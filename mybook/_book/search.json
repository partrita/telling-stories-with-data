[
  {
    "objectID": "03-workflow_ko.html",
    "href": "03-workflow_ko.html",
    "title": "3  재현 가능한 워크플로",
    "section": "",
    "text": "3.1 소개\n선행 학습\n주요 개념 및 기술\n소프트웨어 및 패키지\n과학이 검증 가능한 설명과 예측의 관점에서 지식을 체계적으로 구축하고 조직하는 것이라면, 데이터 과학은 이를 데이터에 집중합니다. 이는 지식을 구축, 조직 및 공유하는 것이 중요한 측면임을 의미합니다. 당신만이 할 수 있는 방식으로 한 번 지식을 창출하는 것은 이 기준을 충족하지 못합니다. 따라서 재현 가능한 데이터 과학 워크플로가 필요합니다.\n(Alexander2019는?) 재현 가능한 연구를 사용된 모든 자료가 주어지면 정확하게 다시 수행할 수 있는 연구로 정의합니다. 이는 코드, 데이터 및 환경을 제공하는 것의 중요성을 강조합니다. 최소한의 기대는 다른 사람이 독립적으로 귀하의 코드, 데이터 및 환경을 사용하여 그림과 표를 포함한 결과를 얻을 수 있다는 것입니다. 아이러니하게도 분야마다 재현성에 대한 정의가 다릅니다. (barba2018terminologies는?) 다양한 분야를 조사하고 주요 언어 사용이 다음 정의를 의미한다고 결론 내립니다.\n우리의 목적을 위해 National Academies of Sciences, Engineering, and Medicine (2019, p. 46)의 정의를 사용합니다. “재현성이란 동일한 입력 데이터, 계산 단계, 방법 및 코드, 분석 조건을 사용하여 일관된 결과를 얻는 것입니다.” 구체적으로 무엇이라고 불리든 (Gelman2016은?) 다양한 사회 과학에서 그것의 부족이 얼마나 큰 문제인지 확인합니다. 재현할 수 없는 작업은 세상에 대한 우리의 지식 축적에 기여하지 않습니다. 이것은 낭비적이며 잠재적으로 비윤리적일 수도 있습니다. Gelman (2016) 이후 많은 사회 과학에서 많은 작업이 이루어졌고 상황이 약간 개선되었지만 아직 많은 작업이 남아 있습니다. 이는 생명 과학(Heil 기타 2021), 암 연구(Begley 와/과 Ellis 2012; Mullard 2021) 및 컴퓨터 과학(Pineau 기타 2021)에서도 마찬가지입니다.\n(Gelman2016이?) 언급한 예 중 일부는 전체적인 관점에서 그렇게 중요하지 않습니다. 그러나 동시에 우리는 큰 영향을 미치는 분야에서 유사한 접근 방식이 사용되는 것을 보았고 계속 보고 있습니다. 예를 들어, 많은 정부는 일부 주장에 신뢰성이 부족하다는 증거가 있음에도 불구하고 공공 정책을 시행하는 “넛지” 단위를 만들었습니다(Sunstein 와/과 Reisch 2017; Maier 기타 2022; Szaszi 기타 2022). 정부는 공개하지 않는 알고리즘을 점점 더 많이 사용하고 있습니다(Chouldechova 기타 2018). 그리고 (herndon2014does는?) 2007-2008년 금융 위기 이후 정부가 긴축 정책을 정당화하는 데 사용한 경제학 연구가 재현 불가능하다는 것을 문서화했습니다.\n최소한, 그리고 몇 가지 예외를 제외하고, 우리는 코드, 데이터셋 및 환경을 공개해야 합니다. 이것들이 없으면 발견이 무엇을 의미하는지 알기 어렵습니다(Miyakawa 2020). 더 평범하게 말하면, 실수나 부주의하게 간과된 측면이 있는지 여부도 알 수 없습니다(Merali 2010; Hillel 2017; Silver 2020). 점점 더 (buckheit1995wavelab을?) 따라 논문을 광고로 간주하고 관련 코드, 데이터 및 환경을 실제 작업으로 간주합니다. Apple의 공동 창립자인 스티브 잡스는 자신의 기술 분야에서 최고인 사람들은 다른 누구도 볼 수 없는 작업의 측면조차도 공개된 측면만큼 잘 마무리되고 고품질인지 확인한다고 말했습니다(Isaacson 2011). 데이터 과학에서도 마찬가지이며, 종종 고품질 작업의 구별되는 측면 중 하나는 README와 코드 주석이 관련 논문의 초록만큼이나 세련되었다는 것입니다.\n워크플로는 문화적, 사회적 맥락 내에 존재하며, 이는 재현 가능해야 할 필요성에 대한 추가적인 윤리적 이유를 부과합니다. 예를 들어, (wang2018deep은?) 동성애자와 이성애자 남성의 얼굴을 구별하기 위해 신경망을 훈련시킵니다. ((murphy2017은?) 논문, 관련 문제 및 저자의 의견에 대한 요약을 제공합니다.) 이를 위해 Wang 와/과 Kosinski (2018, p. 248)은 “성인, 백인, 완전히 보이는, 사용자의 프로필에 보고된 성별과 일치하는 성별”인 사람들의 사진 데이터셋이 필요했습니다. 그들은 특정 작업을 완료하기 위해 근로자에게 소량의 돈을 지불하는 온라인 플랫폼인 Amazon Mechanical Turk를 사용하여 이를 확인했습니다. 이 작업에 대한 Mechanical Turk 근로자에게 제공된 지침은 백인 어머니와 흑인 아버지를 둔 제44대 미국 대통령 버락 오바마를 “흑인”으로 분류해야 하며, 라틴계는 인종이 아닌 민족이라는 점을 명시합니다(Mattson 2017). 분류 작업은 객관적으로 보일 수 있지만, 아마도 무심코 특정 계층과 배경을 가진 미국인의 견해를 반영합니다.\n이것은 Wang 와/과 Kosinski (2018) 워크플로의 한 부분에 대한 한 가지 구체적인 우려 사항일 뿐입니다. (Gelman_2018을?) 포함한 다른 사람들은 더 광범위한 우려를 제기합니다. 주요 문제는 통계 모델이 훈련된 데이터에 특정하다는 것입니다. 그리고 Wang 와/과 Kosinski (2018) 모델에서 발생 가능한 문제를 식별할 수 있는 유일한 이유는 그들이 사용한 특정 데이터셋을 공개하지 않았음에도 불구하고 절차에 대해 공개했기 때문입니다. 우리 작업이 신뢰할 수 있으려면 다른 사람이 재현할 수 있어야 합니다.\n작업의 재현성을 높이기 위해 취할 수 있는 몇 가지 단계는 다음과 같습니다.\n이 책에서 우리가 주장하는 워크플로는 다음과 같습니다.\n\\[\n\\mbox{계획}\\rightarrow\\mbox{시뮬레이션}\\rightarrow\\mbox{수집}\\rightarrow\\mbox{탐색}\\rightarrow\\mbox{공유}\n\\]\n그러나 “주로 읽고 쓰고, 때로는 코딩하는 등 엄청나게 많이 생각한다”고 달리 생각할 수도 있습니다.\n이 워크플로의 재현성을 향상시키는 데 사용할 수 있는 다양한 단계에서 다양한 도구가 있습니다. 여기에는 Quarto, R 프로젝트, Git 및 GitHub가 포함됩니다.",
    "crumbs": [
      "기초",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>재현 가능한 워크플로</span>"
    ]
  },
  {
    "objectID": "03-workflow_ko.html#소개",
    "href": "03-workflow_ko.html#소개",
    "title": "3  재현 가능한 워크플로",
    "section": "",
    "text": "머신 러닝에 대해 명심해야 할 첫 번째 사항은 성능이 하나의 데이터셋의 샘플에서 평가되지만 모델은 반드시 동일한 특성을 따르지 않을 수 있는 샘플에서 프로덕션 환경에서 사용된다는 것입니다\\(\\dots\\) 따라서 “90% 정확도로 평가된 모델을 사용하시겠습니까, 아니면 80% 정확도로 평가된 인간을 사용하시겠습니까?”라는 질문을 할 때 답은 데이터가 평가 프로세스에 따라 일반적인지 여부에 따라 달라집니다. 인간은 적응력이 있지만 모델은 그렇지 않습니다. 상당한 불확실성이 관련된 경우 인간을 선택하십시오. 그들은 (엄청난 양의 데이터로 훈련된 모델에 비해) 패턴 인식 능력이 떨어질 수 있지만 자신이 하는 일을 이해하고 그것에 대해 추론할 수 있으며 새로움에 직면했을 때 즉흥적으로 대처할 수 있습니다.\n프랑수아 숄레, 2020년 2월 20일.\n\n\n\n\n재현 가능한 연구는 “[저자가] 분석을 다시 실행하고 결과를 재현하는 데 필요한 모든 데이터와 컴퓨터 코드를 제공하는 경우”입니다.\n복제는 “새로운 데이터(아마도 다른 방법 사용)를 수집하고 새로운 분석을 완료하여 다른 연구와 동일한 과학적 발견에 도달하는 연구”입니다.\n\n\n\n\n\n\n\n\n전체 워크플로가 문서화되었는지 확인합니다. 여기에는 다음과 같은 질문에 답하는 것이 포함될 수 있습니다.\n\n원본, 편집되지 않은 데이터셋은 어떻게 얻었으며 다른 사람이 액세스할 수 있고 지속적으로 사용할 수 있습니까?\n원본, 편집되지 않은 데이터를 분석된 데이터로 변환하기 위해 어떤 구체적인 단계를 수행하고 있으며 이를 다른 사람에게 어떻게 제공할 수 있습니까?\n어떤 분석이 수행되었으며 이를 얼마나 명확하게 공유할 수 있습니까?\n최종 논문이나 보고서는 어떻게 작성되었으며 다른 사람이 그 과정을 어느 정도까지 따를 수 있습니까?\n\n처음에는 완벽한 재현성에 대해 걱정하지 않고 대신 각 연속적인 프로젝트를 통해 개선하려고 노력하는 데 집중합니다. 예를 들어, 다음 각 요구 사항은 점점 더 부담스러워지며 첫 번째 요구 사항을 수행할 수 있을 때까지 마지막 요구 사항을 수행할 수 없는 것에 대해 걱정할 필요가 없습니다.\n\n전체 워크플로를 다시 실행할 수 있습니까?\n다른 사람이 전체 워크플로를 다시 실행할 수 있습니까?\n“미래의 당신”이 전체 워크플로를 다시 실행할 수 있습니까?\n“미래의 다른 사람”이 전체 워ك플로를 다시 실행할 수 있습니까?\n\n최종 논문이나 보고서에 데이터셋과 접근 방식의 한계에 대한 자세한 논의를 포함합니다.",
    "crumbs": [
      "기초",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>재현 가능한 워크플로</span>"
    ]
  },
  {
    "objectID": "03-workflow_ko.html#콰르토",
    "href": "03-workflow_ko.html#콰르토",
    "title": "3  재현 가능한 워크플로",
    "section": "3.2 콰르토",
    "text": "3.2 콰르토\n\n3.2.1 시작하기\nQuarto는 “문학적 프로그래밍”이라고 불리는 방식으로 코드와 자연어를 통합합니다(Knuth 1984). 이는 R 코드 청크를 포함할 수 있도록 특별히 설계된 Markdown의 변형인 R Markdown의 후속 제품입니다. Quarto는 Microsoft Word와 같은 “보는 대로 얻는 것”(WYSIWYG) 언어와 비교하여 하이퍼텍스트 마크업 언어(HTML) 또는 LaTeX와 유사한 마크업 언어를 사용합니다. 즉, 모든 측면이 일관되며, 예를 들어 모든 최상위 제목은 동일하게 보입니다. 그러나 특정 측면이 어떻게 나타나기를 원하는지 지정하거나 “마크업”해야 합니다. 그리고 문서를 렌더링할 때만 어떻게 보이는지 알 수 있습니다. 시각적 편집기 옵션도 사용할 수 있으며, 이는 사용자가 직접 이 마크업을 수행할 필요성을 숨깁니다.\n앞으로는 Quarto를 사용하는 것이 합리적이지만, R Markdown으로 작성된 많은 자료가 있습니다. 이러한 이유로 ?sec-rmarkdown에서 R Markdown에 해당하는 내용을 제공합니다.\n\n\n\n\n\n\n거인의 어깨 위에 서서\n\n\n\n페르난도 페레즈는 캘리포니아 대학교 버클리 캠퍼스의 통계학 부교수이자 로렌스 버클리 국립 연구소 데이터 과학 및 기술 부서의 교수 과학자입니다. 그는 콜로라도 대학교 볼더에서 입자 물리학 박사 학위를 받았습니다. 박사 과정 중에 Python을 대화형으로 사용할 수 있게 해주는 iPython을 만들었으며, 현재 R Markdown 및 Quarto와 같은 유사한 노트북 접근 방식의 기반이 되는 Project Jupyter의 기초가 되었습니다. (somers2018obsolete는?) 오픈 소스 노트북 접근 방식이 어떻게 극적으로 개선된 과학 컴퓨팅을 가져오는 선순환 피드백 루프를 만드는지 설명합니다. 그리고 (romersays는?) Jupyter와 같은 오픈 소스 접근 방식의 특징을 과학적 합의와 발전을 가능하게 하는 특징과 일치시킵니다. 2017년 페레즈는 ACM(Association for Computing Machinery) 소프트웨어 시스템 상을 수상했습니다.\n\n\n문학적 프로그래밍의 한 가지 장점은 코드가 실행되고 문서의 일부를 형성하는 “라이브” 문서를 얻는다는 것입니다. Quarto의 또 다른 장점은 유사한 코드가 HTML 및 PDF를 포함한 다양한 문서로 컴파일될 수 있다는 것입니다. Quarto에는 제목, 작성자 및 날짜를 포함하기 위한 기본 옵션도 있습니다. 한 가지 단점은 코드를 실행해야 하므로 문서가 컴파일되는 데 시간이 걸릴 수 있다는 것입니다.\n여기에서 Quarto를 다운로드해야 합니다. (Posit Cloud를 사용하는 경우 이미 설치되어 있으므로 이 단계를 건너뛰십시오.) 그런 다음 RStudio 내에서 새 Quarto 문서를 만들 수 있습니다. “파일” \\(\\rightarrow\\) “새 파일” \\(\\rightarrow\\) “Quarto 문서\\(\\dots\\)”.\n새 Quarto 문서를 열고 “소스” 보기를 선택하면 세 개의 대시 쌍 내에 포함된 기본 상단 내용과 몇 가지 마크다운 필수 명령 및 R 청크를 보여주는 텍스트 예제가 표시되며, 각 내용은 다음 섹션에서 자세히 설명합니다.\n\n\n3.2.2 상단 내용\n상단 내용은 제목, 작성자, 날짜와 같은 측면을 정의하는 것으로 구성됩니다. Quarto 문서 상단의 세 개의 대시 안에 포함됩니다. 예를 들어, 다음은 제목, 문서가 렌더링된 날짜로 자동 업데이트되는 날짜, 작성자를 지정합니다.\n---\ntitle: \"내 문서\"\nauthor: \"로한 알렉산더\"\ndate: format(Sys.time(), \"%Y년 %B %d일\")\nformat: html\n---\n초록은 논문의 짧은 요약이며, 상단 내용에 추가할 수 있습니다.\n---\ntitle: \"내 문서\"\nauthor: \"로한 알렉산더\"\ndate: format(Sys.time(), \"%Y년 %B %d일\")\nabstract: \"이것은 내 초록입니다.\"\nformat: html\n---\n기본적으로 Quarto는 HTML 문서를 만들지만 출력 형식을 변경하여 PDF를 생성할 수 있습니다. 이는 백그라운드에서 LaTeX를 사용하며 지원 패키지를 설치해야 합니다. 이렇게 하려면 tinytex를 설치하십시오. 그러나 백그라운드에서 사용되므로 로드할 필요는 없습니다.\n---\ntitle: \"내 문서\"\nauthor: \"로한 알렉산더\"\ndate: format(Sys.time(), \"%Y년 %B %d일\")\nabstract: \"이것은 내 초록입니다.\"\nformat: pdf\n---\n\n\n3.2.3 참고 문헌\n상단 내용에 BibTeX 파일을 지정한 다음 필요에 따라 텍스트 내에서 호출하여 참조를 포함할 수 있습니다.\n---\ntitle: \"내 문서\"\nauthor: \"로한 알렉산더\"\ndate: format(Sys.time(), \"%Y년 %B %d일\")\nformat: pdf\nabstract: \"이것은 내 초록입니다.\"\nbibliography: bibliography.bib\n---\n“bibliography.bib”라는 별도의 파일을 만들고 Quarto 파일 옆에 저장해야 합니다. BibTeX 파일에는 참조할 항목에 대한 항목이 필요합니다. 예를 들어, R에 대한 인용은 citation()으로 얻을 수 있으며 이를 “bibliography.bib” 파일에 추가할 수 있습니다. 패키지에 대한 인용은 패키지 이름을 포함하여 찾을 수 있습니다(예: citation(\"tidyverse\")). 그리고 다시 출력을 “.bib” 파일에 추가합니다. 책이나 기사에 대한 인용을 얻기 위해 Google Scholar 또는 doi2bib을 사용하는 것이 도움이 될 수 있습니다.\n텍스트에서 이 항목을 참조하는 데 사용할 고유 키를 만들어야 합니다. 고유하기만 하면 무엇이든 될 수 있지만 의미 있는 키는 기억하기 더 쉬울 수 있습니다(예: “citeR”).\n@Manual{citeR,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2021},\n    url = {https://www.R-project.org/},\n  }\n@book{tellingstories,\n    title = {Telling Stories with Data},\n    author = {Rohan Alexander},\n    year = {2023},\n    publisher = {Chapman and Hall/CRC},\n    url = {https://tellingstorieswithdata.com}\n  }\nQuarto 문서에서 R을 인용하려면 @citeR을 포함합니다. 그러면 연도 주위에 괄호가 표시됩니다: R Core Team (2024). 또는 [@citeR]을 포함하면 전체 항목 주위에 괄호가 표시됩니다: (R Core Team 2024).\n논문 끝의 참고 문헌 목록은 BibTeX 파일을 호출하고 논문에 참고 문헌을 포함하는 것을 기반으로 자동으로 작성됩니다. Quarto 문서 끝에 “# 참고 문헌”이라는 제목을 포함하면 그 뒤에 실제 인용이 포함됩니다. Quarto 파일이 렌더링되면 Quarto는 내용에서 이를 보고 BibTeX 파일로 이동하여 필요한 참고 문헌 세부 정보를 가져오고 참고 문헌 목록을 작성한 다음 렌더링된 문서 끝에 추가합니다.\nBibTeX은 항목의 대소문자를 조정하려고 시도합니다. 이것은 도움이 될 수 있지만 때로는 특정 대소문자를 고집하는 것이 더 좋습니다. BibTeX이 특정 대소문자를 사용하도록 하려면 항목 주위에 단일 중괄호 대신 이중 중괄호를 사용하십시오. 예를 들어, 위의 예에서 {R Core Team}은 정확한 대소문자로 인쇄되는 반면 {Telling Stories with Data}는 BibTeX의 변덕에 따라 달라집니다. 특정 대소문자를 고집하는 것은 특정 대소문자를 가질 수 있는 R 패키지를 인용할 때와 조직을 저자로 인용할 때 중요합니다. 예를 들어, usethis를 인용할 때 title = {usethis: Automate Package and Project Setup},이 아니라 title = {{usethis: Automate Package and Project Setup}},을 사용해야 합니다. 그리고 예를 들어 토론토 시에서 데이터를 제공한 경우 해당 데이터셋을 인용하기 위해 저자를 지정할 때 author = {City of Toronto},가 아니라 author = {{City of Toronto}},를 사용해야 합니다. 후자는 잘못된 참고 문헌 목록 항목 “Toronto, City of”를 초래하는 반면 전자는 올바른 참고 문헌 목록 항목 “City of Toronto”를 초래합니다.\n\n\n3.2.4 필수 명령\nQuarto는 기본 구문으로 Markdown의 변형을 사용합니다. 필수 Markdown 명령에는 강조, 머리글, 목록, 링크 및 이미지에 대한 명령이 포함됩니다. 이에 대한 알림은 RStudio에 포함되어 있습니다. “도움말” \\(\\rightarrow\\) “Markdown 빠른 참조”. 시각적 편집기 또는 소스 편집기를 사용할지 여부는 사용자의 선택입니다. 그러나 어느 쪽이든 이러한 필수 사항을 이해하는 것이 좋습니다. 왜냐하면 항상 시각적 편집기를 사용할 수 있는 것은 아니기 때문입니다(예: GitHub에서 Quarto 문서를 빠르게 보는 경우). 경험이 많아지면 Sublime Text와 같은 텍스트 편집기나 VS Code와 같은 대체 통합 개발 환경을 사용하는 것이 유용할 수 있습니다.\n\n강조: *기울임꼴*, **굵게**\n머리글 (이것들은 앞뒤에 빈 줄이 있는 자체 줄에 표시됩니다):\n\n         # 첫 번째 수준 머리글\n\n         ## 두 번째 수준 머리글\n\n         ### 세 번째 수준 머리글\n\n하위 목록이 있는 정렬되지 않은 목록:\n\n    * 항목 1\n    * 항목 2\n        + 항목 2a\n        + 항목 2b\n\n하위 목록이 있는 정렬된 목록:\n\n    1. 항목 1\n    2. 항목 2\n    3. 항목 3\n        + 항목 3a\n        + 항목 3b\n\nURL을 추가할 수 있습니다: [이 책](https://www.tellingstorieswithdata.com) 결과는 이 책입니다.\n단락은 빈 줄을 남겨 만듭니다.\n\n아이디어에 대한 단락, 다음 단락과 적절한 간격으로 배치됩니다.\n\n다른 아이디어에 대한 단락, 이전 단락과 다시 간격이 있습니다.\n일부 측면을 추가한 후에는 실제 문서를 보고 싶을 수 있습니다. 문서를 빌드하려면 “렌더링”을 클릭하십시오.\n\n\n3.2.5 R 청크\nQuarto 문서 내의 코드 청크에 R 및 기타 여러 언어에 대한 코드를 포함할 수 있습니다. 문서를 렌더링하면 코드가 실행되고 문서에 포함됩니다.\nR 청크를 만들려면 세 개의 백틱으로 시작한 다음 중괄호 안에 이것이 R 청크임을 Quarto에 알립니다. 이 청크 안의 모든 것은 R 코드로 간주되어 실행됩니다. Applied Econometrics with R이라는 책과 함께 R 패키지 AER을 제공하는 (citeaer의?) 데이터를 사용합니다. tidyverse를 로드하고 AER을 설치 및 로드한 다음 지난 2주 동안 설문 조사 응답자가 의사를 방문한 횟수에 대한 그래프를 만들 수 있습니다.\n```{r}\nlibrary(tidyverse)\nlibrary(AER)\n\ndata(\"DoctorVisits\", package = \"AER\")\n\nDoctorVisits |&gt;\n  ggplot(aes(x = illness)) +\n  geom_histogram(stat = \"count\")\n```\n해당 코드의 출력은 ?fig-doctervisits입니다.\n\n\n\n\n\n\n\n\n그림 3.1: 지난 2주간 질병 발생 건수, 1977-1978년 호주 건강 조사 기준\n\n\n\n\n\n청크에서 사용할 수 있는 다양한 평가 옵션이 있습니다. 청크별 주석 구분 기호 “#|”로 줄을 열고 옵션을 사용하여 각 줄에 이러한 옵션을 포함합니다. 유용한 옵션은 다음과 같습니다.\n\necho: 코드 자체가 문서에 포함되는지 여부를 제어합니다. 예를 들어, #| echo: false는 코드가 실행되고 출력이 표시되지만 코드 자체는 문서에 포함되지 않음을 의미합니다.\ninclude: 코드의 출력이 문서에 포함되는지 여부를 제어합니다. 예를 들어, #| include: false는 코드를 실행하지만 출력이 발생하지 않으며 코드 자체도 문서에 포함되지 않음을 의미합니다.\neval: 코드가 문서에 포함되어야 하는지 여부를 제어합니다. 예를 들어, #| eval: false는 코드가 실행되지 않으므로 포함할 출력이 없지만 코드 자체는 문서에 포함됨을 의미합니다.\nwarning: 경고가 문서에 포함되어야 하는지 여부를 제어합니다. 예를 들어, #| warning: false는 경고가 포함되지 않음을 의미합니다.\nmessage: 메시지가 문서에 포함되어야 하는지 여부를 제어합니다. 예를 들어, #| message: false는 메시지가 문서에 포함되지 않음을 의미합니다.\n\n예를 들어, 출력은 포함하지만 코드는 포함하지 않고 경고는 표시하지 않을 수 있습니다.\n```{r}\n#| echo: false\n#| warning: false\n\nlibrary(tidyverse)\nlibrary(AER)\n\ndata(\"DoctorVisits\", package = \"AER\")\n\nDoctorVisits |&gt;\n  ggplot(aes(x = visits)) +\n  geom_histogram(stat = \"count\")\n```\nR 청크 양쪽에 빈 줄을 남겨두십시오. 그렇지 않으면 제대로 실행되지 않을 수 있습니다. 그리고 논리 값에는 소문자를 사용하십시오(예: “FALSE”가 아닌 “false”).\nMost people did not visit a doctor in the past week.\n\n```{r}\n#| echo: false\n#| warning: false\n\nlibrary(tidyverse)\nlibrary(AER)\n\ndata(\"DoctorVisits\", package = \"AER\")\n\nDoctorVisits |&gt;\n  ggplot(aes(x = visits)) +\n  geom_histogram(stat = \"count\")\n```\n\nThere were some people that visited a doctor once, and then...\nQuarto 문서 자체는 필요한 모든 데이터셋을 로드해야 합니다. 환경에 있는 것만으로는 충분하지 않습니다. 이는 Quarto 문서가 렌더링될 때 문서의 코드를 평가하며 반드시 환경을 평가하는 것은 아니기 때문입니다.\n종종 코드를 작성할 때 여러 줄에 걸쳐 동일한 변경을 하거나 특정 항목의 모든 인스턴스를 변경하고 싶을 수 있습니다. 여러 커서를 사용하여 이를 달성합니다. 여러 연속된 줄에 걸쳐 커서를 원하면 Mac에서는 “option”을, PC에서는 “Alt”를 누른 상태에서 관련 줄 위로 커서를 드래그합니다. 특정 항목의 모든 인스턴스를 선택하려면 한 인스턴스(예: 변수 이름)를 강조 표시한 다음 찾기/바꾸기(Mac에서는 Command + F, PC에서는 CTRL + F)를 사용하고 “모두”를 선택합니다. 그러면 다른 모든 인스턴스에 커서가 활성화됩니다.\n\n\n3.2.6 방정식\nTeX 프로그래밍 언어를 기반으로 하는 LaTeX를 사용하여 방정식을 포함할 수 있습니다. LaTeX에서 수학 모드는 두 개의 달러 기호를 여는 태그와 닫는 태그로 사용하여 호출합니다. 그런 다음 안에 있는 모든 것이 LaTeX 마크업으로 평가됩니다. 예를 들어 복리 이자 공식은 다음과 같이 생성할 수 있습니다.\n$$\nA = P\\left(1+\\frac{r}{n}\\right)^{nt}\n$$\n\\[\nA = P\\left(1+\\frac{r}{n}\\right)^{nt}\n\\]\nLaTeX는 포괄적인 마크업 언어이지만 주로 관심 모델을 지정하는 데 사용할 것입니다. ?sec-its-just-a-linear-model에서 시작하여 활용할 중요한 측면을 포함하는 몇 가지 예를 여기에 포함합니다.\n$$\ny_i|\\mu_i, \\sigma \\sim \\mbox{Normal}(\\mu_i, \\sigma)\n$$\n\\[\ny_i|\\mu_i, \\sigma \\sim \\mbox{Normal}(\\mu_i, \\sigma)\n\\]\n아래 첨자는 밑줄을 사용하여 만듭니다: \\(y_i\\)의 경우 y_i. 그리고 중괄호로 둘러싸서 둘 이상의 항목으로 된 아래 첨자를 만들 수 있습니다: \\(y_{i,c}\\)의 경우 y_{i,c}. 이 경우 줄 안에 수학 모드가 필요했으므로 여는 태그와 닫는 태그로 단일 달러 기호로 둘러쌉니다.\n그리스 문자는 일반적으로 백슬래시가 앞에 옵니다. 일반적인 그리스 문자는 다음과 같습니다: \\(\\alpha\\)의 경우 \\alpha, \\(\\beta\\)의 경우 \\beta, \\(\\delta\\)의 경우 \\delta, \\(\\epsilon\\)의 경우 \\epsilon, \\(\\gamma\\)의 경우 \\gamma, \\(\\lambda\\)의 경우 \\lambda, \\(\\mu\\)의 경우 \\mu, \\(\\phi\\)의 경우 \\phi, \\(\\pi\\)의 경우 \\pi, \\(\\Pi\\)의 경우 \\Pi, \\(\\rho\\)의 경우 \\rho, \\(\\sigma\\)의 경우 \\sigma, \\(\\Sigma\\)의 경우 \\Sigma, \\(\\tau\\)의 경우 \\tau, \\(\\theta\\)의 경우 \\theta.\nLaTeX 수학 모드는 문자를 변수로 간주하여 기울임꼴로 만들지만, “Normal”과 같이 변수가 아니기 때문에 일반 글꼴로 표시하고 싶을 때가 있습니다. 이 경우 \\mbox{}로 둘러쌉니다(예: \\(\\mbox{Normal}\\)의 경우 \\mbox{Normal}).\n\\begin{aligned}와 \\end{aligned}를 사용하여 여러 줄에 걸쳐 방정식을 정렬합니다. 그런 다음 정렬할 항목은 앰퍼샌드로 표시합니다. 다음은 ?sec-multilevel-regression-with-post-stratification에서 추정할 모델입니다.\n$$\n\\begin{aligned}\ny_i|\\pi_i & \\sim \\mbox{Bern}(\\pi_i) \\\\\n\\mbox{logit}(\\pi_i) & = \\beta_0+ \\alpha_{g[i]}^{\\mbox{gender}} + \\alpha_{a[i]}^{\\mbox{age}} + \\alpha_{s[i]}^{\\mbox{state}} + \\alpha_{e[i]}^{\\mbox{edu}} \\\\\n\\beta_0 & \\sim \\mbox{Normal}(0, 2.5)\\\\\n\\alpha_{g}^{\\mbox{gender}} & \\sim \\mbox{Normal}(0, 2.5)\\mbox{ for }g=1, 2\\\\\n\\alpha_{a}^{\\mbox{age}} & \\sim \\mbox{Normal}\\left(0, \\sigma^2_{\\mbox{age}}\\right)\\mbox{ for }a = 1, 2, \\dots, A\\\\\n\\alpha_{s}^{\\mbox{state}} & \\sim \\mbox{Normal}\\left(0, \\sigma^2_{\\mbox{state}}\\right)\\mbox{ for }s = 1, 2, \\dots, S\\\\\n\\alpha_{e}^{\\mbox{edu}} & \\sim \\mbox{Normal}\\left(0, \\sigma^2_{\\mbox{edu}}\\right)\\mbox{ for }e = 1, 2, \\dots, E\\\\\n\\sigma_{\\mbox{gender}} & \\sim \\mbox{Exponential}(1)\\\\\n\\sigma_{\\mbox{state}} & \\sim \\mbox{Exponential}(1)\\\\\n\\sigma_{\\mbox{edu}} & \\sim \\mbox{Exponential}(1)\n\\end{aligned}\n$$\n\\[\n\\begin{aligned}\ny_i|\\pi_i & \\sim \\mbox{Bern}(\\pi_i) \\\\\n\\mbox{logit}(\\pi_i) & = \\beta_0+ \\alpha_{g[i]}^{\\mbox{gender}} + \\alpha_{a[i]}^{\\mbox{age}} + \\alpha_{s[i]}^{\\mbox{state}} + \\alpha_{e[i]}^{\\mbox{edu}} \\\\\n\\beta_0 & \\sim \\mbox{Normal}(0, 2.5)\\\\\n\\alpha_{g}^{\\mbox{gender}} & \\sim \\mbox{Normal}(0, 2.5)\\mbox{ for }g=1, 2\\\\\n\\alpha_{a}^{\\mbox{age}} & \\sim \\mbox{Normal}\\left(0, \\sigma^2_{\\mbox{age}}\\right)\\mbox{ for }a = 1, 2, \\dots, A\\\\\n\\alpha_{s}^{\\mbox{state}} & \\sim \\mbox{Normal}\\left(0, \\sigma^2_{\\mbox{state}}\\right)\\mbox{ for }s = 1, 2, \\dots, S\\\\\n\\alpha_{e}^{\\mbox{edu}} & \\sim \\mbox{Normal}\\left(0, \\sigma^2_{\\mbox{edu}}\\right)\\mbox{ for }e = 1, 2, \\dots, E\\\\\n\\sigma_{\\mbox{gender}} & \\sim \\mbox{Exponential}(1)\\\\\n\\sigma_{\\mbox{state}} & \\sim \\mbox{Exponential}(1)\\\\\n\\sigma_{\\mbox{edu}} & \\sim \\mbox{Exponential}(1)\n\\end{aligned}\n\\]\n마지막으로 특정 함수는 LaTeX에 내장되어 있습니다. 예를 들어, \\log를 사용하여 “log”를 적절하게 조판할 수 있습니다.\n\n\n3.2.7 상호 참조\n그림, 표, 방정식을 상호 참조하는 것이 유용할 수 있습니다. 이렇게 하면 텍스트에서 해당 항목을 더 쉽게 참조할 수 있습니다. 그림에 대해 이 작업을 수행하려면 그림을 만들거나 포함하는 R 청크의 이름을 참조합니다. 예를 들어 다음 코드를 고려하십시오.\n\n```{r}\n#| label: fig-uniquename\n#| fig-cap: 1977-1978년 호주 건강 조사를 기반으로 한 지난 2주간의 질병 발생 건수\n#| warning: false\n\ndata(\"DoctorVisits\", package = \"AER\")\n\nDoctorVisits |&gt;\n  ggplot(aes(x = illness)) +\n  geom_histogram(stat = \"count\")\n```\n\n\n\n\n\n\n\n그림 3.2: 1977-1978년 호주 건강 조사를 기반으로 한 지난 2주간의 질병 발생 건수\n\n\n\n\n\n그러면 (@fig-uniquename)은 R 청크의 이름이 fig-uniquename이므로 (그림 3.2)을 생성합니다. Quarto가 이것이 그림임을 알 수 있도록 청크 이름 시작 부분에 “fig”를 추가해야 합니다. 그런 다음 캡션을 지정하는 R 청크에 “fig-cap:”을 포함합니다.\nQuarto 문서 내의 R 청크에 #| layout-ncol: 2를 추가하여 두 개의 그래프가 나란히 나타나도록 할 수 있습니다(그림 3.3). 여기서 ?fig-doctorgraphsidebyside-1은 최소 테마를 사용하고 ?fig-doctorgraphsidebyside-2는 클래식 테마를 사용합니다. 이들은 모두 R 청크에서 동일한 레이블 #| label: fig-doctorgraphsidebyside을 상호 참조하며, R 청크에 추가 옵션 #| fig-subcap: [\"질병 수\",\"의사 방문 횟수\"]가 추가되어 하위 캡션을 제공합니다. 텍스트 내 문자는 레이블 끝에 “-1”과 “-2”를 추가하여 수행됩니다. 각각 (그림 3.3), 그림 3.3 (a) 및 ?fig-doctorgraphsidebyside-2의 경우 (@fig-doctorgraphsidebyside), @fig-doctorgraphsidebyside-1 및 @fig-doctorgraphsidebyside-2입니다.\n```{r}\n#| eval: true\n#| warning: false\n#| label: fig-doctorgraphsidebyside\n#| fig-cap: \"두 가지 그래프 변형\"\n#| fig-subcap: [\"질병\",\"의사 방문\"]\n#| layout-ncol: 2\n\nDoctorVisits |&gt;\n  ggplot(aes(x = illness)) +\n  geom_histogram(stat = \"count\") +\n  theme_minimal()\n\nDoctorVisits |&gt;\n  ggplot(aes(x = visits)) +\n  geom_histogram(stat = \"count\") +\n  theme_classic()\n```\n\n\n\n\n\n\n\n\n\n\n\n(a) 질병\n\n\n\n\n\n\n\n\n\n\n\n(b) 의사 방문\n\n\n\n\n\n\n\n그림 3.3: 두 가지 그래프 변형\n\n\n\n표를 상호 참조하는 데 비슷한 접근 방식을 사용할 수 있습니다. 예를 들어, (@tbl-docvisittable)은 (표 3.1)을 생성합니다. 이 경우 Quarto가 표임을 알 수 있도록 레이블 시작 부분에 “tbl”을 지정합니다. 그리고 “tbl-cap:”으로 표에 대한 캡션을 지정합니다.\n\n```{r}\n#| label: tbl-docvisittable\n#| tbl-cap: \"의사 방문 횟수 분포\"\n\nDoctorVisits |&gt;\n  count(visits) |&gt;\n  tt() |&gt;\n  style_tt(j = 2, align = \"r\") |&gt;\n  setNames(c(\"Number of visits\", \"Occurrences\"))\n```\n\n\n\n표 3.1: 의사 방문 횟수 분포\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Number of visits\n                Occurrences\n              \n        \n        \n        \n                \n                  0\n                  4141\n                \n                \n                  1\n                  782\n                \n                \n                  2\n                  174\n                \n                \n                  3\n                  30\n                \n                \n                  4\n                  24\n                \n                \n                  5\n                  9\n                \n                \n                  6\n                  12\n                \n                \n                  7\n                  12\n                \n                \n                  8\n                  5\n                \n                \n                  9\n                  1\n                \n        \n      \n    \n\n\n\n\n\n\n마지막으로 방정식도 상호 참조할 수 있습니다. 그러려면 참조할 태그(예: {#eq-macroidentity})를 추가해야 합니다.\n$$\nY = C + I + G + (X - M)\n$$ {#eq-gdpidentity}\n예를 들어, @eq-gdpidentity를 사용하여 ?eq-gdpidentity를 생성합니다.\n\\[\nY = C + I + G + (X - M)\n\\tag{3.1}\\]\n상호 참조를 사용할 때 레이블은 비교적 간단해야 합니다. 일반적으로 이름을 간단하지만 고유하게 유지하고 구두점을 피하고 문자와 하이픈만 사용하십시오. 밑줄은 오류를 유발할 수 있으므로 사용하지 마십시오.",
    "crumbs": [
      "기초",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>재현 가능한 워크플로</span>"
    ]
  },
  {
    "objectID": "03-workflow_ko.html#r-프로젝트-및-파일-구조",
    "href": "03-workflow_ko.html#r-프로젝트-및-파일-구조",
    "title": "3  재현 가능한 워크플로",
    "section": "3.3 R 프로젝트 및 파일 구조",
    "text": "3.3 R 프로젝트 및 파일 구조\n프로젝트는 소프트웨어 개발에서 널리 사용되며 특정 프로젝트와 관련된 모든 파일(데이터, 분석, 보고서 등)을 함께 유지하고 서로 관련시키기 위해 존재합니다. (소프트웨어 개발 의미에서의 “프로젝트” 사용은 프로젝트 관리 의미에서의 “프로젝트”와 구별됩니다.) R 프로젝트는 RStudio에서 만들 수 있습니다. “파일” \\(\\rightarrow\\) “새 프로젝트”를 클릭한 다음 “빈 프로젝트”를 선택하고 R 프로젝트 이름을 지정하고 저장할 위치를 결정합니다. 예를 들어, 산모 사망률에 초점을 맞춘 R 프로젝트는 “maternalmortality”라고 부를 수 있습니다. R 프로젝트를 사용하면 “다른 컴퓨터나 사용자 간에 그리고 시간이 지남에 따라 안정적이고 정중한 동작”이 가능합니다(Bryan 와/과 Hester 2020). 이는 해당 폴더의 컨텍스트를 더 넓은 존재에서 제거하기 때문입니다. 파일은 컴퓨터의 기본이 아니라 R 프로젝트의 기본과 관련하여 존재합니다.\n프로젝트가 생성되면 해당 폴더에 확장자가 “.RProj”인 새 파일이 나타납니다. R 프로젝트, Quarto 문서 및 적절한 파일 구조가 있는 폴더의 예는 여기에서 확인할 수 있습니다. “코드” \\(\\rightarrow\\) “ZIP 다운로드”를 통해 다운로드할 수 있습니다.\nR 프로젝트 사용의 주요 이점은 자체 포함된 방식으로 내부 파일을 참조할 수 있다는 것입니다. 즉, 다른 사람이 우리 작업을 재현하려고 할 때 모든 파일 참조와 구조를 변경할 필요가 없습니다. 모든 것이 “.Rproj” 파일과 관련하여 참조되기 때문입니다. 예를 들어, \"~/Documents/projects/book/data/\"와 같은 곳에서 CSV를 읽는 대신 book/data/에서 읽을 수 있습니다. 다른 사람이 projects 폴더를 가지고 있지 않을 수 있으므로 전자는 작동하지 않지만 후자는 작동합니다.\n프로젝트 사용은 신뢰할 수 있는 작업에 필요한 최소한의 재현성 수준을 충족하는 데 필요합니다. setwd()와 같은 함수 및 컴퓨터별 파일 경로를 사용하면 작업이 특정 컴퓨터에 부적절하게 바인딩됩니다.\n폴더를 설정하는 다양한 방법이 있습니다. 시작할 때 종종 유용한 (wilsongoodenough의?) 변형은 위에 링크된 예제 파일 구조에 나와 있습니다.\nexample_project/\n├── .gitignore\n├── LICENSE.md\n├── README.md\n├── example_project.Rproj\n├── data\n│   ├── 00-simulated_data\n│   │   ├── simulated_data.csv\n│   ├── 01-raw_data\n│   │   ├── raw_data.csv\n│   ├── 02-analysis_data\n│   │   ├── analysis_data.csv\n│   │   └── ...\n├── model\n│   ├── first_model.rds\n├── other\n│   ├── datasheet\n│   │   └── ...\n│   ├── literature\n│   │   └── ...\n│   ├── llm_usage\n│   │   └── ...\n│   ├── sketches\n│   │   └── ...\n├── paper\n│   ├── paper.pdf\n│   ├── paper.qmd\n│   ├── references.bib\n│   └── ...\n├── scripts\n│   ├── 00-simulate_data.R\n│   ├── 01-test_simulated_data.R\n│   ├── 02-download_data.R\n│   ├── 03-clean_data.R\n│   ├── 04-test_analysis_data.R\n│   ├── 05-eda.R\n│   ├── 06-model_data.R\n│   ├── 07-replication.R\n│   └── ...\n└── ...\n여기에는 시뮬레이션된 데이터, 덮어쓰면 안 되는 편집되지 않은 데이터(Wilson 기타 2017) 및 함께 모은 분석 데이터가 포함된 data 폴더가 있습니다. model 폴더에는 저장된 모델 추정치가 포함되어 있습니다. other 폴더에는 데이터시트, 문헌, LLM 사용량 및 스케치와 같은 측면이 포함되어 있으며 상황에 따라 유용합니다. paper 폴더에는 Quarto 문서와 BibTeX 파일이 포함되어 있습니다. 마지막으로 scripts에는 데이터를 시뮬레이션, 다운로드, 테스트 및 분석하는 코드가 포함되어 있습니다.\n유용한 다른 측면으로는 프로젝트에 대한 개요 세부 정보를 지정하는 README.md와 LICENSE가 있습니다. README에 무엇을 넣어야 하는지에 대한 예는 여기에서 확인할 수 있습니다. 이 프로젝트 골격의 또 다른 유용한 변형은 (GoodResearchCode에서?) 제공합니다.",
    "crumbs": [
      "기초",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>재현 가능한 워크플로</span>"
    ]
  },
  {
    "objectID": "03-workflow_ko.html#버전-관리",
    "href": "03-workflow_ko.html#버전-관리",
    "title": "3  재현 가능한 워크플로",
    "section": "3.4 버전 관리",
    "text": "3.4 버전 관리\n이 책에서는 Git과 GitHub의 조합을 통해 버전 관리를 구현합니다. 여기에는 다음과 같은 다양한 이유가 있습니다.\n\n코드와 데이터 공유를 용이하게 하여 작업의 재현성을 향상시킵니다.\n작업 공유를 용이하게 합니다.\n체계적인 접근 방식을 장려하여 워크플로를 개선합니다. 그리고\n팀 작업을 용이하게 합니다.\n\nGit은 흥미로운 역사를 가진 버전 관리 시스템입니다(Brown 2018). 버전 관리를 시작하는 일반적인 방법은 하나의 파일에 대한 다양한 복사본을 갖는 것입니다. “first_go.R”, “first_go-fixed.R”, “first_go-fixed-with-mons-edits.R”. 그러나 이것은 곧 번거로워집니다. 종종 날짜를 사용하게 됩니다. 예를 들어, “2022-01-01-analysis.R”, “2022-01-02-analysis.R”, “2022-01-03-analysis.R” 등입니다. 이것은 기록을 유지하지만, 변경이 이루어진 날짜를 기억하기 어렵기 때문에 다시 돌아가야 할 때 검색하기 어려울 수 있습니다. 어쨌든 정기적으로 작업하는 프로젝트에는 금방 다루기 어려워집니다.\n대신 Git을 사용하여 파일의 한 버전만 갖도록 합니다. Git은 해당 파일의 변경 사항 기록과 특정 시점의 해당 파일 스냅샷을 유지합니다. Git이 해당 스냅샷을 찍는 시기를 결정합니다. 또한 이 스냅샷과 마지막 스냅샷 사이에 변경된 내용을 설명하는 메시지를 추가합니다. 이렇게 하면 파일의 버전이 하나만 있고 기록을 더 쉽게 검색할 수 있습니다.\n한 가지 복잡한 점은 Git이 소프트웨어 개발자 팀을 위해 설계되었다는 것입니다. 따라서 작동하지만 개발자가 아닌 사람에게는 약간 어색할 수 있습니다. 그럼에도 불구하고 Git은 데이터 과학에 유용하게 적용되었으며, 유일한 협력자가 미래의 자신일지라도 마찬가지입니다(Bryan 2018a).\nGitHub, GitLab 및 기타 다양한 회사에서는 Git을 기반으로 하는 사용하기 쉬운 서비스를 제공합니다. 절충안이 있지만 여기서는 GitHub가 지배적인 플랫폼이므로 소개합니다(Eghbal 2020, p. 21). Git과 GitHub는 Posit Cloud에 내장되어 있어 로컬 설치에 문제가 있는 경우 좋은 옵션을 제공합니다. Git의 초기 어려운 측면 중 하나는 용어입니다. 폴더는 “repo”라고 합니다. 스냅샷을 만드는 것을 “커밋”이라고 합니다. 결국 익숙해지지만 처음에는 혼란스러움을 느끼는 것이 정상입니다. (happygit은?) Git과 GitHub를 설정하고 사용하는 데 특히 유용합니다.\n\n3.4.1 Git\n먼저 Git이 설치되어 있는지 확인해야 합니다. RStudio를 열고 터미널로 이동하여 다음을 입력한 다음 Enter/Return을 누릅니다.\n\ngit --version\n\n버전 번호가 표시되면 완료된 것입니다(그림 3.4 (a)).\n\n\n\n\n\n\n\n\n\n\n\n(a) RStudio에서 터미널을 사용하여 Git이 설치되어 있는지 확인\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) RStudio에서 Git에 사용자 이름과 이메일 주소 추가\n\n\n\n\n\n\n\n그림 3.4: Git 설정과 관련된 단계 개요\n\n\n\nGit은 Posit Cloud에 사전 설치되어 있으며 Mac에는 사전 설치되어 있어야 하며 Windows에도 사전 설치되어 있을 수 있습니다. 응답으로 버전 번호가 표시되지 않으면 설치해야 합니다. 그렇게 하려면 Bryan (2020, 5장)의 운영 체제별 지침을 따라야 합니다.\nGit이 설치된 후 사용자 이름과 이메일을 알려줘야 합니다. Git은 스냅샷을 찍을 때마다 또는 Git의 용어로 커밋을 할 때마다 이 정보를 추가하기 때문에 이 작업을 수행해야 합니다.\n다시 터미널 내에서 다음을 입력하고 세부 정보를 사용자 정보로 바꾼 다음 각 줄 다음에 “enter/return”을 누릅니다.\n\ngit config --global user.name \"로한 알렉산더\"\ngit config --global user.email \"rohan.alexander@utoronto.ca\"\ngit config --global --list\n\n이 설정이 제대로 완료되면 “user.name”과 “user.email”에 입력한 값이 마지막 줄 다음에 반환됩니다(그림 3.4 (b)).\n이러한 세부 정보(사용자 이름 및 이메일 주소)는 공개됩니다. 필요한 경우 이메일 주소를 숨기는 다양한 방법이 있으며 GitHub에서 이에 대한 지침을 제공합니다. Bryan (2020, 7장)에서는 이 단계에 대한 자세한 지침과 문제 해결 가이드를 제공합니다.\n\n\n3.4.2 GitHub\n이제 Git이 설정되었으므로 GitHub를 설정해야 합니다. ?sec-fire-hose에서 GitHub 계정을 만들었으며 여기서 다시 사용합니다. github.com에 로그인한 후 먼저 새 폴더를 만들어야 하는데, Git에서는 이를 “repo”라고 합니다. 오른쪽 상단에서 “+”를 찾은 다음 “새 리포지토리”를 선택합니다(그림 3.5 (a)).\n\n\n\n\n\n\n\n\n\n\n\n(a) 새 리포지토리 만들기 프로세스 시작\n\n\n\n\n\n\n\n\n\n\n\n(b) 새 리포지토리의 URL 복사\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Posit Cloud에 프로젝트 추가\n\n\n\n\n\n\n\n\n\n\n\n(d) PAT 만들기\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e) 커밋할 파일 추가\n\n\n\n\n\n\n\n\n\n\n\n(f) 커밋 만들기\n\n\n\n\n\n\n\n그림 3.5: GitHub 설정과 관련된 단계 개요\n\n\n\n이 시점에서 리포지토리에 적절한 이름을 추가할 수 있습니다. 나중에 언제든지 삭제할 수 있으므로 지금은 “공개”로 두십시오. 그리고 “이 리포지토리를 README로 초기화” 확인란을 선택합니다. “Add .gitignore”를 R로 변경합니다. 그런 다음 “리포지토리 만들기”를 클릭합니다.\n이렇게 하면 상당히 비어 있는 화면으로 이동하지만 필요한 세부 정보(URL)는 녹색 “복제 또는 다운로드” 버튼에 있으며 클립보드를 클릭하여 복사할 수 있습니다(그림 3.5 (b)).\n이제 RStudio로 돌아가서 Posit Cloud에서 “Git 리포지토리에서 새 프로젝트”를 사용하여 새 프로젝트를 만듭니다. 방금 복사한 URL을 묻는 메시지가 표시됩니다(그림 3.5 (c)). 로컬 컴퓨터를 사용하는 경우 이 단계는 메뉴를 통해 수행됩니다. “파일” \\(\\rightarrow\\) “새 프로젝트…” \\(\\rightarrow\\) “버전 관리” \\(\\rightarrow\\) “Git”을 선택한 다음 URL을 붙여넣고 폴더에 의미 있는 이름을 지정하고 “새 세션에서 열기”를 선택한 다음 “프로젝트 만들기”를 클릭합니다.\n이 시점에서 사용할 수 있는 새 폴더가 만들어졌습니다. GitHub로 다시 푸시하려면 RStudio 작업 공간을 GitHub 계정과 연결하기 위해 개인 액세스 토큰(PAT)을 사용해야 합니다. 이를 위해 usethis와 gitcreds를 사용합니다. 이들은 각각 반복적인 작업을 자동화하는 패키지와 GitHub로 인증하는 패키지입니다. PAT를 만들려면 브라우저에서 GitHub에 로그인한 후 usethis를 설치하고 로드한 다음 R 세션에서 create_github_token()을 실행합니다. GitHub가 브라우저에서 다양한 옵션이 채워진 상태로 열립니다(그림 3.5 (d)). “참고”를 바꿔 PAT에 정보성 이름을 지정하는 것이 유용할 수 있습니다(예: “RStudio용 PAT”). 그런 다음 “토큰 생성”을 클릭합니다.\n이 토큰을 복사할 기회는 한 번뿐이며 실수하면 새 토큰을 생성해야 합니다. PAT를 R 스크립트나 Quarto 문서에 포함하지 마십시오. 대신 gitcreds를 설치하고 로드한 후 gitcreds_set()을 실행하면 콘솔에 PAT를 추가하라는 메시지가 표시됩니다.\n현재 작업 중인 프로젝트에 GitHub를 사용하려면 다음 절차를 따릅니다.\n\n가장 먼저 해야 할 일은 거의 항상 “풀”을 사용하여 변경 사항을 가져오는 것입니다. 이렇게 하려면 RStudio에서 Git 창을 열고 파란색 아래쪽 화살표를 클릭합니다. 이렇게 하면 GitHub에 있는 폴더의 변경 사항을 자신의 폴더 버전으로 가져옵니다.\n그런 다음 폴더 복사본을 변경할 수 있습니다. 예를 들어 README를 업데이트한 다음 평소와 같이 저장할 수 있습니다.\n이 작업이 완료되면 추가, 커밋 및 푸시해야 합니다. RStudio의 Git 창에서 추가할 파일을 선택합니다. 이렇게 하면 스테이징 영역에 추가됩니다. 그런 다음 “커밋”을 클릭합니다(그림 3.5 (e)). 새 창이 열립니다. 변경 사항에 대한 정보성 메시지를 추가한 다음 해당 새 창에서 “커밋”을 클릭합니다(그림 3.5 (f)). 마지막으로 “푸시”를 클릭하여 변경 사항을 GitHub로 보냅니다.\n\nGit과 GitHub에 관해서는 몇 가지 일반적인 문제점이 있습니다. 버전 관리를 처음 사용하는 경우 특히 정기적으로 커밋하고 푸시하는 것이 좋습니다. 이렇게 하면 필요한 경우 다시 돌아올 수 있는 스냅샷 수가 늘어납니다. 모든 커밋에는 정보성 커밋 메시지가 있어야 합니다. 버전 관리를 처음 사용하는 경우 좋은 커밋 메시지에 대한 기대는 변경 사항에 대한 간략한 요약, 빈 줄, 변경 사항이 무엇인지, 왜 변경하는지를 포함한 변경 사항에 대한 설명입니다. 예를 들어, 커밋이 논문에 그래프를 추가하는 경우 커밋 메시지는 다음과 같을 수 있습니다.\n그래프 추가\n\n데이터 섹션에 실업률 및 인플레이션 그래프 추가됨.\n전반적인 품질과 커밋 행동 사이에는 어떤 관계가 있다는 증거가 있습니다(Sprint 와/과 Conci 2019). 경험이 많아질수록 커밋 메시지가 프로젝트의 일종의 일지 역할을 하는 것이 이상적입니다. 그러나 가장 중요한 것은 정기적으로 커밋하는 것입니다.\nGit과 GitHub는 데이터 과학자가 아닌 소프트웨어 개발자를 위해 설계되었습니다. GitHub는 고려할 파일 크기를 100MB로 제한하며, 50MB만 되어도 경고가 표시될 수 있습니다. 데이터 과학 프로젝트에는 이보다 큰 데이터셋이 정기적으로 포함됩니다. ?sec-store-and-share에서는 프로젝트가 완료되었을 때 특히 유용할 수 있는 데이터 저장소 사용에 대해 논의하지만, 프로젝트를 적극적으로 작업할 때는 적어도 Git과 GitHub에 관한 한 대용량 데이터 파일을 무시하는 것이 유용할 수 있습니다. Git을 사용하여 추적하고 싶지 않은 모든 파일을 나열하는 “.gitignore” 파일을 사용하여 이 작업을 수행합니다. 예제 폴더에는 “.gitignore” 파일이 포함되어 있습니다. 그리고 usethis의 git_vaccinate()를 실행하면 프로젝트별로 수행하는 것을 잊어버릴 경우를 대비하여 다양한 파일을 전역 “.gitignore” 파일에 추가하므로 도움이 될 수 있습니다. Mac 사용자는 이로 인해 “.DS_Store” 파일이 무시되므로 유용하다는 것을 알게 될 것입니다.\nRStudio의 Git 창을 사용하여 터미널을 사용할 필요가 없었지만 GitHub로 이동하여 새 프로젝트를 설정할 필요는 없었습니다. Git과 GitHub를 설정했으므로 usethis를 사용하여 워크플로의 이 측면을 더욱 개선할 수 있습니다.\n먼저 usethis의 git_sitrep()를 사용하여 Git이 설정되어 있는지 확인합니다. 사용자 이름과 이메일에 대한 정보가 인쇄되어야 합니다. 필요한 경우 use_git_config()를 사용하여 이러한 세부 정보를 업데이트할 수 있습니다.\n\nuse_git_config(\n  user.name = \"로한 알렉산더\",\n  user.email = \"rohan.alexander@utoronto.ca\"\n)\n\nGitHub에서 새 프로젝트를 시작한 다음 로컬에 추가하는 대신 이제 use_git()를 사용하여 초기화하고 파일을 커밋할 수 있습니다. 커밋한 후에는 use_github()를 사용하여 GitHub로 푸시하여 GitHub에도 폴더를 만들 수 있습니다.\nGit과 GitHub에 겁을 먹는 것은 정상입니다. 많은 데이터 과학자들은 사용 방법에 대해 약간만 알고 있으며 괜찮습니다. 필요한 경우 최근 스냅샷을 갖도록 정기적으로 푸시하십시오.\n\n\n3.4.3 Git 충돌\n파우스트 박사는 크리스토퍼 말로의 16세기 희곡입니다. 흥미롭게도 두 가지 버전이 있으며 말로가 실제로 어떤 버전을 “그” 버전으로 의도했는지 아무도 모릅니다. 정확히 무엇을 세느냐에 따라 (faustus1604에는?) 약 2,048줄이 있고 (faustus1616에는?) 약 2,852줄이 있습니다. 줄 안에도 변경 사항이 있습니다(그림 3.6). 저자는 오래전에 사망했으므로 인류는 단순히 두 가지 버전이 있다는 이상한 상황에 처해 있습니다. 말로에게 Git이 있었다면 이런 일은 일어나지 않았을 것입니다!\n\n\n\n\n\n\n\n\n\n\n\n(a) 1604년 버전의 첫 몇 줄\n\n\n\n\n\n\n\n\n\n\n\n(b) 1616년 버전의 첫 몇 줄\n\n\n\n\n\n\n\n그림 3.6: 파우스트 박사의 1604년 버전과 1616년 버전 간의 차이점 표시\n\n\n\nGit과 GitHub를 사용할 때 때때로 버전을 체크인합니다. 그러나 동일한 리포지토리에서 작업하는 두 사람이 동일한 줄에 변경 사항을 적용하는 코드를 체크인하려고 하면 어떻게 될까요? Git은 병합 충돌을 설정하고 충돌을 해결하는 것은 충돌을 표면화한 사람, 즉 두 번째 커밋을 한 사람에게 달려 있습니다.\nGit은 파일에 충돌하는 줄을 모두 표시하고 시작 부분에 &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD 식별 마커를 추가하고, 충돌하는 변경 사항을 구분하기 위해 =======를 추가하고, 끝과 충돌을 일으키는 분기를 표시하기 위해 &gt;&gt;&gt;&gt;&gt;&gt;&gt; new_branch를 추가하여 충돌하는 줄을 식별합니다.\n\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n일부 내용\n=======\n일부 충돌하는 내용\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; new_branch\n\n충돌을 해결하려는 사람의 임무는 어떤 내용을 유지할지 선택하는 것입니다. 파일을 편집하고 저장한 다음 일반적인 방법으로 추가하고 커밋합니다.",
    "crumbs": [
      "기초",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>재현 가능한 워크플로</span>"
    ]
  },
  {
    "objectID": "03-workflow_ko.html#실제-r-사용",
    "href": "03-workflow_ko.html#실제-r-사용",
    "title": "3  재현 가능한 워크플로",
    "section": "3.5 실제 R 사용",
    "text": "3.5 실제 R 사용\n\n3.5.1 오류 처리\n\n프로그래밍을 할 때 결국 코드가 깨질 것입니다. 결국이라고 말하지만, 아마 하루에 10번이나 20번 정도일 것입니다.\nGelfand (2021)\n\nR이나 다른 프로그래밍 언어를 사용하는 모든 사람은 어느 시점에서 문제를 발견합니다. 이것은 정상입니다. 프로그래밍은 어렵습니다. 어느 시점에서 코드가 실행되지 않거나 오류가 발생합니다. 이것은 모든 사람에게 발생합니다. 좌절감을 느끼는 것은 일반적이지만 앞으로 나아가기 위해 문제 해결 전략을 개발합니다.\n\n오류 메시지가 표시되면 때때로 유용할 수 있습니다. 유용한 내용이 있는지 주의 깊게 읽어보십시오.\n오류 메시지를 검색해 보십시오. 검색 결과의 적절성을 높이기 위해 검색어에 “tidyverse” 또는 “in R”을 포함하는 것이 유용할 수 있습니다. 때때로 Stack Overflow 결과가 유용할 수 있습니다.\n함수 앞에 “?”를 붙여 함수의 도움말 파일을 살펴보십시오(예: ?pivot_wider()). 일반적인 문제는 약간 잘못된 인수 이름이나 형식(예: 객체 이름 대신 실수로 문자열을 포함하는 것)을 사용하는 것입니다.\n오류가 발생하는 위치를 살펴보고 오류가 해결될 때까지 코드를 제거하거나 주석 처리한 다음 코드를 천천히 다시 추가하십시오.\nclass()를 사용하여 객체의 클래스를 확인하십시오(예: class(data_set$data_column)). 예상되는 것인지 확인하십시오.\nR을 다시 시작하십시오: “세션” \\(\\rightarrow\\) “R 다시 시작 및 출력 지우기”. 그런 다음 모든 것을 다시 로드하십시오.\n컴퓨터를 다시 시작하십시오.\n오류 대신 수행하려는 작업을 검색하고 검색 결과의 적절성을 높이기 위해 검색어에 “tidyverse” 또는 “in R”을 포함해야 합니다. 예를 들어, “ggplot을 사용하여 R에서 그래프의 PDF 저장”입니다. 때때로 도움이 될 관련 블로그 게시물이나 Stack Overflow 답변이 있습니다.\n문제를 격리하고 다른 사람이 도울 수 있도록 작고 독립적인 재현 가능한 예제 “reprex”를 만드십시오.\nQuarto 문서에서 작업하는 경우 청크 옵션에 레이블을 포함하여 실수가 발생하는 위치를 더 쉽게 찾을 수 있도록 하십시오.\n\n더 일반적으로, 항상 가능한 것은 아니지만 휴식을 취하고 다음 날 다시 돌아오는 것이 거의 항상 도움이 됩니다.\n\n\n3.5.2 재현 가능한 예제\n\n아무도 당신에게 조언하거나 도울 수 없습니다. 아무도 없습니다. 당신이 해야 할 일은 단 한 가지뿐입니다. 자신 속으로 들어가십시오.\nRilke ([1929년] 2014)\n\n도움을 요청하는 것은 다른 기술과 마찬가지로 하나의 기술입니다. 연습을 통해 더 잘하게 됩니다. “이것은 작동하지 않습니다”, “모든 것을 시도했습니다”, “당신의 코드는 작동하지 않습니다” 또는 “오류 메시지가 여기 있습니다. 어떻게 해야 합니까?”라고 말하지 않도록 노력하는 것이 중요합니다. 일반적으로 가능한 문제가 너무 많기 때문에 이러한 의견을 바탕으로 도움을 줄 수 없습니다. 다른 사람이 당신을 도울 수 있도록 쉽게 만들어야 합니다. 여기에는 몇 가지 단계가 포함됩니다.\n\n데이터, 코드의 작고 독립적인 예제를 제공하고 무엇이 잘못되었는지 자세히 설명하십시오.\n지금까지 시도한 내용을 문서화하고, 살펴본 Stack Overflow 및 Posit 포럼 게시물과 원하는 내용이 아닌 이유를 포함하십시오.\n원하는 결과에 대해 명확하게 설명하십시오.\n\n최소한의 재현 가능한 예제(REPRoducible EXample), 즉 “reprex”를 만드는 것부터 시작하십시오. 이것은 오류를 재현하는 데 필요한 코드를 포함하지만 필요한 것만 포함합니다. 즉, 코드는 오류를 재현하지만 더 작고 간단한 버전일 가능성이 높습니다.\n때로는 이 과정에서 문제를 해결할 수 있습니다. 그렇지 않은 경우 다른 사람이 도움을 줄 수 있는 기회를 제공합니다. 이전에 아무도 다루지 않은 문제를 겪고 있을 가능성은 거의 없습니다. 주요 어려움은 자신이 하고 싶은 일과 일어나고 있는 일을 다른 사람이 모두 인식할 수 있는 방식으로 전달하려고 노력하는 것일 가능성이 더 큽니다. 끈기를 키우는 것이 중요합니다.\n재현 가능한 예제를 개발하려면 reprex가 특히 유용합니다. 설치 후 다음을 수행합니다.\n\nreprex 패키지를 로드합니다: library(reprex).\n문제가 있는 코드를 강조 표시하고 복사합니다.\n콘솔에서 reprex()를 실행합니다.\n\n코드가 독립적이면 뷰어에서 미리보기가 표시됩니다. 그렇지 않으면 오류가 발생하며 코드를 독립적으로 만들도록 다시 작성해야 합니다.\n오류를 재현하기 위해 데이터가 필요한 경우 R에 내장된 데이터를 사용해야 합니다. R에 내장된 데이터셋은 많이 있으며 library(help = \"datasets\")를 사용하여 볼 수 있습니다. 그러나 가능하면 mtcars 또는 faithful과 같은 일반적인 옵션을 사용해야 합니다. ?sec-fire-hose에서 소개된 GitHub Gist와 reprex를 결합하면 누군가가 당신을 도울 가능성이 높아집니다.\n\n\n3.5.3 정신 자세\n\n어떤 IDE에서 개발하든 어떤 도구를 사용하여 작업을 하든 당신은 실제적이고 유효하며 유능한 사용자이자 프로그래머입니다.\n문을 부수자, 모두를 위한 충분한 공간이 있다.\n샤를라 겔판드, 2020년 3월 10일.\n\n코드를 작성하면 어떻게 하든, 무엇을 위해 사용하든, 누구든 프로그래머입니다. 그러나 훌륭한 프로그래머들이 공통적으로 가지고 있는 몇 가지 특징이 있습니다.\n\n집중: 종종 “R 배우기”와 같은 목표는 실제 끝점이 없기 때문에 문제가 되는 경향이 있습니다. “ggplot2로 2022년 호주 선거에 대한 히스토그램 만들기”와 같이 더 작고 구체적인 목표를 갖는 것이 더 효율적인 경향이 있습니다. 이것은 몇 시간 안에 집중하고 달성할 수 있는 것입니다. “R을 배우고 싶다”와 같이 더 모호한 목표의 문제는 곁길로 새기 쉽고 도움을 받기가 더 어렵다는 것입니다. 이것은 의욕을 꺾고 사람들이 너무 일찍 그만두게 만들 수 있습니다.\n호기심: “한번 해보는 것”은 거의 항상 유용합니다. 즉, 확실하지 않으면 그냥 시도해 보십시오. 일반적으로 최악의 경우는 시간을 낭비하는 것입니다. 돌이킬 수 없을 정도로 무언가를 망가뜨리는 경우는 거의 없습니다. 예를 들어, 데이터프레임 대신 벡터를 ggplot()에 전달하면 어떻게 되는지 알고 싶다면 시도해 보십시오.\n실용적: 동시에 합리적인 범위 내에서 유지하고 매번 작은 변경 사항을 하나씩 적용하는 것이 유용할 수 있습니다. 예를 들어, 일부 회귀 분석을 실행하고 싶고 lm() 대신 rstanarm을 사용할 가능성에 대해 궁금하다고 가정해 보겠습니다. 실용적인 진행 방법은 처음에 rstanarm의 한 가지 측면을 사용한 다음 다음에 다른 변경 사항을 적용하는 것입니다.\n끈기: 다시 말하지만, 이것은 균형 잡힌 행동입니다. 모든 프로젝트에서 예상치 못한 문제와 이슈가 발생합니다. 한편으로는 이러한 문제에도 불구하고 인내하는 것이 좋은 경향입니다. 그러나 다른 한편으로는 돌파구가 가능해 보이지 않으면 무언가를 포기할 준비가 되어 있어야 합니다. 멘토는 무엇이 합리적인지 더 잘 판단하는 경향이 있으므로 유용할 수 있습니다.\n계획적: 무엇을 할 것인지 과도하게 계획하는 것은 거의 항상 유용합니다. 예를 들어, 일부 데이터의 히스토그램을 만들고 싶을 수 있습니다. 필요한 단계를 계획하고 각 단계가 어떻게 구현될 수 있는지 스케치해야 합니다. 예를 들어, 첫 번째 단계는 데이터를 가져오는 것입니다. 어떤 패키지가 유용할 수 있습니까? 데이터는 어디에 있을 수 있습니까? 데이터가 거기에 없으면 백업 계획은 무엇입니까?\n완벽보다 완료: 우리 모두는 다양한 완벽주의적 경향을 가지고 있지만, 어느 정도까지는 처음에는 그것들을 끄려고 노력하는 것이 유용할 수 있습니다. 처음에는 작동하는 코드를 작성하는 것에만 신경 쓰십시오. 언제든지 돌아와서 측면을 개선할 수 있습니다. 그러나 실제로 출시하는 것이 중요합니다. 작업을 완료하는 보기 흉한 코드가 결코 끝나지 않는 아름다운 코드보다 낫습니다.\n\n\n\n3.5.4 코드 주석 및 스타일\n코드는 주석 처리되어야 합니다. 주석은 특정 코드가 작성된 이유와 일반적인 대안이 선택되지 않은 이유에 덜 초점을 맞춰야 합니다. 실제로 코드를 작성하기 전에 주석을 작성하여 무엇을 하고 싶은지, 왜 하고 싶은지 설명한 다음 코드를 작성하는 것이 좋습니다(Fowler 와/과 Beck 2018, p. 59).\n특히 R에서는 코드를 작성하는 방법이 하나만 있는 것은 아닙니다. 그러나 혼자 작업하는 경우에도 더 쉽게 작업할 수 있도록 하는 몇 가지 일반적인 지침이 있습니다. 대부분의 프로젝트는 시간이 지남에 따라 발전하며 코드 주석의 한 가지 목적은 미래의 자신이 수행한 작업과 특정 결정이 내려진 이유를 추적할 수 있도록 하는 것입니다(Bowers 와/과 Voors 2016).\nR 스크립트의 주석은 # 기호를 포함하여 추가할 수 있습니다. (#의 동작은 Quarto 문서의 R 청크 내부 줄에서는 주석으로 작동하지만 R 청크 외부 줄에서는 제목 수준을 설정하는 것과 다릅니다.) 줄 시작 부분에 주석을 달 필요는 없으며 중간에 달 수도 있습니다. 일반적으로 코드의 모든 측면이 무엇을 하는지 주석 처리할 필요는 없지만 명확하지 않은 부분은 주석 처리해야 합니다. 예를 들어, 일부 값을 읽어오면 어디서 가져오는지 주석 처리하고 싶을 수 있습니다.\n왜 무언가를 하고 있는지 주석을 달아야 합니다(Wickham 2021). 무엇을 달성하려고 합니까? 이상한 것을 설명하기 위해 주석을 달아야 합니다. 예를 들어 특정 행(예: 27행)을 제거하는 경우 해당 행을 제거하는 이유는 무엇입니까? 순간에는 명백해 보일 수 있지만 미래의 당신은 기억하지 못할 것입니다.\n코드를 섹션으로 나누어야 합니다. 예를 들어, 작업 공간 설정, 데이터셋 읽기, 데이터셋 조작 및 정리, 데이터셋 분석, 마지막으로 표와 그림 생성입니다. 각 섹션은 무슨 일이 일어나고 있는지 설명하는 주석으로 구분되어야 하며 길이에 따라 때로는 별도의 파일로 구분되어야 합니다.\n또한 각 파일 상단에는 파일의 목적, 전제 조건 또는 종속성, 날짜, 작성자 및 연락처 정보, 마지막으로 위험 신호나 할 일과 같은 기본 정보를 기록하는 것이 중요합니다.\nR 스크립트에는 머리글과 명확한 섹션 구분이 있어야 합니다.\n#### 머리말 ####\n# 목적: 이 스크립트가 하는 일에 대한 간략한 문장\n# 저자: 귀하의 이름\n# 날짜: 작성일\n# 연락처: 이메일 추가\n# 라이선스: 코드가 어떻게 사용될 수 있는지 생각해 보십시오.\n# 전제 조건:\n# - 일부 데이터나 다른 스크립트가 실행되어야 할 수도 있습니까?\n\n\n#### 작업 공간 설정 ####\n# install.packages 줄을 유지하지 마십시오. 필요한 경우 주석 처리하십시오.\n# 패키지 로드\nlibrary(tidyverse)\n\n# 편집되지 않은 데이터를 읽어옵니다.\nraw_data &lt;- read_csv(\"inputs/data/unedited_data.csv\")\n\n\n#### 다음 섹션 ####\n...\n\n마지막으로 사용자가 코드를 주석 처리하고 주석 처리를 해제하거나 디렉터리 지정과 같은 다른 수동 단계를 수행하여 코드가 작동하도록 하는 데 의존하지 마십시오. 이렇게 하면 자동화된 코드 확인 및 테스트를 사용할 수 없습니다.\n이 모든 작업에는 시간이 걸립니다. 대략적인 경험 법칙으로 코드를 작성하는 데 걸린 시간만큼 코드를 주석 처리하고 개선하는 데 시간을 할애해야 합니다. 깔끔하게 주석 처리된 코드의 예로는 Dolatsara 기타 (2021) 및 (burton2021reconsidering이?) 있습니다.\n\n\n3.5.5 테스트\n테스트는 코드 전체에 걸쳐 작성되어야 하며, 마지막에 한꺼번에 작성하는 것이 아니라 진행하면서 작성해야 합니다. 이렇게 하면 속도가 느려집니다. 그러나 생각하고 실수를 수정하는 데 도움이 되어 코드가 더 좋아지고 전반적인 생산성이 향상됩니다. 테스트가 없는 코드는 의심스럽게 봐야 합니다. R 패키지의 테스트 관행에는 개선의 여지가 있으며(Vidoni 2021), 더 일반적으로 R 코드에 대해서는 말할 것도 없습니다.\n다른 사람, 그리고 이상적으로는 자동화된 프로세스가 코드에서 테스트를 실행해야 할 필요성은 우리가 재현성을 강조하는 한 가지 이유입니다. 또한 파일 경로를 하드코딩하지 않고 프로젝트를 사용하고 파일 이름에 공백을 두지 않는 것과 같은 작은 측면을 강조하는 이유이기도 합니다.\n완전하고 일반적인 테스트 모음을 정의하기는 어렵지만 광범위하게 다음을 테스트하려고 합니다.\n\n경계 조건,\n클래스,\n누락된 데이터,\n관찰 및 변수의 수,\n중복, 그리고\n회귀 결과.\n\n이 모든 작업을 처음에 시뮬레이션된 데이터에 대해 수행한 다음 실제 데이터로 이동합니다. 이는 아폴로 프로그램 중 테스트의 진화를 반영합니다. 처음에는 요구 사항에 대한 기대를 기반으로 테스트가 수행되었으며, 이러한 테스트는 나중에 실제 발사 측정을 고려하여 업데이트되었습니다(Simpkinson 1971, p. 21). 무한한 수의 테스트를 작성할 수 있지만 생각 없는 많은 테스트보다 소수의 고품질 테스트가 더 좋습니다.\n테스트 유형 중 하나는 “어설션”입니다. 어설션은 무언가가 참인지 확인하고 그렇지 않으면 코드 실행을 중지하기 위해 코드 전체에 작성됩니다(Irving 기타 2021, p. 272). 예를 들어, 변수가 숫자여야 한다고 어설션할 수 있습니다. 이 어설션에 대해 테스트되었을 때 문자로 판명되면 테스트가 실패하고 스크립트 실행이 중지됩니다. 데이터 과학의 어설션 테스트는 일반적으로 데이터 정리 및 준비 스크립트에서 사용됩니다. 이에 대해서는 ?sec-clean-and-prepare에서 더 자세히 설명합니다. 단위 테스트는 코드의 완전한 측면을 확인합니다(Irving 기타 2021, p. 274). 모델링을 고려할 때 ?sec-its-just-a-linear-model에서 더 자세히 고려할 것입니다.",
    "crumbs": [
      "기초",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>재현 가능한 워크플로</span>"
    ]
  },
  {
    "objectID": "03-workflow_ko.html#효율성",
    "href": "03-workflow_ko.html#효율성",
    "title": "3  재현 가능한 워크플로",
    "section": "3.6 효율성",
    "text": "3.6 효율성\n일반적으로 이 책에서는 무언가를 완료하는 데만 관심을 두고 있으며 앞으로도 그럴 것입니다. 반드시 최선의 방법이나 가장 효율적인 방법으로 완료하는 것은 아닙니다. 왜냐하면 대부분의 경우 그것에 대해 걱정하는 것은 시간 낭비이기 때문입니다. 대부분의 경우 클라우드에 밀어 넣고 합리적인 시간 동안 실행되도록 한 다음 그 시간을 파이프라인의 다른 측면을 걱정하는 데 사용하는 것이 좋습니다. 그러나 그것은 결국 실행 불가능해집니다. 어느 시점에서, 그리고 이것은 상황에 따라 다르지만, 효율성이 중요해집니다. 결국 보기 흉하거나 느린 코드와 특정 작업 방식에 대한 독단적인 주장은 영향을 미칩니다. 그리고 그 시점에서 효율성을 보장하기 위해 새로운 접근 방식에 개방적이어야 합니다. 명백한 성능 향상을 위한 가장 일반적인 영역은 거의 없습니다. 대신 측정, 평가 및 사고 능력을 개발하는 것이 중요합니다.\n코드 효율성을 높이는 가장 좋은 방법 중 하나는 다른 사람이 검토할 수 있도록 준비하는 것입니다. 시간을 최대한 활용하려면 코드를 읽기 쉽게 만드는 것이 중요합니다. 따라서 “코드 린팅”과 “스타일링”부터 시작합니다. 이것은 코드를 직접적으로 빠르게 만들지는 않지만 다른 사람이 코드를 보거나 잠시 쉬었다가 다시 볼 때 더 효율적으로 만듭니다. 이를 통해 공식적인 코드 검토 및 리팩토링이 가능해지며, 이는 코드가 하는 일은 변경하지 않고 더 좋게 만들기 위해 코드를 다시 작성하는 것입니다(동일한 작업을 하지만 다른 방식으로 수행합니다). 그런 다음 실행 시간 측정으로 전환하고 컴퓨터가 동시에 여러 프로세스에 대한 코드를 실행하도록 허용하는 병렬 처리를 도입합니다.\n\n3.6.1 코드 환경 공유\n코드 공유의 필요성에 대해 자세히 논의했으며 GitHub를 사용하는 접근 방식을 제시했습니다. 그리고 ?sec-store-and-share에서는 데이터 공유에 대해 논의할 것입니다. 그러나 다른 사람이 우리 코드를 실행할 수 있도록 하는 또 다른 요구 사항이 있습니다. ?sec-fire-hose에서는 R 자체와 R 패키지가 새로운 기능이 개발되고 오류가 수정되며 기타 일반적인 개선이 이루어짐에 따라 때때로 업데이트되는 방식에 대해 논의했습니다. ?sec-r-essentials에서는 tidyverse의 한 가지 장점이 기본 R보다 더 구체적이기 때문에 더 빠르게 업데이트될 수 있다는 점을 설명합니다. 그러나 이는 우리가 사용하는 모든 코드와 데이터를 공유하더라도 사용 가능해진 소프트웨어 버전으로 인해 오류가 발생할 수 있음을 의미할 수 있습니다.\n이에 대한 해결책은 사용된 환경을 자세히 설명하는 것입니다. 이를 수행하는 방법은 여러 가지가 있으며 복잡성을 더할 수 있습니다. 우리는 사용된 R 및 R 패키지 버전을 문서화하고 다른 사람이 해당 정확한 버전을 더 쉽게 설치할 수 있도록 하는 데만 집중합니다. 기본적으로 재현성에 도움이 되기 때문에 사용한 설정을 격리하는 것입니다(Perkel 2023). R에서는 renv를 사용하여 이 작업을 수행할 수 있습니다.\nrenv가 설치되고 로드되면 init()를 사용하여 필요한 인프라 설정을 가져옵니다. 사용된 패키지와 버전을 기록할 파일을 만들 것입니다. 그런 다음 snapshot()을 사용하여 실제로 사용 중인 것을 문서화합니다. 이렇게 하면 정보를 기록하는 “잠금 파일”이 만들어집니다.\nR 프로젝트에서 사용 중인 패키지를 확인하려면 dependencies()를 사용할 수 있습니다. 예제 폴더에 대해 이 작업을 수행하면 rmarkdown, bookdown, knitr, rmarkdown, bookdown, knitr, palmerpenguins, tidyverse, renv, haven, readr 및 tidyverse 패키지가 사용됨을 나타냅니다.\n원하는 경우 잠금 파일(“renv.lock”)을 열어 정확한 버전을 확인할 수 있습니다. 잠금 파일은 설치된 다른 모든 패키지와 다운로드한 위치도 문서화합니다. 외부에서 이 프로젝트에 오는 사람은 restore()를 사용하여 우리가 사용한 패키지의 정확한 버전을 설치할 수 있습니다.\n\n\n3.6.2 코드 린팅 및 스타일링\n빠른 것이 가치가 있지만 대부분 빠른 반복이 가능한 것이지 반드시 코드가 빠르게 실행되는 것은 아닙니다. Backus (1981, p. 26)는 1954년에도 프로그래머가 컴퓨터만큼이나 비용이 많이 들었고 요즘에는 추가적인 계산 능력이 프로그래머보다 훨씬 저렴하다고 설명합니다. 성능 좋은 코드는 중요하지만 다른 사람의 시간을 효율적으로 사용하는 것도 중요합니다. 코드는 거의 한 번만 작성되지 않습니다. 대신 실수를 수정하기 위해서라도 일반적으로 다시 돌아와야 하며, 이는 코드를 사람이 읽을 수 있어야 함을 의미합니다(Matsumoto 2007, p. 478). 이것이 이루어지지 않으면 효율성 비용이 발생합니다.\n린팅과 스타일링은 코드를 확인하고, 주로 스타일 문제를 확인하고, 코드를 더 읽기 쉽게 재정렬하는 과정입니다. (린팅의 또 다른 측면은 닫는 괄호를 잊는 것과 같은 프로그래밍 오류를 처리하는 것이지만, 여기서는 스타일 문제에 중점을 둡니다.) 종종 최고의 효율성 향상은 다른 사람이 코드를 더 쉽게 읽을 수 있도록 하는 데서 비롯되며, 이는 휴식 후 코드로 돌아오는 자신일지라도 마찬가지입니다. 미국 독점 거래 회사인 제인 스트리트는 위험 완화의 핵심 부분으로 코드를 읽기 쉽게 만드는 데 매우 중점을 둡니다(Minsky 2011). 우리 모두가 코드의 잠재적으로 변덕스러운 관리에 수십억 달러를 가지고 있지는 않겠지만, 우리 모두는 코드가 오류를 생성하지 않기를 바랄 것입니다.\nlintr의 lint()를 사용하여 코드를 린트합니다. 예를 들어, 다음 R 코드를 고려하십시오(“linting_example.R”로 저장됨).\n\nSIMULATED_DATA &lt;-\n  tibble(\n    division = c(1:150, 151),\n    party = sample(\n      x = c(\"Liberal\"),\n      size = 151,\n      replace = T\n    )\n  )\n\n\nlint(filename = \"linting_example.R\")\n\n결과는 “linting_example.R” 파일이 열리고 lint()가 발견한 문제가 “마커”에 인쇄된다는 것입니다(그림 3.7). 그런 다음 문제를 처리하는 것은 사용자에게 달려 있습니다.\n\n\n\n\n\n\n그림 3.7: 예제 R 코드의 린팅 결과\n\n\n\n권장 변경 사항을 적용하면 (tidyversestyleguide에서?) 정의한 모범 사례와 일치하고 더 읽기 쉬운 코드가 됩니다.\n\nsimulated_data &lt;-\n  tibble(\n    division = c(1:150, 151),\n    party = sample(\n      x = c(\"Liberal\"),\n      size = 151,\n      replace = TRUE\n    )\n  )\n\n처음에는 린터가 식별하는 후행 공백이나 큰따옴표만 사용하는 것과 같은 일부 측면이 작고 중요하지 않은 것처럼 보일 수 있습니다. 그러나 더 큰 문제를 해결하는 데 방해가 됩니다. 또한 작은 것을 제대로 처리하지 못하면 어떻게 큰 것을 제대로 처리할 수 있다고 누가 믿을 수 있겠습니까? 따라서 린터가 식별하는 모든 작은 측면을 처리하는 것이 중요합니다.\nlintr 외에도 styler도 사용합니다. 이것은 문제 목록을 제공하는 린터와 달리 스타일 문제를 자동으로 조정합니다. 이를 실행하려면 style_file()을 사용합니다.\n\nstyle_file(path = \"linting_example.R\")\n\n이렇게 하면 공백 및 들여쓰기와 같은 변경 사항이 자동으로 적용됩니다. 따라서 변경 사항을 검토하고 오류가 발생하지 않았는지 확인하기 위해 프로젝트가 끝날 때 한 번만 수행하는 것이 아니라 정기적으로 수행해야 합니다.\n\n\n3.6.3 코드 검토\n이러한 모든 스타일 측면을 처리한 후 코드 검토로 전환할 수 있습니다. 이것은 다른 사람이 코드를 살펴보고 비평하는 과정입니다. 많은 전문 작가에게는 편집자가 있으며 코드 검토는 데이터 과학에서 가장 가까운 것입니다. 코드 검토는 코드 작성의 중요한 부분이며 Irving 기타 (2021, p. 465)는 이를 “버그를 찾는 가장 효과적인 방법”이라고 설명합니다. 피드백을 받는 것이 개선하는 좋은 방법이기 때문에 코딩을 배울 때 특히 도움이 되지만 상당히 부담스러울 수 있습니다.\n다른 사람의 코드를 검토할 때는 정중하고 동료애를 발휘하도록 노력하십시오. 공백 및 구분과 같은 작은 측면은 린터와 스타일러가 처리했어야 하지만 그렇지 않은 경우 일반적인 권장 사항을 제시하십시오. 데이터 과학에서 코드 검토자로서 대부분의 시간은 다음과 같은 측면에 할애해야 합니다.\n\n정보성 README가 있고 어떻게 개선할 수 있습니까?\n파일 이름과 변수 이름이 일관되고 정보가 풍부하며 의미가 있습니까?\n주석을 통해 무언가를 하는 이유를 이해할 수 있습니까?\n테스트가 적절하고 충분합니까? 고려되지 않은 엣지 케이스나 코너 솔루션이 있습니까? 마찬가지로 제거할 수 있는 불필요한 테스트가 있습니까?\n변수로 변경하고 설명할 수 있는 매직 넘버가 있습니까?\n변경할 수 있는 중복 코드가 있습니까?\n해결해야 할 미해결 경고가 있습니까?\n더 작은 것으로 나눌 수 있는 특히 큰 함수나 파이프가 있습니까?\n프로젝트 구조가 적절합니까?\n코드 중 일부를 데이터로 변경할 수 있습니까(Irving 기타 2021, p. 462)?\n\n예를 들어, 총리와 대통령의 이름을 찾는 코드를 생각해 보십시오. 이 코드를 처음 작성했을 때 관련 이름을 코드에 직접 추가했을 가능성이 높습니다. 그러나 코드 검토의 일환으로 대신 이를 변경하도록 권장할 수 있습니다. 관련 이름의 작은 데이터셋을 만들고 해당 데이터셋을 조회하도록 코드를 다시 작성하도록 권장할 수 있습니다.\n코드 검토는 코드를 최소한 다른 한 사람이 이해할 수 있도록 보장합니다. 이것은 세상에 대한 지식을 구축하는 데 중요한 부분입니다. Google에서는 코드 검토가 주로 결함을 찾는 것이 아니라(그럴 수도 있지만) 가독성과 유지 관리성을 보장하고 교육을 제공하는 것입니다(Sadowski 기타 2018). 제인 스트리트에서도 마찬가지이며, 버그를 잡고, 기관 지식을 공유하고, 교육을 지원하고, 직원이 읽을 수 있는 코드를 작성하도록 의무화하기 위해 코드 검토를 사용합니다(Minsky 2015).\n마지막으로 코드 검토는 모든 코드를 읽는 번거로운 며칠이 걸리는 과정일 필요도 없고 그래서도 안 됩니다. 최고의 코드 검토는 단 몇 줄의 변경 사항을 제안하는 데 초점을 맞춘 단일 파일에 대한 빠른 검토입니다. 실제로 한 개인이 아니라 소규모 팀이 검토하는 것이 더 나을 수 있습니다. 한 번에 너무 많은 코드를 검토하지 마십시오. 최대 몇 백 줄 정도이며 약 1시간이 걸립니다. 그 이상은 효능 감소와 관련이 있는 것으로 밝혀졌습니다(Cohen, Teleki, 와/과 Brown 2006, p. 79).\n\n\n3.6.4 코드 리팩토링\n코드 리팩토링이란 새 코드가 이전 코드와 동일한 결과를 달성하지만 새 코드가 더 잘 수행하도록 코드를 다시 작성하는 것을 의미합니다. 예를 들어, (refactornature는?) 중요한 영국 Covid 모델을 뒷받침하는 코드가 처음에 역학자에 의해 작성되었고 몇 달 후 왕립 학회, Microsoft 및 GitHub 팀에 의해 명확해지고 정리되었다고 논의합니다. 이는 동일한 입력을 고려할 때 두 버전 모두 동일한 출력을 생성했지만 모델에 대한 신뢰도를 높였기 때문에 가치가 있었습니다.\n일반적으로 다른 사람이 작성한 코드와 관련하여 코드 리팩토링을 참조합니다. (실제로 우리가 코드를 작성했을 수도 있고, 단지 얼마 전이었을 수도 있습니다.) 코드 리팩토링을 시작할 때 다시 작성된 코드가 원래 코드와 동일한 결과를 달성하는지 확인하고 싶습니다. 즉, 의존할 수 있는 적절한 테스트 모음이 작성되어 있어야 합니다. 이러한 테스트가 없으면 만들어야 할 수도 있습니다.\n다른 사람이 더 쉽게 이해할 수 있도록 코드를 다시 작성하여 결론에 대한 신뢰도를 높입니다. 그러나 그렇게 하기 전에 기존 코드가 무엇을 하는지 이해해야 합니다. 시작하는 한 가지 방법은 코드를 살펴보고 광범위한 주석을 추가하는 것입니다. 이러한 주석은 일반적인 주석과 다릅니다. 각 코드 청크가 무엇을 하려고 하는지, 그리고 이것을 어떻게 개선할 수 있는지 이해하려는 적극적인 과정입니다.\n코드를 리팩토링하는 것은 모범 사례를 충족하는지 확인하는 기회입니다. (Trisovic2022는?) 다음을 포함하여 9,000개의 R 스크립트를 검토한 결과를 바탕으로 몇 가지 핵심 권장 사항을 자세히 설명합니다.\n\nsetwd() 및 모든 절대 경로를 제거하고 “.Rproj” 파일과 관련된 상대 경로만 사용하도록 합니다.\n명확한 실행 순서가 있는지 확인합니다. 처음에는 파일 이름에 숫자를 사용하여 이를 달성하도록 권장했지만 결국 targets(Landau 2021)와 같은 더 정교한 접근 방식을 대신 사용할 수 있습니다.\n코드가 다른 컴퓨터에서 실행될 수 있는지 확인합니다.\n\n예를 들어, 다음 코드를 고려하십시오.\n\nsetwd(\"/Users/rohanalexander/Documents/telling_stories\")\n\nlibrary(tidyverse)\n\nd = read_csv(\"cars.csv\")\n\nmtcars =\n  mtcars |&gt;\n  mutate(K_P_L = mpg / 2.352)\n\nlibrary(datasauRus)\n\ndatasaurus_dozen\n\nR 프로젝트를 만들어 setwd()를 제거하고, 모든 library() 호출을 맨 위에 그룹화하고, “=” 대신 “&lt;-”를 사용하고, 변수 이름과 일관성을 유지하여 변경할 수 있습니다.\n\nlibrary(tidyverse)\nlibrary(datasauRus)\n\ncars_data &lt;- read_csv(\"cars.csv\")\n\nmpg_to_kpl_conversion_factor &lt;- 2.352\n\nmtcars &lt;-\n  mtcars |&gt;\n  mutate(kpl = mpg / mpg_to_kpl_conversion_factor)",
    "crumbs": [
      "기초",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>재현 가능한 워크플로</span>"
    ]
  },
  {
    "objectID": "03-workflow_ko.html#결론",
    "href": "03-workflow_ko.html#결론",
    "title": "3  재현 가능한 워크플로",
    "section": "3.7 결론",
    "text": "3.7 결론\n이 장에서 우리는 많은 것을 다루었으며 압도감을 느끼는 것은 정상입니다. 필요에 따라 Quarto 섹션으로 돌아오십시오. 많은 사람들이 Git과 GitHub에 대해 혼란스러워하며 그럭저럭 사용할 수 있을 만큼만 알고 있습니다. 그리고 효율성에 많은 내용이 있었지만 성능 좋은 코드의 가장 중요한 측면은 다른 사람이 읽기 쉽게 만드는 것입니다. 심지어 그 사람이 휴식 후 돌아온 자신일지라도 말입니다.",
    "crumbs": [
      "기초",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>재현 가능한 워크플로</span>"
    ]
  },
  {
    "objectID": "03-workflow_ko.html#연습-문제",
    "href": "03-workflow_ko.html#연습-문제",
    "title": "3  재현 가능한 워크플로",
    "section": "3.8 연습 문제",
    "text": "3.8 연습 문제\n\n연습\n\n(계획) 다음 시나리오를 고려하십시오. 특정 국가에서는 의회 의석을 차지할 수 있는 정당이 항상 4개뿐입니다. 특정 의석과 관련된 지역에서 과반수 득표를 한 후보가 해당 의석을 차지합니다. 의회는 총 175석으로 구성됩니다. 분석가는 의석별 각 정당의 득표 수에 관심이 있습니다. 데이터셋이 어떻게 생겼는지 스케치한 다음 모든 관찰 결과를 보여주는 그래프를 스케치하십시오.\n(시뮬레이션) 설명된 시나리오를 더 자세히 고려하고 상황을 시뮬레이션하십시오. 아래 코드를 사용하여 적절한 상황을 신중하게 지정하십시오. 그런 다음 시뮬레이션된 데이터를 기반으로 5가지 테스트를 작성하십시오.\n\n\nlibrary(tidyverse)\n\nelection_results &lt;-\n  tibble(\n    seat = rep(1:175, each = 4),\n    party = rep(x = 1:4, times = 175),\n    votes = runif(n = 175 * 4, min = 0, max = 1000) |&gt; floor()\n  )\n\n\n(획득) 관심 있는 국가의 투표에 대한 실제 데이터 출처를 지정하십시오.\n(탐색) 다음 코드로 시작하여 각 정당이 획득한 의석 수 표를 만드십시오.\n\n\nlibrary(tidyverse)\n\nelection_results |&gt;\n  slice_max(votes, n = 1, by = seat) |&gt;\n  count(party) |&gt;\n  tt()\n\n\n(공유) 식별한 출처에서 데이터를 수집한 것처럼(시뮬레이션 대신) 그리고 시뮬레이션된 데이터를 사용하여 만든 표가 실제 상황을 반영하는 것처럼 두 단락을 작성하십시오. 단락에 포함된 정확한 세부 정보는 사실일 필요는 없지만 합리적이어야 합니다(즉, 실제로 데이터를 가져오거나 그래프를 만들 필요는 없습니다). 코드를 R 파일과 Quarto 문서로 적절하게 분리하십시오. README가 있는 GitHub 리포지토리에 대한 링크를 제출하십시오.\n\n\n\n퀴즈\n\n(Gelman2016에?) 따르면 연구자들이 데이터 분석의 유연성을 이용하여 유의미한 결과를 찾는 것을 의미하는 통계적 개념은 무엇입니까 (하나 선택)?\n\n무작위 표본 추출.\nP-해킹.\n귀무 가설 검정.\n베이즈 추론.\n\n(Gelman2016에?) 따르면 “p-해킹”이란 무엇입니까 (하나 선택)?\n\np-값을 수정하는 방법.\n유의미하지 않은 결과가 유의미해질 때까지 데이터 또는 분석을 조작하는 것.\n계산 효율성을 향상시키는 기술.\n데이터 공유에 대한 윤리적 접근 방식.\n\n(Gelman2016에?) 따르면 파일 서랍 문제란 무엇입니까 (하나 선택)?\n\n유의미한 결과만 게시함으로써 발생하는 편향.\n보관된 데이터에 액세스하는 데 어려움.\n데이터 코딩 및 입력 오류.\n오래된 실험을 복제하는 데 어려움.\n\n(Gelman2016에?) 따르면 긍정적인 결과만 게시하는 경향을 나타내는 용어는 무엇입니까 (하나 선택)?\n\n데이터 마이닝.\n출판 편향.\n확증 편향.\n표본 추출 오류.\n\n(Gelman2016에?) 따르면 연구자들이 유의미한 결과로 이어질 수 있는 데이터 분석에서 갖는 다양한 선택을 설명하는 용어는 무엇입니까 (하나 선택)?\n\n연구자 자유도.\n데이터 마이닝.\n표본 편향.\n효과 크기 조작.\n\n(Gelman2016에?) 따르면 갈림길의 정원은 어떤 문제를 의미합니까 (하나 선택)?\n\n머신 러닝에서 의사 결정 트리의 복잡성.\n동일한 데이터로 수행할 수 있는 여러 잠재적 분석.\n시간이 지남에 따라 이론과 응용 작업의 분기.\n학문 분야의 분기.\n\n(Gelman2016에?) 따르면 연구에서 “복제”란 무엇입니까 (하나 선택)?\n\n새로운 데이터를 사용하여 원래 결과를 재현하는 연구.\n이전 방법론을 비판하는 연구.\n여러 연구에 대한 메타 분석.\n원래 연구 원고의 정확한 사본.\n\n(Gelman2016에?) 따르면 사회 과학에서 재현 불가능한 결과에 기여하는 것은 무엇입니까 (하나 선택)?\n\n부적절한 표본 크기.\n고급 통계 소프트웨어 부족.\n선택적 보고로 이어지는 연구자 자유도.\n질적 데이터에 대한 과도한 의존.\n\n(Gelman2016에?) 따르면 복제 위기란 무엇을 의미합니까 (하나 선택)?\n\n새로운 이론을 만드는 데 어려움.\n유사한 연구의 과잉 생산.\n이전 연구 결과를 복제하는 데 어려움.\n실험 참가자 부족.\n\n(Gelman2016에?) 따르면 복제 위기를 완화하는 데 도움이 되는 것은 무엇입니까 (하나 선택)?\n\n데이터 기밀 유지.\n유의미한 결과만 게시.\n연구 및 분석 계획 사전 등록.\n독점 소프트웨어 사용 증가.\n\n(Gelman2016은?) 심리학의 복제 위기에 초점을 맞춥니다. 자신의 경험을 바탕으로 다른 수업에서와 같이 다른 분야를 선택하고 해당 분야에서 복제 문제가 어느 정도 있을 수 있는지, 그리고 그 이유에 대해 작성하십시오.\n익숙한 분야를 선택하십시오. 해당 분야에서 재현성을 향상시킬 수 있는 관행은 무엇입니까? 간략한 설명을 제공하십시오.\n(wilsongoodenough에?) 따르면 다음 중 중요한 데이터 관리 관행은 무엇입니까 (모두 선택)?\n\n원시 데이터와 정리된 버전을 모두 저장합니다.\n데이터 처리 단계를 문서화합니다.\n데이터 저장을 위해 비독점 파일 형식을 사용합니다.\n\n(wilsongoodenough에?) 따르면 프로젝트의 홈 디렉터리에 README 파일을 만드는 것이 중요한 이유는 무엇입니까 (하나 선택)?\n\n원시 데이터 파일을 저장하기 위해.\n프로젝트의 목적을 설명하고 개요를 제공하기 위해.\n프로젝트의 모든 오류와 버그를 나열하기 위해.\n프로젝트 파일의 모든 버전을 추적하기 위해.\n\n(wilsongoodenough에?) 따르면 버전 관리를 사용하는 주요 이점은 무엇입니까 (하나 선택)?\n\n연구원을 위해 코드를 자동으로 작성합니다.\n변경 사항을 추적하고 공동 작업을 돕습니다.\n데이터 백업의 필요성을 대체합니다.\n모든 데이터가 암호화되도록 보장합니다.\n\n(wilsongoodenough에?) 따르면 프로젝트에서 파일 이름을 지정하는 권장 관행은 무엇입니까 (하나 선택)?\n\n파일 이름에 내용이나 기능을 반영합니다.\nresult1.csv, result2.csv와 같은 순차 번호 사용.\n파일 이름을 고유하게 만들기 위해 특수 문자 포함.\n파일 이름에 공백 및 구두점 사용.\n\n(wilsongoodenough에?) 따르면 원시 데이터의 수정되지 않은 사본을 저장해야 하는 이유는 무엇입니까 (하나 선택)?\n\n데이터 저장 공간을 절약하기 위해.\n법적 규정을 준수하기 위해.\n검증 및 재현성을 위해 변경되지 않은 출처를 보장하기 위해.\n소프트웨어 업데이트와의 호환성을 유지하기 위해.\n\n(wilsongoodenough에?) 따르면 개방형 파일 형식을 사용하는 주요 이점은 무엇입니까 (하나 선택)?\n\n처리 속도가 더 빠릅니다.\n독점 소프트웨어 없이 액세스할 수 있습니다.\n데이터를 더 효율적으로 압축합니다.\n데이터 보안을 강화합니다.\n\n(wilsongoodenough에?) 따르면 다음 중 데이터 파일을 구성할 때 권장되는 관행은 무엇입니까 (모두 선택)?\n\n의미 있고 일관된 파일 이름을 사용합니다.\n모든 파일을 단일 폴더에 저장합니다.\n파일을 명확한 디렉터리 구조로 구성합니다.\n버전 추적을 위해 파일 이름에 날짜를 포함합니다.\n\n(wilsongoodenough에?) 따르면 데이터 처리 단계를 문서화하는 것이 중요한 이유는 무엇입니까 (하나 선택)?\n\n데이터 분석 속도를 높입니다.\n데이터 암호화에 도움이 됩니다.\n저장 요구 사항을 줄입니다.\n다른 사람이 분석을 이해하고 재현할 수 있도록 합니다.\n\n재현성의 이점은 무엇입니까?\n\n결과를 독립적으로 확인할 수 있습니다.\n코드 실행 속도를 높입니다.\n데이터 시각화를 더 쉽게 만듭니다.\n문서화의 필요성을 줄입니다.\n\n(Alexander2019에?) 따르면 연구는 다음과 같은 경우 재현 가능합니다 (하나 선택).\n\n동료 심사를 거친 저널에 게시됩니다.\n연구에 사용된 모든 자료가 제공됩니다.\n저자가 자료를 제공하지 않고도 정확하게 재현할 수 있습니다.\n연구에 사용된 모든 자료가 주어지면 정확하게 재현할 수 있습니다.\n\n문학적 프로그래밍이란 무엇입니까 (하나 선택)?\n\n코드와 문서를 다른 파일로 분리합니다.\n코드의 구문 오류를 자동으로 수정합니다.\n코드 문서 생성을 자동화합니다.\n동일한 문서에 코드와 자연어를 통합합니다.\n\n재현 가능한 워크플로에서 git의 주요 기능은 무엇입니까 (하나 선택)?\n\n데이터 정리 자동화.\n병렬로 코드 실행.\n보고서에 데이터 시각화 통합.\n코드에 대한 버전 관리 시스템 제공.\n\n(tidyversestyleguide에?) 따르면 “00_get_data.R”과 “get data.R” 파일은 어떻게 분류됩니까 (하나 선택)?\n\n나쁨; 나쁨.\n좋음; 나쁨.\n나쁨; 좋음.\n좋음; 좋음.\n\n재현 가능한 연구에 Quarto를 사용하는 이점은 무엇입니까 (하나 선택)?\n\n통계 분석을 자동화합니다.\n코드와 텍스트를 통합합니다.\n버전 관리의 필요성을 대체합니다.\n데이터 시각화 기능을 향상시킵니다.\n\nQuarto에서 최상위 제목은 어떻게 표시합니까 (하나 선택)?\n\n3.8.1 제목\n제목\n4 제목\n\n제목\n\n\n다음 중 Quarto에서 굵은 텍스트를 만드는 것은 무엇입니까 (하나 선택)?\n\n**굵게**\n##굵게##\n*굵게*\n#굵게#\n\nQuarto R 코드 청크에서 “echo” 옵션은 무엇을 합니까 (하나 선택)?\n\n코드 출력을 표시하지 않기 위해.\n코드가 문서에 표시되는지 여부를 제어하기 위해.\n코드를 조건부로 평가하기 위해.\n출력에 경고를 포함하기 위해.\n\nQuarto R 청크에서 경고를 숨기는 옵션은 무엇입니까 (하나 선택)?\n\necho: false\neval: false\nwarning: false\nmessage: false\n\nQuarto R 청크에서 R 코드 청크를 실행하고 결과를 표시하지만 코드는 표시하지 않는 옵션은 무엇입니까 (하나 선택)?\n\necho: false\ninclude: false\neval: false\nwarning: false\nmessage: false\n\nR 프로젝트가 중요한 이유는 무엇입니까 (모두 선택)?\n\n재현성에 도움이 됩니다.\n코드 공유를 더 쉽게 만듭니다.\n작업 공간을 더 체계적으로 만듭니다.\n\nR 프로젝트 이름이 리포지토리 내용을 반영하는 것이 중요한 이유는 무엇입니까 (모두 선택)?\n\n일관성.\n전문성.\n세부 사항에 대한 주의.\n\n패키지와 데이터셋이 로드되었다고 가정합니다. 이 코드의 실수는 무엇입니까: DoctorVisits |&gt; filter(visits) (하나 선택)?\n\nDoctorVisits\n|&gt;\nfilter\nvisits\n\nreprex란 무엇이며 하나를 만들 수 있는 것이 중요한 이유는 무엇입니까 (모두 선택)?\n\n오류를 재현할 수 있는 재현 가능한 예제.\n다른 사람이 당신을 돕는 데 도움이 되는 재현 가능한 예제.\n구성 중에 자신의 문제를 해결할 수 있는 재현 가능한 예제.\n실제로 자신을 돕기 위해 노력했음을 보여주는 재현 가능한 예제.\n\n(sharlatalks에?) 따르면 “막힌 곳에서 벗어나려면 첫 번째 단계는 reprex 또는 재현 가능한 예제를 만드는 것입니다. reprex의 목표는 문제가 있는 코드를 다른 사람이 실행하고 당신의 고통을 느낄 수 있도록 패키지하는 것입니다. 그런 다음 바라건대 그들이 해결책을 제공하고 당신을 고통에서 벗어나게 할 수 있습니다.”의 핵심 부분은 무엇입니까 (하나 선택)?\n\n문제가 있는 코드 패키지하기\n다른 사람이 실행하고 당신의 고통을 느낄 수 있도록\n첫 번째 단계는 reprex 만들기\n그들이 해결책을 제공하고 당신을 고통에서 벗어나게 할 수 있도록\n\n(sharlatalks에?) 따르면 도움을 요청할 때 재현 가능한 예제를 만드는 것이 중요한 이유는 무엇입니까 (하나 선택)?\n\n문서화의 필요성을 줄입니다.\n코딩 기술을 보여줍니다.\n다른 사람이 문제를 복제하고 해결책을 제공할 수 있도록 합니다.\n소프트웨어 라이선스를 준수합니다.\n\n다른 사람과 공동 작업할 때 코드 효율성을 향상시키는 관행은 무엇입니까 (하나 선택)?\n\n절대 파일 경로 사용.\n명확한 주석 및 문서 작성.\n함수 사용 최소화.\n지적 재산권을 보호하기 위해 코드 난독화.\n\n다음 중 버전 관리에 Git을 사용하는 이점은 무엇입니까 (모두 선택)?\n\n시간 경과에 따른 변경 사항 추적.\n여러 사용자 간의 공동 작업 촉진.\n데이터 백업 자동화.\n코드 실행 속도 향상.\n\nR 스크립트에서 setwd()를 사용하지 않는 이유는 무엇입니까 (하나 선택)?\n\n코드 실행 속도를 늦출 수 있습니다.\n관리자 권한이 필요합니다.\n코드를 덜 이식 가능하고 재현 가능하게 만듭니다.\n최근 R 버전에서는 더 이상 사용되지 않습니다.\n\n재현성의 맥락에서 renv 패키지의 기능은 무엇입니까 (하나 선택)?\n\n병렬로 코드 청크 실행.\n소프트웨어 환경 문서화 및 공유.\n코드 린팅 자동화.\n시뮬레이션에서 코드 효율성 향상.\n\n다음 중 재현 가능한 워크플로에 기여하지 않는 것은 무엇입니까 (하나 선택)?\n\n작업 디렉터리를 설정하기 위해 setwd() 사용.\n결과뿐만 아니라 코드와 데이터 공유.\n논문에서 R 및 Python 코드를 통합하기 위해 Quarto 사용.\nGit 및 GitHub를 사용한 버전 관리.\n\n(tidyversestyleguide에?) 따르면 다음 변수 이름 중 권장 스타일을 따르는 것은 무엇입니까 (하나 선택)?\n\ntotal-Sales\nTotalSales\ntotal_sales\ntotal sales\n\nR에서 lintr 패키지의 주요 기능은 무엇입니까 (하나 선택)?\n\n병렬로 코드 실행.\n패키지 종속성 설치.\n스타일 일관성을 위한 코드 린팅 제공.\n데이터 분포 시각화.\n\n코드 리팩토링이란 무엇입니까 (하나 선택)?\n\n오류를 수정하기 위해 코드 디버깅.\n동작을 변경하지 않고 구조를 개선하기 위해 코드 다시 작성.\n기존 코드에 새 기능 추가.\n한 언어에서 다른 언어로 코드 변환.\n\n코드에서 “매직 넘버”를 피해야 하는 이유는 무엇입니까 (하나 선택)?\n\n실행 속도를 늦춥니다.\n코드 가독성과 유지 관리성을 저하시킵니다.\n특정 소프트웨어와 호환되지 않습니다.\n구문 오류를 유발합니다.\n\n코드를 작성할 때 린터를 사용하는 주요 목적은 무엇입니까 (하나 선택)?\n\n알고리즘의 논리적 오류 찾기.\n코드를 더 빠르게 실행하기 위해.\n코딩 스타일 가이드라인 적용.\n코드를 기계어로 컴파일하기 위해.\n\n재현성의 맥락에서 “미래의 당신”이란 무엇을 의미합니까 (하나 선택)?\n\n자동 코드 생성.\n나중에 코드를 이해하고 재사용할 수 있는 능력.\n코드의 예측 분석.\n미래 동료와의 공동 작업.\n\n\n\n\n수업 활동\n\n스타터 폴더를 사용하고 새 리포지토리를 만듭니다. 수업의 공유 Google 문서에 GitHub 리포지토리 링크를 추가합니다.\nQuarto를 사용하여 제목, 작성자 및 초록이 있는 PDF를 만듭니다.1\npalmerpenguins::penguins에 대해 종별 평균 부리 길이를 생성하는 세 개의 섹션과 일부 코드를 추가합니다(코드 자체는 숨김).\nR 및 palmerpenguins에 대한 인용을 추가한 다음 성별에 따른 체질량 그래프를 추가합니다.\n그래프에 대한 텍스트 단락과 상호 참조를 추가합니다. 또한 연도별 종 수에 대한 표를 추가합니다.\n[강사는 이 모든 것을 (매우 느리게) 라이브 코딩하고 학생들이 따라 하도록 해야 합니다.] 로컬 컴퓨터에 git을 설정합니다.2 GitHub 리포지토리를 만들고 로컬 복사본을 만들고 일부 변경 사항을 적용하고 푸시합니다.3\n파트너의 GitHub 리포지토리를 찾아 포크하고 변경 사항을 적용하고 풀 리퀘스트를 만듭니다.\n다음 코드는 오류를 생성합니다. ?sec-dealingwitherrors의 전략에 따라 수정하십시오.\n\n\ntibble(year = 1875:1972,\n       level = as.numeric(datasets::LakeHuron)) |&gt;\n  ggplot(aes(x = year, y = level)) |&gt;\n  geom_point()\n\n\n다음 코드는 오류를 생성합니다. ?sec-dealingwitherrors의 전략에 따라 수정하십시오.\n\n\ntibble(year = 1871:1970,\n       annual_nile_flow = as.character(datasets::Nile)) |&gt;\n  ggplot(aes(x = annual_nile_flow)) +\n  geom_histogram()\n\n\n다음 코드는 오류를 생성합니다. ?sec-omgpleasemakeareprexplease에 따라 reprex를 만들고(mtcars와 같은 더 일반적인 데이터셋을 사용하도록 예제를 변경), GitHub Gist에 추가하고 강사에게 이메일로 보내십시오.\n\n\ntibble(year = 1875:1972,\n       level = as.numeric(datasets::LakeHuron)) |&gt;\n  ggplot(aes(x = year, y = level)) |&gt;\n  geom_point()\n\n\n다음 코드는 오류를 생성합니다. ChatGPT 또는 동등한 LLM을 사용하여 수정하십시오. 다음을 논의하십시오. 1) 프롬프트, 2) 수정된 코드.\n\n\npenguins |&gt;\n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm, color = species)) |&gt;\n  geom_point()\n\n\n\n과제 I\n이 과제의 목적은 동료 검토를 주고받는 것입니다. 일반적으로 동료 검토, 특히 코드 검토(Sadowski 기타 2018)는 전문가로서 일하는 데 중요한 부분입니다.\n먼저 usethis::git_vaccinate()를 실행하십시오. 그런 다음 ?sec-fire-hose의 활동에서 스타터 폴더를 사용하도록 작업을 업데이트하십시오. 여기에는 무엇보다도 다운로드 및 정리를 적절한 스크립트로 이동하고 README를 업데이트하고 제목을 추가하는 등의 작업이 포함됩니다. 일반적으로 ?sec-papers의 Donaldson 논문에 대한 채점 기준을 살펴보고 너무 많은 추가 작업을 하지 않고 가능한 한 많이 준수하도록 노력해야 합니다. 그런 다음 다른 사람과 교환하십시오.\nGoogle (2022) 및 (giladpeerreview를?) 읽으십시오. 그런 다음 GitHub Issues를 사용하여 리포지토리 내용에 대한 동료 검토를 수행하십시오. (giladpeerreview에?) 따라 동료 검토는 다음 구조를 사용해야 하며 보기 좋게 형식을 지정해야 합니다.\n\n요약 [검토 중인 원고에 대한 간략한 요약을 추가하십시오.]\n강력한 긍정적 측면: [간단하게 유지하십시오. 두세 개의 항목입니다.]\n필요한 중요한 개선 사항: [이것이 가장 중요한 섹션입니다. 논문 저자가 수정 및/또는 해결해야 하는 문제입니다. 매우 건설적이고 정중하며 부드럽지만 명확하게 설명하고 저자를 돕기 위해 가능한 한 많은 정보를 제공하십시오. 여기에는 실수/오류, 누락된 정보, 간과, 오해 등이 포함될 수 있습니다. 가능하다면 이것이 왜 실수인지 설명하고 수정 사항이나 올바른 정보를 찾을 수 있는 링크를 제공하십시오.]\n개선 제안: [이것은 저자가 더 잘하도록 돕기 위한 것입니다. 있으면 좋은 것입니다. 확실하지 않은 사항에 대해 언급하거나 의견을 제시하거나 오타 또는 사소한 코드 문제를 지적할 수 있지만 이것이 제안이라는 점에 대해 겸손하고 매우 긍정적이고 건설적이어야 합니다. 약 5~6개의 항목이 있어야 합니다.]\n평가: [채점 기준의 각 요소를 추가하고 이에 대한 의견과 점수를 제공하십시오. 이것은 채점에 사용되지 않으며 저자에게 각 요소를 개선하기 위해 얼마나 많은 작업을 수행해야 하는지에 대한 아이디어를 제공하기 위한 것입니다.]\n예상 총점: [Y] 점 만점에 [X] 점.\n기타 의견: [기타 의견.]\n\n\n\n과제 II\n이 과제의 목적은 다음에 대한 편안함을 개발하는 것입니다.\n\n콰르토, 그리고\nGit 및 GitHub.\n\n웹사이트는 커뮤니케이션의 중요한 부분입니다. 예를 들어, 작업 포트폴리오를 공개적으로 제공할 수 있는 장소입니다. 웹사이트를 만드는 한 가지 방법은 Quarto의 내장 웹사이트를 사용하는 것입니다. RStudio에서 GitHub를 설정하면 약 5분 안에 웹사이트를 온라인에 게시할 수 있습니다.\n새 프로젝트를 만들어 시작하십시오(“파일” -&gt; “새 프로젝트” -&gt; “새 디렉터리” -&gt; “Quarto 웹사이트”). 이름을 지정하고 “새 세션에서 열기” -&gt; “프로젝트 만들기”를 선택하십시오(그림 4.1 (a)).\n\n\n\n\n\n\n\n\n\n\n\n(a) Quarto 웹사이트 설정 예시\n\n\n\n\n\n\n\n\n\n\n\n(b) Quarto 웹사이트 빌드\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Quarto 웹사이트 개인화\n\n\n\n\n\n\n\n\n\n\n\n(d) Quarto yaml 업데이트\n\n\n\n\n\n\n\n그림 4.1: Quarto를 사용하여 웹사이트 만들기\n\n\n\n기본 기본 웹사이트는 “빌드” -&gt; “웹사이트 렌더링”으로 생성할 수 있습니다(그림 4.1 (b)). 기본적으로 “뷰어” 창에 표시될 수 있지만 새 창에서도 표시할 수 있습니다. 다시 말하지만, 이 시점에서 세부 정보를 자신에게 맞게 변경하고 싶을 수 있습니다. 특히 “index.qmd”의 제목을 변경하고 자신의 세부 정보를 추가하고 싶을 수 있습니다(그림 4.1 (c)).\n기본 메뉴에 포함된 내용은 “_quarto.yml”에 지정됩니다. 여기에 “contact.qmd”와 같은 다른 페이지를 추가할 수 있으며 포함될 내용을 만들려면 “about.qmd”와 같은 페이지를 복제한 다음 편집할 수 있습니다(그림 4.1 (d)). “_quarto.yml”에서 변경할 수 있는 또 다른 측면은 테마입니다. 기본값은 “cosmo”이지만 여기에 지정된 다른 많은 옵션이 있습니다.\n세부 정보가 개인화되고 웹사이트에 불만이 없으면 GitHub로 푸시한 다음 GitHub Pages로 호스팅할 수 있습니다. 이를 활용하려면 먼저 두 가지 작업을 수행해야 합니다. 첫째, “_site” 대신 “docs” 폴더로 빌드하도록 지정하기 위해 “_quarto.yml”을 약간 수정해야 합니다(그림 4.1 (d)).\n\nproject:\n  type: website\n  output-dir: docs\n\n알아야 할 다른 측면은 이 서비스를 사용할 때 기본적으로 GitHub가 사이트를 빌드하려고 시도한다는 것입니다. 이는 원치 않는 것이므로 먼저 숨겨진 파일을 추가하여 이를 해제해야 합니다. 콘솔에서 다음을 실행합니다.\n\nfile.create(\".nojekyll\")\n\n그런 다음 GitHub가 설정되었다고 가정하면 usethis를 사용하여 새로 만든 프로젝트를 GitHub에 올릴 수 있습니다. use_git()를 사용하여 Git 리포지토리를 초기화한 다음 use_github()를 사용하여 GitHub로 푸시합니다.\n\nuse_git()\nuse_github()\n\n그러면 프로젝트가 GitHub에 있게 됩니다. GitHub 페이지를 사용하여 호스팅할 수 있습니다. “설정 -&gt; 페이지”로 이동한 다음 소스를 설정에 따라 “main” 또는 “master”로 변경하고 마지막으로 “docs”로 변경합니다. 몇 분 동안 다양한 검사를 거친 후 GitHub에서 사이트를 방문하여 공유할 수 있는 주소를 알려줍니다.\n사이트를 업데이트하려면 로컬에서 작업하십시오. 먼저 GitHub에서 변경한 내용이 로컬에 있는지 확인하기 위해 풀하고, 사이트를 편집하고, 다시 렌더링하고, 일반적인 방식으로 GitHub에 푸시합니다. 검사가 완료되면 라이브 웹사이트가 업데이트됩니다.\n웹사이트에 일반 텍스트 단락, 섹션 제목, 글머리 기호가 있는지 확인하십시오. 모든 것이 잘 문서화되고 보기 좋게 형식이 지정되어 있으며 일반적으로 고품질이어야 합니다.\n채점 기준의 관련 구성 요소는 “수업 논문”, “LLM 사용량 문서화됨”, “산문”, “커밋” 및 “재현 가능한 워크플로”입니다. 웹사이트 링크를 제출하십시오.\n\n\n논문\n이 시점에서 ?sec-papers의 도널드슨 논문이 적절할 것입니다.\n\n\n\n\nAlexander, Monica. 2021. “Overcoming barriers to sharing code”. YouTube, 2월. https://youtu.be/yvM2C6aZ94k.\n\n\nArel-Bundock, Vincent. 2024. tinytable: Simple and Configurable Tables in “HTML”, “LaTeX”, “Markdown”, “Word”, “PNG”, “PDF”, and “Typst” Formats. https://vincentarelbundock.github.io/tinytable/.\n\n\nBackus, John. 1981. “The History of FORTRAN I, II, and III”. In History of Programming Languages, 편집자： Richard Wexelblat, 25–74. Academic Press.\n\n\nBegley, Glenn, 와/과 Lee Ellis. 2012. “Raise standards for preclinical cancer research”. Nature 483 (7391): 531--533. https://doi.org/10.1038/483531a.\n\n\nBengtsson, Henrik. 2021. “A Unifying Framework for Parallel and Distributed Processing in R using Futures”. The R Journal 13 (2): 208–27. https://doi.org/10.32614/RJ-2021-048.\n\n\nBowers, Jake, 와/과 Maarten Voors. 2016. “How to improve your relationship with your future self”. Revista de Ciencia Polı́tica 36 (3): 829–48. https://doi.org/10.4067/S0718-090X2016000300011.\n\n\nBrown, Zack. 2018. “A Git Origin Story”. Linux Journal, 7월. https://www.linuxjournal.com/content/git-origin-story.\n\n\nBryan, Jenny. 2018a. “Excuse Me, Do You Have a Moment to Talk About Version Control?” The American Statistician 72 (1): 20–27. https://doi.org/10.1080/00031305.2017.1399928.\n\n\n———. 2018b. “Code smells and feels”. YouTube, 7월. https://youtu.be/7oyiPBjLAWY.\n\n\n———. 2020. Happy Git and GitHub for the useR. https://happygitwithr.com.\n\n\nBryan, Jenny, 와/과 Jim Hester. 2020. What They Forgot to Teach You About R. https://rstats.wtf/index.html.\n\n\nBryan, Jenny, Jim Hester, David Robinson, Hadley Wickham, 와/과 Christophe Dervieux. 2022. reprex: Prepare Reproducible Example Code via the Clipboard. https://CRAN.R-project.org/package=reprex.\n\n\nChouldechova, Alexandra, Diana Benavides-Prado, Oleksandr Fialko, 와/과 Rhema Vaithianathan. 2018. “A case study of algorithm-assisted decision making in child maltreatment hotline screening decisions”. In Proceedings of the 1st Conference on Fairness, Accountability and Transparency, 편집자： Sorelle Friedler 와/과 Christo Wilson, 81:134–48. Proceedings of Machine Learning Research. https://proceedings.mlr.press/v81/chouldechova18a.html.\n\n\nCohen, Jason, Steven Teleki, 와/과 Eric Brown. 2006. Best kept secrets of peer code review. Smart Bear Incorporated.\n\n\nCsárdi, Gábor. 2022. gitcreds: Query “git” Credentials from “R”. https://CRAN.R-project.org/package=gitcreds.\n\n\nDolatsara, Hamidreza Ahady, Ying-Ju Chen, Robert Leonard, Fadel Megahed, 와/과 Allison Jones-Farmer. 2021. “Explaining Predictive Model Performance: An Experimental Study of Data Preparation and Model Choice”. Big Data, 10월. https://doi.org/10.1089/big.2021.0067.\n\n\nEghbal, Nadia. 2020. Working in public: the making and maintenance of open source software. California: Stripe Press.\n\n\nFowler, Martin, 와/과 Kent Beck. 2018. Refactoring: Improving the Design of Existing Code. 2nd ed. New York: Addison-Wesley Professional.\n\n\nGelfand, Sharla. 2021. “Make a ReprEx... please”. YouTube, 2월. https://youtu.be/G5Nm-GpmrLw.\n\n\nGelman, Andrew. 2016. “What has happened down here is the winds have changed”, 9월. https://statmodeling.stat.columbia.edu/2016/09/21/what-has-happened-down-here-is-the-winds-have-changed/.\n\n\nGoogle. 2022. “What to look for in a code review”. Google Engineering Practices Documentation. https://google.github.io/eng-practices/review/reviewer/looking-for.html.\n\n\nHeil, Benjamin, Michael Hoffman, Florian Markowetz, Su-In Lee, Casey Greene, 와/과 Stephanie Hicks. 2021. “Reproducibility standards for machine learning in the life sciences”. Nature Methods 18 (10): 1132–35. https://doi.org/10.1038/s41592-021-01256-7.\n\n\nHester, Jim, Florent Angly, Russ Hyde, Michael Chirico, Kun Ren, Alexander Rosenstock, 와/과 Indrajeet Patil. 2022. lintr: A “Linter” for R Code. https://CRAN.R-project.org/package=lintr.\n\n\nHillel, Wayne. 2017. How Do We Trust Our Science Code? https://www.hillelwayne.com/how-do-we-trust-science-code/.\n\n\nIrving, Damien, Kate Hertweck, Luke Johnston, Joel Ostblom, Charlotte Wickham, 와/과 Greg Wilson. 2021. Research Software Engineering with Python. Chapman; Hall/CRC.\n\n\nIsaacson, Walter. 2011. Steve Jobs. 1st ed. Simon & Schuster.\n\n\nKleiber, Christian, 와/과 Achim Zeileis. 2008. Applied Econometrics with R. New York: Springer-Verlag. https://CRAN.R-project.org/package=AER.\n\n\nKnuth, Donald. 1984. “Literate Programming”. The Computer Journal 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.\n\n\nLandau, William Michael. 2021. “The targets R Package: A Dynamic Make-Like Function-Oriented Pipeline Toolkit for Reproducibility and High-Performance Computing”. Journal of Open Source Software 6 (57): 2959. https://doi.org/10.21105/joss.02959.\n\n\nMaier, Maximilian, František Bartoš, Tom Stanley, David Shanks, Adam Harris, 와/과 Eric-Jan Wagenmakers. 2022. “No evidence for nudging after adjusting for publication bias”. Proceedings of the National Academy of Sciences 119 (31): e2200300119. https://doi.org/10.1073/pnas.2200300119.\n\n\nMatsumoto, Yukihiro. 2007. “Treating Code as an Essay”. In Beautiful Code, 편집자： Andy Oram 와/과 Greg Wilson, 477–81. O’Reilly.\n\n\nMattson, Greggor. 2017. “Artificial Intelligence Discovers Gayface. Sigh.” https://greggormattson.com/2017/09/09/artificial-intelligence-discovers-gayface/amp/.\n\n\nMerali, Zeeya. 2010. “Computational science:... error.” Nature 467 (7317): 775–77. https://doi.org/10.1038/467775a.\n\n\nMinsky, Yaron. 2011. “OCaml for the masses”. Communications of the ACM 54 (11): 53–58. https://doi.org/10.1145/2018396.2018413.\n\n\n———. 2015. “Automated Trading and OCaml with Yaron Minsky”. Hackers — Software Engineering Daily, 11월. https://softwareengineeringdaily.com/2015/11/09/automated-trading-and-ocaml-with-yaron-minsky/.\n\n\nMiyakawa, Tsuyoshi. 2020. “No raw data, no science: another possible source of the reproducibility crisis”. Molecular brain 13 (1): 1–6. https://doi.org/10.1186/s13041-020-0552-2.\n\n\nMullard, Asher. 2021. “Half of top cancer studies fail high-profile reproducibility effort”. Nature 600 (7889): 368--369. https://doi.org/10.1038/d41586-021-03691-0.\n\n\nMüller, Kirill, 와/과 Lorenz Walthert. 2022. styler: Non-Invasive Pretty Printing of R Code. https://CRAN.R-project.org/package=styler.\n\n\nNational Academies of Sciences, Engineering, and Medicine. 2019. Reproducibility and Replicability in Science. 1st ed. National Academies Press. https://doi.org/10.17226/25303.\n\n\nPerkel, Jeffrey. 2023. “The sleight-of-hand trick that can simplify scientific computing”. Nature 617 (7959): 212--213. https://doi.org/10.1038/d41586-023-01469-0.\n\n\nPineau, Joelle, Philippe Vincent-Lamarre, Koustuv Sinha, Vincent Larivière, Alina Beygelzimer, Florence d’Alché-Buc, Emily Fox, 와/과 Hugo Larochelle. 2021. “Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program)”. Journal of Machine Learning Research 22 (164): 1–20. http://jmlr.org/papers/v22/20-303.html.\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nRilke, Rainer Maria. (1929년) 2014. Letters to a Young Poet. Penguin Classics.\n\n\nSadowski, Caitlin, Emma Söderberg, Luke Church, Michal Sipko, 와/과 Alberto Bacchelli. 2018. “Modern Code Review: A Case Study at Google”. In Proceedings of the 40th International Conference on Software Engineering: Software Engineering in Practice, 181–90. ICSE-SEIP ’18. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3183519.3183525.\n\n\nSilver, Nate. 2020. “We Fixed An Issue With How Our Primary Forecast Was Calculating Candidates’ Demographic Strengths”. FiveThirtyEight, 2월. https://fivethirtyeight.com/features/we-fixed-a-mistake-in-how-our-primary-forecast-was-calculating-candidates-demographic-strengths/.\n\n\nSimpkinson, Scott. 1971. “Testing to Ensure Mission Success”. In What Made Apollo A Success, 편집자： NASA, 21–29.\n\n\nSprint, Gina, 와/과 Jason Conci. 2019. “Mining GitHub classroom commit behavior in elective and introductory computer science courses”. Journal of Computing Sciences in Colleges 35 (1): 76–84.\n\n\nSunstein, Cass, 와/과 Lucia Reisch. 2017. The economics of nudge. Routledge.\n\n\nSzaszi, Barnabas, Anthony Higney, Aaron Charlton, Andrew Gelman, Ignazio Ziano, Balazs Aczel, Daniel Goldstein, David Yeager, 와/과 Elizabeth Tipton. 2022. “No reason to expect large and consistent effects of nudge interventions”. Proceedings of the National Academy of Sciences 119 (31): e2200732119. https://doi.org/10.1073/pnas.2200732119.\n\n\nUshey, Kevin. 2022. renv: Project Environments. https://CRAN.R-project.org/package=renv.\n\n\nVidoni, Melina. 2021. “Evaluating Unit Testing Practices in R Packages”. In 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE), 1523–34. https://doi.org/10.1109/ICSE43902.2021.00136.\n\n\nWang, Yilun, 와/과 Michal Kosinski. 2018. “Deep neural networks are more accurate than humans at detecting sexual orientation from facial images.” Journal of Personality and Social Psychology 114 (2): 246–57. https://doi.org/10.1037/pspa0000098.\n\n\nWickham, Hadley. 2021. The tidyverse style guide. https://style.tidyverse.org/index.html.\n\n\nWickham, Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, 기타. 2019. “Welcome to the Tidyverse”. Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Jennifer Bryan, 와/과 Malcolm Barrett. 2022. usethis: Automate Package and Project Setup. https://CRAN.R-project.org/package=usethis.\n\n\nWilson, Greg, Jenny Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt, 와/과 Tracy Teal. 2017. “Good enough practices in scientific computing”. PLOS Computational Biology 13 (6): 1–20. https://doi.org/10.1371/journal.pcbi.1005510.\n\n\nXie, Yihui. 2019. “TinyTeX: A lightweight, cross-platform, and easy-to-maintain LaTeX distribution based on TeX Live”. TUGboat, 호 1: 30–32. https://tug.org/TUGboat/Contents/contents40-1.html.",
    "crumbs": [
      "기초",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>재현 가능한 워크플로</span>"
    ]
  },
  {
    "objectID": "03-workflow_ko.html#footnotes",
    "href": "03-workflow_ko.html#footnotes",
    "title": "3  재현 가능한 워크플로",
    "section": "",
    "text": "항상 소수의 학생들이 로컬에서 PDF를 설정하는 데 어려움을 겪습니다. 최악의 경우 다른 모든 작업을 로컬에서 html로 수행한 다음 Posit Cloud에서 PDF를 빌드합니다.↩︎\nGitHub 이메일을 숨긴 경우 로컬에서 이메일 주소를 추가할 때 별칭을 사용해야 합니다.↩︎\n항상 몇몇 학생들은 로컬에서 git을 작동시키지 못합니다. 가장 좋은 방법은 시연하는 동안 고급 학생과 짝을 지어 분류하고 남은 문제가 있으면 사무실 시간에 개별적으로 처리하는 것입니다.↩︎",
    "crumbs": [
      "기초",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>재현 가능한 워크플로</span>"
    ]
  },
  {
    "objectID": "04-writing_research_ko.html",
    "href": "04-writing_research_ko.html",
    "title": "4  연구 논문 작성",
    "section": "",
    "text": "4.1 소개\n선행 학습\n주요 개념 및 기술\n소프트웨어 및 패키지\n우리는 주로 데이터를 글로 써서 이야기를 전달합니다. 글쓰기는 효율적으로 의사소통할 수 있게 해줍니다. 또한 우리가 무엇을 믿는지 알아내는 방법이며 아이디어에 대한 피드백을 받을 수 있게 해줍니다. 효과적인 논문은 간결하게 작성되고 잘 구성되어 있어 이야기의 흐름이 좋습니다. 적절한 문장 구조, 철자법, 어휘 및 문법은 주의를 산만하게 하는 요소를 제거하고 이야기의 각 측면을 명확하게 표현할 수 있도록 하기 때문에 중요합니다.\n이 장은 글쓰기에 관한 것입니다. 이 장이 끝날 때쯤이면 원하는 내용을 전달하고 독자의 시간을 낭비하지 않는 짧고 상세하며 정량적인 논문을 작성하는 방법에 대해 더 잘 알게 될 것입니다. 우리는 자신을 위해서가 아니라 독자를 위해 글을 씁니다. 구체적으로, 우리는 독자에게 유용하도록 글을 씁니다. 이는 새롭고 진실하며 중요한 것을 명확하게 전달하는 것을 의미합니다(Graham 2020). 그렇긴 하지만, 청중을 위해 글을 쓸 때에도 글쓰기의 가장 큰 이점은 종종 작가에게 돌아갑니다. 이는 글쓰기 과정이 우리가 생각하는 것과 그것을 믿게 된 방법을 알아내는 방법이기 때문입니다.\n이 장의 측면은 약간 목록처럼 느껴질 수 있습니다. 처음에는 해당 측면을 빠르게 살펴본 다음 필요에 따라 다시 돌아올 수 있습니다.",
    "crumbs": [
      "소통",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>연구 논문 작성</span>"
    ]
  },
  {
    "objectID": "04-writing_research_ko.html#소개",
    "href": "04-writing_research_ko.html#소개",
    "title": "4  연구 논문 작성",
    "section": "",
    "text": "작가가 되고 싶다면 다른 무엇보다 두 가지를 해야 합니다. 많이 읽고 많이 쓰는 것입니다. 제가 아는 한 이 두 가지를 피할 방법은 없으며 지름길도 없습니다.\nS. King (2000, p. 145)",
    "crumbs": [
      "소통",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>연구 논문 작성</span>"
    ]
  },
  {
    "objectID": "04-writing_research_ko.html#글쓰기",
    "href": "04-writing_research_ko.html#글쓰기",
    "title": "4  연구 논문 작성",
    "section": "4.2 글쓰기",
    "text": "4.2 글쓰기\n\n글을 쓰는 방법은 한 번이 아니라 세 번 또는 네 번에 걸쳐 하는 것입니다. 저에게 가장 어려운 부분은 처음에 무언가를, 무엇이든, 제 앞에 내놓는 것입니다. 때로는 초조한 마음에 마치 벽에 진흙을 던지듯 단어를 마구 던집니다. 무엇이든, 무엇이든, 초고로 내뱉고, 토해내고, 지껄입니다.\nMcPhee (2017, p. 159)\n\n글쓰기 과정은 다시 쓰기 과정입니다. 중요한 작업은 가능한 한 빨리 초고를 완성하는 것입니다. 완전한 초고가 존재할 때까지는 아무리 나빠 보이더라도 작성된 내용을 삭제하거나 수정하지 않도록 노력하는 것이 유용합니다. 그냥 쓰십시오. (이 조언은 경험이 적은 작가를 대상으로 합니다. 경험이 많아지면 접근 방식이 바뀔 수 있습니다.)\n가장 위협적인 단계 중 하나는 빈 페이지이며, “서론”, “데이터”, “모델”, “결과”, “토론”과 같은 제목을 즉시 추가하여 이를 처리합니다. 그런 다음 “제목”, “날짜”, “저자”, “초록”과 같이 필요한 다양한 조각에 대한 필드를 상단 내용에 추가합니다. 이렇게 하면 논문의 미장 플라스 역할을 할 일반적인 개요가 만들어집니다. 배경 지식으로, 미장 플라스는 전문 주방에서 재료를 분류, 준비 및 쉽게 접근할 수 있도록 배열하는 준비 단계입니다. 이렇게 하면 필요한 모든 것을 불필요한 지연 없이 사용할 수 있습니다. 개요를 작성하는 것은 정량적 논문을 작성할 때 동일한 역할을 하며, 저녁 식사를 준비하는 데 사용할 재료를 조리대에 놓는 것과 유사합니다(McPhee 2017).\n이 일반적인 개요를 설정한 후에는 연구 질문에 대해 깊이 생각하여 탐구하는 내용을 이해해야 합니다. 이론적으로는 연구 질문을 개발하고 답을 찾은 다음 모든 글쓰기를 하지만 실제로는 거의 그렇지 않습니다(Franklin 2005). 대신 일반적으로 질문과 답의 형태에 대한 어떤 아이디어가 있으며 글을 쓰면서 이러한 아이디어가 덜 모호해집니다. 이는 글쓰기 과정을 통해 생각을 다듬기 때문입니다(S. King 2000, p. 131). 연구 질문에 대한 몇 가지 생각을 적어둔 후에는 각 섹션에 점을 추가하고 필요에 따라 정보성 하위 제목이 있는 하위 섹션을 추가할 수 있습니다. 그런 다음 해당 점을 단락으로 확장합니다. 이렇게 하는 동안 우리의 생각은 다른 연구자들의 웹뿐만 아니라 상황과 환경과 같은 다른 측면에도 영향을 받습니다(Latour 1996).\n초고를 쓰는 동안 자신이 충분하지 않거나 불가능하다는 느낌을 무시해야 합니다. 그냥 쓰십시오. 나쁜 글이라도 종이에 단어가 필요하며 초고는 이것을 달성하는 때입니다. 주의를 산만하게 하는 것을 제거하고 글쓰기에 집중하십시오. 완벽주의는 적이며 제쳐두어야 합니다. 때로는 매우 일찍 일어나 글을 쓰거나 마감일을 만들거나 글쓰기 그룹을 구성하여 이를 달성할 수 있습니다. 긴급감을 조성하는 것이 유용할 수 있으며 한 가지 옵션은 진행하면서 적절한 인용을 추가하는 데 신경 쓰지 않고 대신 “[TODO: 여기에 R 인용]”과 같은 것을 추가하는 것입니다. 그래프와 표도 마찬가지입니다. 즉, 실제 그래프와 표 대신 “[TODO: 여기에 시간 경과에 따른 각 국가를 보여주는 그래프 추가]”와 같은 텍스트 설명을 포함합니다. 나쁜 내용이라도 내용을 추가하는 데 집중하십시오. 이 모든 작업이 끝나면 초고가 존재합니다.\n이 초고는 제대로 쓰여지지 않았고 훌륭함과는 거리가 멀 것입니다. 그러나 나쁜 초고를 써야 좋은 두 번째 초고, 훌륭한 세 번째 초고, 그리고 결국 탁월함에 도달할 수 있습니다(Lamott 1994, p. 20). 그 초고는 너무 길고, 말이 안 되고, 뒷받침할 수 없는 주장과 해서는 안 되는 주장을 포함할 것입니다. 초고에 부끄러워하지 않는다면 충분히 빨리 쓰지 않은 것입니다.\n“삭제” 키와 “잘라내기” 및 “붙여넣기”를 광범위하게 사용하여 해당 초고를 두 번째 초고로 만드십시오. 초고를 인쇄하고 빨간 펜을 사용하여 단어, 문장 및 전체 단락을 이동하거나 제거하는 것이 특히 도움이 됩니다. 초고에서 두 번째 초고로 넘어가는 과정은 이야기의 흐름과 일관성을 돕기 위해 한 번에 수행하는 것이 가장 좋습니다. 이 첫 번째 다시 쓰기의 한 가지 측면은 우리가 하고 싶은 이야기를 향상시키는 것입니다. 또 다른 측면은 이야기가 아닌 모든 것을 빼내는 것입니다(S. King 2000, p. 57).\n초고가 되어가는 내용에 잘 맞지 않더라도 좋아 보이는 작업을 제거하는 것은 고통스러울 수 있습니다. 이것을 덜 고통스럽게 만드는 한 가지 방법은 임시 문서(아마도 “debris.qmd”라는 이름)를 만들어 원치 않는 단락을 즉시 삭제하는 대신 저장하는 것입니다. 또 다른 전략은 단락을 주석 처리하는 것입니다. 그렇게 하면 여전히 원시 파일을 보고 유용할 수 있는 측면을 알아차릴 수 있습니다.\n각 섹션에 작성된 내용을 살펴보면서 발전하는 이야기를 어떻게 뒷받침하는지 특별히 고려하여 의미를 부여하려고 노력하십시오. 이 수정 과정은 글쓰기의 본질입니다(McPhee 2017, p. 160). 또한 참고 문헌을 수정하고 실제 그래프와 표를 추가해야 합니다. 이 다시 쓰기 과정의 일부로 논문의 핵심 메시지가 발전하고 연구 질문에 대한 답이 더 명확해지는 경향이 있습니다. 이 시점에서 서론과 같은 측면으로 돌아가 마침내 초록을 작성할 수 있습니다. 오타 및 기타 문제는 작업의 신뢰성에 영향을 미칩니다. 따라서 두 번째 초안의 일부로 수정해야 합니다.\n이 시점에서 초고는 타당해지기 시작합니다. 이제 해야 할 일은 그것을 훌륭하게 만드는 것입니다. 인쇄하여 다시 종이로 살펴보십시오. 이야기에 기여하지 않는 모든 것을 제거하려고 노력하십시오. 이 단계쯤 되면 논문에 너무 가까워지기 시작할 수 있습니다. 이것은 다른 사람에게 의견을 구하기에 좋은 기회입니다. 이야기의 약점에 대한 피드백을 요청하십시오. 이를 해결한 후에는 이번에는 소리 내어 읽으면서 논문을 한 번 더 살펴보는 것이 도움이 될 수 있습니다. 논문은 결코 “완성”되지 않으며 어느 시점에서 시간이 부족하거나 보기 싫어지는 것 이상입니다.",
    "crumbs": [
      "소통",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>연구 논문 작성</span>"
    ]
  },
  {
    "objectID": "04-writing_research_ko.html#질문하기",
    "href": "04-writing_research_ko.html#질문하기",
    "title": "4  연구 논문 작성",
    "section": "4.3 질문하기",
    "text": "4.3 질문하기\n질적 접근 방식과 양적 접근 방식 모두 제자리를 가지고 있습니다. 이 책에서는 양적 접근 방식에 중점을 둡니다. 그럼에도 불구하고 질적 연구는 중요하며 종종 가장 흥미로운 작업에는 두 가지 모두 약간 포함됩니다. 양적 분석을 수행할 때 데이터 품질, 측정 및 관련성과 같은 문제에 직면합니다. 우리는 종종 인과 관계를 밝히려고 특히 노력합니다. 어쨌든 우리는 세상에 대해 무언가를 배우려고 노력하고 있습니다. 우리의 연구 질문은 이 모든 것을 고려해야 합니다.\n광범위하게 그리고 지나치게 단순화할 위험을 무릅쓰고 연구를 수행하는 방법에는 두 가지가 있습니다.\n\n데이터 우선; 또는\n질문 우선.\n\n그러나 이것은 이분법적인 것이 아니며 종종 연구는 연구 퍼즐을 중심으로 데이터와 질문 사이를 반복함으로써 진행됩니다(Gustafsson 와/과 Hagström 2017). Light, Singer, 와/과 Willett (1990, p. 39)는 이 접근 방식을 \\(\\mbox{이론}\\rightarrow\\mbox{데이터}\\rightarrow\\mbox{이론}\\rightarrow\\mbox{데이터}\\) 등의 나선형으로 설명합니다. 예를 들어, 질문 우선 접근 방식은 이론 중심적이거나 데이터 중심적일 수 있으며 데이터 우선 접근 방식도 마찬가지입니다. 대안적인 프레임은 귀납적 또는 특정-일반 접근 방식과 연역적 또는 일반-특정 접근 방식을 연구에 비교하는 것입니다.\n두 가지 예를 고려해 보십시오.\n\n(ahstonanderson은?) 사용자가 콘텐츠를 탐색하는 방식을 이해하기 위해 Spotify 사용자 100,000명의 고유 청취 이벤트 80억 건을 조사합니다. 그들은 연령과 행동 사이에 명확한 관계를 발견하는데, 젊은 사용자는 소비가 더 다양함에도 불구하고 나이든 사용자보다 알려지지 않은 콘텐츠를 덜 탐색합니다. 발견 및 탐색에 대한 연구 질문이 이 논문을 이끌고 있다는 것은 분명하지만, 이 데이터셋에 대한 액세스 없이는 불가능했을 것입니다. 궁극적인 일치가 있기 전에 잠재적인 연구 질문과 잠재적인 데이터셋이 고려되는 반복적인 프로세스가 있었을 가능성이 높습니다.\n?sec-fire-hose에서 소개된 신생아 사망률(NMR)을 탐색하고 싶다고 생각해 보십시오. 20년 후 사하라 사막 이남 아프리카에서 NMR이 어떻게 보일지에 관심이 있을 수 있습니다. 이것은 질문 우선이 될 것입니다. 그러나 이 안에는 다른 양과의 생물학적 관계를 기반으로 무엇을 기대하는지와 같은 이론 중심적인 측면이나 예측을 하기 위해 가능한 한 많은 데이터를 수집하는 것과 같은 데이터 중심적인 측면이 있을 수 있습니다. 대안적인 순전히 데이터 중심적인 접근 방식은 NMR에 액세스한 다음 가능한 것을 알아내는 것입니다.\n\n\n4.3.1 데이터 우선\n데이터 우선일 때 주요 문제는 사용 가능한 데이터로 합리적으로 답할 수 있는 질문을 파악하는 것입니다. 이것이 무엇인지 결정할 때 다음을 고려하는 것이 유용합니다.\n\n이론: 인과 관계를 파악할 수 있다는 합리적인 기대가 있습니까? 예를 들어, 마크 크리스텐슨은 주식 시장을 예측하는 질문이라면 오디세이로 돌아가 불 위에 황소 내장을 읽는 것이 더 나을 것이라고 농담하곤 했습니다. 왜냐하면 적어도 그렇게 하면 하루가 끝날 때 먹을 것이 있을 것이기 때문입니다. 질문은 일반적으로 허위 관계를 피하는 데 도움이 되는 그럴듯한 이론적 토대가 필요합니다. 데이터를 고려하여 이론을 개발하는 한 가지 방법은 “이것은 무엇의 예인가?”를 고려하는 것입니다(Rosenau 1999, p. 7). 이 접근 방식을 따르면 특정 상황을 넘어 일반화하려고 합니다. 예를 들어, 특정 내전을 모든 내전의 예로 생각하는 것입니다. 이것의 이점은 이론 구축에 필요한 일반적인 속성에 주의를 집중시킨다는 것입니다.\n중요성: 답할 수 있는 사소한 질문이 많이 있지만 우리 시간이나 독자의 시간을 낭비하지 않는 것이 중요합니다. 중요한 질문을 갖는 것은 예를 들어 4주 연속 데이터를 정리하고 코드를 디버깅하는 자신을 발견했을 때 동기 부여에 도움이 될 수도 있습니다. 업계에서는 재능 있는 직원과 자금을 유치하는 것이 더 쉬울 수도 있습니다. 그렇긴 하지만 균형이 필요합니다. 질문은 답을 얻을 수 있는 괜찮은 기회가 있어야 합니다. 세대를 정의하는 질문에 도전하는 것은 여러 부분으로 나누는 것이 가장 좋을 수 있습니다.\n가용성: 미래에 추가 데이터를 사용할 수 있다는 합리적인 기대가 있습니까? 이를 통해 관련 질문에 답하고 하나의 논문을 연구 의제로 전환할 수 있습니다.\n반복: 이것은 여러 번 실행할 수 있는 것입니까, 아니면 일회성 분석입니까? 전자라면 특정 연구 질문에 답한 다음 반복할 수 있습니다. 그러나 데이터에 한 번만 액세스할 수 있다면 더 광범위한 질문에 대해 생각해야 합니다.\n\n샤오리 멍이 때때로 언급하는 말에 따르면, 모든 통계는 결측 데이터 문제라고 합니다. 그래서 역설적이게도 데이터 우선 질문을 하는 또 다른 방법은 우리가 가지고 있지 않은 데이터에 대해 생각하는 것입니다. 예를 들어, 앞에서 논의한 신생아 및 산모 사망률 예로 돌아가면 한 가지 문제는 완전한 사망 원인 데이터가 없다는 것입니다. 만약 있다면 관련 사망자 수를 셀 수 있을 것입니다. ((Castro2023은?) 다른 원인과 독립적이지 않은 사망 원인이 때때로 있기 때문에 이 단순한 가설이 실제로는 복잡할 것이라고 상기시켜 줍니다.) 어떤 결측 데이터 문제를 설정한 후에는 데이터 중심 접근 방식을 취할 수 있습니다. 우리가 가진 데이터를 살펴본 다음 가상 데이터셋을 근사화하는 데 어느 정도까지 사용할 수 있는지에 대한 연구 질문을 합니다.\n\n\n\n\n\n\n거인의 어깨 위에 서서\n\n\n\n샤오리 멩은 하버드 대학교의 휘플 V. N. 존스 통계학 교수입니다. 1990년 하버드 대학교에서 통계학 박사 학위를 받은 후 시카고 대학교 조교수로 임명되어 2000년 교수로 승진했습니다. 그는 2001년 하버드로 옮겨 2004년부터 2012년까지 통계학과 학과장을 역임했습니다. 그는 결측 데이터(Meng 1994, 2012)와 데이터 품질(Meng 2018)을 포함한 광범위한 주제에 대해 발표했습니다. 그는 2001년 COPSS 회장상을 수상했습니다.\n\n\n일부 연구자들이 데이터 우선인 한 가지 방법은 특정 지리적 또는 역사적 상황의 데이터에 대한 특정 전문 지식을 개발하는 것입니다. 예를 들어, 그들은 현재 영국이나 19세기 후반 일본에 대해 특히 지식이 풍부할 수 있습니다. 그런 다음 다른 연구자들이 다른 상황에서 묻는 질문을 살펴보고 해당 질문에 자신의 데이터를 적용합니다. 예를 들어, 특정 질문이 처음에 미국에 대해 제기된 다음 많은 연구자들이 영국, 캐나다, 호주 및 기타 여러 국가에 대해 동일한 질문에 답하는 것을 흔히 볼 수 있습니다.\n데이터 우선 연구에는 특히 불확실할 수 있다는 사실을 포함하여 여러 가지 단점이 있습니다. 또한 선택 효과에 대한 우려가 항상 있기 때문에 외부 타당성에 어려움을 겪을 수 있습니다.\n데이터 기반 연구의 한 변형은 모델 기반 연구입니다. 여기서 연구원은 특정 통계적 접근 방식에 대한 전문가가 된 다음 해당 접근 방식을 적절한 맥락에 적용합니다.\n\n\n4.3.2 질문 우선\n질문 우선을 시도할 때 데이터 가용성에 대한 우려라는 반대 문제가 있습니다. “FINER 프레임워크”는 연구 질문 개발을 안내하기 위해 의학에서 사용됩니다. 이는 다음과 같은 질문을 하도록 권장합니다. 실행 가능하고, 흥미롭고, 새롭고, 윤리적이며, 관련성이 있는가(Hulley 기타 2007). (farrugia2010research는?) PICOT으로 FINER을 기반으로 하며, 이는 추가적인 고려 사항을 권장합니다. 인구, 중재, 비교 그룹, 관심 결과 및 시간.\n질문을 작성하려고 할 때 압도감을 느낄 수 있습니다. 한 가지 방법은 매우 구체적인 질문을 하는 것입니다. 또 다른 방법은 기술적, 예측적, 추론적 또는 인과적 분석에 관심이 있는지 결정하는 것입니다. 그런 다음 이러한 분석은 다양한 유형의 질문으로 이어집니다. 예를 들어 다음과 같습니다.\n\n기술 분석: “\\(x\\)는 어떻게 생겼습니까?”;\n예측 분석: “\\(x\\)에 무슨 일이 일어날까요?”;\n추론: “\\(x\\)를 어떻게 설명할 수 있습니까?”; 그리고\n인과 관계: “\\(x\\)가 \\(y\\)에 미치는 영향은 무엇입니까?”.\n\n이들 각각은 해야 할 역할이 있습니다. 신뢰성 혁명 이후(Angrist 와/과 Pischke 2010), 특정 접근 방식으로 답하는 인과 관계 질문이 우세했습니다. 이것은 약간의 이점을 가져왔지만 비용이 들지 않은 것은 아닙니다. 기술 분석은 그만큼, 때로는 더, 계몽적일 수 있으며 중요합니다(Sen 1980). 묻는 질문의 성격은 진정으로 답하는 데 관심이 있는 것보다 덜 중요합니다.\n시간은 종종 흥미로운 방식으로 제약을 받으며 이는 연구 질문의 세부 사항을 안내할 수 있습니다. 유명인의 발표가 주식 시장에 미치는 영향에 관심이 있다면 발표 전후의 주가를 살펴봄으로써 이를 수행할 수 있습니다. 그러나 암 치료제가 장기적인 결과에 미치는 영향에 관심이 있다면 어떨까요? 효과가 20년이 걸린다면 잠시 기다리거나 20년 전에 치료받은 사람들을 살펴봐야 합니다. 그러면 오늘날 약물을 투여하는 경우와 비교하여 선택 효과와 다른 상황이 발생합니다. 종종 합리적인 유일한 방법은 통계 모델을 구축하는 것이지만 이는 다른 문제를 야기합니다.",
    "crumbs": [
      "소통",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>연구 논문 작성</span>"
    ]
  },
  {
    "objectID": "04-writing_research_ko.html#질문에-답하기",
    "href": "04-writing_research_ko.html#질문에-답하기",
    "title": "4  연구 논문 작성",
    "section": "4.4 질문에 답하기",
    "text": "4.4 질문에 답하기\n\n4.4.1 반사실과 편향\n반사실의 생성은 종종 질문에 답할 때 중요합니다. 반사실은 “만약”이 거짓인 만약-그러면 문장입니다. 루이스 캐럴의 거울 나라의 앨리스에 나오는 험프티 덤프티의 예를 생각해 보십시오.\n\n“정말 쉬운 수수께끼를 내시는군요!” 험프티 덤프티가 으르렁거렸다. “물론 그렇게 생각하지 않아요! 만약 제가 떨어진다면… 그럴 가능성은 없지만… 만약 떨어진다면…” 여기서 그는 입술을 오므리고 너무 엄숙하고 위엄 있어 보여서 앨리스는 웃음을 참을 수 없었다. “만약 제가 떨어진다면,” 그가 계속했다. “왕께서 약속하셨어요… 직접 말씀하시기를… 모든 말과 모든 부하를 보내시겠다고…” 앨리스가 다소 경솔하게 끼어들었다.\nCarroll (1871)\n\n험프티는 자신이 결코 떨어지지 않을 것이라고 확신하지만, 만약 떨어진다면 무슨 일이 일어날지에 만족합니다. 질문에 대한 답을 결정하는 것은 종종 이 비교 그룹입니다. 예를 들어, ?sec-causality-from-observational-data에서는 VO2 max가 사이클리스트의 경기 승리 가능성에 미치는 영향을 고려합니다. 일반 인구와 비교하면 중요한 변수입니다. 그러나 잘 훈련된 운동선수만 비교하면 선택 때문에 덜 중요합니다.\n연구 질문을 결정할 때 데이터의 두 가지 측면에 특히 유의해야 합니다. 선택 편향과 측정 편향입니다.\n선택 편향은 결과가 표본에 누가 포함되는지에 따라 달라질 때 발생합니다. 선택 편향의 해로운 측면 중 하나는 이에 대해 조치를 취하려면 그 존재를 알아야 한다는 것입니다. 그러나 많은 기본 진단에서는 선택 편향을 식별하지 못합니다. ?sec-hunt-data에서 논의하는 A/B 테스트에서 A/A 테스트는 그룹을 만들고 치료를 적용하기 전에 비교하는 약간의 변형입니다(따라서 A/A 명명법). 그룹이 처음에 동일한지 확인하려는 이러한 노력은 선택 편향을 식별하는 데 도움이 될 수 있습니다. 더 일반적으로 연령 그룹, 성별 및 교육과 같은 표본의 속성을 인구의 특성과 비교하는 것도 도움이 될 수 있습니다. 그러나 선택 편향과 관찰 데이터의 근본적인 문제는 데이터가 있는 사람들은 데이터가 없는 사람들과 적어도 한 가지 면에서 다르다는 것을 알고 있다는 것입니다! 그러나 다른 어떤 면에서 다를 수 있는지는 알 수 없습니다.\n선택 편향은 분석의 여러 측면에 만연할 수 있습니다. 처음에는 대표적인 표본이라도 시간이 지남에 따라 편향될 수 있습니다. 예를 들어, ?sec-farm-data에서 논의하는 설문 조사 패널은 아무것도 얻지 못하는 사람들이 응답을 중단하기 때문에 때때로 업데이트해야 합니다.\n주의해야 할 또 다른 편향은 측정 편향으로, 데이터 수집 방식에 따라 결과가 영향을 받을 때 발생합니다. 이에 대한 일반적인 예는 응답자에게 소득을 물어보면 온라인 설문 조사와 비교하여 직접 만났을 때 다른 답변을 얻을 수 있다는 것입니다.\n\n\n4.4.2 추정 대상\n일반적으로 데이터로 질문에 답하는 데 관심이 있으며 세부 사항을 명확히 하는 것이 중요합니다. 예를 들어, 흡연이 기대 수명에 미치는 영향에 관심이 있을 수 있습니다. 이 경우 결코 알 수 없는 실제 효과가 있으며 이 실제 효과를 “추정 대상”이라고 합니다(Little 와/과 Lewis 2021). 논문의 어느 시점에서, 이상적으로는 서론에서 추정 대상을 정의하는 것이 중요합니다(Lundberg, Johnson, 와/과 Stewart 2021). 분석 계획의 특정 측면을 약간 변경하여 실수로 다른 것을 추정하게 되기 쉽기 때문입니다(Kahan 기타 2022). 일부 의약품 규제 기관에서는 이를 요구하기 시작했습니다(Kahan 기타 2024). 추정 대상의 경우 효과가 무엇을 나타내는지에 대한 명확한 설명이 필요합니다(Kahan 기타 2023). “추정량”은 사용 가능한 데이터를 사용하여 추정 대상의 “추정치”를 생성하는 프로세스입니다. (steinsparadox는?) 추정량 및 관련 우려 사항에 대한 논의를 제공합니다.\nBueno de Mesquita 와/과 Fowler (2021, p. 94)는 추정치와 추정 대상 간의 관계를 다음과 같이 설명합니다.\n\\[\n\\mbox{추정치 = 추정 대상 + 편향 + 잡음}\n\\]\n편향은 추정량이 체계적으로 추정 대상과 다른 추정치를 제공하는 문제점을 의미하며, 잡음은 비체계적인 차이점을 의미합니다. 예를 들어, 표준 정규 분포를 생각해 봅시다. 우리는 평균을 이해하는 데 관심이 있을 수 있으며, 이것이 우리의 추정 대상이 될 것입니다. 우리는 (실제 데이터로는 결코 알 수 없는 방식으로) 추정 대상이 0이라는 것을 알고 있습니다. 해당 분포에서 10번 추출해 봅시다. 추정치를 생성하는 데 사용할 수 있는 한 가지 추정량은 추출한 값을 합산하고 추출 횟수로 나누는 것입니다. 또 다른 방법은 추출한 값을 정렬하고 중간 관찰값을 찾는 것입니다. 더 구체적으로 말하면, 이 상황을 시뮬레이션할 것입니다(표 4.1).\n\nset.seed(853)\n\ntibble(\n  num_draws = c(\n    rep(10, times = 10),\n    rep(100, times = 100),\n    rep(1000, times = 1000),\n    rep(10000, times = 10000)\n  ),\n  draw = rnorm(\n    n = length(num_draws),\n    mean = 0,\n    sd = 1)\n  ) |&gt;\n  summarise(\n    estimator_one = sum(draw) / unique(num_draws),\n    estimator_two = sort(draw)[round(unique(num_draws) / 2, 0)],\n    .by = num_draws\n  ) |&gt;\n  tt() |&gt;\n  style_tt(j = 2:3, align = \"r\") |&gt;\n  format_tt(digits = 2, num_mark_big = \",\", num_fmt = \"decimal\") |&gt;\n  setNames(c(\"Number of draws\", \"Estimator one\", \"Estimator two\"))\n\n\n\n표 4.1: 추출 횟수가 증가함에 따라 임의 추출 평균의 두 추정량 결과 비교\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Number of draws\n                Estimator one\n                Estimator two\n              \n        \n        \n        \n                \n                  10\n                  -0.58\n                  -0.82\n                \n                \n                  100\n                  -0.06\n                  -0.07\n                \n                \n                  1,000\n                  0.06\n                  0.04\n                \n                \n                  10,000\n                  -0.01\n                  -0.01\n                \n        \n      \n    \n\n\n\n\n\n\n추출 횟수가 증가함에 따라 잡음의 영향이 제거되고 추정치는 추정량의 편향을 보여줍니다. 이 예에서는 진실이 무엇인지 알고 있지만 실제 데이터를 고려할 때는 무엇을 해야 할지 알기가 더 어려울 수 있습니다. 따라서 추정치를 생성하기 전에 추정 대상이 무엇인지 명확히 하는 것이 중요합니다.\n\n\n4.4.3 방향성 비순환 그래프\n질문에 답하는 데 사용할 변수에 대해 생각할 때 우리가 의미하는 바를 구체적으로 명시하는 것이 도움이 될 수 있습니다. 관찰 데이터에 휘말려 자신을 속이기 쉽습니다. 열심히 생각하고 사용 가능한 모든 도구를 사용해야 합니다. 데이터에 대해 열심히 생각하는 데 도움이 될 수 있는 한 가지 프레임워크는 방향성 비순환 그래프(DAG)를 사용하는 것입니다. DAG는 흐름도의 멋진 이름이며 변수 간의 관계를 나타내기 위해 변수 사이에 화살표와 선을 그리는 것을 포함합니다.\n이를 구성하기 위해 그래프 시각화를 위한 오픈 소스 패키지인 Graphviz를 사용하며 Quarto에 내장되어 있습니다. 코드는 “R” 대신 “dot” 청크로 래핑해야 하며 청크 옵션은 “#|” 대신 “//|”로 설정됩니다. 이것이 필요하지 않은 대안으로는 DiagrammeR (Iannone 2022) 및 ggdag (Barrett 2021) 사용이 있습니다. 첫 번째 DAG에 대한 전체 청크를 제공하지만 다른 DAG에 대해서는 코드만 제공합니다.\n\n```{dot}\n//| label: fig-dot-firstdag-quarto\n//| fig-cap: \"x가 y에 영향을 미치는 x와 y 사이의 인과 관계를 예상합니다.\"\n//| fig-width: 4\ndigraph D {\n  node [shape=plaintext, fontname = \"helvetica\"];\n\n  {rank=same x y};\n\n  x -&gt; y;\n}\n```\n\n\n\n\n\n\n\nD\n\n\n\nx\nx\n\n\n\ny\ny\n\n\n\nx-&gt;y\n\n\n\n\n\n\n\n\n그림 4.1: x가 y에 영향을 미치는 x와 y 사이의 인과 관계를 예상합니다.\n\n\n\n\n\n?fig-dot-firstdag-quarto에서 우리는 x가 y를 유발한다고 말하고 있습니다.\n상황이 덜 명확한 또 다른 DAG를 만들 수 있습니다. 예를 더 쉽게 따르도록 하기 위해 소득과 행복 사이의 가상 관계에 대해 생각하고 해당 관계에 영향을 미칠 수 있는 변수를 고려하는 것으로 전환합니다. 이 첫 번째 예에서는 소득과 행복 사이의 관계와 교육을 함께 고려합니다(그림 4.2).\n\ndigraph D {\n\n  node [shape=plaintext, fontname = \"helvetica\"];\n\n  a [label = \"소득\"];\n  b [label = \"행복\"];\n  c [label = \"교육\"];\n\n  { rank=same a b};\n\n  a-&gt;b;\n  c-&gt;{a, b};\n}\n\n\n\n\n\n\n\nD\n\n\n\na\n소득\n\n\n\nb\n행복\n\n\n\na-&gt;b\n\n\n\n\n\nc\n교육\n\n\n\nc-&gt;a\n\n\n\n\n\nc-&gt;b\n\n\n\n\n\n\n\n\n그림 4.2: 교육은 소득과 행복 사이의 관계에 영향을 미치는 교란 변수입니다.\n\n\n\n\n\n?fig-dot-educationasconfounder-quarto에서 우리는 소득이 행복을 유발한다고 생각합니다. 그러나 우리는 또한 교육이 행복을 유발하고 교육이 소득도 유발한다고 생각합니다. 그 관계는 “백도어 경로”이며, 회귀 분석에서 교육을 조정하지 않으면 분석에서 소득과 행복 사이의 관계 정도를 과장하거나 심지어 허위 관계를 만들 수 있습니다. 즉, 소득 변화가 행복 변화를 유발한다고 생각할 수 있지만 교육이 둘 다를 변화시키고 있을 수 있습니다. 이 경우 교육과 같은 해당 변수를 “교란 변수”라고 합니다.\nHernán 와/과 Robins (2023, p. 83)은 한 사람이 하늘을 올려다보는 것이 다른 사람도 하늘을 올려다보게 하는지 여부에 관심이 있는 연구자의 흥미로운 사례를 논의합니다. 두 사람의 반응 사이에는 명확한 관계가 있었습니다. 그러나 하늘에 소음이 있는 경우도 있었습니다. 두 번째 사람이 첫 번째 사람이 올려다보았기 때문에 올려다보았는지, 아니면 소음 때문에 둘 다 올려다보았는지 불분명했습니다. 실험 데이터를 사용할 때 무작위화를 통해 이러한 우려를 피할 수 있지만 관찰 데이터로는 그렇게 할 수 없습니다. 더 큰 데이터가 반드시 이 문제를 해결해 주는 것도 아닙니다. 대신 상황에 대해 신중하게 생각해야 하며 DAG가 도움이 될 수 있습니다.\n교란 변수가 있지만 여전히 인과 관계에 관심이 있다면 이를 조정해야 합니다. 한 가지 방법은 회귀 분석에 포함하는 것입니다. 그러나 이것의 타당성에는 몇 가지 가정이 필요합니다. 특히 Gelman 와/과 Hill (2007, p. 169)은 모든 교란 변수를 포함하고 올바른 모델을 사용하는 경우에만 추정치가 표본의 평균 인과 효과에 해당한다고 경고합니다. 두 번째 요구 사항은 제쳐두고 첫 번째 요구 사항에만 집중하면 교란 변수에 대해 생각하고 관찰하지 않으면 조정하기 어려울 수 있습니다. 그리고 이것은 영역 전문 지식과 이론 모두 분석에 상당한 비중을 둘 수 있는 영역입니다.\n?fig-dot-luxuryasmediator-quarto에서 우리는 다시 소득이 행복을 유발한다고 생각합니다. 그러나 소득이 자녀도 유발하고 자녀도 행복을 유발한다면 소득이 행복에 미치는 영향을 이해하기 어려운 상황에 처하게 됩니다.\n\ndigraph D {\n\n  node [shape=plaintext, fontname = \"helvetica\"];\n\n  a [label = \"소득\"];\n  b [label = \"행복\"];\n  c [label = \"자녀\"];\n\n  { rank=same a b};\n\n  a-&gt;{b, c};\n  c-&gt;b;\n}\n\n\n\n\n\n\n\nD\n\n\n\na\n소득\n\n\n\nb\n행복\n\n\n\na-&gt;b\n\n\n\n\n\nc\n자녀\n\n\n\na-&gt;c\n\n\n\n\n\nc-&gt;b\n\n\n\n\n\n\n\n\n그림 4.3: 소득과 행복 사이의 매개 변수로서의 자녀\n\n\n\n\n\n?fig-dot-luxuryasmediator-quarto에서 자녀는 “매개 변수”라고 하며 소득이 행복에 미치는 영향에 관심이 있다면 조정하지 않을 것입니다. 조정하면 소득에 기인하는 일부가 자녀 때문일 것입니다.\n마지막으로 ?fig-dot-residenceascollider-quarto에서는 소득이 행복을 유발한다고 생각하는 또 다른 유사한 상황이 있습니다. 그러나 이번에는 소득과 행복 모두 운동을 유발합니다. 예를 들어, 돈이 더 많으면 운동하기가 더 쉬울 수 있지만 행복하면 운동하기가 더 쉬울 수도 있습니다.\n\ndigraph D {\n\n  node [shape=plaintext, fontname = \"helvetica\"];\n\n  a [label = \"소득\"];\n  b [label = \"행복\"];\n  c [label = \"운동\"];\n\n  { rank=same a b};\n\n  a-&gt;{b c};\n  b-&gt;c;\n}\n\n\n\n\n\n\n\nD\n\n\n\na\n소득\n\n\n\nb\n행복\n\n\n\na-&gt;b\n\n\n\n\n\nc\n운동\n\n\n\na-&gt;c\n\n\n\n\n\nb-&gt;c\n\n\n\n\n\n\n\n\n그림 4.4: 소득과 행복 간의 관계에 영향을 미치는 충돌 변수로서의 운동\n\n\n\n\n\n이 경우 운동은 “충돌 변수”라고 하며, 만약 우리가 그것을 조건으로 한다면 오해의 소지가 있는 관계를 만들 것입니다. 소득은 운동에 영향을 미치지만, 사람의 행복도 이것에 영향을 미칩니다. 운동은 관심 있는 예측 변수와 결과 변수 모두에 영향을 미치기 때문에 충돌 변수입니다.\n이 점을 명확히 하겠습니다. 모델을 직접 만들어야 하는 것처럼 DAG도 직접 만들어야 합니다. 우리를 위해 만들어 줄 것은 아무것도 없습니다. 즉, 상황에 대해 신중하게 생각해야 합니다. DAG에서 무언가를 보고 그것에 대해 조치를 취하는 것은 한 가지이지만, 그것이 거기에 있다는 것조차 모르는 것은 또 다른 문제입니다. McElreath ([2015년] 2020, p. 180)는 이것을 유령 DAG라고 설명합니다. DAG는 도움이 되지만 상황에 대해 깊이 생각하는 데 도움이 되는 도구일 뿐입니다.\n모델을 구축할 때 가능한 한 많은 예측 변수를 포함하고 싶은 유혹이 있을 수 있습니다. DAG는 왜 더 신중해야 하는지 명확하게 보여줍니다. 예를 들어, 변수가 교란 변수이면 조정하고 싶지만 변수가 충돌 변수이면 조정하지 않을 것입니다. 우리는 결코 진실을 알 수 없으며 이론, 관심사, 연구 설계, 데이터의 한계 또는 연구자로서의 한계와 같은 측면에 의해 정보를 얻습니다. 한계를 아는 것은 모델을 보고하는 것만큼 중요합니다. 결함이 있는 데이터와 모델은 결함을 인정하면 여전히 유용합니다. 상황에 대해 생각하는 작업은 결코 끝나지 않으며 다른 사람에게 의존하므로 가능한 한 모든 작업을 재현 가능하게 만들어야 합니다.",
    "crumbs": [
      "소통",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>연구 논문 작성</span>"
    ]
  },
  {
    "objectID": "04-writing_research_ko.html#논문의-구성-요소",
    "href": "04-writing_research_ko.html#논문의-구성-요소",
    "title": "4  연구 논문 작성",
    "section": "4.5 논문의 구성 요소",
    "text": "4.5 논문의 구성 요소\n\n나는 교수를 시작하기 전에 아무것도 출판하지 않았지만, 거의 구성되자마자 파괴된 많은 미숙한 노력 속에서 한때 가졌을지도 모르는 장식적이고 중복된 구성에 대한 취향을 극복하고 평범하고 소박한 것을 선호하게 되었다.\n교수 (Brontë 1857)\n\n다음 구성 요소를 논의합니다. 제목, 초록, 서론, 데이터, 결과, 토론, 그림, 표, 방정식 및 전문 용어.1 논문 전체에 걸쳐 가능한 한 간결하고 구체적으로 작성하도록 노력하십시오. 대부분의 독자는 제목을 넘기지 않을 것입니다. 초록 이상을 읽는 사람은 거의 없을 것입니다. 섹션 및 하위 섹션 제목과 그래프 및 표 캡션은 주변 텍스트 없이 자체적으로 작동해야 합니다. 왜냐하면 많은 사람들이 논문을 읽는 방식이 그러한 훑어보기이기 때문입니다(Keshav 2007).\n\n4.5.1 제목\n제목은 독자를 우리 이야기에 참여시킬 수 있는 첫 번째 기회입니다. 이상적으로는 우리가 발견한 것을 독자에게 정확하게 전달할 수 있어야 합니다. 효과적인 제목은 매우 중요합니다. 그렇지 않으면 독자가 논문을 무시할 수 있기 때문입니다. 제목이 “귀엽게” 보일 필요는 없지만 의미가 있어야 합니다. 즉, 이야기를 명확하게 전달해야 합니다.\n충분히 좋은 제목의 한 예는 “2016년 브렉시트 국민투표에 관하여”입니다. 이 제목은 독자가 논문의 내용이 무엇인지 알 수 있기 때문에 유용합니다. 그러나 특별히 유익하거나 매력적이지는 않습니다. 약간 더 나은 제목은 “2016년 브렉시트 국민투표에서 탈퇴 투표 결과에 관하여”가 될 수 있습니다. 이 변형은 유익한 구체성을 더합니다. 우리는 가장 좋은 제목은 “2016년 브렉시트 국민투표에서 농촌 지역에서 탈퇴 투표가 우세함: 베이즈 계층 모델의 증거”와 같은 것이라고 주장합니다. 여기서 독자는 논문의 접근 방식과 주요 내용을 알 수 있습니다.\n몇 가지 특히 효과적인 제목의 예를 살펴보겠습니다. (hug2019national은?) “1990년에서 2017년 사이 신생아 사망률의 국가, 지역 및 전 세계 수준 및 추세, 2030년까지의 시나리오 기반 예측: 체계적인 분석”을 사용합니다. 여기서 논문의 내용과 사용된 방법이 명확합니다. (Alexander2020은?) “1901년에서 2018년 사이 호주 연방 의회에서 논의된 주제에 대한 선거 및 총리 교체의 영향 증가”를 사용합니다. 주요 발견 사항은 내용이 무엇인지에 대한 상당한 정보와 함께 제목에서 명확합니다. (alexander2018trends는?) “1979-2015년 미국 흑인 및 백인 아편유사제 사망률 추세”를 사용합니다. (Frei2022는?) “미국 세금 허점 폐쇄가 투자자 포트폴리오에 미치는 영향”을 사용합니다. 아마도 역대 최고의 제목 중 하나는 Bickel, Hammel, 와/과 O’Connell (1975) “대학원 입학에서의 성차별: 버클리 데이터: 편향 측정은 일반적으로 가정하는 것보다 어렵고 증거는 때때로 예상과 반대입니다”이며, 이는 ?sec-causality-from-observational-data에서 다시 다룹니다.\n제목은 종종 논문의 마지막 측면 중 하나입니다. 초고를 작성하는 동안 일반적으로 작업을 수행하는 작업 제목을 사용합니다. 그런 다음 다시 작성하는 과정에서 다듬습니다. 제목은 논문의 최종 이야기를 반영해야 하며, 이는 일반적으로 처음에 알 수 있는 것이 아닙니다. 독자가 논문을 읽을 만큼 충분히 관심을 갖도록 하는 것과 내용을 충분히 전달하여 유용하도록 하는 것 사이에서 균형을 이루어야 합니다(Hayot 2014). 두 가지 훌륭한 예는 토마스 배빙턴 매콜리의 제임스 2세 즉위 이후의 영국 역사와 윈스턴 처칠의 영어 사용 민족의 역사입니다. 둘 다 내용이 무엇인지 명확하며 대상 독자에게 관심을 불러일으킵니다.\n한 가지 구체적인 접근 방식은 “흥미로운 내용: 구체적인 내용” 형식입니다. 예를 들어, “뿌리로 돌아가기: 2016년 브렉시트 국민투표에서 탈퇴 투표 성과 조사”입니다. (kennedy2020know는?) “인구를 알고 모델을 알라: 관찰된 표본을 넘어 결과를 일반화하기 위해 모델 기반 회귀 및 사후 계층화 사용”으로 이 접근 방식의 특히 좋은 예를 제공하며, (craiu2019hiring은?) “채용 전략: 투퍼 데이터 과학자 찾기”로 그렇게 합니다. 이와 밀접한 변형은 “질문? 그리고 접근 방식”입니다. 예를 들어, (cahill2020increase는?) “2030년까지 75% 수요 만족을 달성하기 위해 FP2020 국가에서 현대 피임법 사용량이 얼마나 증가해야 합니까? 가속화된 전환 방법 및 가족 계획 추정 모델을 사용한 평가”입니다. 이 변형에 익숙해지면 답변 부분을 삭제하면서도 효과적인 경우를 알 수 있게 됩니다. 예를 들어 (briggs2021does는?) “왜 원조는 가장 가난한 사람들을 대상으로 하지 않는가?”입니다. 또 다른 구체적인 접근 방식은 “구체적인 내용 다음 광범위한 내용” 또는 그 반대입니다. 예를 들어, “농촌성, 엘리트 및 2016년 브렉시트 국민투표에서 탈퇴 투표 지지” 또는 “2016년 브렉시트 국민투표에서 탈퇴 투표 지지, 농촌성 및 엘리트”입니다. 이 접근 방식은 (tolley2021gender가?) “성별, 지방 자치 단체 정당 정치 및 몬트리올 최초의 여성 시장”에서 사용합니다.\n때로는 부제를 포함할 수 있습니다. 이것이 가능할 때 이를 활용하는 좋은 방법은 찾은 주요 정량적 결과의 일부 세부 정보를 포함하는 데 사용하는 것입니다. 해당 결과에 대한 적절한 수준의 세부 정보와 추상화를 얻는 것은 어려우며 다시 작성하고 다른 사람의 의견을 구해야 합니다.\n\n\n4.5.2 초록\n10~15페이지 분량의 논문의 경우 좋은 초록은 3~5문장으로 구성된 단락입니다. 더 긴 논문의 경우 초록은 약간 더 길 수 있습니다. 초록은 논문의 이야기를 명시해야 합니다. 또한 무엇을 했고 왜 중요한지 전달해야 합니다. 그렇게 하기 위해 초록은 일반적으로 작업의 맥락, 목표, 접근 방식 및 결과를 다룹니다.\n더 구체적으로 말하면, 좋은 초록의 요리법은 다음과 같습니다. 첫 번째 문장: 논문의 일반적인 영역을 명시하고 독자를 격려합니다. 두 번째 문장: 일반적인 수준에서 데이터셋과 방법을 명시합니다. 세 번째 문장: 주요 결과를 명시합니다. 그리고 네 번째 문장은 함의에 관한 것입니다.\n다양한 초록에서 이 패턴을 볼 수 있습니다. 예를 들어, (tolley2021gender는?) 400년 만에 최초의 여성 시장 당선을 언급하여 첫 문장으로 독자를 끌어들입니다. 두 번째 문장은 논문에서 무엇을 하는지 명확하게 보여줍니다. 세 번째 문장은 독자에게 어떻게 하는지, 즉 설문 조사를 통해 알려주고 네 번째 문장은 약간의 세부 정보를 추가합니다. 다섯 번째이자 마지막 문장은 주요 요점을 명확하게 만듭니다.\n\n2017년 몬트리올은 400년 역사상 최초의 여성 시장인 발레리 플랑트를 선출했습니다. 이 선거를 사례 연구로 사용하여 성별이 결과에 어떤 영향을 미쳤고 미치지 않았는지 보여줍니다. 몬트리올 유권자 설문 조사에 따르면 성별은 투표 선택에 중요한 요소가 아니었습니다. 성별은 유권자에게 그다지 중요하지 않았지만 캠페인 및 정당 조직을 형성했습니다. 우리는 플랑트의 승리가 덜 지도자 중심적인 정당과 여성의 정치 지도자 직책 부적합성에 대한 고정 관념을 상쇄하는 데 도움이 된 성별 없는 캠페인을 선보인 전략 때문이라고 주장합니다.\n\n마찬가지로 (beauregard2021antiwomen은?) 처음 두 문장 내에서 더 넓은 환경을 명확히 하고 해당 환경에 대한 이 논문의 구체적인 기여를 명시합니다. 세 번째와 네 번째 문장은 데이터 출처와 주요 결과를 명확히 합니다. 다섯 번째와 여섯 번째 문장은 이 초록의 잠재적 독자, 즉 학술 정치학자에게 흥미로울 수 있는 구체성을 더합니다. 마지막 문장에서는 저자의 입장이 명확해집니다.\n\n성별 할당제 지지에 대한 이전 연구는 성 평등과 정부 개입에 대한 태도를 설명으로 초점에 맞춘다. 우리는 정치에서 여성의 존재를 늘리는 것을 목표로 하는 정책에 대한 지지를 이해하는 데 있어 여성에 대한 태도의 역할이 양가적이라고 주장한다. 적대적 성차별과 자애적 성차별 모두 다른 방식으로 지지를 이해하는 데 기여한다. 호주 응답자의 확률 기반 표본에 대한 설문 조사에서 얻은 원본 데이터를 사용하여 우리 연구 결과는 적대적 성차별주의자가 성별 할당제 채택을 통해 정치에서 여성의 존재를 늘리는 것에 반대할 가능성이 더 높다는 것을 보여준다. 반면에 자애적 성차별주의자는 자애적 성차별 수준이 낮은 응답자보다 이러한 정책을 지지할 가능성이 더 높다. 우리는 이것이 자애적 성차별이 여성이 순수하고 보호가 필요하다고 여기기 때문이라고 주장한다. 그들은 할당제의 도움 없이는 정치에서 성공할 수 있는 자질이 없다. 마지막으로, 여성이 할당제를 지지할 가능성이 더 높지만 양가적 성차별은 여성과 남성 모두에게 지지와 동일한 관계를 갖는다는 것을 보여준다. 이러한 연구 결과는 성별 할당제에 대한 대중적 지지 수준이 반드시 일반적으로 성 평등에 대한 더 큰 수용을 나타내는 것은 아님을 시사한다.\n\n또 다른 훌륭한 초록 예는 (sidesvavreckwarshaw입니다?). 단 다섯 문장으로 그들은 무엇을 하고, 어떻게 하고, 무엇을 발견하고, 왜 중요한지 명확하게 전달합니다.\n\n우리는 2000년부터 2018년까지 미국 선거 결과에 대한 텔레비전 광고의 영향에 대한 포괄적인 평가를 제공합니다. 우리는 대통령, 상원, 하원, 주지사, 법무장관 및 주 재무장관 선거를 포함하고 광고의 인과 관계를 파악하는 데 도움이 되는 차이-차이 및 경계-불연속성 연구 설계를 모두 사용하여 이전 연구를 확장합니다. 우리는 방송 캠페인 광고가 투표 용지 위아래로 중요하지만 대통령 선거보다 하위 투표 선거에서 훨씬 더 큰 영향을 미친다는 것을 발견했습니다. 여러 선거 주기의 설문 조사 및 유권자 등록 데이터를 사용하여 광고 효과의 주요 메커니즘은 당파 동원이 아니라 설득이라는 것도 보여줍니다. 우리 결과는 캠페인 및 선거 연구뿐만 아니라 유권자 의사 결정 및 정보 처리에도 영향을 미칩니다.\n\n최고의 초록은 단어 대비 내용 비율이 너무 높아서 약간 간결하게 느껴질 수도 있습니다. 예를 들어, (touvron의?) 초록에는 낭비되는 단어가 없으며 단 네 문장으로 많은 양의 정보를 전달합니다.\n\n우리는 70억에서 650억 개의 매개변수에 이르는 기본 언어 모델 모음인 LLaMA를 소개합니다. 우리는 수조 개의 토큰으로 모델을 훈련시키고 독점적이고 접근 불가능한 데이터셋에 의존하지 않고 공개적으로 사용 가능한 데이터셋만 사용하여 최첨단 모델을 훈련하는 것이 가능하다는 것을 보여줍니다. 특히 LLaMA-13B는 대부분의 벤치마크에서 GPT-3(175B)를 능가하며 LLaMA-65B는 최고의 모델인 Chinchilla-70B 및 PaLM-540B와 경쟁력이 있습니다. 우리는 모든 모델을 연구 커뮤니티에 공개합니다.\n\n(kasymatching은?) 더 통계적인 초록의 훌륭한 예를 제공합니다. 그들은 무엇을 하는지, 왜 중요한지 명확하게 밝힙니다.\n\n우리는 자원을 참가자에게 반복적으로 매칭해야 하고 개별 선택된 매칭의 수익은 알 수 없지만 학습할 수 있는 실험적 환경을 고려합니다. 우리 환경은 난민 재정착, 사회 주택 할당 및 위탁 양육과 같이 (잠재적으로 복잡한) 수용 능력 제약이 있는 양면 및 일면 매칭을 다룹니다. 우리는 이러한 적응형 조합 할당 문제를 해결하기 위해 톰슨 샘플링 알고리즘의 변형을 제안합니다. 우리는 이 알고리즘에 대한 엄격하고 사전 독립적이며 유한 표본의 예상 후회에 대한 경계를 제공합니다. 할당 수는 매칭 수에 따라 기하급수적으로 증가하지만 우리 경계는 그렇지 않습니다. 베이즈 계층 모델을 사용하여 난민 재정착 데이터를 기반으로 한 시뮬레이션에서 우리는 알고리즘이 고용 확률에 대한 완벽한 지식을 기반으로 한 최적 매칭에서 얻을 수 있는 고용 이득의 절반(현상 유지 대비)을 달성한다는 것을 발견했습니다.\n\n마지막으로 (briggs2021does는?) 의심할 여지 없이 사실인 것처럼 보이는 주장으로 시작합니다. 두 번째 문장에서는 그것이 거짓이라고 말합니다! 세 번째 문장은 이 주장의 범위를 명시하고 네 번째 문장은 더 자세한 내용을 제공하기 전에 이 입장에 도달한 방법을 자세히 설명합니다. 마지막 두 문장은 더 넓은 의미와 중요성을 말합니다.\n\n해외 원조 프로젝트는 일반적으로 지역적 효과를 가지므로 빈곤을 줄이려면 가난한 사람들과 가까운 곳에 배치해야 합니다. 저는 세계은행(WB) 프로젝트 원조가 지역 인구 수준을 조건으로 국가의 부유한 지역을 대상으로 한다는 것을 보여줍니다. 이 관계는 시간과 세계 지역에 걸쳐 유지됩니다. 저는 WB 태스크 팀 리더(TTL)에 대한 사전 등록된 공동 실험을 사용하여 부유층 대상 지정에 대한 5가지 기증자 측 설명을 테스트합니다. TTL은 원조 수혜국 정부가 원조를 정치적으로 대상 지정하고 실행을 통제하는 데 가장 관심이 있다고 인식합니다. 그들은 또한 원조가 더 가난하거나 더 외딴 지역에서 더 잘 작동하지만 이러한 지역에서의 실행이 유난히 어렵다고 생각합니다. 이러한 결과는 분배 정치, 원조에 대한 국제 협상 및 국제기구의 주체-대리인 문제에 대한 논쟁에 시사하는 바가 있습니다. 결과는 또한 프로젝트 실행의 용이성을 덜 중요하게 만드는 WB 인센티브 구조의 조정이 원조가 국가의 더 가난한 지역으로 흘러가도록 장려할 수 있음을 시사합니다.\n\n네이처라는 과학 저널은 초록 작성 가이드를 제공합니다. 그들은 6개 부분으로 구성되어 약 200단어로 요약되는 초록 구조를 권장합니다.\n\n광범위한 청중이 이해할 수 있는 소개 문장.\n잠재적 독자와 관련된 더 자세한 배경 문장.\n일반적인 문제를 명시하는 문장.\n주요 결과를 요약하고 설명하는 문장.\n일반적인 맥락에 대한 문장.\n그리고 마지막으로 더 넓은 관점에 대한 문장.\n\n초록의 첫 문장은 공허해서는 안 됩니다. 독자가 제목을 지나 계속 읽었다고 가정하면, 이 첫 문장은 우리가 그들에게 우리 논문을 계속 읽도록 간청할 수 있는 다음 기회입니다. 그리고 초록의 두 번째 문장 등등입니다. 초록이 너무 좋아서 그것만 읽어도 괜찮을 정도로 작업하고 다시 작업하십시오. 왜냐하면 종종 그런 경우가 많기 때문입니다.\n\n\n4.5.3 소개\n서론은 독립적이어야 하며 독자가 알아야 할 모든 것을 전달해야 합니다. 우리는 미스터리 소설을 쓰는 것이 아닙니다. 대신 서론에서 가장 중요한 요점을 제시하고 싶습니다. 10~15페이지 분량의 논문의 경우 서론은 주요 내용이 2~3단락일 수 있습니다. Hayot (2014, p. 90)은 서론의 목표는 독자의 참여를 유도하고 어떤 분야와 배경에 위치시킨 다음 논문의 나머지 부분에서 무슨 일이 일어나는지 알려주는 것이라고 말합니다. 완전히 독자 중심이어야 합니다.\n서론은 배경을 설정하고 독자에게 약간의 배경 지식을 제공해야 합니다. 예를 들어, 일반적으로 약간 더 넓게 시작합니다. 이것은 논문에 약간의 맥락을 제공합니다. 그런 다음 논문이 해당 맥락에 어떻게 부합하는지 설명하고, 특히 이야기의 주요 부분인 핵심 결과에 초점을 맞춘 높은 수준의 결과를 제공합니다. 초록에서 제공한 것보다 더 자세한 내용을 여기서 제공하지만 전체 범위는 아닙니다. 그리고 한두 문장으로 다음 단계를 광범위하게 논의합니다. 마지막으로 논문의 구조를 강조하는 추가적인 짧은 마지막 단락으로 서론을 마무리합니다.\n예를 들어 (만들어진 세부 정보 포함):\n\n영국 보수당은 항상 농촌 선거구에서 좋은 성과를 거두었습니다. 그리고 2016년 브렉시트 투표도 농촌과 도시 지역 간의 지지율에 상당한 차이가 있는 등 다르지 않았습니다. 그러나 보수적인 문제에 대한 농촌의 지지 기준에 비추어 보더라도 “탈퇴 투표”에 대한 지지는 이례적으로 강력했으며, “탈퇴 투표”는 이스트 미들랜즈와 잉글랜드 동부에서 가장 강력하게 지지를 받았고, “잔류 투표”에 대한 가장 강력한 지지는 그레이터 런던에서 나타났습니다.\n이 논문에서는 2016년 브렉시트 국민투표에서 “탈퇴 투표”의 성과가 농촌성과 그토록 상관관계가 있었던 이유를 살펴봅니다. 우리는 투표 지역 수준에서 “탈퇴 투표”에 대한 지지가 해당 지역의 농장 수, 평균 인터넷 연결성 및 중간 연령으로 설명되는 모델을 구성합니다. 우리는 해당 지역의 중간 연령이 증가함에 따라 해당 지역이 “탈퇴 투표”를 지지할 가능성이 14% 포인트 감소한다는 것을 발견했습니다. 향후 연구에서는 보수당 의원을 두는 것의 효과를 살펴볼 수 있으며, 이를 통해 이러한 효과에 대한 더 미묘한 이해가 가능해질 것입니다.\n이 논문의 나머지 부분은 다음과 같이 구성됩니다. 2절에서는 데이터를 논의하고, 3절에서는 모델을 논의하고, 4절에서는 결과를 제시하고, 마지막으로 5절에서는 우리의 발견과 몇 가지 약점을 논의합니다.\n\n서론은 독립적이어야 하며 독자에게 알아야 할 거의 모든 것을 알려주어야 합니다. 독자는 서론만 읽고도 전체 논문의 모든 주요 측면을 정확하게 파악할 수 있어야 합니다. 서론에 그래프나 표를 포함하는 경우는 드뭅니다. 서론은 논문의 구조를 예고하며 마무리해야 합니다.\n\n\n4.5.4 데이터\n린든 존슨의 전기 작가인 로버트 카로는 전기를 쓸 때 “장소감”을 전달하는 것의 중요성을 설명합니다(Caro 2019, p. 141). 그는 이것을 “책의 행동이 일어나는 물리적 환경: 행동이 일어나는 동안 자신이 직접 있는 것처럼 느낄 수 있을 만큼 충분히 자세하고 명확하게 보는 것”으로 정의합니다. 그는 다음 예를 제공합니다.\n\n레베카가 그 작은 집 현관문을 나섰을 때 아무것도 없었다. 부리에 길고 젖은 것을 물고 바위 뒤로 쏜살같이 달려가는 로드러너나 덤불 주위로 너무 빨리 사라져서 하얀 꼬리만 번쩍 보인 토끼 정도였을까, 그 외에는 아무것도 없었다. 흩어진 나무들의 잎사귀가 살랑거리는 것 외에는 움직임이 없었고, 끊임없이 속삭이는 바람 소리 외에는 소리가 없었다\\(\\dots\\) 레베카가 거의 절망에 빠져 집 뒤 언덕을 올랐을 때, 그 꼭대기에서 본 것은 더 많은 언덕, 끝없이 펼쳐진 언덕, 단 하나의 집도 보이지 않는 언덕\\(\\dots\\) 아무것도 움직이지 않는 언덕, 텅 빈 언덕 위에 텅 빈 하늘, 머리 위를 조용히 맴도는 매 한 마리가 사건이었다. 그러나 무엇보다도 인간적인 것은 아무것도 없었고, 이야기할 사람도 없었다.\nCaro (2019, p. 146)\n\n존슨의 어머니 레베카 베인스 존슨의 상황을 얼마나 철저하게 상상할 수 있겠습니까? 논문을 쓸 때 힐 카운티에 대해 카로가 제공하는 것과 같은 장소감을 데이터에 대해 달성해야 합니다. 가능한 한 명시적으로 그렇게 합니다. 일반적으로 이에 대한 전체 섹션이 있으며 이는 독자에게 가능한 한 가깝게 우리 이야기의 기초가 되는 실제 데이터를 보여주기 위해 설계되었습니다.\n데이터 섹션을 작성할 때 우리는 주장에 대한 중요한 질문, 즉 이것을 어떻게 알 수 있는가에 대한 답을 시작합니다(McPhee 2017, p. 78). 데이터 섹션의 훌륭한 예는 (doll1950smoking에서?) 제공합니다. 그들은 대조군과 치료군 간의 흡연 효과에 관심이 있습니다. 데이터셋을 명확하게 설명한 후 표를 사용하여 관련 교차표를 표시하고 그래프를 사용하여 그룹을 대조합니다.\n데이터 섹션에서는 사용 중인 데이터셋의 변수를 철저히 논의해야 합니다. 사용할 수 있었지만 사용하지 않은 다른 데이터셋이 있는 경우 이를 언급하고 선택을 정당화해야 합니다. 변수가 구성되거나 결합된 경우 이 과정과 동기를 설명해야 합니다.\n독자가 결과의 기초가 되는 데이터가 어떻게 생겼는지 이해하기를 바랍니다. 즉, 분석에 사용된 데이터 또는 가능한 한 가까운 데이터를 그래프로 표시해야 합니다. 그리고 요약 통계 표도 포함해야 합니다. 데이터셋이 다른 출처에서 만들어진 경우 해당 원본 출처의 예를 포함하는 것도 도움이 될 수 있습니다. 예를 들어, 데이터셋이 설문 조사 응답에서 만들어진 경우 기본 설문 조사 질문을 부록에 포함해야 합니다.\n데이터 섹션의 그림과 표에 관해서는 약간의 판단이 필요합니다. 독자는 세부 사항을 이해할 기회가 있어야 하지만 일부는 부록에 더 적합할 수 있습니다. 그림과 표는 사람들에게 이야기를 설득하는 데 중요한 측면입니다. 그래프에서는 데이터를 보여주고 독자가 스스로 결정하도록 할 수 있습니다. 그리고 표를 사용하여 데이터셋을 요약할 수 있습니다. 최소한 모든 변수는 그래프로 표시되고 표로 요약되어야 합니다. 너무 많으면 일부는 부록으로 보내고 중요한 관계는 본문에 표시할 수 있습니다. 그림과 표에는 번호를 매기고 텍스트에서 상호 참조해야 합니다(예: “그림 1은 \\(\\dots\\)를 보여줍니다”, “표 1은 \\(\\dots\\)를 설명합니다”). 모든 그래프와 표에는 주요 측면을 설명하고 추가 세부 정보를 추가하는 텍스트가 함께 제공되어야 합니다.\n?sec-static-communication에서 제목과 레이블을 포함한 그래프와 표의 구성 요소를 논의합니다. 그러나 여기서는 텍스트와 그래프 또는 표 사이에 있는 캡션에 대해 논의할 것입니다. 캡션은 유익하고 독립적이어야 합니다. (borkin2015beyond는?) 시각화가 인식되고 회상되는 방식을 이해하기 위해 시선 추적을 사용합니다. 그들은 캡션이 그림의 핵심 메시지를 명확하게 전달해야 하며 중복성이 있어야 한다는 것을 발견했습니다. Cleveland ([1985년] 1994, p. 57)에서 말했듯이 “그래프, 캡션 및 텍스트 간의 상호 작용은 미묘하지만” 독자는 캡션만 읽고도 그래프나 표가 무엇을 보여주는지 이해할 수 있어야 합니다. 두 줄 길이의 캡션이 반드시 부적절한 것은 아닙니다. 그리고 그래프나 표의 모든 측면을 설명해야 합니다. 예를 들어, Bowley (1901, p. 151)의 ?fig-bowleygraphisnice와 ?fig-bowleytableisnice를 모두 고려하십시오. 명확하고 독립적입니다.\n\n\n\n\n\n\n\n\n\n\n\n(a) 잘 캡션된 그림의 예\n\n\n\n\n\n\n\n\n\n\n\n(b) 잘 캡션된 표의 예\n\n\n\n\n\n\n\n그림 4.5: (bowley의?) 그래프 및 표 예시\n\n\n\n표와 그래프 중 어떤 것을 선택할지는 전달할 정보의 양에 따라 결정됩니다. 일반적으로 요약 통계와 같이 고려해야 할 특정 정보가 있는 경우 표가 좋은 옵션입니다. 독자가 비교하고 추세를 이해하는 데 관심이 있다면 그래프가 좋은 옵션입니다(Gelman, Pasarica, 와/과 Dodhia 2002).\n\n\n4.5.5 모델\n우리는 종종 데이터를 탐색하는 데 사용할 통계 모델을 구축하며 이에 대한 특정 섹션을 갖는 것이 일반적입니다. 최소한 사용 중인 모델을 설명하는 방정식을 지정하고 해당 구성 요소를 평이한 언어와 상호 참조로 설명해야 합니다.\n모델 섹션은 일반적으로 모델을 작성하고 설명하고 정당화하는 것으로 시작합니다. 예상 독자에 따라 약간의 배경 지식이 필요할 수 있습니다. 적절한 수학적 표기법으로 모델을 지정하고 상호 참조한 후 모델의 구성 요소를 정의하고 설명해야 합니다. 표기법의 각 측면을 정의하려고 노력하십시오. 이는 독자에게 모델이 잘 선택되었음을 설득하고 논문의 신뢰성을 높이는 데 도움이 됩니다. 모델의 변수는 데이터 섹션에서 논의된 변수와 일치해야 하며 두 섹션 간에 명확한 연결을 만들어야 합니다.\n특징이 모델에 어떻게 입력되고 그 이유는 무엇인지에 대한 논의가 있어야 합니다. 몇 가지 예는 다음과 같습니다.\n\n왜 연령 그룹 대신 연령을 사용합니까?\n왜 주/도는 수준 효과를 갖습니까?\n왜 성별은 범주형 변수입니까? 일반적으로 우리는 이것이 상황에 적합한 모델이라는 느낌을 전달하려고 합니다. 독자가 데이터 섹션에서 논의된 측면이 내려진 모델링 결정에 어떻게 나타나는지 이해하기를 바랍니다.\n\n모델 섹션은 모델의 기초가 되는 가정에 대한 논의로 마무리되어야 합니다. 또한 대체 모델이나 변형에 대한 간략한 논의도 있어야 합니다. 강점과 약점이 명확하고 독자가 이 특정 모델이 선택된 이유를 알기를 원합니다.\n이 섹션의 어느 시점에서 모델을 실행하는 데 사용된 소프트웨어를 지정하고 모델이 적절하지 않을 수 있는 상황에 대한 생각의 증거를 제공하는 것이 일반적으로 적절합니다. 두 번째 요점은 일반적으로 토론 섹션에서 확장됩니다. 그리고 모델 유효성 검사 및 확인, 모델 수렴 및/또는 진단 문제에 대한 증거가 있어야 합니다. 다시 말하지만, 여기서 균형이 필요하며 이 내용 중 일부는 부록에 더 적절하게 배치될 수 있습니다.\n전문 용어가 사용될 때는 익숙하지 않은 독자를 위해 평이한 언어로 간략하게 설명해야 합니다. 예를 들어, (monicababynames는?) 독자를 끌어들이는 지니 계수에 대한 설명을 통합합니다.\n\n아기 이름의 집중도를 보기 위해 각 국가, 성별 및 연도에 대한 지니 계수를 계산해 보겠습니다. 지니 계수는 빈도 분포 값 간의 분산 또는 불평등을 측정합니다. 0에서 1 사이의 모든 값을 가질 수 있습니다. 소득 분포의 경우 지니 계수 1은 한 사람이 모든 소득을 갖는다는 것을 의미합니다. 이 경우 지니 계수 1은 모든 아기가 같은 이름을 갖는다는 것을 의미합니다. 반대로 지니 계수 0은 이름이 모든 아기에게 고르게 분포되어 있음을 의미합니다.\n\n통계 모델을 포함하지 않는 논문이 있을 수 있습니다. 이 경우 이 “모델” 섹션은 더 광범위한 “방법론” 섹션으로 대체되어야 합니다. 수행된 시뮬레이션을 설명하거나 접근 방식에 대한 더 일반적인 세부 정보를 포함할 수 있습니다.\n\n\n4.5.6 결과\n결과 섹션의 훌륭한 예 두 가지는 (kharecha2013prevented와?) (kiang2021racial에서?) 제공합니다. 결과 섹션에서는 분석 결과를 명확하게 전달하고 함의에 대한 논의에 너무 집중하지 않으려고 합니다. 결과 섹션에는 요약 통계, 표 및 그래프가 필요할 가능성이 높습니다. 이러한 각 측면은 상호 참조되어야 하며 각 그림에 표시된 내용을 자세히 설명하는 텍스트가 함께 제공되어야 합니다. 이 섹션은 결과를 전달해야 합니다. 즉, 결과가 무엇을 의미하는지보다는 결과가 무엇인지에 관심이 있습니다.\n이 섹션에는 일반적으로 모델링을 기반으로 한 계수 추정치의 표와 그래프도 포함됩니다. 추정치의 다양한 특징을 논의하고 모델 간의 차이점을 설명해야 합니다. 데이터의 다른 하위 집합을 별도로 고려할 수 있습니다. 다시 말하지만, 모든 그래프와 표에는 평이한 언어로 된 텍스트가 함께 제공되어야 합니다. 대략적인 지침은 텍스트 양이 표와 그래프가 차지하는 공간의 양과 최소한 같아야 한다는 것입니다. 예를 들어, 계수 추정치 표를 표시하는 데 전체 페이지가 사용되는 경우 해당 표에 대한 약 전체 페이지의 텍스트가 상호 참조되고 함께 제공되어야 합니다.\n\n\n4.5.7 토론\n토론 섹션은 논문의 마지막 섹션일 수 있으며 일반적으로 4~5개의 하위 섹션으로 구성됩니다.\n토론 섹션은 일반적으로 논문에서 수행된 작업에 대한 간략한 요약으로 구성된 하위 섹션으로 시작합니다. 그런 다음 이 논문에서 세상에 대해 배우는 핵심 사항에 할애된 2~3개의 하위 섹션이 이어집니다. 이러한 하위 섹션은 논문에서 전달되는 이야기의 함의를 정당화하거나 자세히 설명하는 주요 기회입니다. 일반적으로 이러한 하위 섹션에는 새로 도입된 그래프나 표가 표시되지 않고 대신 이전 섹션에서 도입된 그래프나 표에서 무엇을 배우는지에 초점을 맞춥니다. 일부 결과는 다른 사람이 발견한 것과 관련하여 논의될 수 있으며 여기서 차이점을 조정하려고 시도할 수 있습니다.\n세상에 대해 배우는 이러한 하위 섹션 다음에는 일반적으로 수행된 작업의 약점에 초점을 맞춘 하위 섹션이 있습니다. 여기에는 사용된 데이터, 접근 방식 및 모델과 같은 측면이 포함될 수 있습니다. 모델의 경우 특히 결과에 영향을 미칠 수 있는 측면에 관심이 있습니다. 이는 기계 학습 모델의 경우 특히 어려울 수 있으며 (realml은?) 고려할 측면에 대한 지침을 제공합니다. 그리고 마지막 하위 섹션은 일반적으로 무엇을 배워야 하는지, 그리고 향후 작업이 어떻게 진행될 수 있는지 명시하는 몇 단락입니다.\n일반적으로 이 섹션은 전체 논문의 25% 이상을 차지할 것으로 예상합니다. 즉, 8페이지 분량의 논문에서는 최소 2페이지의 토론이 필요합니다.\n\n\n4.5.8 간결함, 오타 및 문법\n간결함은 중요합니다. 부분적으로는 독자를 위해 글을 쓰고 독자에게는 다른 우선순위가 있기 때문입니다. 그러나 작가로서 가장 중요한 요점이 무엇인지, 어떻게 가장 잘 뒷받침할 수 있는지, 주장이 가장 약한 부분이 어디인지 고려하도록 강요하기 때문이기도 합니다. 전 캐나다 총리인 장 크레티앵은 Chrétien (2007, p. 105)에서 “\\(\\dots\\) 관리들에게 문서를 2~3페이지로 요약하고 나머지 자료는 배경 정보로 첨부하도록 요청하곤 했다. 곧 이것이 자신이 무엇을 말하는지 정말로 모르는 사람들에게만 문제라는 것을 알게 되었다”고 썼습니다.\n이 경험은 캐나다에만 국한된 것이 아니며 새로운 것도 아닙니다. (institueforgovernment에서?) 전 영국 내각 구성원인 올리버 레트윈은 “일부 부서에서 엄청나게 많은 끔찍한 헛소리가 엄청나게, 거대하게, 엄청나게 길게 나오고 있다”고 설명하며 “길이를 4분의 1로 줄여달라”고 요청한 방법을 설명합니다. 그는 부서들이 중요한 것을 잃지 않고 이 요청을 수용할 수 있다는 것을 발견했습니다. 윈스턴 처칠은 제2차 세계 대전 중에 “실제 요점을 간결하게 설명하는 훈련이 더 명확한 사고에 도움이 될 것”이라고 말하며 간결함을 요청했습니다. 맨해튼 프로젝트의 촉매제가 된 실라르드와 아인슈타인이 FDR에게 보낸 편지는 단 두 페이지였습니다!\n(zinsser는?) 더 나아가 “좋은 글쓰기의 비결”을 “모든 문장을 가장 깨끗한 구성 요소로 벗겨내는 것”이라고 설명합니다. 모든 문장은 본질로 단순화되어야 합니다. 그리고 기여하지 않는 모든 단어는 제거해야 합니다.\n불필요한 단어, 오타 및 문법적 문제는 논문에서 제거해야 합니다. 이러한 실수는 주장의 신뢰성에 영향을 미칩니다. 독자가 맞춤법 검사기를 사용하는 것을 신뢰할 수 없다면 왜 로지스틱 회귀를 사용하는 것을 신뢰해야 합니까? RStudio에는 맞춤법 검사기가 내장되어 있지만 Microsoft Word와 Google Docs는 유용한 추가 검사입니다. Quarto 문서에서 복사하여 Word에 붙여넣은 다음 빨간색과 녹색 줄을 찾고 Quarto 문서에서 수정하십시오.\n우리는 n번째 수준의 문법 내용에 대해 걱정하지 않습니다. 대신, 대화 언어 사용에서 발생하는 문법과 문장 구조에 관심이 있습니다(S. King 2000, p. 118). 편안함을 개발하는 방법은 널리 읽고 다른 사람에게도 작품을 읽어달라고 요청하는 것입니다. 또 다른 유용한 전략은 글을 소리 내어 읽는 것인데, 이는 소리가 어떻게 들리는지에 따라 이상한 문장을 감지하는 데 유용할 수 있습니다. 정기적으로 나타날 작은 측면 중 하나는 1에서 10까지의 모든 숫자는 단어로 작성해야 하고 11 이상은 숫자로 작성해야 한다는 것입니다.\n\n\n4.5.9 규칙\n다양한 저자들이 글쓰기 규칙을 확립했습니다. 여기에는 유명하게도 (politicsandtheenglishlanguage의?) 규칙이 포함되며, 이는 (johnsontheeconomist에?) 의해 재해석되었습니다. 데이터로 이야기를 전달하는 데 초점을 맞춘 글쓰기 규칙을 추가로 재해석하면 다음과 같을 수 있습니다.\n\n독자와 독자의 요구에 집중하십시오. 다른 모든 것은 해설입니다.\n구조를 설정한 다음 이야기를 전달하기 위해 해당 구조에 의존하십시오.\n가능한 한 빨리 초고를 작성하십시오.\n해당 초고를 광범위하게 다시 작성하십시오.\n간결하고 직접적으로 작성하십시오. 가능한 한 많은 단어를 제거하십시오.\n단어를 정확하게 사용하십시오. 예를 들어, 주가는 개선되거나 악화되는 것이 아니라 상승하거나 하락합니다.\n가능한 한 짧은 문장을 사용하십시오.\n전문 용어를 피하십시오.\n마치 작품이 신문 1면에 실릴 것처럼 쓰십시오.\n참신함을 주장하거나 “X를 연구한 최초의 사람”이라고 주장하지 마십시오. 항상 먼저 도착한 다른 사람이 있습니다.\n\n(fiske2021words에는?) 과학 논문에 대한 규칙 목록이 있으며 (pineau2021improving의?) 부록에는 기계 학습 논문에 대한 체크리스트가 있습니다. 그러나 아마도 마지막 말은 (Savage2019에서?) 가져와야 할 것입니다.\n\n논문의 가장 좋은 버전을 쓰려고 노력하십시오. 당신이 좋아하는 버전 말입니다. 익명의 독자를 만족시킬 수는 없지만 자신은 만족시킬 수 있어야 합니다. 당신의 논문은, 바라건대, 후세를 위한 것입니다.\nSavage 와/과 Yeh (2019, p. 442)",
    "crumbs": [
      "소통",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>연구 논문 작성</span>"
    ]
  },
  {
    "objectID": "04-writing_research_ko.html#연습-문제",
    "href": "04-writing_research_ko.html#연습-문제",
    "title": "4  연구 논문 작성",
    "section": "4.6 연습 문제",
    "text": "4.6 연습 문제\n\n연습\n\n(계획) 다음 시나리오를 고려하십시오. 한 아이와 부모가 아파트 창문에서 전차를 봅니다. 8시간 동안 매시간 지나가는 전차 수를 기록합니다. 데이터셋이 어떻게 생겼는지 스케치한 다음 모든 관찰 결과를 보여주는 그래프를 스케치하십시오.\n(시뮬레이션) 설명된 시나리오를 더 자세히 고려하고 상황을 시뮬레이션하십시오. 그런 다음 시뮬레이션된 데이터를 기반으로 5가지 테스트를 작성하십시오.\n(획득) 관심 있는 도시의 대중교통의 일부 측면에 대한 실제 데이터 출처를 지정하십시오.\n(탐색) 시뮬레이션된 데이터를 사용하여 그래프와 표를 만드십시오.\n(공유) 실제 상황을 반영하는 것처럼 그래프와 표에 첨부할 텍스트를 작성하십시오. 단락에 포함된 정확한 세부 정보는 사실일 필요는 없지만 합리적이어야 합니다(즉, 실제로 데이터를 가져오거나 그래프를 만들 필요는 없습니다). 코드를 R 파일과 Quarto 문서로 적절하게 분리하십시오. README가 있는 GitHub 리포지토리에 대한 링크를 제출하십시오.\n\n\n\n퀴즈\n\n좋은 연구 질문의 세 가지 특징은 무엇입니까 (한두 단락 작성)?\n(bydesignplanningresearch는?) 광범위한 주제에서 상세한 연구 계획으로 나아가는 방법을 어떻게 권장합니까 (하나 선택)?\n\n전문가와 상담하십시오.\n사용 가능한 데이터를 식별하십시오.\n구체적인 연구 질문 세트를 명확히 하십시오.\n\n(bydesignplanningresearch가?) 연구 질문이 매우 중요하다고 믿는 이유는 무엇입니까 (모두 선택)?\n\n합리적인 계획 결정을 내릴 수 있는 유일한 근거입니다.\n표본을 추출할 대상 모집단을 식별합니다.\n적절한 집계 수준을 결정합니다.\n결과 변수를 식별합니다.\n주요 예측 변수를 식별합니다.\n측정 및 데이터 수집에 대한 과제를 제기합니다.\n\n(bydesignplanningresearch에?) 따르면 연구에서 “이론과 데이터의 나선”의 목적은 무엇입니까 (하나 선택)?\n\n이론을 개발하기 전에 데이터를 수집하기 위해.\n이론과 데이터 사이를 이동하여 둘 다 반복적으로 개선하기 위해.\n이론적 분석 전에 데이터 수집이 완료되도록 보장하기 위해.\n데이터 없이 이론적 프레임워크에만 집중하기 위해.\n\n연구 접근 방식의 맥락에서 데이터 우선이란 무엇을 의미합니까 (하나 선택)?\n\n데이터 가용성을 고려하지 않고 연구 질문을 개발하는 것.\n미리 정의된 질문에 답하기 위해 특별히 설계된 새 데이터를 수집하는 것.\n경험적 증거보다 이론적 프레임워크를 우선시하는 것.\n사용 가능한 데이터로 시작한 다음 답할 수 있는 질문을 결정하는 것.\n\n데이터 우선 접근 방식의 이점은 무엇입니까 (하나 선택)?\n\n이론적 프레임워크의 필요성을 제거합니다.\n연구자가 사용 가능한 데이터를 기반으로 질문을 공식화할 수 있도록 합니다.\n인과 관계를 확립할 수 있음을 보장합니다.\n연구에서 모든 형태의 편향을 방지합니다.\n\n데이터 우선 접근 방식의 단점은 무엇입니까 (하나 선택)?\n\n“가로등 아래에서 찾는 것”에 대한 우려.\n이론에 기여할 수 있는지에 대한 우려.\n인과 관계를 분리하기 어려울 것이라는 우려.\n외부 타당성에 대한 우려.\n\n반사실이란 무엇입니까 (예와 참고 문헌을 포함하고 최소 3단락 작성)?\n반사실이란 무엇입니까 (하나 선택)?\n\n주요 이론과 모순되는 대안 가설.\n만약이 거짓인 만약-그러면 문장.\n논문의 주요 주장에 반하는 사실.\n교란 변수를 조정하는 데 사용되는 통계적 방법.\n\n“FINER” 프레임워크는 무엇을 의미합니까 (하나 선택)?\n\n유연하고, 혁신적이며, 중립적이고, 경험적이며, 재현 가능합니다.\n형식적이고, 해석적이며, 새롭고, 실험적이며, 견고합니다.\n집중적이고, 통합적이며, 자연스럽고, 효율적이며, 신뢰할 수 있습니다.\n실행 가능하고, 흥미롭고, 새롭고, 윤리적이며, 관련성이 있습니다.\n\n추정 대상이란 무엇입니까 (하나 선택)?\n\n오류로 측정되는 변수.\n편향된 추정량.\n데이터를 사용하여 추정치를 계산하는 프로세스.\n추정하려는 실제 효과 또는 관심 수량.\n\n추정 대상이란 무엇입니까 (하나 선택)?\n\n관찰된 데이터를 기반으로 주어진 수량의 추정치를 계산하는 규칙.\n조사 대상.\n특정 데이터셋과 접근 방식이 주어졌을 때의 결과.\n\n추정량이란 무엇입니까 (하나 선택)?\n\n관찰된 데이터를 기반으로 주어진 수량의 추정치를 계산하는 규칙.\n조사 대상.\n특정 데이터셋과 접근 방식이 주어졌을 때의 결과.\n\n추정량의 역할은 무엇입니까 (하나 선택)?\n\n추정하려는 실제 효과입니다.\n데이터에서 추정치를 계산하는 규칙 또는 방법입니다.\n데이터셋과 방법이 주어졌을 때 계산된 값입니다.\n통계 모델의 오차항입니다.\n\n추정치란 무엇입니까 (하나 선택)?\n\n관찰된 데이터를 기반으로 주어진 수량의 추정치를 계산하는 규칙.\n조사 대상.\n특정 데이터셋과 접근 방식이 주어졌을 때의 결과.\n\n선택 편향이란 무엇입니까 (하나 선택)?\n\n참가자가 시간이 지남에 따라 연구에서 탈락할 때.\n데이터 측정 방식에 따라 결과가 영향을 받을 때.\n표본이 모집단을 대표하지 않을 때.\n실험에서 변수가 제대로 통제되지 않을 때.\n\n측정 편향이란 무엇입니까 (하나 선택)?\n\n장비 고장으로 인해 데이터가 부정확하게 기록될 때.\n데이터 수집 방법이 체계적으로 실제 값보다 높거나 낮게 평가할 때.\n측정 과정이 결과에 영향을 미칠 때.\n표본 크기가 너무 작아 결론을 도출할 수 없을 때.\n\n방향성 비순환 그래프(DAG)의 목적은 무엇입니까 (하나 선택)?\n\n복잡한 모집단에서 무작위 표본을 생성하기 위해.\n비선형 데이터에 대한 통계적 검정을 수행하기 위해.\n통계 모델을 자동으로 생성하기 위해.\n변수 간의 인과 관계를 시각적으로 표현하기 위해.\n\nDAG 구축의 이점은 무엇입니까 (하나 선택)?\n\n데이터에서 인과 관계를 자동으로 식별합니다.\n통계 분석의 필요성을 제거합니다.\n연구자가 변수 관계에 대해 신중하게 생각하도록 돕습니다.\n인과 효과의 정확한 추정치를 제공합니다.\n\n교란 변수란 무엇입니까 (하나 선택)?\n\n예측 변수와 결과 변수 모두에 영향을 받는 변수.\n예측 변수와 결과 변수 모두에 영향을 미치는 변수.\n예측 변수에 영향을 받고 결과 변수에 영향을 미치는 변수.\n\n매개 변수란 무엇입니까 (하나 선택)?\n\n예측 변수와 결과 변수 모두에 영향을 받는 변수.\n예측 변수와 결과 변수 모두에 영향을 미치는 변수.\n예측 변수에 영향을 받고 결과 변수에 영향을 미치는 변수.\n\n충돌 변수란 무엇입니까 (하나 선택)?\n\n예측 변수와 결과 변수 모두에 영향을 받는 변수.\n예측 변수와 결과 변수 모두에 영향을 미치는 변수.\n예측 변수에 영향을 받고 결과 변수에 영향을 미치는 변수.\n\n(zinsser의?) 2장에 따르면 좋은 글쓰기의 비결은 무엇입니까 (하나 선택)?\n\n올바른 문장 구조와 문법.\n긴 단어, 부사 및 수동태 사용.\n모든 문장을 가장 깨끗한 구성 요소로 벗겨내는 것.\n철저한 계획.\n\n(zinsser의?) 2장에 따르면 작가는 끊임없이 무엇을 물어야 합니까 (하나 선택)?\n\n누구를 위해 글을 쓰고 있습니까?\n무엇을 말하려고 합니까?\n이것을 어떻게 다시 쓸 수 있습니까?\n이것이 왜 중요합니까?\n\n논문 작성 과정에서 중요한 작업 중 하나는 무엇입니까 (하나 선택)?\n\n글쓰기를 시작하기 전에 가능한 한 많은 데이터를 수집하는 것.\n초고의 각 문장을 완성하는 데 많은 시간을 할애하는 것.\n가능한 한 빨리 초고를 완성하는 것.\n글쓰기 전에 상세한 그래프와 표를 만드는 데 집중하는 것.\n\n초고를 빨리 완성하는 것이 중요한 이유는 무엇입니까 (하나 선택)?\n\n초기 초고에 실수가 없도록 보장합니다.\n수정하고 개선할 수 있는 완전한 버전을 제공합니다.\n작가가 진행하면서 각 문장을 완성할 수 있도록 합니다.\n글쓰기에 소요되는 전체 시간을 줄입니다.\n\n“사랑하는 것을 죽여라”는 무엇을 의미합니까 (하나 선택)?\n\n논란의 여지가 있는 주제에 대해 글을 쓰지 않는 것.\n가혹한 비판을 사용하여 작업을 개선하는 것.\n좋아하지만 주요 이야기에 도움이 되지 않는 불필요한 내용을 제거하는 것.\n전체 초고를 처음부터 다시 쓰는 것.\n\n독자에게 초점을 맞출 때에도 작가에게 글쓰기의 주요 이점 중 하나는 무엇입니까 (하나 선택)?\n\n작가가 논문을 다시 쓰는 것을 피할 수 있도록 합니다.\n작가가 자신이 무엇을 믿는지, 그리고 어떻게 믿게 되었는지 알아내는 데 도움이 됩니다.\n동료로부터 필요한 피드백 양을 줄입니다.\n작가의 작업이 출판되도록 보장합니다.\n\n예를 들어 3장에서 (zinsser의?) 조언을 특징짓는 두 개의 반복되는 단어는 무엇입니까 (하나 선택)?\n\n다시 쓰기, 다시 쓰기.\n단순화, 단순화.\n제거, 제거.\n덜, 덜.\n\n논문에서 불필요한 단어, 오타 및 문법적 문제를 제거하는 주된 이유는 무엇입니까 (하나 선택)?\n\n단어 수 제한을 충족하기 위해.\n고급 어휘로 검토자를 감동시키기 위해.\n논문을 더 길게 만들기 위해.\n주장의 신뢰성을 높이기 위해.\n\n다음 중 가장 좋은 제목은 무엇입니까 (하나 선택)?\n\n“소규모 표본에서 얻은 추정치의 표준 오차”\n“표준 오차”\n“문제 세트 2”\n\n제목을 쓰는 한 가지 전략은 무엇입니까 (하나 선택)?\n\n전문 독자를 감동시키기 위해 전문 용어 사용.\n일반적인 주제와 주요 결과에 대한 구체적인 정보를 모두 포함합니다.\n한두 단어만 사용하여 가능한 한 짧게 만듭니다.\n독자의 참여를 유도하기 위해 제목을 질문으로 제시합니다.\n\n(fourcade2017seeing에?) 대한 새 제목을 작성하십시오.\n다음 중 초록을 작성할 때 권장되지 않는 것은 무엇입니까 (하나 선택)?\n\n핵심 사항을 설명하기 위해 그림이나 표 추가.\n주요 결과 및 함의 포함.\n정확하고 간결한 언어 사용.\n초록을 독립적으로 만듭니다.\n\n초록을 작성하는 일반적인 구조는 무엇입니까 (하나 선택)?\n\n함의로 시작하여 방법, 맥락으로 끝납니다.\n첫 번째 문장은 일반적인 영역, 두 번째 문장은 방법, 세 번째 문장은 주요 결과, 네 번째 문장은 함의에 관한 것입니다.\n한계로 시작하여 데이터 출처, 결과 순으로 이어집니다.\n논문이 답할 일련의 질문.\n\nXKCD 단순 작성기에 따르면 영어에서 가장 많이 사용되는 1,000개 단어만 사용하여 (chambliss1989mundanity의?) 초록을 원래 의미를 유지하도록 다시 작성하십시오.\n(king2006publication에?) 따르면 부제목의 주요 임무는 무엇입니까 (하나 선택)?\n\n논문을 문헌에 통합하기 위해 약어 사용.\n독자가 논문의 중요성에 감탄하도록 광범위하고 포괄적으로 작성.\n무작위로 잠들지만 계속 페이지를 넘기는 독자가 자신이 어디에 있는지 알 수 있도록 합니다.\n\n데이터 섹션에서 무엇을 달성하고 싶습니까 (하나 선택)?\n\n독자를 감동시키기 위해 데이터의 복잡성을 보여주기 위해.\n데이터를 철저히 설명하여 장소감을 조성하기 위해.\n가능한 한 많은 그래프와 표를 포함하기 위해.\n데이터의 약점을 숨기기 위해.\n\n연구 논문에서 데이터 섹션의 주요 목표는 무엇입니까 (하나 선택)?\n\n가능한 한 많은 표와 그래프를 제시하기 위해.\n독자가 결과의 기초를 이해하도록 데이터를 철저히 설명하기 위해.\n분석의 복잡성에 대해 독자를 설득하기 위해.\n사용하지 않은 데이터 출처를 포함하여 가능한 모든 데이터 출처를 논의하기 위해.\n\n(king2006publication에?) 따르면 표준 오차가 0.05인 경우 다음 계수 특정성 중 어리석은 것은 무엇입니까 (모두 선택)?\n\n2.7182818\n2.718282\n2.71828\n2.7183\n2.718\n2.72\n2.7\n3\n\n좋은 그림이나 표 캡션은 무엇을 달성해야 합니까 (하나 선택)?\n\n가능한 한 간결하게, 이상적으로는 한 줄로 작성합니다.\n독립적이며 주요 내용을 설명합니다.\n전문성을 보여주기 위해 복잡한 전문 용어를 포함합니다.\n독자가 텍스트를 읽도록 장려하기 위해 최소한의 정보를 제공합니다.\n\n모델 섹션에는 무엇이 있어야 합니까 (하나 선택)?\n\n문헌에서 사용된 다른 모델 요약.\n방정식 없이 최종 결과만.\n수학적 표기법 없는 일반적인 설명.\n방정식, 설명 및 모든 구성 요소의 정의.\n\n모델 섹션에서 대체 모델이나 변형을 논의하는 것이 중요한 이유는 무엇입니까 (하나 선택)?\n\n철저한 고려를 보여주고 선택한 모델을 정당화하기 위해.\n다른 모델이 열등하다는 것을 보여주기 위해.\n여러 옵션으로 독자를 혼란스럽게 하기 위해.\n논문의 길이를 늘리기 위해.\n\n결과 섹션의 목적은 무엇입니까 (하나 선택)?\n\n결과를 해석하고 그 함의를 논의합니다.\n다른 연구자의 결과를 비판합니다.\n향후 연구 방향을 제안합니다.\n광범위한 해석 없이 분석 결과를 명확하게 제시합니다.\n\n결과 섹션에서 그래프와 표는 어떻게 통합되어야 합니까 (하나 선택)?\n\n첨부된 텍스트 없이 독립적으로 존재해야 합니다.\n혼란을 피하기 위해 최소화해야 합니다.\n텍스트에서 상호 참조되고 논의되어야 합니다.\n흐름을 방해하지 않도록 부록에 배치해야 합니다.\n\n토론 섹션의 목적은 무엇입니까 (하나 선택)?\n\n결과를 더 자세히 반복하기 위해.\n상세한 방법론을 제공하기 위해.\n결과를 해석하고, 함의를 논의하고, 약점을 인정하기 위해.\n해결책을 제시하지 않고 연구의 모든 한계를 나열하기 위해.\n\n(Savage2019는?) 왜 그 구두점, 그 단어, 그 문장, 그 단락 또는 그 섹션 없이 원래 메시지를 보존할 수 있는지 자문해 보라고 권장합니까 (하나 선택)?\n\n오류 가능성을 줄이기 위해.\n명확성을 달성하기 위해.\n논문을 짧게 유지하기 위해.\n\n재작성 과정의 주요 측면은 무엇입니까 (모두 선택)?\n\n불필요한 단어를 제거하기 위해 빨간 펜으로 검토합니다.\n논문을 인쇄하고 실제 사본을 읽습니다.\n흐름을 향상시키기 위해 잘라내고 붙여넣습니다.\n소리 내어 읽습니다.\n다른 사람과 교환합니다.\n\n잘못된 문법과 오타가 논문의 신뢰성에 영향을 미치는 이유는 무엇입니까 (하나 선택)?\n\n논문의 편안한 분위기를 향상시킵니다.\n논문을 더 짧게 만듭니다.\n세부 사항에 대한 주의 부족을 나타냅니다.\n내용이 좋으면 허용됩니다.\n\n\n\n\n수업 활동\n\n연구에 대한 선호하는 접근 방식(데이터 우선/질문 우선/기타)과 그 이유를 논의하십시오.\n예를 참조하여 추정 대상, 추정량 및 추정치가 무엇인지 설명하십시오.\n“선택 편향”을 고려하고 (monicababynames가?) 지니 계수에 대해 하는 것과 같은 방식으로 정의를 문장에 포함하십시오.\nChatGPT 또는 동등한 LLM을 사용하여 “선택 효과란 무엇입니까?”라는 질문에 답하는 프롬프트를 만드십시오. 파트너와 함께 컨텍스트, 참고 문헌을 추가하고 (필요한 경우) 사실로 만들어 응답을 개선하십시오. 세 가지 측면을 논의하십시오. 1) 프롬프트, 2) 원래 답변, 3) 보강된 답변.\n잘 쓰여진 정량적 논문 중 하나를 선택하십시오.\n\n원본 제목을 적으십시오. 마음에 드는 점과 마음에 들지 않는 점은 무엇입니까? 이에 대한 대체 제목을 작성하십시오.\n초록을 적으십시오. 마음에 드는 점과 마음에 들지 않는 점은 무엇입니까?\nChatGPT 또는 동등한 LLM에 대체 초록을 만들도록 프롬프트를 작성하십시오(논의할 수 있도록 프롬프트를 복사하십시오).\n이 모든 것을 바탕으로 개선된 초록을 작성한 다음 모든 것을 논의하십시오.\n\n이 수업이 끝날 때까지 의미 있는 논문을 작성하는 방법에 대해 (king2006publication을?) 기반으로 계획을 세우십시오. (박사 과정 학생의 경우: 제출할 3개의 저널/컨퍼런스를 순서대로 자세히 설명하고 각 저널/컨퍼런스에 논문이 적합한 이유를 설명하십시오.)\n논문 검토: (Gerring2012를?) 읽고 한 페이지 분량의 검토를 작성하십시오.\n\n\n\n과제\nCaro (2019, p. xii)은 거의 매일 최소 1,000단어를 씁니다. 이 과제의 목적은 여러분에게도 그렇게 할 기회를 제공하는 것입니다. 전제 조건에 명시된 논문 중 하나를 선택하고 다음 과제를 완료하십시오.\n\n1일차: 전체 서론을 직접 각 단어를 써서 필사하십시오.\n2일차: 서론을 5줄(또는 10%, 둘 중 더 적은 쪽) 짧게 다시 작성하십시오.\n3일차: 초록 전체를 직접 각 단어를 써서 필사하십시오.\n4일차: 논문에 대한 새로운 4문장 초록을 다시 작성하십시오.\n5일차: 여기에 정의된 영어에서 가장 많이 사용되는 1,000개 단어만 사용하여 새 초록의 두 번째 버전을 작성하십시오.\n6일차: 논문 작성 방식에 대해 마음에 드는 점 3가지를 자세히 설명하십시오.\n7일차: 논문 작성 방식에 대해 마음에 들지 않는 점 1가지를 자세히 설명하십시오.\n\nQuarto를 사용하여 일주일 동안 단일 PDF를 작성하십시오. 매일 작업 후 정보성 커밋 메시지와 함께 작업을 커밋하고 푸시하십시오. 제출물을 구성하기 위해 제목과 부제목을 활용하십시오. 고품질 리포지토리 링크를 제출하십시오.\n\n\n\n\nAngrist, Joshua, 와/과 Jörn-Steffen Pischke. 2010. “The credibility revolution in empirical economics: How better research design is taking the con out of econometrics”. Journal of Economic Perspectives 24 (2): 3–30. https://doi.org/10.1257/jep.24.2.3.\n\n\nArel-Bundock, Vincent. 2024. tinytable: Simple and Configurable Tables in “HTML”, “LaTeX”, “Markdown”, “Word”, “PNG”, “PDF”, and “Typst” Formats. https://vincentarelbundock.github.io/tinytable/.\n\n\nBarrett, Malcolm. 2021. ggdag: Analyze and Create Elegant Directed Acyclic Graphs. https://CRAN.R-project.org/package=ggdag.\n\n\nBarron, Alexander, Jenny Huang, Rebecca Spang, 와/과 Simon DeDeo. 2018. “Individuals, institutions, and innovation in the debates of the French Revolution”. Proceedings of the National Academy of Sciences 115 (18): 4607–12. https://doi.org/10.1073/pnas.1717729115.\n\n\nBickel, Peter, Eugene Hammel, 와/과 William O’Connell. 1975. “Sex Bias in Graduate Admissions: Data from Berkeley: Measuring bias is harder than is usually assumed, and the evidence is sometimes contrary to expectation”. Science 187 (4175): 398–404. https://doi.org/10.1126/science.187.4175.398.\n\n\nBirkmeyer, John, Jonathan Finks, Amanda O’Reilly, Mary Oerline, Arthur Carlin, Andre Nunn, Justin Dimick, Mousumi Banerjee, 와/과 Nancy Birkmeyer. 2013. “Surgical Skill and Complication Rates after Bariatric Surgery”. New England Journal of Medicine 369 (15): 1434–42. https://doi.org/10.1056/nejmsa1300625.\n\n\nBland, Martin, 와/과 Douglas Altman. 1986. “Statistical methods for assessing agreement between two methods of clinical measurement”. The Lancet 327 (8476): 307–10. https://doi.org/10.1016/S0140-6736(86)90837-8.\n\n\nBowley, Arthur Lyon. 1901. Elements of Statistics. London: P. S. King.\n\n\nBronner, Laura. 2021. “Quantitative Editing”. YouTube, 6월. https://youtu.be/LI5m9RzJgWc.\n\n\nBrontë, Charlotte. 1857. The Professor. https://www.gutenberg.org/files/1028/1028-h/1028-h.htm.\n\n\nBueno de Mesquita, Ethan, 와/과 Anthony Fowler. 2021. Thinking clearly with data: A guide to quantitative reasoning and analysis. New Jersey: Princeton University Press.\n\n\nCaro, Robert. 2019. Working. 1st ed. New York: Knopf.\n\n\nCarroll, Lewis. 1871. Through the Looking-Glass. Macmillan. https://www.gutenberg.org/files/12/12-h/12-h.htm.\n\n\nChambliss, Daniel. 1989. “The Mundanity of Excellence: An Ethnographic Report on Stratification and Olympic Swimmers”. Sociological Theory 7 (1): 70–86. https://doi.org/10.2307/202063.\n\n\nChrétien, Jean. 2007. My Years as Prime Minister. 1st ed. Toronto: Knopf Canada.\n\n\nCleveland, William. (1985년) 1994. The Elements of Graphing Data. 2nd ed. New Jersey: Hobart Press.\n\n\nDoll, Richard, 와/과 Bradford Hill. 1950. “Smoking and Carcinoma of the Lung”. British Medical Journal 2 (4682): 739–48. https://doi.org/10.1136/bmj.2.4682.739.\n\n\nFourcade, Marion, 와/과 Kieran Healy. 2017. “Seeing like a market”. Socio-Economic Review 15 (1): 9–29. https://doi.org/10.1093/ser/mww033.\n\n\nFranklin, Laura. 2005. “Exploratory experiments”. Philosophy of Science 72 (5): 888–99. https://doi.org/10.1086/508117.\n\n\nGelman, Andrew, 와/과 Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Models. 1st ed. Cambridge University Press.\n\n\nGelman, Andrew, Cristian Pasarica, 와/과 Rahul Dodhia. 2002. “Let’s Practice What We Preach: Turning Tables into Graphs”. The American Statistician 56 (2): 121–30. https://doi.org/10.1198/000313002317572790.\n\n\nGraham, Paul. 2020. “How to write usefully”, 2월. http://paulgraham.com/useful.html.\n\n\nGustafsson, Karl, 와/과 Linus Hagström. 2017. “what is the point? teaching graduate students how to construct political science research puzzles”. European Political Science 17 (4): 634–48. https://doi.org/10.1057/s41304-017-0130-y.\n\n\nHayot, Eric. 2014. The Elements of Academic Style. New York: Columbia University Press.\n\n\nHernán, Miguel, 와/과 James Robins. 2023. What If. 1st ed. Boca Raton: Chapman & Hall/CRC. https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/.\n\n\nHulley, Stephen, Steven Cummings, Warren Browner, Deborah Grady, 와/과 Thomas Newman. 2007. Designing clinical research. 3rd ed. Lippincott Williams & Wilkins.\n\n\nIannone, Richard. 2022. DiagrammeR: Graph/Network Visualization. https://CRAN.R-project.org/package=DiagrammeR.\n\n\nJoyner, Michael. 1991. “Modeling: optimal marathon performance on the basis of physiological factors”. Journal of Applied Physiology 70 (2): 683–87. https://doi.org/10.1152/jappl.1991.70.2.683.\n\n\nKahan, Brennan, Suzie Cro, Fan Li, 와/과 Michael Harhay. 2023. “Eliminating ambiguous treatment effects using estimands”. American Journal of Epidemiology, 2월. https://doi.org/10.1093/aje/kwad036.\n\n\nKahan, Brennan, Joanna Hindley, Mark Edwards, Suzie Cro, 와/과 Tim Morris. 2024. “The estimands framework: a primer on the ICH E9(R1) addendum”. BMJ, 1월, e076316. https://doi.org/10.1136/bmj-2023-076316.\n\n\nKahan, Brennan, Fan Li, Andrew Copas, 와/과 Michael Harhay. 2022. “Estimands in cluster-randomized trials: choosing analyses that answer the right question”. International Journal of Epidemiology, 7월. https://doi.org/10.1093/ije/dyac131.\n\n\nKeshav, Srinivasan. 2007. “How to read a paper”. ACM SIGCOMM Computer Communication Review 37 (3): 83–84. https://doi.org/10.1145/1273445.1273458.\n\n\nKharecha, Pushker, 와/과 James Hansen. 2013. “Prevented Mortality and Greenhouse Gas Emissions from Historical and Projected Nuclear Power”. Environmental Science & Technology 47 (9): 4889–95. https://doi.org/10.1021/es3051197.\n\n\nKing, Gary. 2006. “Publication, Publication”. PS: Political Science & Politics 39 (1): 119–25. https://doi.org/10.1017/S1049096506060252.\n\n\nKing, Stephen. 2000. On Writing: A Memoir of the Craft. 1st ed. Scribner.\n\n\nKoenker, Roger, 와/과 Achim Zeileis. 2009. “On reproducible econometric research”. Journal of Applied Econometrics 24 (5): 833–47. https://doi.org/10.1002/jae.1083.\n\n\nLamott, Anne. 1994. Bird by Bird: Some instructions on writing and life. Anchor Books.\n\n\nLatour, Bruno. 1996. “On actor-network theory: A few clarifications”. Soziale Welt 47 (4): 369–81. http://www.jstor.org/stable/40878163.\n\n\nLight, Richard, Judith Singer, 와/과 John Willett. 1990. By Design: Planning Research on Higher Education. 1st ed. Cambridge: Harvard University Press.\n\n\nLittle, Roderick, 와/과 Roger Lewis. 2021. “Estimands, Estimators, and Estimates”. JAMA 326 (10): 967. https://doi.org/10.1001/jama.2021.2886.\n\n\nLucas, Robert. 1978. “Asset prices in an exchange economy”. Econometrica 46 (6): 1429–45. https://doi.org/10.2307/1913837.\n\n\nLundberg, Ian, Rebecca Johnson, 와/과 Brandon Stewart. 2021. “What Is Your Estimand? Defining the Target Quantity Connects Statistical Evidence to Theory”. American Sociological Review 86 (3): 532–65. https://doi.org/10.1177/00031224211004187.\n\n\nMcElreath, Richard. (2015년) 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. 2nd ed. Chapman; Hall/CRC.\n\n\nMcPhee, John. 2017. Draft No. 4. 1st ed. Farrar, Straus; Giroux.\n\n\nMeng, Xiao-Li. 1994. “Multiple-Imputation Inferences with Uncongenial Sources of Input”. Statistical Science 9 (4): 538–58. https://doi.org/10.1214/ss/1177010269.\n\n\n———. 2012. “You Want Me to Analyze Data I Don’t Have? Are You Insane?” Shanghai Archives of Psychiatry 24 (5): 297–301. https://doi.org/10.3969/j.issn.1002-0829.2012.05.011.\n\n\n———. 2018. “Statistical paradises and paradoxes in big data (I): Law of large populations, big data paradox, and the 2016 US presidential election”. The Annals of Applied Statistics 12 (2): 685–726. https://doi.org/10.1214/18-AOAS1161SF.\n\n\nRosenau, James N. 1999. “A Transformed Observer In A Transforming World”. Studia Diplomatica 52 (1/2): 5–14. http://www.jstor.org/stable/44838096.\n\n\nSamuel, Arthur. 1959. “Some studies in machine learning using the game of checkers”. IBM Journal of research and development 3 (3): 210–29. https://doi.org/10.1147/rd.33.0210.\n\n\nSavage, Van, 와/과 Pamela Yeh. 2019. “Novelist Cormac McCarthy’s tips on how to write a great science paper”. Nature 574 (7778): 441–42. https://doi.org/10.1038/d41586-019-02918-5.\n\n\nSen, Amartya. 1980. “Description as Choice”. Oxford Economic Papers 32 (3): 353–69. https://doi.org/10.1093/oxfordjournals.oep.a041484.\n\n\nStudent. 1908. “The probable error of a mean”. Biometrika 6 (1): 1–25. https://doi.org/10.2307/2331554.\n\n\nWardrop, Robert. 1995. “Simpson’s paradox and the hot hand in basketball”. The American Statistician 49 (1): 24–28. https://doi.org/10.2307/2684806.\n\n\nWickham, Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, 기타. 2019. “Welcome to the Tidyverse”. Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nXie, Yihui. 2023. knitr: A General-Purpose Package for Dynamic Report Generation in R. https://yihui.org/knitr/.\n\n\nZinsser, William. 1976. On Writing Well. New York: HarperCollins.",
    "crumbs": [
      "소통",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>연구 논문 작성</span>"
    ]
  },
  {
    "objectID": "04-writing_research_ko.html#footnotes",
    "href": "04-writing_research_ko.html#footnotes",
    "title": "4  연구 논문 작성",
    "section": "",
    "text": "때로는 별도의 문헌 검토 섹션이 필요할 수도 있지만, 또 다른 접근 방식은 필요에 따라 논문 전체에 걸쳐 관련 문헌을 논의하는 것입니다. 예를 들어, 데이터와 관련된 문헌이 있는 경우 이 섹션에서 논의해야 하며, 모델, 결과 또는 토론과 관련된 문헌은 해당 섹션에서 적절하게 언급해야 합니다.↩︎",
    "crumbs": [
      "소통",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>연구 논문 작성</span>"
    ]
  },
  {
    "objectID": "01-introduction_ko.html",
    "href": "01-introduction_ko.html",
    "title": "1  데이터로 이야기하기",
    "section": "",
    "text": "1.1 이야기하기에 관하여\n선행 학습\n많은 부모들이 자녀가 태어나면 가장 먼저 하는 일 중 하나는 이야기를 읽어주는 것입니다. 그렇게 함으로써 그들은 수천 년 동안 이어져 온 전통을 이어갑니다. 신화, 우화, 동화는 우리 주변 어디에서나 보고 들을 수 있습니다. 그것들은 재미있을 뿐만 아니라 우리가 세상에 대해 배울 수 있게 해줍니다. 에릭 칼의 매우 배고픈 애벌레는 데이터를 다루는 세계와는 상당히 거리가 멀어 보일 수 있지만, 유사점이 있습니다. 둘 다 이야기를 하고 지식을 전달하는 것을 목표로 합니다.\n데이터를 사용할 때 우리는 설득력 있는 이야기를 하려고 노력합니다. 그것은 선거 예측만큼 흥미로울 수도 있고, 인터넷 광고 클릭률 증가만큼 평범할 수도 있고, 질병의 원인을 찾는 것만큼 심각할 수도 있고, 농구 경기 예측만큼 재미있을 수도 있습니다. 어떤 경우든 핵심 요소는 동일합니다. 20세기 초 영국 작가 E. M. 포스터는 모든 소설에 공통적인 측면을 이야기, 인물, 줄거리, 환상, 예언, 패턴, 리듬으로 묘사했습니다(Forster 1927). 마찬가지로, 설정에 관계없이 데이터로 이야기를 할 때 공통적인 관심사가 있습니다.\n과거에는 데이터로 이야기를 하는 특정 요소가 더 쉬웠습니다. 예를 들어, 실험 설계는 농업 및 의학, 물리학, 화학 분야에서 길고 견고한 전통을 가지고 있습니다. 스튜던트의 t-분포는 1900년대 초 맥주 제조업체인 기네스에서 일했던 화학자 윌리엄 실리 고셋에 의해 확인되었습니다(Boland 1984). 그가 맥주를 무작위로 샘플링하고 한 번에 한 가지 측면을 변경하는 것은 비교적 간단했을 것입니다.\n오늘날 우리가 사용하는 통계 방법의 많은 기본 사항은 그러한 환경에서 개발되었습니다. 그러한 상황에서는 일반적으로 통제 그룹을 설정하고 무작위화하는 것이 가능했으며 윤리적 우려도 적었습니다. 그 결과 데이터로 만들어진 이야기는 상당히 설득력이 있었을 것입니다.\n불행히도, 통계 방법이 적용되는 설정의 다양성을 고려할 때 오늘날에는 이러한 것이 거의 적용되지 않습니다. 반면에 우리는 많은 이점을 가지고 있습니다. 예를 들어, 우리는 잘 개발된 통계 기법, 대규모 데이터셋에 대한 더 쉬운 접근, R 및 Python과 같은 오픈 소스 언어를 가지고 있습니다. 그러나 전통적인 실험을 수행하는 것이 어렵다는 것은 설득력 있는 이야기를 하기 위해 다른 측면에도 의존해야 함을 의미합니다.",
    "crumbs": [
      "기초",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>데이터로 이야기하기</span>"
    ]
  },
  {
    "objectID": "01-introduction_ko.html#이야기하기에-관하여",
    "href": "01-introduction_ko.html#이야기하기에-관하여",
    "title": "1  데이터로 이야기하기",
    "section": "",
    "text": "데이터셋은 무엇인가? 누가 데이터셋을 생성했고 왜 생성했는가?\n데이터셋의 기반이 되는 프로세스는 무엇인가? 해당 프로세스를 고려할 때 데이터셋에서 누락되었거나 제대로 측정되지 않은 것은 무엇인가? 다른 데이터셋이 생성될 수 있었고, 만약 그렇다면 우리가 가진 것과 얼마나 다를 수 있었는가?\n데이터셋은 무엇을 말하려고 하며, 어떻게 그렇게 말하게 할 수 있는가? 그 밖에 무엇을 말할 수 있는가? 이들 중에서 어떻게 결정하는가?\n이 데이터셋에서 다른 사람들이 무엇을 보기를 바라는가, 그리고 어떻게 그들을 설득할 수 있는가? 그들을 설득하기 위해 얼마나 많은 작업을 해야 하는가?\n이 데이터셋과 관련된 프로세스 및 결과에 의해 누가 영향을 받는가? 그들이 데이터셋에 어느 정도 표현되어 있으며, 분석에 참여했는가?",
    "crumbs": [
      "기초",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>데이터로 이야기하기</span>"
    ]
  },
  {
    "objectID": "01-introduction_ko.html#워크플로-구성-요소",
    "href": "01-introduction_ko.html#워크플로-구성-요소",
    "title": "1  데이터로 이야기하기",
    "section": "1.2 워크플로 구성 요소",
    "text": "1.2 워크플로 구성 요소\n데이터로 이야기를 전달하는 데 필요한 워크플로에는 5가지 핵심 구성 요소가 있습니다.\n\n최종 목표를 계획하고 스케치합니다.\n시뮬레이션된 데이터를 시뮬레이션하고 고려합니다.\n실제 데이터를 수집하고 준비합니다.\n실제 데이터를 탐색하고 이해합니다.\n수행한 작업과 발견한 내용을 공유합니다.\n\n우리는 최종 목표를 계획하고 스케치하는 것부터 시작합니다. 이렇게 하면 우리가 가고 싶은 곳에 대해 신중하게 생각할 수 있기 때문입니다. 이는 우리가 상황을 깊이 고려하도록 강요하고, 집중적이고 효율적으로 유지하며, 범위蔓延을 줄이는 데 도움이 됩니다. 루이스 캐럴의 이상한 나라의 앨리스에서 앨리스는 체셔 고양이에게 어느 길로 가야 하는지 묻습니다. 체셔 고양이는 앨리스가 어디로 가고 싶은지 물어봄으로써 대답합니다. 그리고 앨리스가 어딘가에 도착하기만 하면 상관없다고 대답하자 체셔 고양이는 “충분히 오래 걸으면” 항상 어딘가에 도착할 것이기 때문에 방향은 중요하지 않다고 말합니다. 우리 경우의 문제는 일반적으로漫然히 오래 걸을 여유가 없다는 것입니다. 최종 목표를 변경해야 할 수도 있지만, 이것이 신중하고 합리적인 결정이어야 한다는 것이 중요합니다. 그리고 그것은 초기 목표가 주어졌을 때만 가능합니다. 이것으로부터 많은 가치를 얻기 위해 너무 많은 시간을 할애할 필요는 없습니다. 종종 종이와 펜으로 10분이면 충분합니다.\n다음 단계는 데이터를 시뮬레이션하는 것입니다. 그렇게 하면 세부 사항에 집중하게 되기 때문입니다. 데이터셋의 클래스와 예상되는 값의 분포에 집중하게 되므로 데이터셋 정리 및 준비에 도움이 됩니다. 예를 들어, 연령대가 정치적 선호도에 미치는 영향에 관심이 있다면 연령대 변수는 “18-29”, “30-44”, “45-59”, “60+”의 네 가지 가능한 값을 가진 요인이 될 것으로 예상할 수 있습니다. 시뮬레이션 프로세스는 실제 데이터셋이 충족해야 하는 명확한 특징을 제공합니다. 이러한 특징을 사용하여 데이터 정리 및 준비를 안내하는 테스트를 정의할 수 있습니다. 예를 들어, 실제 데이터셋에서 해당 네 가지 값이 아닌 연령대를 확인할 수 있습니다. 해당 테스트를 통과하면 연령대 변수에 예상되는 값만 포함되어 있다고 확신할 수 있습니다.\n데이터 시뮬레이션은 통계 모델링으로 전환할 때도 중요합니다. 해당 단계에 있을 때 우리는 모델이 데이터셋에 있는 내용을 반영하는지 여부에 관심이 있습니다. 문제는 실제 데이터셋을 모델링하는 데 바로 착수하면 모델에 문제가 있는지 여부를 알 수 없다는 것입니다. 우리는 기본 데이터 생성 프로세스를 정확히 알 수 있도록 처음에 데이터를 시뮬레이션합니다. 그런 다음 시뮬레이션된 데이터셋에 모델을 적용합니다. 입력한 내용을 얻으면 모델이 적절하게 수행되고 있음을 알 수 있으며 실제 데이터셋으로 전환할 수 있습니다. 시뮬레이션된 데이터에 대한 초기 적용 없이는 모델에 대한 확신을 갖기가 더 어려울 것입니다.\n시뮬레이션은 종종 저렴합니다. 현대 컴퓨팅 리소스와 프로그래밍 언어를 고려하면 거의 무료입니다. 그리고 빠릅니다. 그것은 “상황에 대한 친밀한 느낌”을 제공합니다(Hamming [1997년] 2020, p. 239). 필수적인 것만 포함하는 시뮬레이션부터 시작하여 작동하도록 한 다음 복잡하게 만드십시오.\n우리가 관심 있는 데이터를 획득하고 준비하는 것은 워크플로에서 종종 간과되는 단계입니다. 이는 가장 어려운 단계 중 하나일 수 있고 많은 결정을 내려야 하기 때문에 놀랍습니다. 점점 더 많은 연구의 대상이 되고 있으며, 이 단계에서 내린 결정이 통계 결과에 영향을 미칠 수 있다는 것이 밝혀졌습니다(Huntington-Klein 기타 2021; Dolatsara 기타 2021; Gould 기타 2023).\n워크플로의 이 단계에서는 약간 압도감을 느끼는 것이 일반적입니다. 일반적으로 우리가 얻을 수 있는 데이터는 우리를 약간 두렵게 만듭니다. 너무 적을 수도 있고, 이 경우 통계 기계를 어떻게 작동시킬 수 있을지 걱정합니다. 또는 반대의 문제가 있을 수도 있고, 그렇게 많은 양의 데이터를 어떻게 처리해야 할지 걱정할 수도 있습니다.\n\n어쩌면 우리 삶의 모든 용들은 우리가 한 번만이라도 아름다움과 용기를 가지고 행동하기를 기다리는 공주들일지도 모릅니다. 어쩌면 우리를 두렵게 하는 모든 것은 가장 깊은 본질에서 우리의 사랑을 원하는 무력한 것일지도 모릅니다.\nRilke ([1929년] 2014)\n\n워크플로의 이 단계에서 편안함을 개발하면 나머지 부분이 잠금 해제됩니다. 설득력 있는 이야기를 하는 데 필요한 데이터셋이 거기에 있습니다. 그러나 조각가처럼 우리는 필요하지 않은 모든 것을 반복적으로 제거한 다음 필요한 것을 모양을 만들어야 합니다.\n데이터셋이 있으면 해당 데이터셋의 특정 관계를 탐색하고 이해하려고 합니다. 일반적으로 기술 통계로 프로세스를 시작한 다음 통계 모델로 이동합니다. 데이터의 의미를 이해하기 위해 통계 모델을 사용하는 것은 편향이 없거나 “진실”이 아닙니다. 그들은 우리가 하라고 하는 대로 합니다. 데이터로 이야기를 할 때 통계 모델은 그래프와 표를 사용하는 것과 같은 방식으로 데이터셋을 탐색하는 데 사용하는 도구이자 접근 방식입니다. 그것들은 우리에게 명확한 결과를 제공하는 것이 아니라 특정 방식으로 데이터셋을 더 명확하게 이해할 수 있게 해줍니다.\n워크플로의 이 단계에 도달할 때쯤이면 모델은 기본 데이터 생성 프로세스 유형만큼이나 초기 단계, 특히 획득 및 정리 단계에서 내려진 결정을 반영하게 됩니다. 정교한 모델러는 자신의 통계 모델이 빙산의 일각과 같다는 것을 알고 있습니다. 즉, 그들은 대부분이 아래에 있는 것, 이 경우에는 데이터에 기반을 두고 있으며 그것 때문에만 가능합니다. 그러나 전체 데이터 과학 워크플로의 전문가가 모델링을 사용할 때, 그들은 얻은 결과가 누구의 데이터가 중요한지에 대한 선택, 데이터를 측정하고 기록하는 방법에 대한 결정, 그리고 데이터가 특정 워크플로에 사용 가능해지기 훨씬 전에 세상이 있는 그대로를 반영하는 기타 측면 때문이라는 것을 인식합니다.\n마지막으로, 우리가 한 일과 발견한 것을 가능한 한 높은 충실도로 공유해야 합니다. 당신만이 아는 지식에 대해 이야기하는 것은 당신을 박식하게 만들지 않으며, 여기에는 “과거의 당신”만이 아는 지식도 포함됩니다. 의사소통할 때, 우리가 내린 결정, 왜 그렇게 했는지, 우리의 발견, 그리고 우리 접근 방식의 약점에 대해 명확해야 합니다. 우리는 중요한 것을 밝혀내는 것을 목표로 하므로 처음에는 모든 것을 적어야 하지만, 이 서면 의사소통은 나중에 다른 형태의 의사소통으로 보완될 수 있습니다. 이 워크플로에서 내려야 할 결정이 너무 많아서 처음부터 끝까지 모든 것에 대해 공개해야 합니다. 이것은 통계 모델링과 그래프 및 표 작성뿐만 아니라 모든 것을 의미합니다. 이것이 없으면 데이터를 기반으로 한 이야기는 신뢰성이 부족합니다.\n세상은 모든 것이 신중하고 현명하게 평가되는 합리적인 능력주의 사회가 아닙니다. 대신, 우리는 경험을 바탕으로 지름길, 편법, 경험적 방법을 사용합니다. 불분명한 의사소통은 최고의 작업조차도 무용지물로 만들 것입니다. 왜냐하면 그것은 철저하게 다루어지지 않을 것이기 때문입니다. 의사소통에는 최소한의 기준이 있지만, 그것이 얼마나 인상적일 수 있는지에 대한 상한선은 없습니다. 잘 짜여진 워크플로의 정점일 때, 그것은 심지어 어떤 스프레차투라, 즉 연구된 부주의함을 얻을 수도 있습니다. 그러한 숙달을 이루려면 수년간의 노력이 필요합니다.",
    "crumbs": [
      "기초",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>데이터로 이야기하기</span>"
    ]
  },
  {
    "objectID": "01-introduction_ko.html#데이터로-이야기하기",
    "href": "01-introduction_ko.html#데이터로-이야기하기",
    "title": "1  데이터로 이야기하기",
    "section": "1.3 데이터로 이야기하기",
    "text": "1.3 데이터로 이야기하기\n데이터를 기반으로 한 설득력 있는 이야기는 약 10~20페이지로 전달할 수 있습니다. 이보다 적으면 일부 세부 사항이 너무 부족할 가능성이 높습니다. 그리고 훨씬 더 많이 쓰는 것은 쉽지만, 종종 약간의 성찰을 통해 간결하게 만들거나 여러 이야기를 분리할 수 있습니다.\n전통적인 실험을 수행할 수 없을 때에도 설득력 있는 이야기를 하는 것이 가능합니다. 이러한 접근 방식은 “빅 데이터”(만병통치약이 아님 (Meng 2018; Bradley 기타 2021))에 의존하는 대신 사용 가능한 데이터를 더 잘 활용하는 데 의존합니다. 연구와 독립적인 학습, 이론과 응용의 조화, 실용적인 기술, 정교한 워크플로, 그리고 자신이 모르는 것에 대한 이해가 결합되면 종종 지속적인 지식을 창출하기에 충분합니다.\n데이터를 기반으로 한 최고의 이야기는 여러 분야에 걸쳐 있는 경향이 있습니다. 그들은 필요한 분야에서 무엇이든 가져오지만 거의 항상 통계학, 소프트웨어 공학, 경제학, 공학 (몇 가지만 언급하자면)을 활용합니다. 따라서 종단 간 워크플로에는 이러한 분야의 기술 조합이 필요합니다. 이러한 기술을 배우는 가장 좋은 방법은 실제 데이터를 사용하여 다음과 같은 연구 프로젝트를 수행하는 것입니다.\n\n연구 질문을 개발합니다.\n관련 데이터셋을 확보하고 정리합니다.\n해당 질문에 답하기 위해 데이터를 탐색합니다. 그리고\n의미 있는 방식으로 소통합니다.\n\n데이터로 설득력 있는 이야기를 하는 핵심 요소는 다음과 같습니다.\n\n소통.\n재현성.\n윤리학.\n질문.\n측정.\n데이터 수집.\n데이터 정제.\n탐색적 데이터 분석.\n모델링.\n확장.\n\n이러한 요소들은 몇 가지 다른 범주 내에서 고려될 수 있습니다. 여기에는 좋은 연구 수행(윤리 및 질문), 신뢰할 수 있는 답변 도출(측정, 수집, 정제, 탐색적 데이터 분석 및 모델링), 설득력 있는 설명 생성(소통, 재현성 및 확장)이 포함됩니다. 이러한 요소들은 워크플로가 구축되는 기반입니다(그림 1.1).\n\n\n\n\n\n\n그림 1.1: 워크플로는 다양한 요소 위에 구축됩니다\n\n\n\n이것은 숙달해야 할 것이 많지만, 소통이 가장 중요합니다. 잘 전달된 간단한 분석은 제대로 전달되지 않은 복잡한 분석보다 더 가치가 있습니다. 후자는 다른 사람들이 이해하거나 신뢰할 수 없기 때문입니다. 명확한 소통의 부족은 때때로 연구자가 무슨 일이 일어나고 있는지, 심지어 자신이 무엇을 하고 있는지 이해하지 못하는 것을 반영합니다. 따라서 분석 수준은 데이터셋, 도구, 작업 및 기술 수준과 일치해야 하지만, 명확성과 복잡성 사이에서 절충이 필요한 경우 명확성 쪽으로 기우는 것이 합리적일 수 있습니다.\n명확한 의사소통은 청중을 함께 데려가는 방식으로 표, 그래프, 모델의 도움을 받아 평이한 언어로 작성하는 것을 의미합니다. 무엇을 했고 왜 했는지, 그리고 무엇을 발견했는지 명시하는 것을 의미합니다. 최소한의 기준은 다른 사람이 독립적으로 당신이 한 일을 하고 당신이 발견한 것을 찾을 수 있을 정도로 수행되는 것입니다. 한 가지 과제는 데이터에 몰입할수록 처음 접했을 때 어땠는지 기억하기 어려울 수 있다는 것입니다. 그러나 대부분의 청중은 거기서부터 시작할 것입니다. 적절한 수준의 미묘함과 세부 사항을 제공하는 법을 배우는 것은 어려울 수 있지만 청중의 이익을 위해 글쓰기에 집중하면 더 쉬워집니다.\n재현성은 세상에 대한 지속적인 지식을 창출하는 데 필요합니다. 이는 수행된 모든 것, 즉 종단 간 모든 것을 독립적으로 다시 수행할 수 있음을 의미합니다. 이상적으로는 자율적인 종단 간 재현성이 가능해야 합니다. 누구나 코드, 데이터 및 환경을 얻어 수행된 모든 것을 확인할 수 있습니다(Heil 기타 2021). 코드에 대한 무제한 액세스는 거의 항상 가능합니다. 데이터에 대해서도 이것이 기본 기대치이지만 항상 합리적인 것은 아닙니다. 예를 들어, 심리학 연구에는 개인을 식별할 수 있는 소규모 샘플이 있을 수 있습니다. 한 가지 방법은 유사한 속성을 가진 시뮬레이션된 데이터를 공개적으로 공유하고, 적절한 선의가 주어지면 실제 데이터에 액세스할 수 있는 프로세스를 정의하는 것입니다. 통계 모델은 일반적으로 광범위한 수동 검사를 받습니다. 재현성의 또 다른 측면은 유사하게 광범위한 자동화된 테스트를 포함해야 한다는 것입니다.\n윤리에 대한 적극적인 고려가 필요합니다. 왜냐하면 데이터셋은 아마도 인간과 관련이 있기 때문입니다. 이것은 다음과 같은 것들을 고려하는 것을 의미합니다: 누가 데이터셋에 있고, 누가 빠져 있으며, 왜 빠져 있는가? 우리의 이야기가 과거를 어느 정도까지 영속시킬 것인가? 그리고 이것이 일어나야 할 일인가? 데이터셋이 인간과 관련이 없더라도 이야기는 아마도 인간에 의해 만들어지고 있으며, 우리는 거의 모든 것에 영향을 미칩니다. 이것은 우리가 환경 영향과 불평등에 대한 우려를 가지고 데이터를 윤리적으로 사용할 책임이 있음을 의미합니다.\n윤리에 대한 많은 정의가 있지만, 데이터로 이야기를 하는 경우 최소한 데이터셋의 전체 맥락을 고려하는 것을 의미합니다(D’Ignazio 와/과 Klein 2020). 법학에서 법에 대한 텍스트적 접근 방식은 인쇄된 대로 법의 단어를 문자 그대로 고려하는 것을 의미하는 반면, 목적론적 접근 방식은 법이 더 넓은 맥락에서 해석되는 것을 의미합니다. 데이터로 이야기를 하는 윤리적 접근 방식은 후자의 접근 방식을 채택하고 우리 세계, 따라서 우리 데이터를 형성하는 사회적, 문화적, 역사적, 정치적 힘을 고려하는 것을 의미합니다(Crawford 2021).\n호기심은 데이터셋과 관련 프로세스를 적절한 범위까지 탐색하려는 내적 동기를 제공합니다. 질문은 질문을 낳는 경향이 있으며, 데이터셋을 이해하는 과정이 진행됨에 따라 일반적으로 개선되고 정제됩니다. 종종 가르치는 가설 검증의 일반적인 포퍼적 접근 방식과는 대조적으로, 질문은 일반적으로 지속적이고 진화하는 과정을 통해 개발됩니다(Franklin 2005). 초기 질문을 찾는 것은 어려울 수 있습니다. 연구 질문을 합리적으로 이용 가능한 측정 가능한 변수로 조작하는 것은 특히 어렵습니다. 관심 분야를 선택하는 것이 도움이 될 수 있으며, 특정 질문으로 발전시키려는 의도로 광범위한 주장을 스케치하고 마지막으로 두 가지 다른 영역을 통합하는 것도 도움이 될 수 있습니다.\n실제 데이터의 복잡함 속에서 편안함과 용이함을 개발하는 것은 데이터가 업데이트될 때마다 새로운 질문을 할 수 있게 된다는 것을 의미합니다. 그리고 데이터셋을 자세히 알면 예상치 못한 그룹이나 값이 나타나 해당 분야 전문가와 협력하여 이해할 수 있게 됩니다. 다양한 분야에 걸쳐 지식 기반을 개발하여 일종의 “하이브리드”가 되는 것은 특히 가치가 있으며, 처음에는 어리석은 질문을 할 가능성에 대해 편안해지는 것도 마찬가지입니다.\n측정과 데이터 수집은 우리 세상이 어떻게 데이터가 될 것인지를 결정하는 것입니다. 그것들은 어렵습니다. 세상은 너무나 활기차서 일관되게 측정하고 수집할 수 있는 것으로 축소하기가 어렵습니다. 예를 들어, 누군가의 키를 생각해 보십시오. 우리는 아마도 키를 재기 전에 신발을 벗어야 한다는 데 모두 동의할 것입니다. 그러나 우리의 키는 하루 동안 변합니다. 그리고 줄자로 누군가의 키를 재는 것은 레이저를 사용하는 것과 다른 결과를 줄 것입니다. 따라서 사람 간 또는 시간 경과에 따른 키를 비교하는 경우 매일 같은 시간에 같은 방법을 사용하여 측정하는 것이 중요해집니다. 그러나 그것은 금방 실행 불가능해집니다. 그리고 이것은 이러한 데이터의 데이터베이스 표현에 관한 문제를 제쳐두고 있습니다(Kent 1993).\n우리가 관심 있는 대부분의 질문은 키보다 더 복잡한 데이터를 사용할 것입니다. 누군가가 얼마나 슬픈지 어떻게 측정합니까? 고통을 어떻게 측정합니까? 무엇을 측정하고 어떻게 측정할지 누가 결정합니까? 세상을 가치로 축소하고 이것들을 비교할 수 있다고 생각하는 데는 어떤 오만함이 필요합니다. 궁극적으로 우리는 그렇게 해야 하지만, 측정할 대상을 일관되게 정의하기는 어렵습니다. 이 과정은 가치 중립적이지 않습니다. 이 잔인한 축소를 합리적으로 받아들이는 유일한 방법은 우리가 측정하고 수집하는 것을 깊이 이해하고 존중하는 것입니다. 핵심 본질은 무엇이며, 무엇을 제거할 수 있습니까?\n20세기 스페인 화가 파블로 피카소는 단 하나의 선으로 동물의 윤곽을 묘사한 일련의 그림을 가지고 있습니다(그림 1.2). 단순함에도 불구하고 우리는 어떤 동물이 묘사되고 있는지 인식합니다. 그림은 그 동물이 고양이가 아닌 개라는 것을 알기에 충분합니다. 이것이 개가 아픈지 여부를 판단하는 데 사용될 수 있을까요? 아마도 아닐 것입니다. 우리는 아마도 다른 묘사를 원할 것입니다. 어떤 것을 측정해야 하는지, 그리고 우리가 고려하기로 결정한 것들 중에서 어떤 특징을 측정하고 수집하고 어떤 것을 무시해야 하는지에 대한 결정은 맥락과 목적에 따라 달라집니다.\n\n\n\n\n\n\n그림 1.2: 파블로 피카소의 이 그림은 단 한 줄이지만 분명히 개입니다\n\n\n\n데이터 정제 및 준비는 데이터 사용의 중요한 부분입니다. 사용 가능한 데이터를 우리가 사용할 수 있는 데이터셋으로 마사지해야 합니다. 이를 위해서는 많은 결정을 내려야 합니다. 데이터 정제 및 준비 단계는 중요하며 다른 어떤 단계만큼이나 많은 관심과 주의를 기울일 가치가 있습니다.\n(kennedy2020using에?) 따르면, 성별이라는 잠재적으로 민감한 주제에 대한 정보를 수집한 설문 조사를 생각해 보십시오. 이 설문 조사는 “남성”, “여성”, “응답하지 않음”, “기타”의 네 가지 옵션을 사용했으며, “기타”는 공개 텍스트 상자로 이어졌습니다. 해당 데이터셋을 보면 대부분의 응답이 “남성” 또는 “여성”임을 알 수 있습니다. “응답하지 않음”에 대해 어떻게 처리할지 결정해야 합니다. 데이터셋에서 이를 삭제하면 이러한 응답자를 적극적으로 무시하는 것입니다. 삭제하지 않으면 분석이 더 복잡해집니다. 마찬가지로 공개 텍스트 응답을 어떻게 처리할지 결정해야 합니다. 다시 말하지만, 이러한 응답을 삭제할 수 있지만 이는 일부 응답자의 경험을 무시하는 것입니다. 또 다른 옵션은 이를 “응답하지 않음”과 병합하는 것이지만, 이는 응답자가 해당 옵션을 구체적으로 선택하지 않았기 때문에 응답자를 무시하는 것을 보여줍니다.\n많은 데이터 정리 및 준비 상황에서 쉽거나 항상 올바른 선택은 없습니다. 상황과 목적에 따라 다릅니다. 데이터 정리 및 준비에는 이와 같은 많은 선택이 포함되며, 다른 사람들이 무엇을 했고 왜 했는지 이해할 수 있도록 모든 단계를 기록하는 것이 중요합니다. 데이터는 결코 스스로 말하지 않습니다. 데이터는 데이터를 정리하고 준비한 복화술사의 꼭두각시입니다.\n데이터셋의 모양과 느낌을 이해하는 과정을 탐색적 데이터 분석(EDA)이라고 합니다. 이것은 개방형 프로세스입니다. 공식적으로 모델링하기 전에 데이터셋의 모양을 이해해야 합니다. EDA 프로세스는 요약 통계, 그래프, 표를 생성하고 때로는 일부 모델링까지 포함하는 반복적인 프로세스입니다. 공식적으로 끝나지 않는 프로세스이며 다양한 기술이 필요합니다.\nEDA가 어디에서 끝나고 공식적인 통계 모델링이 시작되는지 구분하기는 어렵습니다. 특히 신념과 이해가 어떻게 발전하는지 고려할 때 그렇습니다(Hullman 와/과 Gelman 2021). 그러나 핵심적으로는 데이터에서 시작하여 데이터에 몰입하는 것을 포함합니다(Cook, Reid, 와/과 Tanaka 2021). EDA는 일반적으로 최종 이야기에 명시적으로 포함되지 않습니다. 그러나 우리가 하는 이야기를 이해하는 방식에 중심적인 역할을 합니다. EDA 중에 수행된 모든 단계를 기록하고 공유하는 것이 중요합니다.\n통계 모델링은 길고 견고한 역사를 가지고 있습니다. 통계 지식은 수백 년에 걸쳐 구축되었습니다. 통계는 일련의 건조한 정리와 증명이 아니라 세상을 탐구하는 방법입니다. 그것은 “외국어 또는 대수학 지식과 유사합니다. 언제 어떤 상황에서도 유용할 수 있습니다.”(Bowley 1901, p. 4). 통계 모델은 만약-이렇다면-저렇다는 식으로 순진하게 따라야 할 레시피가 아니라 데이터를 이해하는 방법입니다(James 기타 [2013년] 2021). 모델링은 일반적으로 데이터에서 통계적 패턴을 추론하는 데 필요합니다. 더 공식적으로, 통계적 추론은 “데이터를 사용하여 데이터를 생성한 분포를 추론하는 프로세스”입니다(Wasserman 2005 p. 87).\n통계적 유의성은 과학적 유의성과 동일하지 않으며, 우리는 지배적인 패러다임이었던 것의 대가를 깨닫고 있습니다. 데이터에 임의의 합격/불합격 통계 테스트를 사용하는 것은 거의 적절하지 않습니다. 대신, 통계 모델링의 적절한 사용은 일종의 반향 정위와 같습니다. 우리는 모델에서 우리에게 돌아오는 것을 듣고 세상의 모양에 대해 배우는 데 도움을 받지만, 그것이 세상의 한 가지 표현일 뿐이라는 것을 인식합니다.\nR 및 Python과 같은 프로그래밍 언어 사용은 작업을 신속하게 확장할 수 있게 해줍니다. 이는 입력과 출력 모두를 의미합니다. 10개의 관찰값을 고려하는 것이 1,000개 또는 심지어 1,000,000개를 고려하는 것만큼이나 쉽습니다. 이를 통해 우리 이야기가 어느 정도까지 적용되는지 더 빨리 알 수 있습니다. 또한 우리 결과물은 한 사람이 소비하는 것만큼이나 10명 또는 100명이 쉽게 소비할 수 있습니다. 애플리케이션 프로그래밍 인터페이스(API)를 사용하면 우리 이야기가 초당 수천 번 고려될 수도 있습니다.",
    "crumbs": [
      "기초",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>데이터로 이야기하기</span>"
    ]
  },
  {
    "objectID": "01-introduction_ko.html#우리-세상은-어떻게-데이터가-되는가",
    "href": "01-introduction_ko.html#우리-세상은-어떻게-데이터가-되는가",
    "title": "1  데이터로 이야기하기",
    "section": "1.4 우리 세상은 어떻게 데이터가 되는가?",
    "text": "1.4 우리 세상은 어떻게 데이터가 되는가?\n\n에딩턴의 유명한 이야기가 있습니다. 어떤 사람들이 그물로 바다에서 낚시를 하러 갔습니다. 그들이 잡은 물고기의 크기를 조사한 후, 그들은 바다에는 물고기의 최소 크기가 있다고 결정했습니다! 그들의 결론은 현실이 아니라 사용된 도구에서 비롯되었습니다.\nHamming ([1997년] 2020, p. 177)\n\n어느 정도 우리는 시간을 낭비하고 있습니다. 우리는 세상의 완벽한 모델을 가지고 있습니다. 바로 세상 그 자체입니다! 하지만 너무 복잡합니다. 모든 것이 그것에 영향을 미치는 셀 수 없는 요인에 의해 어떻게 영향을 받는지 완벽하게 안다면, 동전 던지기, 주사위 굴리기, 그리고 겉보기에 무작위적인 다른 모든 과정을 매번 완벽하게 예측할 수 있을 것입니다. 하지만 우리는 할 수 없습니다. 대신, 우리는 그럴듯하게 측정 가능한 것으로 단순화해야 하며, 그것이 우리가 데이터로 정의하는 것입니다. 우리의 데이터는 그것들이 파생된 지저분하고 복잡한 세상의 단순화입니다.\n“그럴듯하게 측정 가능한” 것에는 다양한 근사치가 있습니다. 따라서 데이터셋은 항상 선택의 결과입니다. 우리는 그것들이 당면한 작업에 대해 그럼에도 불구하고 합리적인지 여부를 결정해야 합니다. 우리는 통계 모델을 사용하여 데이터에 대해 깊이 생각하고, 탐색하고, 바라건대 더 잘 이해하는 데 도움을 받습니다.\n많은 통계학은 우리가 가진 데이터를 철저히 고려하는 데 중점을 둡니다. 이는 우리 데이터가 농업, 천문학 또는 물리 과학에서 비롯되었을 때 적절했습니다. 이것은 비인간적 맥락에서 체계적 편향이 존재하거나 영향을 미칠 수 없다는 것을 말하는 것이 아니라, 데이터 과학의 부상과 함께, 부분적으로는 인간이 생성한 데이터셋에 대한 적용 가치 때문에, 우리는 또한 우리 데이터셋에 없는 것을 적극적으로 고려해야 합니다. 우리 데이터셋에서 체계적으로 누락된 사람은 누구입니까? 누구의 데이터가 우리가 사용하는 접근 방식에 잘 맞지 않아 부적절하게 단순화되고 있습니까? 세상이 데이터가 되는 과정에 추상화와 단순화가 필요하다면, 언제 합리적으로 단순화할 수 있고 언제 부적절한지 명확히 해야 합니다.\n우리 세상이 데이터가 되는 과정에는 필연적으로 측정이 포함됩니다. 역설적이게도, 측정을 하고 세부 사항에 깊이 몰두하는 사람들은 종종 그것과 동떨어진 사람들보다 데이터에 대한 신뢰가 낮습니다. 거리 측정, 경계 정의, 인구 계산과 같이 겉보기에 명확한 작업조차도 실제로는 놀랍도록 어렵습니다. 우리 세상을 데이터로 바꾸는 데는 많은 결정이 필요하고 많은 오류가 발생합니다. 다른 많은 고려 사항 중에서 무엇을 측정할 것인지, 얼마나 정확하게 측정할 것인지, 누가 측정할 것인지 결정해야 합니다.\n\n\n\n\n\n\n아, 그것에 대해 좋은 데이터가 있다고 생각하시는군요!\n\n\n\n겉보기에 간단한 것이 얼마나 빨리 어려워지는지에 대한 중요한 예는 산모 관련 사망입니다. 이는 임신 중이거나 중절 후 곧 임신 또는 그 관리와 관련된 원인으로 사망하는 여성의 수를 의미합니다(World Health Organization 2019). 그러한 사망의 비극을 원인별 데이터로 바꾸는 것은 어렵지만 중요합니다. 왜냐하면 그것이 미래의 사망을 완화하는 데 도움이 되기 때문입니다. 일부 국가는 모든 사망에 대한 데이터를 수집하는 잘 개발된 시민 등록 및 생명 통계(CRVS)를 가지고 있습니다. 그러나 많은 국가에는 CRVS가 없어 사망이 기록되지 않습니다. 사망이 기록되더라도 사망 원인을 정의하는 것은 어려울 수 있으며, 특히 자격을 갖춘 의료진이나 장비가 부족한 경우 더욱 그렇습니다. 산모 사망은 일반적으로 원인이 많기 때문에 특히 어렵습니다. 일부 CRVS 시스템에는 사망이 산모 사망으로 계산되어야 하는지 여부를 지정하기 위해 사망 등록 양식에 확인란이 있습니다(Dattani 2024). 그러나 일부 선진국조차도 최근에야 이를 채택했습니다. 예를 들어, 미국에서는 2003년에야 도입되었으며, 2015년에도 앨라배마, 캘리포니아, 웨스트버지니아는 표준 질문을 채택하지 않았습니다(MacDorman 와/과 Declercq 2018). 이는 산모 사망이 과소 보고되거나 잘못 분류될 위험이 있음을 의미합니다.\n\n\n우리는 일반적으로 세상을 데이터로 바꾸기 위해 다양한 도구를 사용합니다. 천문학에서는 더 나은 망원경의 개발과 결국 위성 및 탐사선을 통해 다른 세계에 대한 새로운 이해가 가능해졌습니다. 마찬가지로, 우리 자신의 세상을 데이터로 바꾸는 새로운 도구가 매일 개발되고 있습니다. 한때 인구 조사가 세대를 정의하는 행사였지만, 이제는 정기적인 설문 조사, 초 단위로 제공되는 거래 데이터, 인터넷상의 거의 모든 상호 작용이 어떤 종류의 데이터가 됩니다. 이러한 도구의 개발은 흥미로운 새로운 이야기를 가능하게 했습니다.\n우리 세상은 불완전하게 데이터가 됩니다. 그럼에도 불구하고 데이터를 사용하여 세상에 대해 배우려면 그 불완전성과 그 불완전성의 의미를 적극적으로 이해하려고 노력해야 합니다.",
    "crumbs": [
      "기초",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>데이터로 이야기하기</span>"
    ]
  },
  {
    "objectID": "01-introduction_ko.html#데이터-과학이란-무엇이며-세상을-배우는-데-어떻게-사용해야-할까요",
    "href": "01-introduction_ko.html#데이터-과학이란-무엇이며-세상을-배우는-데-어떻게-사용해야-할까요",
    "title": "1  데이터로 이야기하기",
    "section": "1.5 데이터 과학이란 무엇이며 세상을 배우는 데 어떻게 사용해야 할까요?",
    "text": "1.5 데이터 과학이란 무엇이며 세상을 배우는 데 어떻게 사용해야 할까요?\n데이터 과학에 대한 합의된 정의는 없습니다. (r4ds는?) 그것이 “…원시 데이터를 이해, 통찰력 및 지식으로 전환할 수 있게 해준다”고 말합니다. 마찬가지로 (leekandpeng는?) 그것이 “\\(\\dots\\)데이터로 답할 수 있는 정량적 질문을 공식화하고, 데이터를 수집 및 정리하고, 데이터를 분석하고, 관련 청중에게 질문에 대한 답변을 전달하는 과정”이라고 주장합니다. (moderndatascience는?) 그것을 “\\(\\dots\\)데이터에서 의미 있는 정보를 추출하는 과학”으로 간주합니다. 그리고 (timbersandfriends는?) 그것을 “재현 가능하고 감사 가능한 프로세스를 통해 데이터에서 통찰력을 생성하는 프로세스”로 정의합니다. 더 이전 시대의 (foster는?) “(통계)는 대량의 데이터 처리 및 분석과 데이터에서 정보를 추출하는 수학적 방법 개발에 관심이 있습니다. 이 모든 활동을 컴퓨터 방법과 결합하면 부분의 합보다 더 큰 무언가를 얻게 됩니다.”라고 말하며 현재 우리가 데이터 과학이라고 부르는 것을 명확하게 지적합니다.\n(craiu2019hiring은?) 데이터 과학이 무엇인지에 대한 불확실성은 중요하지 않을 수 있다고 주장합니다. 왜냐하면 “\\(\\dots\\)누가 정말로 누군가를 시인이나 과학자로 만드는지 말할 수 있겠습니까?” 그는 계속해서 데이터 과학자는 “\\(\\dots\\)데이터 중심 연구 의제를 가지고 있고, 통계 방법의 원칙적인 구현을 고수하거나 열망하며 효율적인 계산 기술을 사용하는 사람”이라고 광범위하게 말합니다.\n어쨌든 구체적이고 기술적인 정의와 함께 약간의 구체성을 잃더라도 간단한 정의를 갖는 것은 가치가 있습니다. 확률은 종종 비공식적으로 “사물을 세는 것”으로 정의됩니다(McElreath [2015년] 2020, p. 10). 유사한 비공식적인 의미에서 데이터 과학은 다음과 같이 정의할 수 있습니다: 인간이 사물을 측정하고, 일반적으로 다른 인간과 관련되며, 설명하고 예측하기 위해 정교한 평균을 사용하는 것. 우리는 ?sec-concluding-remarks에서 이것을 다시 방문하여 더 자세한 정의를 제공합니다.\n약간 귀엽게 들릴지 모르지만, 19세기 통계학자이자 경제학자인 프랜시스 에지워스는 통계를 “사회 현상에 의해 제시되는 평균의 과학”으로 간주했으므로 좋은 동료를 찾은 셈입니다(Edgeworth 1885). 어쨌든 이 정의의 한 가지 특징은 데이터를 테라 눌리우스, 즉 누구의 땅도 아닌 것으로 취급하지 않는다는 것입니다. 통계학자들은 데이터를 우리가 결코 알 수 없는 어떤 과정의 결과로 보지만, 데이터를 사용하여 이해하려고 노력합니다. 많은 통계학자들은 데이터와 측정에 깊은 관심을 가지고 있지만, 통계학에서 데이터가 그냥 나타나는 경우가 많습니다. 즉, 누구에게도 속하지 않습니다. 그러나 실제로는 그렇지 않습니다.\n데이터는 생성된 다음 수집, 정리 및 준비되어야 하며 이러한 결정은 중요합니다. 모든 데이터셋은 독자적인 종류, 즉 그 자체로 하나의 클래스이므로 한 데이터셋을 잘 알게 되면 모든 데이터셋이 아니라 하나의 데이터셋만 알게 됩니다.\n데이터 과학의 많은 부분은 “과학”에 초점을 맞추지만 “데이터”에도 초점을 맞추는 것이 중요합니다. 그리고 그것이 데이터 과학에 대한 그 귀여운 정의의 또 다른 특징입니다. 일부 데이터 과학자는 광범위한 문제에 관심이 있는 제너럴리스트입니다. 종종 이러한 것들을 통합하는 것은 지저분한 데이터를 수집, 정리 및 준비해야 한다는 것입니다. 그리고 자주 가장 많은 시간이 필요하고 가장 자주 업데이트되며 가장 완전한 주의를 기울일 가치가 있는 것은 해당 데이터의 세부 사항입니다.\n(Jordan2019Artificial은?) 진료실에 있었고, 당시 태아였던 그의 아이가 다운 증후군을 앓고 있을 확률을 산전 초기 검사를 통해 받았다고 설명합니다. 배경 지식으로, 확실히 알기 위해 검사를 할 수 있지만, 그 검사는 태아가 생존하지 못할 위험이 따르므로 이 초기 검사를 하고 부모는 일반적으로 그 초기 검사에서 나온 다운 증후군 확률을 사용하여 확정 검사를 할지 여부를 결정합니다. (Jordan2019Artificial은?) 초기 검사에서 제공된 확률이 10년 전에 영국에서 수행된 연구를 기반으로 결정되고 있음을 발견했습니다. 문제는 그 후 10년 동안 영상 기술이 향상되어 초기 검사에서 그렇게 고해상도 이미지를 예상하지 못했고, 그 후 초기 검사에서 다운 증후군 진단이 (거짓으로) 증가했다는 것입니다. 데이터가 문제였습니다.\n\n\n\n\n\n\n거인의 어깨 위에 서서\n\n\n\n마이클 조던 박사는 캘리포니아 대학교 버클리 캠퍼스의 페홍 첸 특훈 교수입니다. 1985년 캘리포니아 대학교 샌디에이고 캠퍼스에서 인지 과학 박사 학위를 받은 후 MIT 조교수로 임명되었고, 1997년 정교수로 승진했으며, 1998년 버클리로 옮겼습니다. 그의 연구 분야 중 하나는 통계적 기계 학습입니다. 예를 들어, 특히 중요한 논문 중 하나는 (Blei2003latent인데?), 이 논문은 텍스트를 그룹화하여 주제를 정의하는 방법을 정의했으며, ?sec-text-as-data에서 이를 다룹니다.\n\n\n어려운 것은 “과학” 부분만이 아니라 “데이터” 부분이기도 합니다. 예를 들어, 연구자들은 컴퓨터 과학에서 가장 인기 있는 텍스트 데이터셋 중 하나를 다시 조사했고, 데이터의 약 30%가 부적절하게 중복되었다는 것을 발견했습니다(Bandy 와/과 Vincent 2021). 이러한 유형의 데이터셋을 전문으로 하는 전체 분야(언어학)가 있으며, 부적절한 데이터 사용은 어느 한 분야가 헤게모니를 장악할 때 발생하는 위험 중 하나입니다. 데이터 과학의 강점은 다양한 배경과 훈련을 가진 사람들을 모아 어떤 데이터셋에 대해 배우는 작업에 참여시킨다는 것입니다. 과거에 수행된 작업에 제약을 받지 않습니다. 이는 우리가 우리 자신의 전통에서 오지 않았지만 우리만큼 데이터셋에 관심이 있는 사람들에게 존경심을 표하기 위해 노력해야 함을 의미합니다. 데이터 과학은 여러 분야에 걸쳐 있으며 점점 더 중요해지고 있습니다. 따라서 우리 세계를 반영해야 합니다. 데이터 과학에는 다양한 배경, 접근 방식 및 분야가 필요합니다.\n우리 세상은 지저분하고, 우리 데이터도 마찬가지입니다. 데이터로 성공적으로 이야기를 하려면 그 과정이 어려울 것이라는 사실에 익숙해져야 합니다. 영국 수학자 해나 프라이는 문제를 해결하기 전에 코드를 다시 작성하는 데 6개월을 보냈다고 설명합니다(Thornhill 2021). 끈기 있게 노력하는 법을 배워야 합니다. 때로는 실패를 받아들여야 하며, 회복력을 키우고 내재적 동기를 가짐으로써 그렇게 합니다. 데이터의 세계는 가능성과 확률을 고려하고 그 사이에서 절충하는 법을 배우는 것입니다. 우리가 확실히 아는 것은 거의 없으며 완벽한 분석도 없습니다.\n궁극적으로 우리 모두는 데이터로 이야기를 하고 있을 뿐이지만, 이러한 이야기는 점점 더 세상에서 가장 중요한 이야기가 되고 있습니다.",
    "crumbs": [
      "기초",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>데이터로 이야기하기</span>"
    ]
  },
  {
    "objectID": "01-introduction_ko.html#연습-문제",
    "href": "01-introduction_ko.html#연습-문제",
    "title": "1  데이터로 이야기하기",
    "section": "1.6 연습 문제",
    "text": "1.6 연습 문제\n\n퀴즈\n\n데이터 과학이란 무엇입니까 (자신의 말로)?\n(register2020에?) 따르면 데이터 결정은 다음에 영향을 미칩니까 (하나 선택)?\n\n실제 사람들.\n아무도.\n훈련 세트에 있는 사람들.\n테스트 세트에 있는 사람들.\n\n(keyes2019에?) 따르면 데이터 과학이란 무엇입니까 (하나 선택)?\n\n데이터 과학은 과학적 방법, 프로세스, 알고리즘 및 시스템을 사용하여 많은 구조화 및 비구조화 데이터에서 지식과 통찰력을 추출하는 학제 간 분야입니다.\n의사 결정을 목적으로 하는 대량 데이터의 정량적 분석.\n인류를 셀 수 있는 것으로 비인간적으로 축소하는 것.\n\n(keyes2019에?) 따르면 표준화된 범주를 요구하는 데이터 시스템의 한 가지 결과는 무엇입니까 (하나 선택)?\n\n사용자 경험 저하.\n보안 조치 손상.\n기술 혁신 증가.\n개인의 정체성과 경험 소멸.\n\n(kieranskitchen에?) 따르면 데이터 작업에 대한 일반적인 비판은 무엇입니까 (하나 선택)?\n\n너무 시간이 많이 걸리고 비효율적이라는 것.\n숫자 뒤에 있는 인간 삶의 현실에서 멀어지게 한다는 것.\n분석을 위해 값비싼 소프트웨어와 광범위한 교육이 필요하다는 것.\n\n(kieranskitchen에?) 따르면 그 비판에 대한 반응은 무엇입니까 (하나 선택)?\n\n데이터 작업은 의미에 대한 질문에 직면하도록 강요합니다.\n데이터 분석은 수행되어서는 안 됩니다.\n데이터는 자동화된 프로세스로만 분석되어야 합니다.\n질적 접근 방식이 지배적인 접근 방식이어야 합니다.\n\n(keyes2019와?) (kieranskitchen을?) 어떻게 조화시킬 수 있습니까?\n윤리가 데이터 과학의 핵심 요소인 이유는 무엇입니까 (하나 선택)?\n\n데이터 과학은 항상 민감한 개인 정보를 포함하기 때문입니다.\n윤리적 고려 사항으로 인해 분석을 더 쉽게 수행할 수 있기 때문입니다.\n데이터셋은 아마도 인간과 관련이 있으며 맥락을 고려해야 하기 때문입니다.\n규정에 따라 모든 데이터 분석에 대해 윤리 승인이 필요하기 때문입니다.\n\n이 장에서 설명한 (crawford에?) 따르면 다음 중 우리 세계와 따라서 우리 데이터를 형성하는 힘은 무엇입니까 (모두 선택)?\n\n정치적.\n물리적.\n역사적.\n문화적.\n사회적.\n\n(nottomford에?) 따르면 컴파일러란 무엇입니까 (하나 선택)?\n\n파일에 입력한 기호를 가져와 하위 수준 명령으로 변환하는 소프트웨어.\n누군가가 입력했거나 다른 곳에서 복사하거나 붙여넣은 (일반적인 키보드 문자를 사용하여 어떤 종류의 파일에 저장된) 기호 시퀀스.\n혜택이 있는 시계.\n펀치 카드에 구멍을 뚫고 상자에 넣은 다음 로드하면 컴퓨터가 카드를 넘겨 구멍이 있는 위치를 식별하고 메모리 일부를 업데이트합니다.\n\n성별에 대해 질문한 설문 조사 결과를 고려하십시오. 다음과 같은 개수를 찾습니다: “남성: 879”, “여성: 912”, “논바이너리: 10”, “응답하지 않음: 3”, “기타: 1”. “응답하지 않음”을 고려하는 적절한 방법은 무엇입니까 (하나 선택)?\n\n삭제합니다.\n경우에 따라 다릅니다.\n포함합니다.\n“기타”에 병합합니다.\n\n인종 및/또는 성적 지향을 예측 변수로 포함하면 모델 성능이 향상되는 직업을 가지고 있다고 상상해 보십시오. 분석에 이러한 요소를 포함할지 여부를 결정할 때 어떤 요소를 고려하시겠습니까 (자신의 말로)?\n데이터 과학에서 재현성이란 무엇을 의미합니까 (하나 선택)?\n\n다른 데이터셋으로 유사한 결과를 생성할 수 있는 것.\n분석의 모든 단계를 다른 사람이 독립적으로 다시 수행할 수 있도록 보장하는 것.\n동료 심사를 거친 저널에 결과를 게시하는 것.\n데이터를 보호하기 위해 독점 소프트웨어를 사용하는 것.\n\n측정과 관련된 과제는 무엇입니까 (하나 선택)?\n\n일반적으로 간단하며 거의 주의를 기울일 필요가 없습니다.\n무엇을 어떻게 측정할지 결정하는 것은 복잡하고 상황에 따라 다릅니다.\n데이터 수집은 객관적이며 편향이 없습니다.\n측정값은 항상 정확하고 시간이 지나도 일관됩니다.\n\n조각가에 대한 비유에서 조각 행위는 데이터 워크플로에서 무엇을 나타냅니까 (하나 선택)?\n\n데이터에 적합한 복잡한 모델 생성.\n원시 데이터 획득.\n필요한 데이터셋을 드러내기 위해 데이터 정리 및 준비.\n결과 시각화.\n\n탐색적 데이터 분석(EDA)이 개방형 프로세스인 이유는 무엇입니까 (하나 선택)?\n\n따라야 할 고정된 단계 집합이 있기 때문입니다.\n데이터의 모양과 패턴을 이해하기 위해 지속적인 반복이 필요하기 때문입니다.\n구조화된 방식으로 가설을 테스트하는 것을 포함하기 때문입니다.\n자동화할 수 있기 때문입니다.\n\n통계 모델을 신중하게 사용해야 하는 이유는 무엇입니까 (하나 선택)?\n\n항상 명확한 결과를 제공하기 때문입니다.\n이전 단계에서 내린 결정을 반영할 수 있기 때문입니다.\n대부분의 청중에게 너무 복잡하기 때문입니다.\n데이터가 잘 제시되면 불필요하기 때문입니다.\n\n키 측정의 어려움에 대해 생각하면서 얻을 수 있는 한 가지 교훈은 무엇입니까 (하나 선택)?\n\n키는 변동성이 거의 없는 간단한 측정값입니다.\n모든 측정값은 올바른 도구로 수행하면 정확합니다.\n간단한 측정값조차도 데이터 품질에 영향을 미치는 복잡성을 가질 수 있습니다.\n키는 데이터 분석에서 유용한 변수가 아닙니다.\n\n데이터셋에서 누락된 사람을 고려하지 않을 경우의 위험은 무엇입니까 (하나 선택)?\n\n분석에 큰 영향을 미치지 않습니다.\n데이터 양을 줄여 분석을 단순화합니다.\n전체 맥락을 나타내지 않는 결론으로 이어질 수 있습니다.\n\n통계 모델링의 목적은 무엇입니까 (하나 선택)?\n\n데이터를 탐색하고 이해하는 데 도움이 되는 도구로서.\n가설을 증명하기 위해.\n탐색적 데이터 분석을 대체하기 위해.\n\n“우리 데이터는 지저분하고 복잡한 세상의 단순화입니다”는 무엇을 의미합니까 (하나 선택)?\n\n데이터는 현실의 모든 측면을 완벽하게 포착합니다.\n데이터는 분석을 가능하게 하기 위해 현실을 단순화하지만 모든 세부 사항을 포착할 수는 없습니다.\n데이터는 항상 부정확하고 쓸모가 없습니다.\n\n\n\n\n수업 활동\n\n강사는 수업 사진을 찍은 다음 화면에 사진을 표시해야 합니다. 소그룹으로 학생들은 사진이 보여주는 세 가지 측면과 사진이 보여주지 않는 세 가지 측면을 식별해야 합니다. 이것이 데이터 과학과 어떻게 관련되는지 토론하십시오.\n강사는 각 그룹에 측정에 사용할 다른 항목을 제공해야 하며, 그중 일부는 다른 항목보다 더 유용합니다(예: 줄자, 종이, 자, 마커, 저울 등). 그런 다음 학생들은 항목을 사용하여 다음 질문에 답해야 합니다. “머리카락 길이는 얼마나 됩니까?”. 숫자를 스프레드시트에 추가합니다. 스프레드시트만 있다면 머리카락 길이에 대해 무엇을 이해하고 무엇을 이해하지 못하겠습니까? 이것을 데이터 과학과 더 광범위하게 관련시키십시오.\n\n\n\n과제\n이 과제의 목적은 겉보기에 간단한 것조차도 측정의 어려움, 따라서 더 복잡한 영역에서 측정 문제의 가능성을 명확히 하는 것입니다.\n무, 겨자잎 또는 아루굴라와 같이 빨리 자라는 식물의 씨앗을 구하십시오. 씨앗을 심고 사용한 흙의 양을 측정하십시오. 물을 주고 사용한 물의 양을 측정하십시오. 매일 변경 사항을 기록하십시오. 더 일반적으로 가능한 한 많이 측정하고 기록하십시오. 측정의 어려움에 대한 생각을 기록하십시오. 결국 씨앗이 싹트고 어떻게 자라는지 측정해야 합니다.\n\n\n\n\nBandy, John, 와/과 Nicholas Vincent. 2021. “Addressing ‘Documentation Debt’ in Machine Learning: A Retrospective Datasheet for BookCorpus”. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, 편집자： J. Vanschoren 와/과 S. Yeung. Vol 1. https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/file/54229abfcfa5649e7003b83dd4755294-Paper-round1.pdf.\n\n\nBoland, Philip. 1984. “A biographical glimpse of William Sealy Gosset”. The American Statistician 38 (3): 179–83. https://doi.org/10.2307/2683648.\n\n\nBowley, Arthur Lyon. 1901. Elements of Statistics. London: P. S. King.\n\n\nBradley, Valerie, Shiro Kuriwaki, Michael Isakov, Dino Sejdinovic, Xiao-Li Meng, 와/과 Seth Flaxman. 2021. “Unrepresentative big surveys significantly overestimated US vaccine uptake”. Nature 600 (7890): 695–700. https://doi.org/10.1038/s41586-021-04198-4.\n\n\nCook, Dianne, Nancy Reid, 와/과 Emi Tanaka. 2021. “The Foundation is Available for Thinking about Data Visualization Inferentially”. Harvard Data Science Review 3 (3). https://doi.org/10.1162/99608f92.8453435d.\n\n\nCrawford, Kate. 2021. Atlas of AI. 1st ed. New Haven: Yale University Press.\n\n\nD’Ignazio, Catherine, 와/과 Lauren Klein. 2020. Data Feminism. Massachusetts: The MIT Press. https://data-feminism.mitpress.mit.edu.\n\n\nDattani, Saloni. 2024. “The rise in reported maternal mortality rates in the US is largely due to a change in measurement”. Our World in Data.\n\n\nDolatsara, Hamidreza Ahady, Ying-Ju Chen, Robert Leonard, Fadel Megahed, 와/과 Allison Jones-Farmer. 2021. “Explaining Predictive Model Performance: An Experimental Study of Data Preparation and Model Choice”. Big Data, 10월. https://doi.org/10.1089/big.2021.0067.\n\n\nEdgeworth, Francis Ysidro. 1885. “Methods of Statistics”. Journal of the Statistical Society of London, 181–217.\n\n\nFord, Paul. 2015. “What is Code?” Bloomberg Businessweek, 6월. https://www.bloomberg.com/graphics/2015-paul-ford-what-is-code/.\n\n\nForster, Edward Morgan. 1927. Aspects of the Novel. London: Edward Arnold.\n\n\nFranklin, Laura. 2005. “Exploratory experiments”. Philosophy of Science 72 (5): 888–99. https://doi.org/10.1086/508117.\n\n\nGould, Elliot, Hannah Fraser, Timothy Parker, Shinichi Nakagawa, Simon Griffith, Peter Vesk, 와/과 Fiona Fidler. 2023. “Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology”, 10월. https://doi.org/10.32942/x2gg62.\n\n\nHamming, Richard. (1997년) 2020. The Art of Doing Science and Engineering. 2nd ed. Stripe Press.\n\n\nHealy, Kieran. 2020. “The Kitchen Counter Observatory”, 5월. https://kieranhealy.org/blog/archives/2020/05/21/the-kitchen-counter-observatory/.\n\n\nHeil, Benjamin, Michael Hoffman, Florian Markowetz, Su-In Lee, Casey Greene, 와/과 Stephanie Hicks. 2021. “Reproducibility standards for machine learning in the life sciences”. Nature Methods 18 (10): 1132–35. https://doi.org/10.1038/s41592-021-01256-7.\n\n\nHullman, Jessica, 와/과 Andrew Gelman. 2021. “Designing for Interactive Exploratory Data Analysis Requires Theories of Graphical Inference”. Harvard Data Science Review 3 (3). https://doi.org/10.1162/99608f92.3ab8a587.\n\n\nHuntington-Klein, Nick, Andreu Arenas, Emily Beam, Marco Bertoni, Jeffrey Bloem, Pralhad Burli, Naibin Chen, 기타. 2021. “The influence of hidden researcher decisions in applied microeconomics”. Economic Inquiry 59: 944–60. https://doi.org/10.1111/ecin.12992.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, 와/과 Robert Tibshirani. (2013년) 2021. An Introduction to Statistical Learning with Applications in R. 2nd ed. Springer. https://www.statlearning.com.\n\n\nKent, William. 1993. “My Height: A Model For Numeric Information”. https://www.bkent.net/Doc/myheight.htm.\n\n\nKeyes, Os. 2019. “Counting the Countless”. Real Life. https://reallifemag.com/counting-the-countless/.\n\n\nMacDorman, Marian, 와/과 Eugene Declercq. 2018. “The failure of United States maternal mortality reporting and its impact on women’s lives”. Birth 45 (2): 105–8. https://doi.org/1111/birt.12333.\n\n\nMcElreath, Richard. (2015년) 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. 2nd ed. Chapman; Hall/CRC.\n\n\nMeng, Xiao-Li. 2018. “Statistical paradises and paradoxes in big data (I): Law of large populations, big data paradox, and the 2016 US presidential election”. The Annals of Applied Statistics 12 (2): 685–726. https://doi.org/10.1214/18-AOAS1161SF.\n\n\nRegister, Yim. 2020. “Data Science Ethics in 6 Minutes”. YouTube, 12월. https://youtu.be/mA4gypAiRYU.\n\n\nRilke, Rainer Maria. (1929년) 2014. Letters to a Young Poet. Penguin Classics.\n\n\nThornhill, John. 2021. “Lunch with the FT: Mathematician Hannah Fry”. Financial Times, 7월. https://www.ft.com/content/a5e33e5a-99b9-4bbc-948f-8a527c7675c3.\n\n\nWasserman, Larry. 2005. All of Statistics. Springer.\n\n\nWorld Health Organization. 2019. “Trends in maternal mortality 2000 to 2017: estimates by WHO, UNICEF, UNFPA, World Bank Group and the United Nations Population Division”. https://apps.who.int/iris/handle/10665/327596.",
    "crumbs": [
      "기초",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>데이터로 이야기하기</span>"
    ]
  },
  {
    "objectID": "02-drinking_from_a_fire_hose_ko.html",
    "href": "02-drinking_from_a_fire_hose_ko.html",
    "title": "2  소방 호스에서 물 마시기",
    "section": "",
    "text": "2.1 안녕하세요, 세상!\n선행 학습\n주요 개념 및 기술\n소프트웨어 및 패키지\n시작하는 방법은 시작하는 것입니다. 이 장에서는 이 책에서 권장하는 데이터 과학 워크플로의 세 가지 완전한 예를 살펴봅니다. 이는 다음을 의미합니다.\n\\[\n\\mbox{계획} \\rightarrow \\mbox{시뮬레이션} \\rightarrow \\mbox{수집} \\rightarrow \\mbox{탐색} \\rightarrow \\mbox{공유}\n\\] R을 처음 사용하는 경우 일부 코드가 약간 익숙하지 않을 수 있습니다. 통계를 처음 사용하는 경우 일부 개념이 익숙하지 않을 수 있습니다. 걱정하지 마십시오. 곧 모든 것이 익숙해질 것입니다.\n이야기를 하는 법을 배우는 유일한 방법은 스스로 이야기를 시작하는 것입니다. 즉, 이러한 예를 작동시켜야 합니다. 스케치를 직접 하고, 모든 것을 직접 입력하고(R을 처음 사용하고 로컬에 설치되어 있지 않은 경우 Posit Cloud 사용) 모든 것을 실행하십시오. 처음에는 어려울 것이라는 점을 깨닫는 것이 중요합니다. 이것은 정상입니다.\n여기서 철저하게 안내해 드릴 것입니다. 데이터로 이야기를 하는 즐거움을 경험함으로써 계속할 수 있는 힘을 얻기를 바랍니다.\n워크플로의 첫 번째 단계는 계획입니다. 나중에 상황에 대해 더 많이 알게 되면서 업데이트해야 할 수도 있지만 최종 목표를 설정해야 하기 때문에 이렇게 합니다. 그런 다음 계획의 세부 사항에 집중하기 위해 시뮬레이션합니다. 일부 프로젝트에서는 데이터 수집이 데이터셋을 다운로드하는 것만큼 간단할 수 있지만, 다른 프로젝트에서는 예를 들어 설문 조사를 수행하는 경우 데이터 수집이 훨씬 더 중요할 수 있습니다. 다양한 정량적 방법을 사용하여 데이터를 탐색하여 이해합니다. 그리고 마지막으로 청중의 요구에 초점을 맞춘 방식으로 이해한 내용을 공유합니다.\n시작하려면 Posit Cloud로 이동하여 계정을 만드십시오. 지금은 무료 버전으로 충분합니다. 처음에는 데스크톱 대신 이를 사용하여 모든 사람이 동일하게 시작할 수 있도록 하지만 비용을 지불하지 않으려면 나중에 로컬 설치로 변경해야 합니다. 계정을 만들고 로그인하면 ?fig-02-rstudio_cloud-1과 같이 보일 것입니다.\n“내 프로젝트”에 있을 것입니다. 여기서 새 프로젝트를 시작해야 합니다. “새 프로젝트” \\(\\rightarrow\\) “새 RStudio 프로젝트” (그림 2.1 (b)). “제목 없는 프로젝트”를 클릭하고 이름을 바꾸어 프로젝트에 이름을 지정할 수 있습니다.\n이제 호주 선거, 토론토 쉼터 사용, 신생아 사망률의 세 가지 작업 예를 살펴보겠습니다. 이러한 예는 복잡성을 증가시키지만 첫 번째 예부터 데이터로 이야기를 할 것입니다. 여기서 많은 측면을 간략하게 설명하지만 거의 모든 것이 책의 나머지 부분에서 훨씬 더 자세히 설명됩니다.",
    "crumbs": [
      "기초",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>소방 호스에서 물 마시기</span>"
    ]
  },
  {
    "objectID": "02-drinking_from_a_fire_hose_ko.html#안녕하세요-세상",
    "href": "02-drinking_from_a_fire_hose_ko.html#안녕하세요-세상",
    "title": "2  소방 호스에서 물 마시기",
    "section": "",
    "text": "새로운 도구를 배울 때마다 오랫동안 어려움을 겪을 것입니다\\(\\dots\\) 하지만 좋은 소식은 그것이 일반적이라는 것입니다. 모든 사람에게 일어나는 일이며 일시적일 뿐입니다.\n해들리 위컴, Barrett (2021) 인용.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) 처음으로 Posit Cloud 열기\n\n\n\n\n\n\n\n\n\n\n\n(b) 새 RStudio 프로젝트 열기\n\n\n\n\n\n\n\n그림 2.1: Posit Cloud 및 새 프로젝트 시작하기",
    "crumbs": [
      "기초",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>소방 호스에서 물 마시기</span>"
    ]
  },
  {
    "objectID": "02-drinking_from_a_fire_hose_ko.html#호주-선거",
    "href": "02-drinking_from_a_fire_hose_ko.html#호주-선거",
    "title": "2  소방 호스에서 물 마시기",
    "section": "2.2 호주 선거",
    "text": "2.2 호주 선거\n호주는 하원에 151석이 있는 의회 민주주의 국가이며, 하원은 정부가 구성되는 곳입니다. 두 개의 주요 정당(“자유당”과 “노동당”)과 두 개의 소수 정당(“국민당”과 “녹색당”), 그리고 많은 소규모 정당과 무소속 의원이 있습니다. 이 예에서는 2022년 연방 선거에서 각 정당이 획득한 의석 수를 그래프로 만들 것입니다.\n\n2.2.1 계획\n이 예에서는 두 가지 측면을 계획해야 합니다. 첫 번째는 필요한 데이터셋이 어떻게 생겼는지, 두 번째는 최종 그래프가 어떻게 생겼는지입니다.\n데이터셋의 기본 요구 사항은 의석 이름(호주에서는 때때로 “선거구”라고 함)과 당선된 사람의 정당이 있어야 한다는 것입니다. 필요한 데이터셋의 간단한 스케치는 그림 2.2 (a) 입니다.\n\n\n\n\n\n\n\n\n\n\n\n(a) 호주 선거 분석에 유용할 수 있는 데이터셋의 간략한 스케치\n\n\n\n\n\n\n\n\n\n\n\n(b) 각 정당이 획득한 의석 수에 대한 가능한 그래프의 간략한 스케치\n\n\n\n\n\n\n\n그림 2.2: 호주 선거와 관련된 잠재적 데이터셋 및 그래프 스케치\n\n\n\n또한 관심 있는 그래프를 계획해야 합니다. 각 정당이 획득한 의석 수를 표시하고 싶으므로 목표로 할 수 있는 간단한 스케치는 그림 2.2 (b) 입니다.\n\n\n2.2.2 시뮬레이션\n이제 스케치에 구체성을 부여하기 위해 일부 데이터를 시뮬레이션합니다.\n시작하려면 Posit Cloud 내에서 새 Quarto 문서를 만듭니다. “파일” \\(\\rightarrow\\) “새 파일” \\(\\rightarrow\\) “Quarto 문서\\(\\dots\\)”. “2022년 호주 선거 탐색”과 같은 제목을 지정하고 작성자로 이름을 추가하고 “시각적 마크다운 편집기 사용” 선택을 취소합니다(그림 2.3 (a)). 다른 옵션은 기본값으로 두고 “만들기”를 클릭합니다.\n\n\n\n\n\n\n\n\n\n\n\n(a) 새 Quarto 문서 만들기\n\n\n\n\n\n\n\n\n\n\n\n(b) 필요한 경우 rmarkdown 설치\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) 초기 설정 후 및 머리글 포함\n\n\n\n\n\n\n\n\n\n\n\n(d) 청크를 실행하기 위해 녹색 화살표 강조 표시\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e) 메시지를 제거하기 위해 X 강조 표시\n\n\n\n\n\n\n\n\n\n\n\n(f) 렌더링 버튼 강조 표시\n\n\n\n\n\n\n\n그림 2.3: Quarto 문서 시작하기\n\n\n\n“패키지 rmarkdown이 필요합니다\\(\\dots\\)”와 같은 알림을 받을 수 있습니다 (그림 2.3 (b)). 이 경우 “설치”를 클릭하십시오. 이 예에서는 모든 것을 이 하나의 Quarto 문서에 넣을 것입니다. “australian_elections.qmd”로 저장해야 합니다. “파일” \\(\\rightarrow\\) “다른 이름으로 저장\\(\\dots\\)”.\n거의 모든 기본 내용을 제거한 다음 제목 자료 아래에 새 R 코드 청크를 만듭니다. “코드” \\(\\rightarrow\\) “청크 삽입”. 그런 다음 다음을 설명하는 머리글 설명서를 추가합니다.\n\n문서의 목적\n저자 및 연락처 정보\n파일 작성 또는 마지막 업데이트 날짜\n파일이 의존하는 전제 조건.\n\n\n#### 머리말 ####\n# 목적: 2022년 호주 선거 데이터를 읽어 각 정당이 획득한 의석 수 그래프 만들기.\n# 저자: 로한 알렉산더\n# 이메일: rohan.alexander@utoronto.ca\n# 날짜: 2023년 1월 1일\n# 전제 조건: 호주 선거 데이터를 어디서 얻을 수 있는지 알기.\n\nR에서 “#”으로 시작하는 줄은 주석입니다. 즉, R에서 코드로 실행되지 않고 대신 사람이 읽도록 설계되었습니다. 이 머리글의 각 줄은 “#”으로 시작해야 합니다. 또한 이것이 머리글 섹션임을 명확히 하기 위해 “####”으로 둘러싸십시오. 결과는 ?fig-quarto-australian-elections-3과 같아야 합니다.\n이후 작업 공간을 설정해야 합니다. 여기에는 필요한 패키지를 설치하고 로드하는 작업이 포함됩니다. 패키지는 컴퓨터당 한 번만 설치하면 되지만 사용할 때마다 로드해야 합니다. 이 경우 tidyverse 및 janitor 패키지를 사용할 것입니다. 처음 사용하므로 설치해야 하며 각 패키지를 로드해야 합니다.\n\n\n\n\n\n\n거인의 어깨 위에 서서\n\n\n\n해들리 위컴은 RStudio의 수석 과학자입니다. 2008년 아이오와 주립대학교에서 통계학 박사 학위를 받은 후 라이스 대학교 조교수로 임명되었으며, 2013년 현재 Posit인 RStudio의 수석 과학자가 되었습니다. 그는 tidyverse 패키지 모음을 개발했으며 R for Data Science(Wickham, Çetinkaya-Rundel, 와/과 Grolemund [2016년] 2023) 및 Advanced R(Wickham 2019)을 포함한 많은 책을 출판했습니다. 그는 2019년 COPSS 회장상을 수상했습니다.\n\n\n패키지 설치 예는 다음과 같습니다. R 코드 청크와 관련된 작은 녹색 화살표를 클릭하여 이 코드를 실행합니다(그림 2.3 (d)).\n\n#### 작업 공간 설정 ####\ninstall.packages(\"tidyverse\")\ninstall.packages(\"janitor\")\n\n이제 패키지가 설치되었으므로 로드해야 합니다. 해당 패키지 설치 단계는 컴퓨터당 한 번만 수행하면 되므로 실수로 실행되지 않도록 해당 코드를 주석 처리하거나 제거해야 합니다. 또한 패키지를 설치할 때 인쇄된 메시지를 제거할 수 있습니다(그림 2.3 (e)).\n\n#### 작업 공간 설정 ####\n# install.packages(\"tidyverse\")\n# install.packages(\"janitor\")\n\nlibrary(tidyverse)\nlibrary(janitor)\n\n“렌더링”을 클릭하여 전체 문서를 렌더링할 수 있습니다(그림 2.3 (f)). 이렇게 하면 일부 패키지를 설치하라는 메시지가 표시될 수 있습니다. 이 경우 동의해야 합니다. 이렇게 하면 HTML 문서가 생성됩니다.\n방금 설치한 패키지에 대한 소개는 각 패키지에 해당 패키지와 해당 기능에 대한 정보를 제공하는 도움말 파일이 포함되어 있습니다. 패키지 이름 앞에 물음표를 붙인 다음 콘솔에서 해당 코드를 실행하여 액세스할 수 있습니다. 예를 들어 ?tidyverse입니다.\n데이터를 시뮬레이션하려면 “선거구”와 “정당”이라는 두 변수와 각 변수에 대한 일부 값을 사용하여 데이터셋을 만들어야 합니다. “선거구”의 경우 합리적인 값은 151개 호주 선거구 중 하나의 이름입니다. “정당”의 경우 합리적인 값은 “자유당”, “노동당”, “국민당”, “녹색당” 또는 “기타” 중 하나입니다. 다시 말하지만, 이 코드는 R 코드 청크와 관련된 작은 녹색 화살표를 클릭하여 실행할 수 있습니다.\n\nsimulated_data &lt;-\n  tibble(\n    # 각 선거구를 나타내기 위해 1부터 151까지 사용\n    \"Division\" = 1:151,\n    # 옵션을 무작위로 151번 복원 추출\n    \"Party\" = sample(\n      x = c(\"Liberal\", \"Labor\", \"National\", \"Green\", \"Other\"),\n      size = 151,\n      replace = TRUE\n    )\n  )\n\nsimulated_data\n\n# A tibble: 151 × 2\n   Division Party   \n      &lt;int&gt; &lt;chr&gt;   \n 1        1 Green   \n 2        2 Other   \n 3        3 Liberal \n 4        4 Other   \n 5        5 Green   \n 6        6 Green   \n 7        7 Liberal \n 8        8 Labor   \n 9        9 National\n10       10 Other   \n# ℹ 141 more rows\n\n\n어느 시점에서 코드가 실행되지 않고 다른 사람에게 도움을 요청하고 싶을 것입니다. 코드의 작은 조각을 스크린샷으로 찍고 그것을 기반으로 누군가가 도움을 줄 수 있을 것이라고 기대하지 마십시오. 그들은 거의 확실히 그럴 수 없습니다. 대신, 그들이 실행할 수 있는 방식으로 전체 스크립트를 제공해야 합니다. ?sec-reproducible-workflows에서 GitHub이 무엇인지 더 자세히 설명하겠지만, 지금은 도움이 필요하면 GitHub Gist를 순진하게 만들어 스크린샷을 찍는 것보다 더 도움이 되는 방식으로 코드를 공유할 수 있도록 해야 합니다. 첫 번째 단계는 GitHub에서 무료 계정을 만드는 것입니다(그림 2.4 (a)). 이 사용자 이름은 전문 프로필의 일부가 되므로 적절한 사용자 이름을 생각하는 것이 중요합니다. 전문적이고 과정과 독립적이며 이상적으로는 실제 이름과 관련된 사용자 이름을 사용하는 것이 합리적입니다. 그런 다음 오른쪽 상단에서 “+”를 찾고 “새 gist”를 선택합니다(그림 2.4 (b)).\n\n\n\n\n\n\n\n\n\n\n\n(a) GitHub 가입 화면\n\n\n\n\n\n\n\n\n\n\n\n(b) 새 GitHub Gist\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) 코드를 공유하기 위한 공개 GitHub Gist 만들기\n\n\n\n\n\n\n\n그림 2.4: 도움을 요청할 때 코드를 공유하기 위한 Gist 만들기\n\n\n\n여기서 오류가 발생하는 마지막 비트뿐만 아니라 모든 코드를 해당 Gist에 추가해야 합니다. 그리고 끝에 “.R”을 포함하는 의미 있는 파일 이름을 지정하십시오(예: “australian_elections.R”). ?fig-githubgisttwo에서는 library(Tidyverse) 대신 library(tidyverse)와 같이 대소문자가 잘못된 것으로 판명될 것입니다.\n“공개 gist 만들기”를 클릭합니다. 그런 다음 이 Gist의 URL을 도움을 요청하는 사람과 공유하고 문제가 무엇인지, 무엇을 달성하려고 하는지 설명할 수 있습니다. 모든 코드를 사용할 수 있으므로 그들이 돕기가 더 쉬울 것입니다.\n\n\n2.2.3 획득\n이제 실제 데이터를 얻고 싶습니다. 필요한 데이터는 호주 연방 선거를 조직하는 비당파 기관인 호주 선거관리위원회(AEC)에서 제공합니다. 웹사이트 페이지를 readr의 read_csv()에 전달할 수 있습니다. readr는 tidyverse의 일부이므로 명시적으로 로드할 필요가 없습니다. &lt;- 또는 “할당 연산자”는 read_csv()의 출력을 “raw_elections_data”라는 객체에 할당합니다.\n\n#### 데이터 읽어오기 ####\nraw_elections_data &lt;-\n  read_csv(\n    file =\n      \"https://results.aec.gov.au/27966/website/Downloads/HouseMembersElectedDownload-27966.csv\",\n    show_col_types = FALSE,\n    skip = 1\n  )\n\n# AEC 웹사이트에서 데이터를 읽었습니다.\n# 무슨 일이 생기거나 이동할 경우를 대비하여 저장하고 싶을 수 있습니다.\nwrite_csv(\n  x = raw_elections_data,\n  file = \"australian_voting.csv\"\n)\n\nhead()를 사용하여 데이터셋을 빠르게 살펴볼 수 있으며, 이는 처음 6개 행을 표시하고 tail()은 마지막 6개 행을 표시합니다.\n\nhead(raw_elections_data)\n\n# A tibble: 6 × 8\n  DivisionID DivisionNm StateAb CandidateID GivenNm   Surname   PartyNm  PartyAb\n       &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;  \n1        179 Adelaide   SA            36973 Steve     GEORGANAS Austral… ALP    \n2        197 Aston      VIC           36704 Alan      TUDGE     Liberal  LP     \n3        198 Ballarat   VIC           36409 Catherine KING      Austral… ALP    \n4        103 Banks      NSW           37018 David     COLEMAN   Liberal  LP     \n5        180 Barker     SA            37083 Tony      PASIN     Liberal  LP     \n6        104 Barton     NSW           36820 Linda     BURNEY    Austral… ALP    \n\ntail(raw_elections_data)\n\n# A tibble: 6 × 8\n  DivisionID DivisionNm StateAb CandidateID GivenNm    Surname  PartyNm  PartyAb\n       &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;  \n1        152 Wentworth  NSW           37451 Allegra    SPENDER  Indepen… IND    \n2        153 Werriwa    NSW           36810 Anne Maree STANLEY  Austral… ALP    \n3        150 Whitlam    NSW           36811 Stephen    JONES    Austral… ALP    \n4        178 Wide Bay   QLD           37506 Llew       O'BRIEN  Liberal… LNP    \n5        234 Wills      VIC           36452 Peter      KHALIL   Austral… ALP    \n6        316 Wright     QLD           37500 Scott      BUCHHOLZ Liberal… LNP    \n\n\n데이터를 사용하려면 정리해야 합니다. 계획 단계에서 원했던 데이터셋과 유사하게 만들려고 합니다. 계획에서 벗어나는 것은 괜찮지만, 이것은 신중하고 합리적인 결정이어야 합니다. 저장한 데이터셋을 읽은 후 가장 먼저 할 일은 변수 이름을 조정하는 것입니다. janitor의 clean_names()를 사용하여 이 작업을 수행합니다.\n\n#### 기본 정리 ####\nraw_elections_data &lt;-\n  read_csv(\n    file = \"australian_voting.csv\",\n    show_col_types = FALSE\n  )\n\n\n# 이름을 더 쉽게 입력할 수 있도록 만듭니다.\ncleaned_elections_data &lt;-\n  clean_names(raw_elections_data)\n\n# 처음 6개 행을 살펴봅니다.\nhead(cleaned_elections_data)\n\n# A tibble: 6 × 8\n  division_id division_nm state_ab candidate_id given_nm  surname   party_nm    \n        &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;       \n1         179 Adelaide    SA              36973 Steve     GEORGANAS Australian …\n2         197 Aston       VIC             36704 Alan      TUDGE     Liberal     \n3         198 Ballarat    VIC             36409 Catherine KING      Australian …\n4         103 Banks       NSW             37018 David     COLEMAN   Liberal     \n5         180 Barker      SA              37083 Tony      PASIN     Liberal     \n6         104 Barton      NSW             36820 Linda     BURNEY    Australian …\n# ℹ 1 more variable: party_ab &lt;chr&gt;\n\n\nRStudio가 자동으로 완성하므로 이름을 더 빨리 입력할 수 있습니다. 이렇게 하려면 변수 이름을 입력하기 시작한 다음 “tab” 키를 사용하여 완성합니다.\n데이터셋에는 많은 변수가 있으며, 주로 “division_nm”과 “party_nm”이라는 두 가지 변수에 관심이 있습니다. tidyverse의 일부로 로드한 dplyr의 select()를 사용하여 관심 있는 특정 변수를 선택할 수 있습니다. “파이프 연산자”, |&gt;는 한 줄의 출력을 다음 줄의 함수 첫 번째 입력으로 푸시합니다.\n\ncleaned_elections_data &lt;-\n  cleaned_elections_data |&gt;\n  select(\n    division_nm,\n    party_nm\n  )\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 × 2\n  division_nm party_nm              \n  &lt;chr&gt;       &lt;chr&gt;                 \n1 Adelaide    Australian Labor Party\n2 Aston       Liberal               \n3 Ballarat    Australian Labor Party\n4 Banks       Liberal               \n5 Barker      Liberal               \n6 Barton      Australian Labor Party\n\n\n일부 변수 이름은 약자로 되어 있어 여전히 명확하지 않습니다. names()를 사용하여 이 데이터셋의 열 이름을 볼 수 있습니다. 그리고 dplyr의 rename()을 사용하여 이름을 변경할 수 있습니다.\n\nnames(cleaned_elections_data)\n\n[1] \"division_nm\" \"party_nm\"   \n\n\n\ncleaned_elections_data &lt;-\n  cleaned_elections_data |&gt;\n  rename(\n    division = division_nm,\n    elected_party = party_nm\n  )\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 × 2\n  division elected_party         \n  &lt;chr&gt;    &lt;chr&gt;                 \n1 Adelaide Australian Labor Party\n2 Aston    Liberal               \n3 Ballarat Australian Labor Party\n4 Banks    Liberal               \n5 Barker   Liberal               \n6 Barton   Australian Labor Party\n\n\n이제 unique()를 사용하여 “elected_party” 열의 고유한 값을 볼 수 있습니다.\n\ncleaned_elections_data$elected_party |&gt;\n  unique()\n\n[1] \"Australian Labor Party\"              \n[2] \"Liberal\"                             \n[3] \"Liberal National Party of Queensland\"\n[4] \"The Greens\"                          \n[5] \"The Nationals\"                       \n[6] \"Independent\"                         \n[7] \"Katter's Australian Party (KAP)\"     \n[8] \"Centre Alliance\"                     \n\n\n원했던 것보다 더 자세한 내용이 있으므로 dplyr의 case_match()를 사용하여 시뮬레이션한 것과 일치하도록 정당 이름을 단순화하고 싶을 수 있습니다.\n\ncleaned_elections_data &lt;-\n  cleaned_elections_data |&gt;\n  mutate(\n    elected_party =\n      case_match(\n        elected_party,\n        \"Australian Labor Party\" ~ \"Labor\",\n        \"Liberal National Party of Queensland\" ~ \"Liberal\",\n        \"Liberal\" ~ \"Liberal\",\n        \"The Nationals\" ~ \"Nationals\",\n        \"The Greens\" ~ \"Greens\",\n        \"Independent\" ~ \"Other\",\n        \"Katter's Australian Party (KAP)\" ~ \"Other\",\n        \"Centre Alliance\" ~ \"Other\"\n      )\n  )\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 × 2\n  division elected_party\n  &lt;chr&gt;    &lt;chr&gt;        \n1 Adelaide Labor        \n2 Aston    Liberal      \n3 Ballarat Labor        \n4 Banks    Liberal      \n5 Barker   Liberal      \n6 Barton   Labor        \n\n\n이제 데이터가 계획과 일치합니다(그림 2.2 (a)). 모든 선거구에 대해 당선된 사람의 정당이 있습니다.\n이제 데이터셋을 깔끔하게 정리했으므로 다음 단계에서 해당 정리된 데이터셋으로 시작할 수 있도록 저장해야 합니다. 원시 데이터를 바꾸지 않고 나중에 정리된 데이터셋을 쉽게 식별할 수 있도록 새 파일 이름으로 저장해야 합니다.\n\nwrite_csv(\n  x = cleaned_elections_data,\n  file = \"cleaned_elections_data.csv\"\n)\n\n\n\n2.2.4 탐색\n생성한 데이터셋을 탐색하고 싶을 수 있습니다. 데이터셋을 더 잘 이해하는 한 가지 방법은 그래프를 만드는 것입니다. 특히, 여기서는 ?fig-australiaexample-graph에서 계획한 그래프를 만들고 싶습니다.\n먼저 방금 만든 데이터셋을 읽어옵니다.\n\n#### 데이터 읽어오기 ####\ncleaned_elections_data &lt;-\n  read_csv(\n    file = \"cleaned_elections_data.csv\",\n    show_col_types = FALSE\n  )\n\ndplyr의 count()를 사용하여 각 정당이 획득한 의석 수를 빠르게 계산할 수 있습니다.\n\ncleaned_elections_data |&gt;\n  count(elected_party)\n\n# A tibble: 5 × 2\n  elected_party     n\n  &lt;chr&gt;         &lt;int&gt;\n1 Greens            4\n2 Labor            77\n3 Liberal          48\n4 Nationals        10\n5 Other            12\n\n\n관심 있는 그래프를 만들려면 tidyverse의 일부인 ggplot2를 사용합니다. 이 패키지의 핵심 측면은 “+”를 사용하여 레이어를 추가하여 그래프를 만드는 것입니다. 이를 “추가 연산자”라고 합니다. 특히 ggplot2의 geom_bar()를 사용하여 막대 차트를 만듭니다(그림 2.5 (a)).\ncleaned_elections_data |&gt;\n  ggplot(aes(x = elected_party)) + # aes는 \"aesthetics\"의 약자입니다.\n  geom_bar()\n\ncleaned_elections_data |&gt;\n  ggplot(aes(x = elected_party)) +\n  geom_bar() +\n  theme_minimal() + # 테마를 더 깔끔하게 만듭니다.\n  labs(x = \"Party\", y = \"Number of seats\") # 레이블을 더 의미 있게 만듭니다.\n\n\n\n\n\n\n\n\n\n\n\n(a) 기본 옵션\n\n\n\n\n\n\n\n\n\n\n\n(b) 개선된 테마 및 레이블\n\n\n\n\n\n\n\n그림 2.5: 2022년 호주 연방 선거에서 정당별 획득 의석 수\n\n\n\n?fig-canadanice-1은 우리가 설정한 목표를 달성합니다. 그러나 기본 옵션을 수정하고 레이블을 개선하여 좀 더 보기 좋게 만들 수 있습니다(그림 2.5 (b)).\n\n\n2.2.5 공유\n지금까지 일부 데이터를 다운로드하고 정리하고 그래프를 만들었습니다. 일반적으로 우리가 한 일에 대해 어느 정도 자세히 전달해야 합니다. 이 경우, 우리가 한 일, 왜 했는지, 그리고 워크플로를 마무리하기 위해 무엇을 찾았는지에 대해 몇 단락을 작성할 수 있습니다. 다음은 예입니다.\n\n호주는 하원에 151석이 있는 의회 민주주의 국가이며, 하원은 정부가 구성되는 곳입니다. 두 개의 주요 정당(“자유당”과 “노동당”)과 두 개의 소수 정당(“국민당”과 “녹색당”), 그리고 많은 소규모 정당이 있습니다. 2022년 연방 선거는 5월 21일에 치러졌으며 약 1,500만 표가 투표되었습니다. 우리는 각 정당이 획득한 의석 수에 관심이 있었습니다.\n호주 선거관리위원회 웹사이트에서 의석별 결과를 다운로드했습니다. 통계 프로그래밍 언어 R (R Core Team 2024)과 tidyverse (Wickham 기타 2019) 및 janitor (Firke 2023)를 사용하여 데이터셋을 정리하고 정돈했습니다. 그런 다음 각 정당이 획득한 의석 수 그래프를 만들었습니다(그림 2.5).\n노동당이 77석을 얻었고 자유당이 48석을 얻었습니다. 소수 정당은 국민당이 10석, 녹색당이 4석을 얻었습니다. 마지막으로 10명의 무소속 의원과 소규모 정당 후보가 당선되었습니다.\n의석 분포는 두 주요 정당에 치우쳐 있으며, 이는 호주 유권자의 비교적 안정적인 선호도를 반영하거나 전국적인 네트워크나 자금 지원과 같은 주요 정당이 이미 가지고 있는 이점으로 인한 관성일 수 있습니다. 이러한 분포의 이유에 대한 더 나은 이해는 향후 연구에서 관심사입니다. 데이터셋은 투표한 모든 사람으로 구성되지만 호주에서는 일부 사람들이 체계적으로 투표에서 제외되고 다른 사람들보다 투표하기가 훨씬 더 어렵다는 점에 유의할 가치가 있습니다.\n\n특히 주의해야 할 한 가지 측면은 이 의사소통이 청중의 요구와 이야기 전달에 초점을 맞추도록 하는 것입니다. 데이터 저널리즘은 분석을 청중에게 맞게 조정해야 하는 방법에 대한 훌륭한 예를 제공합니다(예: Cardoso (2020) 및 Bronner (2020)).",
    "crumbs": [
      "기초",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>소방 호스에서 물 마시기</span>"
    ]
  },
  {
    "objectID": "02-drinking_from_a_fire_hose_ko.html#토론토의-노숙자-인구",
    "href": "02-drinking_from_a_fire_hose_ko.html#토론토의-노숙자-인구",
    "title": "2  소방 호스에서 물 마시기",
    "section": "2.3 토론토의 노숙자 인구",
    "text": "2.3 토론토의 노숙자 인구\n토론토에는 많은 노숙자 인구가 있습니다(City of Toronto 2021). 겨울이 혹독하기 때문에 쉼터에 충분한 장소가 있는 것이 중요합니다. 이 예에서는 2021년 쉼터 사용 현황표를 만들어 각 달의 평균 사용량을 비교합니다. 12월과 같이 추운 달에는 7월과 같이 더운 달에 비해 사용량이 더 많을 것으로 예상합니다.\n\n2.3.1 계획\n관심 있는 데이터셋에는 날짜, 쉼터, 그날 밤 점유된 침대 수가 있어야 합니다. 작동할 데이터셋의 간단한 스케치는 ?fig-torontohomeless-data입니다. 매일 밤 점유된 침대의 월평균 수를 보여주는 표를 만드는 데 관심이 있습니다. 표는 아마도 ?fig-torontohomeless-table과 같이 보일 것입니다.\n\n\n\n\n\n\n\n\n\n\n\n(a) 데이터셋의 간략한 스케치\n\n\n\n\n\n\n\n\n\n\n\n(b) 매월 점유된 침대 수 평균 표의 간략한 스케치\n\n\n\n\n\n\n\n그림 2.6: 토론토 쉼터 사용과 관련된 데이터셋 및 표 스케치\n\n\n\n\n\n2.3.2 시뮬레이션\n다음 단계는 데이터셋과 유사할 수 있는 일부 데이터를 시뮬레이션하는 것입니다. 시뮬레이션은 데이터 생성 프로세스에 대해 깊이 생각할 수 있는 기회를 제공합니다. 분석으로 전환할 때 가이드가 될 것입니다. 시뮬레이션을 먼저 사용하지 않고 분석을 수행하는 것은 목표 없이 화살을 쏘는 것과 같다고 생각할 수 있습니다. 확실히 무언가를 하고 있지만 잘하고 있는지는 명확하지 않습니다.\nPosit Cloud에서 새 Quarto 문서를 만들고 저장한 다음 새 R 코드 청크를 만들고 머리글 설명서를 추가합니다. 그런 다음 필요한 패키지를 설치 및/또는 로드합니다. 다시 tidyverse와 janitor를 사용할 것입니다. 이전에 설치했으므로 다시 설치할 필요는 없습니다. lubridate도 사용할 것입니다. 이것은 tidyverse의 일부이므로 독립적으로 설치할 필요는 없지만 로드해야 합니다. opendatatoronto와 knitr도 사용할 것이며 이들은 설치하고 로드해야 합니다.\n\n#### 머리말 ####\n# 목적: 2021년 쉼터 사용 데이터 가져오기 및 표 만들기\n# 저자: 로한 알렉산더\n# 이메일: rohan.alexander@utoronto.ca\n# 날짜: 2022년 7월 1일\n# 전제 조건: -\n\n#### 작업 공간 설정 ####\ninstall.packages(\"opendatatoronto\")\ninstall.packages(\"knitr\")\n\nlibrary(knitr)\nlibrary(janitor)\nlibrary(lubridate)\nlibrary(opendatatoronto)\nlibrary(tidyverse)\n\n이전 예에 좀 더 자세한 내용을 추가하자면, 패키지에는 다른 사람들이 작성한 코드가 포함되어 있습니다. 이 책에서 정기적으로 보게 될 몇 가지 일반적인 패키지가 있으며, 특히 tidyverse가 그렇습니다. 패키지를 사용하려면 먼저 설치한 다음 로드해야 합니다. 패키지는 컴퓨터당 한 번만 설치하면 되지만 매번 로드해야 합니다. 즉, 이전에 설치한 패키지는 여기서 다시 설치할 필요가 없습니다.\n\n\n\n\n\n\n거인의 어깨 위에 서서\n\n\n\n로버트 젠틀맨 박사는 R의 공동 개발자입니다. 1988년 워싱턴 대학교에서 통계학 박사 학위를 받은 후 오클랜드 대학교로 옮겼습니다. 그 후 23andMe를 비롯한 다양한 직책을 거쳐 현재 하버드 의과대학 계산 생의학 센터의 전무 이사입니다.\n\n\n\n\n\n\n\n\n거인의 어깨 위에 서서\n\n\n\n로스 이하카 박사는 R의 공동 개발자입니다. 그는 1985년 캘리포니아 대학교 버클리에서 통계학 박사 학위를 받았습니다. 그는 마오리족 지진의 신인 “루아우모코”라는 제목의 논문을 썼습니다. 그 후 오클랜드 대학교로 옮겨 평생 그곳에서 근무했습니다. 그는 2008년 뉴질랜드 왕립 학회 테 아파랑기에서 피커링 메달을 수상했습니다.\n\n\n사람들이 R과 우리가 사용하는 패키지를 만드는 데 시간을 기부한다는 점을 고려할 때 그들을 인용하는 것이 중요합니다. 필요한 정보를 얻으려면 citation()을 사용합니다. 인수 없이 실행하면 R 자체에 대한 인용 정보를 제공하고 패키지 이름인 인수를 사용하여 실행하면 해당 패키지에 대한 인용 정보를 제공합니다.\n\ncitation() # R에 대한 인용 정보 가져오기\n\nTo cite R in publications use:\n\n  R Core Team (2025). _R: A Language and Environment for Statistical\n  Computing_. R Foundation for Statistical Computing, Vienna, Austria.\n  &lt;https://www.R-project.org/&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2025},\n    url = {https://www.R-project.org/},\n  }\n\nWe have invested a lot of time and effort in creating R, please cite it\nwhen using it for data analysis. See also 'citation(\"pkgname\")' for\nciting R packages.\n\ncitation(\"ggplot2\") # 패키지에 대한 인용 정보 가져오기\n\nTo cite ggplot2 in publications, please use\n\n  H. Wickham. ggplot2: Elegant Graphics for Data Analysis.\n  Springer-Verlag New York, 2016.\n\nA BibTeX entry for LaTeX users is\n\n  @Book{,\n    author = {Hadley Wickham},\n    title = {ggplot2: Elegant Graphics for Data Analysis},\n    publisher = {Springer-Verlag New York},\n    year = {2016},\n    isbn = {978-3-319-24277-4},\n    url = {https://ggplot2.tidyverse.org},\n  }\n\n\n시뮬레이션으로 돌아가서 “날짜”, “쉼터”, “점유율”이라는 세 가지 변수가 필요합니다. 이 예는 set.seed()를 사용하여 시드를 추가하여 이전 예를 기반으로 합니다. 시드를 사용하면 동일한 코드를 실행할 때마다 항상 동일한 임의 데이터를 생성할 수 있습니다. 모든 정수를 시드로 사용할 수 있습니다. 이 경우 시드는 853입니다. 이를 시드로 사용하면 이 예와 동일한 임의의 숫자를 얻을 수 있습니다. 다른 시드를 사용하면 다른 임의의 숫자를 예상해야 합니다. 마지막으로 rep()를 사용하여 특정 횟수만큼 무언가를 반복합니다. 예를 들어, “쉼터 1”을 365번 반복하는데, 이는 약 1년을 차지합니다.\n\n#### 시뮬레이션 ####\nset.seed(853)\n\nsimulated_occupancy_data &lt;-\n  tibble(\n    date = rep(x = as.Date(\"2021-01-01\") + c(0:364), times = 3),\n    # Eddelbuettel 기반: https://stackoverflow.com/a/21502386\n    shelter = c(\n      rep(x = \"Shelter 1\", times = 365),\n      rep(x = \"Shelter 2\", times = 365),\n      rep(x = \"Shelter 3\", times = 365)\n    ),\n    number_occupied =\n      rpois(\n        n = 365 * 3,\n        lambda = 30\n      ) # 포아송 분포에서 1,095번 추출\n  )\n\nhead(simulated_occupancy_data)\n\n# A tibble: 6 × 3\n  date       shelter   number_occupied\n  &lt;date&gt;     &lt;chr&gt;               &lt;int&gt;\n1 2021-01-01 Shelter 1              28\n2 2021-01-02 Shelter 1              29\n3 2021-01-03 Shelter 1              35\n4 2021-01-04 Shelter 1              25\n5 2021-01-05 Shelter 1              21\n6 2021-01-06 Shelter 1              30\n\n\n이 시뮬레이션에서는 먼저 2021년의 모든 날짜 목록을 만듭니다. 해당 목록을 세 번 반복합니다. 연중 매일 세 개의 쉼터에 대한 데이터를 가정합니다. 매일 밤 점유된 침대 수를 시뮬레이션하기 위해 포아송 분포에서 추출하며, 쉼터당 평균 30개의 침대가 점유된다고 가정하지만 이는 임의적인 선택일 뿐입니다. 배경 지식으로, 포아송 분포는 개수 데이터가 있을 때 자주 사용되며 ?sec-its-just-a-generalized-linear-model에서 다시 다룹니다.\n\n\n2.3.3 획득\n토론토 시에서 제공하는 토론토 쉼터 사용에 대한 데이터를 사용합니다. 쉼터 사용은 매일 밤 오전 4시에 점유된 침대 수를 세어 측정합니다. 데이터에 액세스하려면 opendatatoronto를 사용한 다음 자체 사본을 저장합니다.\n\n#### 획득 ####\ntoronto_shelters &lt;-\n  # 각 패키지는 Open Data Toronto의 관련 페이지 \"개발자용\" 탭에서 찾을 수 있는 고유 ID와 연결됩니다.\n  # https://open.toronto.ca/dataset/daily-shelter-overnight-service-occupancy-capacity/\n  list_package_resources(\"21c83b32-d5a8-4106-a54f-010dbe49f6f2\") |&gt;\n  # 해당 패키지 내에서 2021년 데이터셋에 관심이 있습니다.\n  filter(name ==\n    \"daily-shelter-overnight-service-occupancy-capacity-2021.csv\") |&gt;\n  # 데이터셋을 한 행으로 줄인 후 리소스를 가져올 수 있습니다.\n  get_resource()\n\nwrite_csv(\n  x = toronto_shelters,\n  file = \"toronto_shelters.csv\"\n)\n\nhead(toronto_shelters)\n\n\nhead(toronto_shelters)\n\n# A tibble: 6 × 32\n   X_id OCCUPANCY_DATE ORGANIZATION_ID ORGANIZATION_NAME        SHELTER_ID\n  &lt;dbl&gt; &lt;chr&gt;                    &lt;dbl&gt; &lt;chr&gt;                         &lt;dbl&gt;\n1     1 21-01-01                    24 COSTI Immigrant Services         40\n2     2 21-01-01                    24 COSTI Immigrant Services         40\n3     3 21-01-01                    24 COSTI Immigrant Services         40\n4     4 21-01-01                    24 COSTI Immigrant Services         40\n5     5 21-01-01                    24 COSTI Immigrant Services         40\n6     6 21-01-01                    24 COSTI Immigrant Services         40\n# ℹ 27 more variables: SHELTER_GROUP &lt;chr&gt;, LOCATION_ID &lt;dbl&gt;,\n#   LOCATION_NAME &lt;chr&gt;, LOCATION_ADDRESS &lt;chr&gt;, LOCATION_POSTAL_CODE &lt;chr&gt;,\n#   LOCATION_CITY &lt;chr&gt;, LOCATION_PROVINCE &lt;chr&gt;, PROGRAM_ID &lt;dbl&gt;,\n#   PROGRAM_NAME &lt;chr&gt;, SECTOR &lt;chr&gt;, PROGRAM_MODEL &lt;chr&gt;,\n#   OVERNIGHT_SERVICE_TYPE &lt;chr&gt;, PROGRAM_AREA &lt;chr&gt;, SERVICE_USER_COUNT &lt;dbl&gt;,\n#   CAPACITY_TYPE &lt;chr&gt;, CAPACITY_ACTUAL_BED &lt;dbl&gt;, CAPACITY_FUNDING_BED &lt;dbl&gt;,\n#   OCCUPIED_BEDS &lt;dbl&gt;, UNOCCUPIED_BEDS &lt;dbl&gt;, UNAVAILABLE_BEDS &lt;dbl&gt;, …\n\n\n관심 있는 데이터셋과 유사하게 만들기 위해 이것에 많은 작업이 필요하지 않습니다(그림 2.6 (a)). clean_names()를 사용하여 이름을 더 쉽게 입력할 수 있도록 변경하고 select()를 사용하여 관련 열만으로 줄여야 합니다.\n\ntoronto_shelters_clean &lt;-\n  clean_names(toronto_shelters) |&gt;\n  mutate(occupancy_date = ymd(occupancy_date)) |&gt;\n  select(occupancy_date, occupied_beds)\n\nhead(toronto_shelters_clean)\n\n# A tibble: 6 × 2\n  occupancy_date occupied_beds\n  &lt;date&gt;                 &lt;dbl&gt;\n1 2021-01-01                NA\n2 2021-01-01                NA\n3 2021-01-01                NA\n4 2021-01-01                NA\n5 2021-01-01                NA\n6 2021-01-01                 6\n\n\n남은 것은 정리된 데이터셋을 저장하는 것뿐입니다.\n\nwrite_csv(\n  x = toronto_shelters_clean,\n  file = \"cleaned_toronto_shelters.csv\"\n)\n\n\n\n2.3.4 탐색\n먼저 방금 만든 데이터셋을 로드합니다.\n\n#### 탐색 ####\ntoronto_shelters_clean &lt;-\n  read_csv(\n    \"cleaned_toronto_shelters.csv\",\n    show_col_types = FALSE\n  )\n\n데이터셋에는 각 쉼터에 대한 일일 기록이 포함되어 있습니다. 각 달의 평균 사용량을 이해하는 데 관심이 있습니다. 이렇게 하려면 lubridate의 month()를 사용하여 월 열을 추가해야 합니다. 기본적으로 month()는 월 번호를 제공하므로 월의 전체 이름을 얻기 위해 “label”과 “abbr”이라는 두 가지 인수를 포함합니다. tidyverse의 일부인 tidyr의 drop_na()를 사용하여 침대 수에 대한 데이터가 없는 행을 제거합니다. 여기서는 시작에 초점을 맞추고 있으므로 생각 없이 이 작업을 수행하지만 이것은 중요한 결정이며 장 6 및 ?sec-exploratory-data-analysis에서 누락된 데이터에 대해 더 자세히 설명합니다. 그런 다음 dplyr의 summarise()를 사용하여 월별 그룹을 기준으로 요약 통계를 만듭니다. tinytable의 tt()를 사용하여 ?tbl-homelessoccupancyd를 만듭니다.\n\ntoronto_shelters_clean |&gt;\n  mutate(occupancy_month = month(\n    occupancy_date,\n    label = TRUE,\n    abbr = FALSE\n  )) |&gt;\n  arrange(month(occupancy_date)) |&gt;\n  drop_na(occupied_beds) |&gt;\n  summarise(number_occupied = mean(occupied_beds),\n            .by = occupancy_month) |&gt;\n  tt()\n\n\n\n표 2.1: 2021년 토론토 쉼터 이용 현황\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                occupancy_month\n                number_occupied\n              \n        \n        \n        \n                \n                  January\n                  28.55708\n                \n                \n                  February\n                  27.73821\n                \n                \n                  March\n                  27.18521\n                \n                \n                  April\n                  26.31561\n                \n                \n                  May\n                  27.42596\n                \n                \n                  June\n                  28.88300\n                \n                \n                  July\n                  29.67137\n                \n                \n                  August\n                  30.83975\n                \n                \n                  September\n                  31.65405\n                \n                \n                  October\n                  32.32991\n                \n                \n                  November\n                  33.26980\n                \n                \n                  December\n                  33.52426\n                \n        \n      \n    \n\n\n\n\n\n\n이전과 마찬가지로 이것은 괜찮아 보이며 우리가 설정한 목표를 달성합니다. 그러나 기본값을 약간 수정하여 더 보기 좋게 만들 수 있습니다(표 2.2). 특히 열 이름을 읽기 쉽게 만들고 적절한 소수 자릿수만 표시하고 정렬을 변경합니다(j는 관심 있는 열 번호를 지정하는 데 사용되고 r은 정렬 유형(즉, 오른쪽)입니다).\n\ntoronto_shelters_clean |&gt;\n  mutate(occupancy_month = month(\n    occupancy_date,\n    label = TRUE,\n    abbr = FALSE\n  )) |&gt;\n  arrange(month(occupancy_date)) |&gt;\n  drop_na(occupied_beds) |&gt;\n  summarise(number_occupied = mean(occupied_beds),\n            .by = occupancy_month) |&gt;\n  tt(\n    digits = 1\n  ) |&gt;\n  style_tt(j = 2, align = \"r\") |&gt;\n  setNames(c(\"Month\", \"Average daily number of occupied beds\"))\n\n\n\n표 2.2: 2021년 토론토 쉼터 이용 현황\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Month\n                Average daily number of occupied beds\n              \n        \n        \n        \n                \n                  January\n                  29\n                \n                \n                  February\n                  28\n                \n                \n                  March\n                  27\n                \n                \n                  April\n                  26\n                \n                \n                  May\n                  27\n                \n                \n                  June\n                  29\n                \n                \n                  July\n                  30\n                \n                \n                  August\n                  31\n                \n                \n                  September\n                  32\n                \n                \n                  October\n                  32\n                \n                \n                  November\n                  33\n                \n                \n                  December\n                  34\n                \n        \n      \n    \n\n\n\n\n\n\n\n\n2.3.5 공유\n우리가 한 일, 왜 했는지, 그리고 우리가 발견한 것을 요약하기 위해 몇 개의 짧은 단락을 작성해야 합니다. 다음은 예입니다.\n\n토론토에는 많은 노숙자 인구가 있습니다. 겨울이 혹독하기 때문에 쉼터에 충분한 장소가 있는 것이 중요합니다. 우리는 추운 달과 더운 달을 비교하여 쉼터 사용량이 어떻게 변하는지 이해하는 데 관심이 있습니다.\n토론토 시에서 제공하는 토론토 쉼터 침대 점유율에 대한 데이터를 사용합니다. 구체적으로, 매일 밤 오전 4시에 점유된 침대 수를 계산합니다. 우리는 이것을 월별로 평균내는 데 관심이 있습니다. 통계 프로그래밍 언어 R (R Core Team 2024)과 tidyverse (Wickham 2017), janitor (Firke 2023), opendatatoronto (Gelfand 2022), lubridate (Grolemund 와/과 Wickham 2011), knitr (Xie 2023)를 사용하여 데이터셋을 정리, 정돈 및 분석했습니다. 그런 다음 각 달의 매일 밤 평균 점유 침대 수 표를 만들었습니다(표 2.2).\n2021년 12월의 일일 평균 점유 침대 수는 2021년 7월보다 높았으며, 12월에는 34개의 침대가 점유된 반면 7월에는 30개의 침대가 점유되었습니다(표 2.2). 더 일반적으로 7월과 12월 사이에 일일 평균 점유 침대 수가 꾸준히 증가했으며 매달 약간의 전반적인 증가가 있었습니다.\n데이터셋은 쉼터를 기준으로 하므로 특히 크거나 작은 쉼터에 특정한 변경 사항으로 인해 결과가 왜곡될 수 있습니다. 특정 쉼터는 추운 달에 특히 매력적일 수 있습니다. 또한 점유된 침대 수에 관심이 있었지만 계절에 따라 침대 공급이 변경되면 관심 있는 추가 통계는 점유율입니다.\n\n이 예는 몇 단락에 불과하지만 요약본을 만들거나 각 단락을 섹션으로 확장하여 전체 보고서를 만들 수 있습니다. 첫 번째 단락은 일반적인 개요이고, 두 번째 단락은 데이터에 초점을 맞추고, 세 번째 단락은 결과에 초점을 맞추고, 네 번째 단락은 토론입니다. (hao2019의?) 예를 따라 네 번째 단락은 편향이 스며들었을 수 있는 영역을 고려하기에 좋은 곳입니다.",
    "crumbs": [
      "기초",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>소방 호스에서 물 마시기</span>"
    ]
  },
  {
    "objectID": "02-drinking_from_a_fire_hose_ko.html#신생아-사망률",
    "href": "02-drinking_from_a_fire_hose_ko.html#신생아-사망률",
    "title": "2  소방 호스에서 물 마시기",
    "section": "2.4 신생아 사망률",
    "text": "2.4 신생아 사망률\n신생아 사망률은 생후 첫 달 이내에 발생하는 사망을 의미합니다. 신생아 사망률(NMR)은 출생 1,000명당 신생아 사망자 수입니다(UN IGME 2021). 세 번째 지속 가능한 개발 목표(SDG)는 NMR을 12로 줄이는 것을 요구합니다. 이 예에서는 아르헨티나, 호주, 캐나다, 케냐의 지난 50년간 추정 NMR 그래프를 만듭니다.\n\n2.4.1 계획\n이 예에서는 데이터셋이 어떻게 보여야 하고 그래프가 어떻게 보여야 하는지 생각해 봐야 합니다.\n데이터셋에는 국가와 연도를 지정하는 변수가 있어야 합니다. 또한 해당 연도 해당 국가의 NMR 추정치가 있는 변수도 있어야 합니다. 대략적으로 ?fig-nmrexample-data와 같이 보여야 합니다.\n\n\n\n\n\n\n\n\n\n\n\n(a) 잠재적으로 유용한 NMR 데이터셋의 간략한 스케치\n\n\n\n\n\n\n\n\n\n\n\n(b) 시간 경과에 따른 국가별 NMR 그래프의 간략한 스케치\n\n\n\n\n\n\n\n그림 2.7: 신생아 사망률(NMR)에 대한 데이터셋 및 그래프 스케치\n\n\n\nx축에는 연도, y축에는 추정 NMR이 있는 그래프를 만들고 싶습니다. 각 국가는 자체 계열을 가져야 합니다. 우리가 찾고 있는 것의 간단한 스케치는 ?fig-nmrexample-graph입니다.\n\n\n2.4.2 시뮬레이션\n계획과 일치하는 일부 데이터를 시뮬레이션하고 싶습니다. 이 경우 국가, 연도 및 NMR이라는 세 개의 열이 필요합니다.\nPosit Cloud 내에서 새 Quarto 문서를 만들고 저장합니다. 머리글 설명서를 추가하고 작업 공간을 설정합니다. tidyverse, janitor 및 lubridate를 사용할 것입니다.\n\n#### 머리말 ####\n# 목적: 지난 50년간 4개국의 신생아 사망률에 대한 데이터를 확보하고 준비하여 그래프를 만듭니다.\n# 저자: 로한 알렉산더\n# 이메일: rohan.alexander@utoronto.ca\n# 날짜: 2022년 7월 1일\n# 전제 조건: -\n\n#### 작업 공간 설정 ####\nlibrary(janitor)\nlibrary(lubridate)\nlibrary(tidyverse)\n\n패키지에 포함된 코드는 저자가 업데이트하고 새 버전을 출시함에 따라 때때로 변경될 수 있습니다. packageVersion()을 사용하여 사용 중인 패키지 버전을 확인할 수 있습니다. 예를 들어, tidyverse 버전 2.0.0과 janitor 버전 2.2.0을 사용하고 있습니다.\n\npackageVersion(\"tidyverse\")\n\n[1] '2.0.0'\n\npackageVersion(\"janitor\")\n\n[1] '2.2.1'\n\n\n설치한 모든 패키지의 버전을 업데이트하려면 update.packages()를 사용합니다. tidyverse_update()를 사용하여 tidyverse 패키지만 설치할 수 있습니다. 매일 실행할 필요는 없지만 때때로 패키지를 업데이트하는 것이 좋습니다. 많은 패키지가 이전 버전과의 호환성을 보장하기 위해 주의를 기울이지만 어느 시점에서는 이것이 불가능해집니다. 패키지를 업데이트하면 이전 코드를 다시 작성해야 할 수 있습니다. 시작할 때는 큰 문제가 아니며 어쨌든 ?sec-reproducible-workflows에서 다루는 특정 버전을 로드하기 위한 도구가 있습니다.\n시뮬레이션으로 돌아가서, rep()를 사용하여 각 국가의 이름을 50번 반복하고 50년이 지나도록 합니다. 마지막으로 runif()를 사용하여 균일 분포에서 추출하여 해당 연도 해당 국가의 추정 NMR 값을 시뮬레이션합니다.\n\n#### 데이터 시뮬레이션 ####\nset.seed(853)\n\nsimulated_nmr_data &lt;-\n  tibble(\n    country =\n      c(rep(\"Argentina\", 50), rep(\"Australia\", 50),\n        rep(\"Canada\", 50), rep(\"Kenya\", 50)),\n    year =\n      rep(c(1971:2020), 4),\n    nmr =\n      runif(n = 200, min = 0, max = 100)\n  )\n\nhead(simulated_nmr_data)\n\n# A tibble: 6 × 3\n  country    year   nmr\n  &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt;\n1 Argentina  1971 35.9 \n2 Argentina  1972 12.0 \n3 Argentina  1973 48.4 \n4 Argentina  1974 31.6 \n5 Argentina  1975  3.74\n6 Argentina  1976 40.4 \n\n\n이 시뮬레이션은 작동하지만, 50년 대신 예를 들어 60년을 시뮬레이션하기로 결정하면 시간이 많이 걸리고 오류가 발생하기 쉽습니다. 이 코드를 개선하는 한 가지 방법은 모든 50을 변수로 바꾸는 것입니다.\n\n#### 데이터 시뮬레이션 ####\nset.seed(853)\n\nnumber_of_years &lt;- 50\n\nsimulated_nmr_data &lt;-\n  tibble(\n    country =\n      c(rep(\"Argentina\", number_of_years), rep(\"Australia\", number_of_years),\n        rep(\"Canada\", number_of_years), rep(\"Kenya\", number_of_years)),\n    year =\n      rep(c(1:number_of_years + 1970), 4),\n    nmr =\n      runif(n = number_of_years * 4, min = 0, max = 100)\n  )\n\nhead(simulated_nmr_data)\n\n# A tibble: 6 × 3\n  country    year   nmr\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 Argentina  1971 35.9 \n2 Argentina  1972 12.0 \n3 Argentina  1973 48.4 \n4 Argentina  1974 31.6 \n5 Argentina  1975  3.74\n6 Argentina  1976 40.4 \n\n\n결과는 동일하지만 이제 50년에서 60년으로 변경하려면 한 곳에서만 변경하면 됩니다.\n이 시뮬레이션된 데이터셋은 비교적 간단하고 코드를 직접 작성했기 때문에 신뢰할 수 있습니다. 그러나 실제 데이터셋으로 전환하면 그것이 주장하는 것인지 확신하기가 더 어렵습니다. 데이터를 신뢰하더라도 다른 사람과 그 신뢰를 공유할 수 있어야 합니다. 한 가지 방법은 데이터가 제대로 되어 있는지 여부에 대한 몇 가지 테스트를 설정하는 것입니다. 예를 들어 다음과 같이 예상합니다.\n\n“국가”는 “아르헨티나”, “호주”, “캐나다” 또는 “케냐” 중 하나여야 합니다.\n반대로 “국가”에는 해당 4개국이 모두 포함되어야 합니다.\n“연도”는 1971년보다 작지 않고 2020년보다 크지 않으며 문자나 소수점 이하 자릿수가 있는 숫자가 아닌 정수여야 합니다.\n“nmr”은 0에서 1,000 사이의 값이며 숫자여야 합니다.\n\n이러한 기능을 기반으로 데이터셋이 통과할 것으로 예상되는 일련의 테스트를 작성할 수 있습니다.\n\nsimulated_nmr_data$country |&gt;\n  unique() == c(\"Argentina\", \"Australia\", \"Canada\", \"Kenya\")\n\nsimulated_nmr_data$country |&gt;\n  unique() |&gt;\n  length() == 4\n\nsimulated_nmr_data$year |&gt; min() == 1971\nsimulated_nmr_data$year |&gt; max() == 2020\nsimulated_nmr_data$nmr |&gt; min() &gt;= 0\nsimulated_nmr_data$nmr |&gt; max() &lt;= 1000\nsimulated_nmr_data$nmr |&gt; class() == \"numeric\"\n\n이러한 테스트를 통과하면 시뮬레이션된 데이터셋에 대한 확신을 가질 수 있습니다. 더 중요한 것은 이러한 테스트를 실제 데이터셋에 적용할 수 있다는 것입니다. 이를 통해 해당 데이터셋에 대한 확신을 높이고 다른 사람과 그 확신을 공유할 수 있습니다.\n\n\n2.4.3 획득\nUN 아동 사망률 추정 기관 간 그룹(IGME)은 우리가 다운로드하고 저장할 수 있는 NMR 추정치를 제공합니다.\n\n#### 데이터 획득 ####\nraw_igme_data &lt;-\n  read_csv(\n    file =\n      \"https://childmortality.org/wp-content/uploads/2021/09/UNIGME-2021.csv\",\n    show_col_types = FALSE\n  )\n\nwrite_csv(x = raw_igme_data, file = \"igme.csv\")\n\n이와 같이 확립된 데이터의 경우 데이터에 대한 지원 자료를 읽는 것이 유용할 수 있습니다. 이 경우 코드북은 여기에서 사용할 수 있습니다. 이 후 데이터셋을 더 잘 이해하기 위해 빠르게 살펴볼 수 있습니다. head() 및 tail()을 사용하여 데이터셋이 어떻게 생겼는지, names()를 사용하여 열 이름이 무엇인지에 관심이 있을 수 있습니다.\n\nhead(raw_igme_data)\n\n# A tibble: 6 × 29\n  `Geographic area` Indicator              Sex   `Wealth Quintile` `Series Name`\n  &lt;chr&gt;             &lt;chr&gt;                  &lt;chr&gt; &lt;chr&gt;             &lt;chr&gt;        \n1 Afghanistan       Neonatal mortality ra… Total Total             Multiple Ind…\n2 Afghanistan       Neonatal mortality ra… Total Total             Multiple Ind…\n3 Afghanistan       Neonatal mortality ra… Total Total             Multiple Ind…\n4 Afghanistan       Neonatal mortality ra… Total Total             Multiple Ind…\n5 Afghanistan       Neonatal mortality ra… Total Total             Multiple Ind…\n6 Afghanistan       Neonatal mortality ra… Total Total             Afghanistan …\n# ℹ 24 more variables: `Series Year` &lt;chr&gt;, `Regional group` &lt;chr&gt;,\n#   TIME_PERIOD &lt;chr&gt;, OBS_VALUE &lt;dbl&gt;, COUNTRY_NOTES &lt;chr&gt;, CONNECTION &lt;lgl&gt;,\n#   DEATH_CATEGORY &lt;lgl&gt;, CATEGORY &lt;chr&gt;, `Observation Status` &lt;chr&gt;,\n#   `Unit of measure` &lt;chr&gt;, `Series Category` &lt;chr&gt;, `Series Type` &lt;chr&gt;,\n#   STD_ERR &lt;dbl&gt;, REF_DATE &lt;dbl&gt;, `Age Group of Women` &lt;chr&gt;,\n#   `Time Since First Birth` &lt;chr&gt;, DEFINITION &lt;chr&gt;, INTERVAL &lt;dbl&gt;,\n#   `Series Method` &lt;chr&gt;, LOWER_BOUND &lt;dbl&gt;, UPPER_BOUND &lt;dbl&gt;, …\n\n\n\nnames(raw_igme_data)\n\n [1] \"Geographic area\"        \"Indicator\"              \"Sex\"                   \n [4] \"Wealth Quintile\"        \"Series Name\"            \"Series Year\"           \n [7] \"Regional group\"         \"TIME_PERIOD\"            \"OBS_VALUE\"             \n[10] \"COUNTRY_NOTES\"          \"CONNECTION\"             \"DEATH_CATEGORY\"        \n[13] \"CATEGORY\"               \"Observation Status\"     \"Unit of measure\"       \n[16] \"Series Category\"        \"Series Type\"            \"STD_ERR\"               \n[19] \"REF_DATE\"               \"Age Group of Women\"     \"Time Since First Birth\"\n[22] \"DEFINITION\"             \"INTERVAL\"               \"Series Method\"         \n[25] \"LOWER_BOUND\"            \"UPPER_BOUND\"            \"STATUS\"                \n[28] \"YEAR_TO_ACHIEVE\"        \"Model Used\"            \n\n\n이름을 정리하고 관심 있는 행과 열만 유지하고 싶습니다. 계획에 따라 “성별”이 “전체”이고, “계열 이름”이 “UN IGME 추정치”이고, “지리적 영역”이 “아르헨티나”, “호주”, “캐나다”, “케냐” 중 하나이고, “지표”가 “신생아 사망률”인 행에 관심이 있습니다. 이 후 “geographic_area”, “time_period”, “obs_value”라는 몇 개의 열에만 관심이 있습니다.\n\ncleaned_igme_data &lt;-\n  clean_names(raw_igme_data) |&gt;\n  filter(\n    sex == \"Total\",\n    series_name == \"UN IGME estimate\",\n    geographic_area %in% c(\"Argentina\", \"Australia\", \"Canada\", \"Kenya\"),\n    indicator == \"Neonatal mortality rate\"\n    ) |&gt;\n  select(geographic_area, time_period, obs_value)\n\nhead(cleaned_igme_data)\n\n# A tibble: 6 × 3\n  geographic_area time_period obs_value\n  &lt;chr&gt;           &lt;chr&gt;           &lt;dbl&gt;\n1 Argentina       1970-06          24.9\n2 Argentina       1971-06          24.7\n3 Argentina       1972-06          24.6\n4 Argentina       1973-06          24.6\n5 Argentina       1974-06          24.5\n6 Argentina       1975-06          24.1\n\n\n두 가지 다른 측면을 수정해야 합니다. “time_period”의 클래스는 문자인데 연도여야 하고, “obs_value”의 이름은 더 정보를 제공하기 위해 “nmr”이어야 합니다.\n\ncleaned_igme_data &lt;-\n  cleaned_igme_data |&gt;\n  mutate(\n    time_period = str_remove(time_period, \"-06\"),\n    time_period = as.integer(time_period)\n  ) |&gt;\n  filter(time_period &gt;= 1971) |&gt;\n  rename(nmr = obs_value, year = time_period, country = geographic_area)\n\nhead(cleaned_igme_data)\n\n# A tibble: 6 × 3\n  country    year   nmr\n  &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt;\n1 Argentina  1971  24.7\n2 Argentina  1972  24.6\n3 Argentina  1973  24.6\n4 Argentina  1974  24.5\n5 Argentina  1975  24.1\n6 Argentina  1976  23.3\n\n\n마지막으로 시뮬레이션된 데이터셋을 기반으로 개발한 테스트를 데이터셋이 통과하는지 확인할 수 있습니다.\n\ncleaned_igme_data$country |&gt;\n  unique() == c(\"Argentina\", \"Australia\", \"Canada\", \"Kenya\")\n\n[1] TRUE TRUE TRUE TRUE\n\ncleaned_igme_data$country |&gt;\n  unique() |&gt;\n  length() == 4\n\n[1] TRUE\n\ncleaned_igme_data$year |&gt; min() == 1971\n\n[1] TRUE\n\ncleaned_igme_data$year |&gt; max() == 2020\n\n[1] TRUE\n\ncleaned_igme_data$nmr |&gt; min() &gt;= 0\n\n[1] TRUE\n\ncleaned_igme_data$nmr |&gt; max() &lt;= 1000\n\n[1] TRUE\n\ncleaned_igme_data$nmr |&gt; class() == \"numeric\"\n\n[1] TRUE\n\n\n남은 것은 깔끔하게 정리된 데이터셋을 저장하는 것뿐입니다.\n\nwrite_csv(x = cleaned_igme_data, file = \"cleaned_igme_data.csv\")\n\n\n\n2.4.4 탐색\n정리된 데이터셋을 사용하여 추정 NMR 그래프를 만들고 싶습니다. 먼저 데이터셋을 읽어옵니다.\n\n#### 탐색 ####\ncleaned_igme_data &lt;-\n  read_csv(\n    file = \"cleaned_igme_data.csv\",\n    show_col_types = FALSE\n  )\n\n이제 NMR이 시간에 따라 어떻게 변했는지, 그리고 국가 간의 차이점을 보여주는 그래프를 만들 수 있습니다(그림 2.8).\n\ncleaned_igme_data |&gt;\n  ggplot(aes(x = year, y = nmr, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"Year\", y = \"Neonatal Mortality Rate (NMR)\", color = \"Country\") +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n그림 2.8: 아르헨티나, 호주, 캐나다, 케냐의 신생아 사망률(NMR) (1971-2020)\n\n\n\n\n\n\n\n2.4.5 공유\n지금까지 일부 데이터를 다운로드하고 정리하고 일부 테스트를 작성하고 그래프를 만들었습니다. 일반적으로 우리가 한 일에 대해 어느 정도 자세히 전달해야 합니다. 이 경우, 우리가 한 일, 왜 했는지, 그리고 무엇을 찾았는지에 대해 몇 단락을 작성할 것입니다.\n\n신생아 사망은 생후 첫 달 이내에 발생하는 사망을 의미합니다. 특히 신생아 사망률(NMR)은 출생 1,000명당 신생아 사망자 수입니다. 우리는 지난 50년간 아르헨티나, 호주, 캐나다, 케냐 4개국의 NMR 추정치를 얻습니다.\nUN 아동 사망률 추정 기관 간 그룹(IGME)은 웹사이트 https://childmortality.org/에서 NMR 추정치를 제공합니다. 우리는 그들의 추정치를 다운로드한 다음 통계 프로그래밍 언어 R (R Core Team 2024)을 사용하여 데이터셋을 정리하고 정돈했습니다.\n관심 있는 4개국 간에 시간 경과에 따른 추정 NMR에 상당한 변화가 있음을 발견했습니다(그림 2.8). 1970년대는 추정 NMR 감소와 관련이 있는 경향이 있음을 발견했습니다. 호주와 캐나다는 그 시점에 낮은 NMR을 가진 것으로 추정되었으며 2020년까지 약간의 추가 감소와 함께 그 상태를 유지했습니다. 아르헨티나와 케냐의 추정치는 2020년까지 상당한 감소를 계속했습니다.\n우리 결과는 시간 경과에 따른 추정 NMR의 상당한 개선을 시사합니다. NMR 추정치는 통계 모델과 기본 데이터를 기반으로 합니다. 데이터의 이중 부담은 결과가 더 나쁜 그룹, 이 경우 국가에 대해 고품질 데이터를 덜 쉽게 사용할 수 있다는 것입니다. 우리 결론은 추정치를 뒷받침하는 모델과 기본 데이터의 품질에 따라 달라지며, 이들 중 어느 것도 독립적으로 확인하지 않았습니다.",
    "crumbs": [
      "기초",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>소방 호스에서 물 마시기</span>"
    ]
  },
  {
    "objectID": "02-drinking_from_a_fire_hose_ko.html#결론",
    "href": "02-drinking_from_a_fire_hose_ko.html#결론",
    "title": "2  소방 호스에서 물 마시기",
    "section": "2.5 결론",
    "text": "2.5 결론\n이 장에서 많은 내용을 다루었으며 모든 내용을 따라가지 못하는 것은 정상입니다. 가장 좋은 진행 방법은 세 가지 사례 연구를 각자 자신의 시간에 살펴보는 것입니다. 복사하여 붙여넣는 대신 모든 코드를 직접 입력하고, 무엇을 하는지 완전히 이해하지 못하더라도 조금씩 실행해 보십시오. 그런 다음 자신만의 주석을 추가해 보십시오.\n또한 이 시점에서 이 장의 모든 내용을 완전히 이해할 필요는 없습니다. 일부 학생들은 이 책의 다음 몇 장을 계속 읽고 나중에 이 장으로 돌아오는 것이 가장 좋다고 생각합니다. 흥미롭게도 한두 시간의 작업만으로도 데이터를 사용하여 세상에 대해 무언가를 배울 수 있음을 보여주었습니다. 이러한 기술을 개발함에 따라 작업의 광범위한 영향에 대해 점점 더 정교하게 고려하고 싶습니다.\n\n“우리 작업의 사회적 영향에 대해 생각할 필요는 없습니다. 왜냐하면 그것은 어렵고 다른 사람들이 우리를 위해 할 수 있기 때문입니다.”는 정말 나쁜 주장입니다. 저는 제 작업이 미치는 영향을 보았기 때문에 CV [컴퓨터 비전] 연구를 중단했습니다. 저는 그 작업을 사랑했지만 군사적 응용과 개인 정보 보호 문제는 결국 무시할 수 없게 되었습니다. 그러나 기본적으로 모든 안면 인식 작업은 광범위한 영향 섹션을 진지하게 받아들이면 게시되지 않을 것입니다. 거의 장점이 없고 엄청난 단점 위험이 있습니다. 공정하게 말하면 여기서 많은 겸손을 가져야 합니다. 대학원 대부분 동안 저는 과학은 비정치적이며 연구는 주제가 무엇이든 객관적으로 도덕적이고 선하다는 신화에 빠져 있었습니다.\n조 레드먼, 2020년 2월 20일\n\n“데이터 과학”이라는 용어는 학계, 산업계, 심지어 더 일반적으로 널리 퍼져 있지만, 우리가 보았듯이 정의하기는 어렵습니다. 데이터 과학에 대한 의도적으로 적대적인 정의 중 하나는 “[인류를 셀 수 있는 것으로 비인간적으로 축소하는 것]”(Keyes 2019)입니다. 의도적으로 논란의 여지가 있지만, 이 정의는 지난 10년 동안 데이터 과학과 정량적 방법에 대한 수요가 증가한 한 가지 이유를 강조합니다. 즉, 개인과 그들의 행동이 이제 그 중심에 있다는 것입니다. 많은 기술이 수십 년 동안 존재했지만, 지금 인기를 얻게 된 것은 이러한 인간 중심적 초점 때문입니다.\n불행히도 많은 작업이 개인에게 초점을 맞추고 있음에도 불구하고 개인 정보 보호 및 동의 문제, 그리고 더 광범위하게는 윤리적 우려가 거의 최우선 순위로 고려되지 않는 것 같습니다. 몇 가지 예외는 있지만 일반적으로 AI, 기계 학습 및 데이터 과학이 사회를 혁신할 것이라고 주장하는 동시에 이러한 유형의 문제에 대한 고려는 혁명을 받아들이기 전에 생각하고 싶을 수 있는 것이라기보다는 있으면 좋은 것으로 크게 취급되는 것 같습니다.\n대부분의 경우 이러한 유형의 문제는 새로운 것이 아닙니다. 과학 분야에서는 CRISPR 기술과 유전자 편집에 대한 광범위한 윤리적 고려가 있었습니다(Brokowski 와/과 Adli 2019; Marchese 2022). 그리고 이전 시대에는 예를 들어 나치 독일을 위해 로켓을 만들었던 베르너 폰 브라운이 미국을 위해 로켓을 만들도록 허용된 것에 대한 유사한 대화가 있었습니다(Neufeld 2002; Wilford 1977). 의학에서는 이러한 우려가 한동안 최우선 과제였습니다(American Medical Association and New York Academy of Medicine 1848). 데이터 과학은 다른 분야의 경험을 바탕으로 이러한 문제에 대해 생각하고 사전에 해결하는 대신 자체적인 터스키기 순간을 갖기로 결심한 것 같습니다.\n그럼에도 불구하고 일부 데이터 과학자들이 실천을 둘러싼 윤리에 대해 더 많은 관심을 갖기 시작했다는 몇 가지 증거가 있습니다. 예를 들어, 권위 있는 기계 학습 컨퍼런스인 NeurIPS는 2020년부터 모든 제출물에 윤리에 관한 진술서를 첨부하도록 요구하고 있습니다.\n\n균형 잡힌 관점을 제공하기 위해 저자는 윤리적 측면과 미래 사회적 결과를 포함하여 작업의 잠재적인 광범위한 영향에 대한 진술을 포함해야 합니다. 저자는 긍정적인 결과와 부정적인 결과를 모두 논의하도록 주의해야 합니다.\nNeurIPS 2020 컨퍼런스 논문 모집\n\n데이터 과학의 광범위한 영향에 대한 윤리적 고려와 우려의 목적은 규범적으로 어떤 것을 허용하거나 배제하는 것이 아니라 가장 중요해야 할 몇 가지 문제를 제기할 기회를 제공하는 것입니다. 데이터 과학 응용 프로그램의 다양성, 분야의 상대적인 젊음, 변화의 속도는 그러한 고려 사항이 때때로 의도적으로 무시되고 이것이 나머지 분야에 수용 가능하다는 것을 의미합니다. 이는 과학, 의학, 공학 및 회계와 같은 분야와 대조됩니다. 아마도 해당 분야는 더 자각적일 것입니다(그림 2.9).\n\n\n\n\n\n\n그림 2.9: 숫자는 맥락에서 제거할 수 없습니다. 랜들 먼로의 “확률”에 설명되어 있습니다: https://xkcd.com/881/.",
    "crumbs": [
      "기초",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>소방 호스에서 물 마시기</span>"
    ]
  },
  {
    "objectID": "02-drinking_from_a_fire_hose_ko.html#연습-문제",
    "href": "02-drinking_from_a_fire_hose_ko.html#연습-문제",
    "title": "2  소방 호스에서 물 마시기",
    "section": "2.6 연습 문제",
    "text": "2.6 연습 문제\n\n연습\n\n(계획) 다음 시나리오를 고려하십시오. 1년 동안 매일 한 사람이 1달러, 2달러 또는 3달러를 기부했는지 기록합니다. 데이터셋이 어떻게 생겼는지 스케치한 다음 모든 관찰 결과를 보여주는 그래프를 스케치하십시오.\n(시뮬레이션) 설명된 시나리오를 더 자세히 고려하고 상황을 시뮬레이션하십시오. sample()을 사용하여 적절한 상황을 신중하게 지정하십시오. 그런 다음 시뮬레이션된 데이터를 기반으로 5가지 테스트를 작성하십시오.\n(획득) 관심 있는 국가에서 자선 단체에 기부되는 금액에 대한 실제 데이터 출처를 지정하십시오.\n(탐색) tidyverse를 로드하고 geom_bar()를 사용하여 막대 차트를 만드십시오.\n(공유) 식별한 출처에서 데이터를 수집한 것처럼(시뮬레이션 대신) 그리고 시뮬레이션된 데이터를 사용하여 만든 그래프가 실제 상황을 반영하는 것처럼 두 단락을 작성하십시오. 단락에 포함된 정확한 세부 정보는 사실일 필요는 없지만 합리적이어야 합니다(즉, 실제로 데이터를 가져오거나 그래프를 만들 필요는 없습니다). GitHub Gist에 대한 링크를 제출하십시오.\n\n\n\n퀴즈\n\n(citeBarrett은?) 데이터 과학 학습에 대해 무엇을 강조합니까 (하나 선택)?\n\n빠르고 강렬한 학습 세션.\n실패를 통한 학습.\n일회성 대규모 프로젝트.\n작고 일관된 행동.\n\n(chambliss1989mundanity에?) 따르면 탁월함이란 무엇입니까 (하나 선택)?\n\n세계적 수준에서의 장기간의 성과.\n모든 올림픽 메달 수상자.\n지속적인 성과의 우월성.\n모든 국가 수준의 선수.\n\n(chambliss1989mundanity에?) 따르면 탁월함으로 이끄는 핵심 요소는 무엇입니까 (하나 선택)?\n\n타고난 재능.\n자원에 대한 접근성.\n특별한 훈련 방법.\n훈련, 기술 및 태도.\n\nChambliss (1989.81)의 다음 인용문을 생각하고 데이터 과학에서 탁월함을 달성하는 데 도움이 될 수 있는 세 가지 작은 기술이나 활동을 나열하십시오.\n\n\n탁월함은 평범합니다. 최상의 성과는 실제로 수십 가지의 작은 기술이나 활동의 합류점이며, 각 기술이나 활동은 학습되거나 우연히 발견되어 습관으로 신중하게 훈련된 다음 종합적인 전체로 결합됩니다. 이러한 행동 중 어느 것도 특별하거나 초인적인 것은 없습니다. 단지 일관되고 정확하게 수행되고 모두 함께 수행되어 탁월함을 만들어낸다는 사실뿐입니다.\n\n\n(hao2019의?) 주요 초점은 무엇입니까 (하나 선택)?\n\n고용 관행의 편향.\nAI 모델이 편향을 영속시키는 방식.\n의사 결정에서 AI의 이점.\n코딩 기술을 통한 편향 감소.\n\n(hao2019에서?) 언급된 편향 완화를 위한 네 가지 과제 중 하나가 아닌 것은 무엇입니까 (하나 선택)?\n\n알려지지 않은 미지수.\n불완전한 프로세스.\n이익 고려 사항을 고려한 무관심.\n사회적 맥락 부족.\n공정성의 정의.\n\ntidyverse에 대한 도움말 파일의 첫 번째 문장은 무엇입니까 (힌트: 콘솔에서 ?tidyverse 실행) (하나 선택)?\n\n“’tidyverse’는 데이터 과학의 일반적인 작업을 돕기 위해 설계된 독단적인 패키지 모음입니다.”\n“’tidyverse’에 오신 것을 환영합니다.”\n“‘tidyverse’는 공통 데이터 표현과 ’API’ 디자인을 공유하기 때문에 조화롭게 작동하는 패키지 세트입니다.”\n\n도움말 파일을 사용하여 다음 중 read_csv()의 인수인 것을 확인하십시오 (모두 선택).\n\n“all_cols”\n“file”\n“col_types”\n“show_col_types”\n\n데이터로 이야기하기 워크플로에서 데이터 과학 프로젝트의 첫 번째 단계는 무엇입니까 (하나 선택)?\n\n탐색.\n시뮬레이션.\n공유.\n계획.\n\n핵심 tidyverse 내에서 주로 데이터 조작에 사용되는 R 패키지는 무엇입니까 (하나 선택)?\n\nggplot2\ndplyr\njanitor\nlubridate\n\n열 이름을 더 쉽게 작업할 수 있도록 하는 데 사용되는 함수는 무엇입니까 (하나 선택)?\n\nrename()\nmutate()\nclean_names()\nfilter()\n\nggplot2의 주요 목적은 무엇입니까 (하나 선택)?\n\n통계 분석 수행.\n데이터 시각화 생성 및 사용자 지정.\n지저분한 데이터 정리.\n데이터 입력 자동화.\n\nR에서 한 함수의 출력을 다른 함수의 입력으로 전달하는 데 사용되는 연산자는 무엇입니까 (하나 선택)?\n\n|&gt;\n~\n-&gt;\n+\n\ndplyr 패키지의 mutate() 함수는 무엇을 합니까 (하나 선택)?\n\n행 필터링.\n데이터 그룹화.\n열 생성 또는 수정.\n데이터 정리.\n\n시뮬레이션 중에 set.seed()를 사용하는 것이 중요한 이유는 무엇입니까 (하나 선택)?\n\n프로세스를 더 빠르게 만들기 위해.\n임의의 결과를 더 재현 가능하게 만들기 위해.\n코드의 오류를 줄이기 위해.\n데이터 수집을 자동화하기 위해.\n\n데이터로 이야기하기 워크플로에서 데이터를 시뮬레이션하는 이유는 무엇입니까 (모두 선택)?\n\n계획의 세부 사항에 집중하고 구체성을 부여합니다.\n데이터 생성 프로세스에 대해 깊이 생각할 수 있는 기회를 제공합니다.\n시뮬레이션만 있으면 됩니다.\n팀 작업에 도움이 됩니다.\n\nR의 sample() 함수를 사용할 때 “replace = TRUE”는 무엇을 합니까 (하나 선택)?\n\n이전 값은 이후 값으로 대체되어 재현성에 도움이 됩니다.\n각 값은 고유합니다.\n동일한 값을 두 번 이상 선택할 수 있습니다.\n\nrpois()는 어떤 분포에서 샘플링합니까 (하나 선택)?\n\n정규.\n균일.\n포아송.\n지수.\n\nrunif()는 어떤 분포에서 샘플링합니까 (하나 선택)?\n\n정규.\n균일.\n포아송.\n지수.\n\n다음 중 각각 정규 분포와 이항 분포에서 추출하는 데 사용할 수 있는 것은 무엇입니까 (하나 선택)?\n\nrnorm() 및 rbinom().\nrnorm() 및 rbinomial().\nrnormal() 및 rbinomial().\nrnormal() 및 rbinom().\n\n시드가 “853”으로 설정되었을 때 sample(x = letters, size = 2)의 결과는 무엇입니까? 시드가 “1234”로 설정되었을 때는 어떻습니까 (하나 선택)?\n\n‘“i” “q”’ 및 ‘“e” “r”’.\n‘“e” “l”’ 및 ‘“e” “r”’.\n‘“i” “q”’ 및 ‘“p” “v”’.\n‘“e” “l”’ 및 ‘“p” “v”’.\n\nR을 인용하기 위해 권장되는 인용을 제공하는 함수는 무엇입니까 (하나 선택)?\n\ncite(\"R\").\ncitation().\ncitation(\"R\").\ncite().\n\nopendatatoronto에 대한 인용 정보는 어떻게 얻습니까 (하나 선택)?\n\ncite()\ncitation()\ncitation(\"opendatatoronto\")\ncite(\"opendatatoronto\")\n\n패키지를 업데이트하는 데 사용되는 함수는 무엇입니까 (하나 선택)?\n\nupdate.packages()\nupgrade.packages()\nrevise.packages()\nrenovate.packages()\n\n연도라고 주장하는 열에서 일반적으로 예상할 수 있는 몇 가지 특징은 무엇입니까 (모두 선택)?\n\n클래스는 “character”입니다.\n음수는 없습니다.\n열에 문자가 있습니다.\n각 항목에는 네 자리 숫자가 있습니다.\n\n다음 코드에 작은 실수를 추가하십시오. 그런 다음 GitHub Gist에 추가하고 URL을 제출하십시오.\n\n\nmidwest |&gt;\n  ggplot(aes(x = poptotal, y = popdensity, color = state)) +\n  geom_point() +\n  scale_x_log10() +\n  scale_y_log10()\n\n\n데이터셋을 시뮬레이션하는 이유는 무엇입니까 (최소 3가지 항목 작성)?\n이 코드 library(datasauRus)가 실행되고 이 오류 “Error in library(datasauRus) : there is no package called ‘datasauRus’”가 발생합니다. 가장 가능성이 높은 문제는 무엇입니까 (하나 선택)?\n\n`datasauRus 패키지가 설치되지 않았습니다.\ndatasauRus 이름에 오타가 있습니다.\ndatasauRus와 다른 패키지 간에 패키지 충돌이 있습니다.\n\n신생아 사망률에 대한 데이터셋에서 국가 이름을 저장하는 데 사용되는 변수에 가장 적합한 이름은 무엇입니까 (하나 선택)?\n\n“ctry”\n“geo_area”\n“country”\n\n\n\n\n수업 활동\n\ndplyr 동사 중 하나(mutate(), select(), filter(), arrange(), summarize())를 선택하고 시뮬레이션한 예의 맥락에서 무엇을 하는지 설명하십시오.\nsimulation.R 스크립트에서 평균 5, 표준 편차 2인 균일 분포에서 100개의 추출을 시뮬레이션합니다. tests.R 스크립트에서 이 데이터셋에 대한 테스트 하나를 작성합니다.\nsimulation.R 스크립트에서 람다 10인 포아송 분포에서 50개의 추출을 시뮬레이션합니다. tests.R 스크립트에서 이 데이터셋에 대한 두 가지 테스트를 작성합니다.\ngather.R 스크립트에서 Open Data Toronto를 사용하여 토론토의 결혼 허가 통계에 대한 일부 데이터를 수집합니다. cleaning.R 스크립트에서 정리합니다.1 Quarto 문서에서 그래프로 표시합니다.\n다음 코드는 오류를 생성합니다. GitHub Gist에 추가한 다음 적절한 장소에서 도움을 요청하십시오.\n\n\ntibble(year = 1875:1972,\n       level = as.numeric(datasets::LakeHuron)) |&gt;\n  ggplot(aes(x = year, y = level)) |&gt;\n  geom_point()\n\n\n다음 코드는 날짜 측면에서 이상하게 보이는 그래프를 만듭니다. ggplot() 앞에 함수를 추가하여 문제를 식별하고 수정하십시오.\n\n\nset.seed(853)\n\ndata &lt;-\n  tibble(date = as.character(sample(seq(\n    as.Date(\"2022-01-01\"),\n    as.Date(\"2022-12-31\"),\n    by = \"day\"\n  ),\n  10)), # https://stackoverflow.com/a/21502397\n  number = rcauchy(n = 10, location = 1) |&gt; round(0))\n\ndata |&gt;\n  # 여기서 변경하십시오\n  ggplot(aes(x = date, y = number)) +\n  geom_col()\n\n\n그래프를 만들기 위한 다음 코드를 고려하십시오. 범례를 아래로 옮기고 싶지만 그렇게 하는 ggplot2 함수를 기억할 수 없습니다. LLM을 사용하여 필요한 변경 사항을 식별하십시오. 프롬프트를 공유하십시오.\n\n\npenguins |&gt;\n  drop_na() |&gt;\n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm, color = species)) +\n  geom_point()\n\n\n\n과제\n이 과제의 목적은 호주 선거 예제를 다시 하되 캐나다에 대해 수행하는 것입니다. 캐나다 상황에는 몇 가지 차이점이 있지만 호주 예제는 가드레일을 제공하므로 현실적인 환경에서 작업할 수 있는 기회입니다.\n배경 지식으로 캐나다 의회에는 하원에 338석(선거구라고도 함)이 있습니다. 주요 정당은 자유당과 보수당이고, 소수 정당은 블록 퀘벡, 신민주당, 녹색당이며, 일부 소규모 정당과 무소속 의원이 있습니다. 따라야 할 단계는 다음과 같습니다.\n\n계획:\n\n데이터셋: 각 관찰에는 선거구 이름과 당선된 후보자의 정당이 포함되어야 합니다.\n그래프: 그래프는 각 정당이 획득한 선거구 수를 보여주어야 합니다.\n\n시뮬레이션:\n\nQuarto 문서를 만듭니다.\n필요한 패키지 로드: tidyverse 및 janitor.\n정당을 선거구에 무작위로 할당하여 선거 결과를 시뮬레이션합니다. 선거구에 대한 숫자를 추가한 다음 sample()을 사용하여 6가지 옵션 중 하나를 338번 복원 추출합니다.\n\n획득:\n\nElections Canada에서 CSV 파일을 다운로드합니다여기.\n이름을 정리한 다음 관심 있는 두 열(“electoral_district_name_nom_de_circonscription” 및 “elected_candidate_candidat_elu”)을 선택합니다. 마지막으로 프랑스어를 제거하고 이름을 단순화하도록 열 이름을 바꿉니다.\n필요한 열은 당선된 후보자에 관한 것입니다. 여기에는 당선된 후보자의 성, 쉼표, 이름, 공백, 영어와 프랑스어로 된 정당 이름(슬래시로 구분)이 포함됩니다. tidyr의 separate()를 사용하여 이 열을 조각으로 나눈 다음 select()를 사용하여 정당 정보만 유지합니다(일부 도우미 코드는 아래에 있음).\n마지막으로 시뮬레이션한 것과 일치하도록 정당 이름을 프랑스어에서 영어로 다시 코딩합니다.\n\n\n\ncleaned_elections_data &lt;-\n  cleaned_elections_data |&gt;\n  separate(\n    col = elected_candidate,\n    into = c(\"Other\", \"party\"),\n    sep = \"/\"\n  ) |&gt;\n  select(-Other)\n\n\n탐색:\n\n2021년 캐나다 연방 선거에서 각 정당이 획득한 선거구 수에 대한 멋진 그래프를 만드십시오.\n\n공유:\n\n무엇을 했는지, 왜 했는지, 무엇을 찾았는지에 대해 몇 단락을 작성하십시오. GitHub Gist에 대한 링크를 제출하십시오.\n\n\n\n\n\n\nAmerican Medical Association and New York Academy of Medicine. 1848. Code of medical ethics. Academy of Medicine. https://hdl.handle.net/2027/chi.57108026.\n\n\nArel-Bundock, Vincent. 2024. tinytable: Simple and Configurable Tables in “HTML”, “LaTeX”, “Markdown”, “Word”, “PNG”, “PDF”, and “Typst” Formats. https://vincentarelbundock.github.io/tinytable/.\n\n\nBarrett, Malcolm. 2021. Data science as an atomic habit. https://malco.io/articles/2021-01-04-data-science-as-an-atomic-habit.\n\n\nBrokowski, Carolyn, 와/과 Mazhar Adli. 2019. “CRISPR Ethics: Moral Considerations for Applications of a Powerful Tool”. Journal of Molecular Biology 431 (1): 88–101. https://doi.org/10.1016/j.jmb.2018.05.044.\n\n\nBronner, Laura. 2020. “Why Statistics Don’t Capture The Full Extent Of The Systemic Bias In Policing”. FiveThirtyEight, 6월. https://fivethirtyeight.com/features/why-statistics-dont-capture-the-full-extent-of-the-systemic-bias-in-policing/.\n\n\nCardoso, Tom. 2020. “Bias behind bars: A Globe investigation finds a prison system stacked against Black and Indigenous inmates”. The Globe and Mail, 10월. https://www.theglobeandmail.com/canada/article-investigation-racial-bias-in-canadian-prison-risk-assessments/.\n\n\nChambliss, Daniel. 1989. “The Mundanity of Excellence: An Ethnographic Report on Stratification and Olympic Swimmers”. Sociological Theory 7 (1): 70–86. https://doi.org/10.2307/202063.\n\n\nCity of Toronto. 2021. 2021 Street Needs Assessment. https://www.toronto.ca/city-government/data-research-maps/research-reports/housing-and-homelessness-research-and-reports/.\n\n\nFirke, Sam. 2023. janitor: Simple Tools for Examining and Cleaning Dirty Data. https://CRAN.R-project.org/package=janitor.\n\n\nGelfand, Sharla. 2022. opendatatoronto: Access the City of Toronto Open Data Portal. https://CRAN.R-project.org/package=opendatatoronto.\n\n\nGrolemund, Garrett, 와/과 Hadley Wickham. 2011. “Dates and Times Made Easy with lubridate”. Journal of Statistical Software 40 (3): 1–25. https://doi.org/10.18637/jss.v040.i03.\n\n\nHao, Karen. 2019. “This is How AI Bias Really Happens-And Why It’s So Hard To Fix”. MIT Technology Review, 2월. https://www.technologyreview.com/2019/02/04/137602/this-is-how-ai-bias-really-happensand-why-its-so-hard-to-fix/.\n\n\nKeyes, Os. 2019. “Counting the Countless”. Real Life. https://reallifemag.com/counting-the-countless/.\n\n\nMarchese, David. 2022. “Her Discovery Changed the World. How Does She Think We Should Use It?” The New York Times, 8월. https://www.nytimes.com/interactive/2022/08/15/magazine/jennifer-doudna-crispr-interview.html.\n\n\nNeufeld, Michael. 2002. “Wernher von Braun, the SS, and concentration camp labor: Questions of moral, political, and criminal responsibility”. German Studies Review 25 (1): 57–78. https://doi.org/10.2307/1433245.\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nUN IGME. 2021. “Levels and Trends in Child Mortality, 2021”. https://childmortality.org/wp-content/uploads/2021/12/UNICEF-2021-Child-Mortality-Report.pdf.\n\n\nWickham, Hadley. 2016. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\n———. 2017. tidyverse: Easily Install and Load the “Tidyverse”. https://CRAN.R-project.org/package=tidyverse.\n\n\n———. 2019. Advanced R. 2nd ed. Chapman; Hall/CRC. https://adv-r.hadley.nz.\n\n\n———. 2022. stringr: Simple, Consistent Wrappers for Common String Operations. https://CRAN.R-project.org/package=stringr.\n\n\nWickham, Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, 기타. 2019. “Welcome to the Tidyverse”. Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, 와/과 Garrett Grolemund. (2016년) 2023. R for Data Science. 2nd ed. O’Reilly Media. https://r4ds.hadley.nz.\n\n\nWickham, Hadley, Romain François, Lionel Henry, 와/과 Kirill Müller. 2022. dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, Jim Hester, 와/과 Jenny Bryan. 2022. readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr.\n\n\nWickham, Hadley, Davis Vaughan, 와/과 Maximilian Girlich. 2023. tidyr: Tidy Messy Data. https://CRAN.R-project.org/package=tidyr.\n\n\nWilford, John Noble. 1977. “Wernher von Braun, Rocket Pioneer, Dies”. The New York Times, 6월. https://www.nytimes.com/1977/06/18/archives/wernher-von-braun-rocket-pioneer-dies-wernher-von-braun-pioneer-in.html.\n\n\nXie, Yihui. 2023. knitr: A General-Purpose Package for Dynamic Report Generation in R. https://yihui.org/knitr/.",
    "crumbs": [
      "기초",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>소방 호스에서 물 마시기</span>"
    ]
  },
  {
    "objectID": "02-drinking_from_a_fire_hose_ko.html#footnotes",
    "href": "02-drinking_from_a_fire_hose_ko.html#footnotes",
    "title": "2  소방 호스에서 물 마시기",
    "section": "",
    "text": "날짜에 대해 separate()를 고려한 다음 lubridate::ymd()를 고려하십시오.↩︎",
    "crumbs": [
      "기초",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>소방 호스에서 물 마시기</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "데이터로 이야기하기",
    "section": "",
    "text": "서문\n추천사\n이 책은 데이터를 통해 이야기를 전달하는 데 도움을 줄 것입니다. 이 책은 여러분이 관찰한 데이터를 기반으로 관심 있는 세상의 한 측면에 대한 지식을 구축하고 공유할 수 있는 토대를 마련합니다. 불 주위에 작은 그룹으로 이야기를 나누는 것은 인간과 사회의 발전에 중요한 역할을 했습니다 (Wiessner 2014). 오늘날 데이터에 기반한 우리의 이야기는 수백만 명에게 영향을 미칠 수 있습니다.\n이 책에서 우리는 데이터를 탐색하고, 조사하고, 밀어붙이고, 조작하고, 반죽하고, 궁극적으로는 그 함의를 이해하려고 노력할 것입니다. 다양한 특징들이 이 책의 선택을 이끌어냅니다.\n제가 박사 학위를 받은 대학의 모토는 naturam primum cognoscere rerum 또는 대략 “사물의 본질을 먼저 배우는 것”입니다. 그러나 원래 인용문은 temporis aeterni quoniam 또는 대략 “영원한 시간을 위해”로 이어집니다. 우리는 이 두 가지를 모두 할 것입니다. 저는 지속적이고 재현 가능한 지식을 구축할 수 있도록 하는 도구, 접근 방식 및 워크플로우에 중점을 둡니다.\n이 책에서 데이터에 대해 이야기할 때, 일반적으로 인간과 관련될 것입니다. 인간은 우리 이야기의 대부분의 중심에 있을 것이며, 우리는 사회적, 문화적, 경제적 이야기를 할 것입니다. 특히, 이 책 전체에서 저는 사회 현상과 데이터 모두에서 불평등에 주목할 것입니다. 대부분의 데이터 분석은 세상을 있는 그대로 반영합니다. 가장 취약한 사람들 중 많은 이들이 이 점에서 이중 부담에 직면합니다. 즉, 불이익을 받을 뿐만 아니라 그 정도를 측정하기가 더 어렵습니다. 우리 데이터셋에 있는 사람들의 데이터를 존중하는 것이 주요 관심사이며, 우리 데이터셋에 체계적으로 포함되지 않은 사람들을 생각하는 것도 마찬가지입니다.\n데이터는 종종 다양한 맥락과 분야에 특화되어 있지만, 데이터를 이해하는 데 사용되는 접근 방식은 유사한 경향이 있습니다. 데이터는 또한 점점 더 전 세계적으로 다양한 출처에서 자원과 기회를 얻을 수 있습니다. 따라서 저는 많은 분야와 지역의 예시를 활용합니다.\n지식이 되려면 우리의 발견은 다른 사람들에게 전달되고, 이해되고, 신뢰되어야 합니다. 과학적, 경제적 진보는 다른 사람들의 작업을 기반으로 할 때만 이루어질 수 있습니다. 그리고 이것은 우리가 그들이 무엇을 했는지 이해할 수 있을 때만 가능합니다. 마찬가지로, 우리가 세상에 대한 지식을 창출하려면 다른 사람들이 우리가 무엇을 했는지, 무엇을 발견했는지, 그리고 우리의 작업을 어떻게 수행했는지 정확하게 이해할 수 있도록 해야 합니다. 따라서 이 책에서는 커뮤니케이션과 재현성에 대해 특히 규범적으로 다룰 것입니다.\n양적 작업의 품질을 향상시키는 것은 엄청난 도전이지만, 그것은 우리 시대의 도전입니다. 데이터는 우리 주변에 있지만, 지속적인 지식은 거의 창출되지 않고 있습니다. 이 책은 작은 방식으로나마 그것을 바꾸는 데 기여하기를 바랍니다.",
    "crumbs": [
      "서문"
    ]
  },
  {
    "objectID": "index.html#독자-및-가정된-배경-지식",
    "href": "index.html#독자-및-가정된-배경-지식",
    "title": "데이터로 이야기하기",
    "section": "독자 및 가정된 배경 지식",
    "text": "독자 및 가정된 배경 지식\n이 책을 읽는 일반적인 독자는 1학년 학부 통계에 어느 정도 익숙하며, 예를 들어 회귀 분석을 실행해 본 적이 있습니다. 그러나 특정 수준을 대상으로 하는 것이 아니라 거의 모든 정량 과정과 관련된 측면을 제공합니다. 저는 학부, 대학원 및 전문가 수준에서 이 책으로 가르쳤습니다. 모든 사람은 고유한 요구 사항을 가지고 있지만 이 책의 일부 측면이 여러분에게 도움이 되기를 바랍니다.\n열정과 관심은 사람들을 멀리 데려갔습니다. 그것들이 있다면 다른 것에 대해 너무 걱정하지 마십시오. 가장 성공적인 학생 중 일부는 정량적 또는 코딩 배경이 전혀 없는 학생들이었습니다.\n이 책은 많은 내용을 다루지만 특정 측면에 대해 깊이 다루지는 않습니다. 따라서 데이터 과학: 첫 번째 소개(Timbers, Campbell, 와/과 Lee 2022), R for Data Science(Wickham, Çetinkaya-Rundel, 와/과 Grolemund [2016년] 2023), 통계 학습 입문(James 기타 [2013년] 2021) 및 통계적 재고찰(McElreath [2015년] 2020)과 같은 더 자세한 책을 특히 보완합니다. 이러한 책에 관심이 있다면 이 책으로 시작하는 것이 좋을 수 있습니다.",
    "crumbs": [
      "서문"
    ]
  },
  {
    "objectID": "index.html#구조-및-내용",
    "href": "index.html#구조-및-내용",
    "title": "데이터로 이야기하기",
    "section": "구조 및 내용",
    "text": "구조 및 내용\n이 책은 6개의 파트로 구성되어 있습니다. I) 기초, II) 의사소통, III) 수집, IV) 준비, V) 모델링, VI) 응용.\n1부—기초—는 1  데이터로 이야기하기으로 시작하며, 이 책으로 무엇을 달성하려고 하는지, 왜 읽어야 하는지에 대한 개요를 제공합니다. ?sec-fire-hose는 세 가지 작업 예를 살펴봅니다. 이들의 의도는 무슨 일이 일어나고 있는지에 대해 너무 걱정하지 않고 이 책에서 권장하는 전체 워크플로를 경험할 수 있도록 하는 것입니다. 해당 워크플로는 계획, 시뮬레이션, 수집, 모델링 및 전달입니다. 이 장의 모든 내용을 처음에 따라가지 못하는 것은 정상이지만, 코드를 직접 입력하고 실행하면서 살펴보아야 합니다. 이 책의 한 장만 읽을 시간이 있다면 해당 장을 추천합니다. ?sec-reproducible-workflows는 제가 주장하는 워크플로에서 사용되는 재현성을 위한 몇 가지 주요 도구를 소개합니다. 이는 Quarto, R 프로젝트, Git 및 GitHub, 그리고 실제로 R을 사용하는 것과 같은 측면입니다.\n2부—의사소통—은 서면 및 정적 의사소통을 고려합니다. ?sec-on-writing은 정량적 글쓰기가 가져야 할 특징과 간결하고 정량적인 연구 논문을 작성하는 방법을 자세히 설명합니다. ?sec-static-communication의 정적 의사소통은 그래프, 표 및 지도와 같은 특징을 소개합니다.\n3부—수집—은 우리 세상을 데이터로 바꾸는 데 중점을 둡니다. ?sec-farm-data는 측정으로 시작한 다음 데이터에 대한 우리 접근 방식을 지배하는 표본 추출의 필수 개념을 단계별로 살펴봅니다. 그런 다음 인구 조사 및 기타 정부 통계와 같이 데이터로 사용하도록 명시적으로 제공되는 데이터셋을 고려합니다. 이것들은 일반적으로 깨끗하고 잘 문서화된 사전 패키지된 데이터셋입니다. ?sec-gather-data는 애플리케이션 프로그래밍 인터페이스(API) 사용, 데이터 스크래핑, PDF에서 데이터 가져오기 및 광학 문자 인식(OCR)과 같은 측면을 다룹니다. 아이디어는 데이터가 사용 가능하지만 반드시 데이터셋으로 설계된 것은 아니며 우리가 직접 가져와야 한다는 것입니다. 마지막으로 ?sec-hunt-data는 우리에게 더 많은 것이 기대되는 측면을 다룹니다. 예를 들어, 실험을 수행하거나 A/B 테스트를 실행하거나 일부 설문 조사를 수행해야 할 수 있습니다.\n4부—준비—는 원본, 편집되지 않은 데이터를 탐색하고 공유할 수 있는 것으로 정중하게 변환하는 방법을 다룹니다. ?sec-clean-and-prepare는 데이터 정리 및 준비 작업에 접근할 때 따라야 할 몇 가지 원칙을 자세히 설명하는 것으로 시작한 다음 수행할 특정 단계와 구현할 검사를 살펴봅니다. ?sec-store-and-share는 R 데이터 패키지 및 파케이 사용을 포함하여 해당 데이터셋을 저장하고 검색하는 방법에 중점을 둡니다. 그런 다음 데이터셋을 가능한 한 광범위하게 배포하는 동시에 해당 데이터의 기반이 되는 사람들을 존중할 때 고려하고 수행해야 할 사항으로 이어집니다.\n5부—모델링—은 ?sec-exploratory-data-analysis에서 탐색적 데이터 분석으로 시작합니다. 이것은 데이터셋을 이해하는 중요한 과정이지만 일반적으로 최종 제품에는 포함되지 않는 것입니다. 과정 자체가 목적입니다. ?sec-its-just-a-linear-model에서는 데이터를 탐색하기 위한 선형 모델 사용이 소개됩니다. 그리고 ?sec-its-just-a-generalized-linear-model은 로지스틱, 포아송 및 음이항 회귀를 포함한 일반화 선형 모델을 고려합니다. 또한 다단계 모델링을 소개합니다.\n6부—응용—은 모델링의 세 가지 응용 프로그램을 제공합니다. ?sec-causality-from-observational-data는 관찰 데이터에서 인과 관계를 주장하는 데 중점을 두고 차이-차이, 회귀 불연속성 및 도구 변수와 같은 접근 방식을 다룹니다. ?sec-multilevel-regression-with-post-stratification은 사후 계층화를 사용한 다단계 회귀를 소개하며, 이는 통계 모델을 사용하여 알려진 편향에 대해 표본을 조정하는 것입니다. ?sec-text-as-data는 텍스트-데이터에 중점을 둡니다.\n?sec-concluding-remarks는 몇 가지 결론적인 발언을 제공하고, 몇 가지 미해결 문제를 자세히 설명하고, 몇 가지 다음 단계를 제안합니다.\n온라인 부록은 페이지 크기 제약으로 인해 다루기 어렵거나 인쇄된 책에 대해 합리적인 것보다 더 자주 업데이트해야 할 가능성이 있는 중요한 측면을 제공합니다. Online Appendix A — R 필수 사항는 이 책에서 사용되는 통계 프로그래밍 언어인 R의 몇 가지 필수 작업을 살펴봅니다. 참고 장이 될 수 있으며 일부 학생들은 책의 나머지 부분을 진행하면서 이 장으로 돌아갑니다. Online Appendix D — 데이터셋는 평가에 유용할 수 있는 데이터셋 목록을 제공합니다. 이 책의 핵심은 Quarto를 중심으로 하지만 이전 버전인 R Markdown은 아직 중단되지 않았으며 사용할 수 있는 자료가 많습니다. 따라서 Online Appendix E — R 마크다운에는 ?sec-reproducible-workflows의 Quarto 관련 측면에 해당하는 R Markdown 내용이 포함되어 있습니다. 논문 세트는 Online Appendix F — 논문에 포함되어 있습니다. 이것들을 작성하면 관심 있는 주제에 대한 독창적인 연구를 수행하게 됩니다. 개방형 연구가 처음일 수 있지만, 자신의 질문을 개발하고 정량적 방법을 사용하여 탐색하고 결과를 전달할 수 있는 정도가 이 책의 성공 척도입니다. Online Appendix C — SQL 필수 사항은 SQL 필수 사항에 대한 간략한 개요를 제공합니다. Online Appendix G — 생산은 모델 추정치와 예측을 더 널리 사용할 수 있도록 하는 방법을 고려합니다.",
    "crumbs": [
      "서문"
    ]
  },
  {
    "objectID": "index.html#교육학-및-주요-특징",
    "href": "index.html#교육학-및-주요-특징",
    "title": "데이터로 이야기하기",
    "section": "교육학 및 주요 특징",
    "text": "교육학 및 주요 특징\n여러분은 직접 작업을 해야 합니다. 자료와 코드를 직접 적극적으로 살펴보아야 합니다. (stephenking은?) “아마추어는 영감을 기다리지만, 우리는 일어나서 일하러 간다”고 말합니다. 이 책을 수동적으로 읽지 마십시오. 저의 역할은 Hamming ([1997년] 2020, p. 2-3)에 가장 잘 설명되어 있습니다:\n\n저는 말하자면 코치일 뿐입니다. 저는 여러분을 위해 마일을 달릴 수 없습니다. 기껏해야 스타일을 논하고 여러분의 스타일을 비판할 수 있을 뿐입니다. 여러분은 육상 코스가 여러분에게 도움이 되려면 마일을 달려야 한다는 것을 알고 있습니다. 따라서 이 책에서 듣고 읽는 내용을 신중하게 생각해야 합니다. 그래야만 여러분을 변화시키는 데 효과적일 수 있습니다. 이것이 분명히 목적이어야 합니다…\n\n이 책은 밀도 높은 12주 입문 과정을 중심으로 구성되어 있습니다. 고급 독자들이 도전할 수 있을 만큼 충분한 자료를 제공하면서도, 모든 독자들이 숙달해야 할 핵심을 확립합니다. 일반적인 과정은 ?sec-its-just-a-generalized-linear-model까지의 대부분의 자료를 다루고, 그 다음 특별히 관심 있는 다른 장을 선택합니다. 그러나 이는 학생들의 배경과 관심사에 따라 다릅니다.\n?sec-fire-hose부터 여러분은 데이터로 설득력 있는 이야기를 전달할 수 있는 워크플로우(계획, 시뮬레이션, 획득, 모델링, 커뮤니케이션)를 갖게 될 것입니다. 각 후속 장에서는 이 워크플로우에 깊이를 더할 것입니다. 이를 통해 여러분은 점점 더 정교하고 신뢰할 수 있는 방식으로 이야기할 수 있게 될 것입니다. 이 워크플로우는 학계와 산업계 모두에서 일반적으로 요구되는 기술을 포함합니다. 여기에는 커뮤니케이션, 윤리, 재현성, 연구 질문 개발, 데이터 수집, 데이터 정리, 데이터 보호 및 배포, 탐색적 데이터 분석, 통계 모델링 및 확장이 포함됩니다.\n이 책의 특징 중 하나는 윤리 및 불평등 문제가 한 장에 몰려 쉽게 무시될 수 있는 것이 아니라, 전체에 통합되어 있다는 것입니다. 이것들은 중요하지만, 그 가치를 즉시 파악하기 어려울 수 있으므로 긴밀하게 통합되어 있습니다.\n이 책은 또한 잠재적 고용주에게 보여줄 수 있는 작업 포트폴리오를 구축할 수 있도록 설계되었습니다. 산업계에서 일자리를 원한다면, 이것이 아마도 가장 중요한 일일 것입니다. Robinson 와/과 Nolis (2020, p. 55)는 포트폴리오가 여러분이 할 수 있는 것을 보여주는 프로젝트 모음이며, 구직에 성공하는 데 도움이 될 수 있다고 설명합니다.\n소설 마지막 사무라이 (DeWitt 2000 p. 326)에서 한 등장인물은 다음과 같이 말합니다:\n\n[학자는] 구절의 어떤 단어를 보더라도 즉시 그 단어가 나타났던 다른 구절을 떠올릴 수 있어야 한다. … [그래서] 텍스트는 빙산 덩어리 같아서 각 단어는 눈 덮인 봉우리이고 그 표면 아래에는 거대한 교차 참조의 얼어붙은 덩어리가 있다.\n\n유사하게, 이 책은 자체 포함된 텍스트와 지침을 제공할 뿐만 아니라 전문 지식이 구축되는 중요한 지식 덩어리를 개발하는 데 도움을 줍니다. 어떤 장도 최종적인 결론을 내리지 않고, 다른 작업과 관련하여 작성됩니다.\n이 책을 수업에서 사용하는 방법은 다양합니다. 전통적인 판서 강의도 효과적이지만, 학생들이 수업 전에 장을 읽는 데 전념할 수 있다면(주간 퀴즈나 중간고사를 통해 동기 부여), 그룹 기반 프로젝트와 토론을 위해 수업을 활용하는 것이 즐겁습니다. 매주 2~4명의 학생으로 구성된 소그룹을 만들고(학생들이 새로운 사람들과 함께 작업할 기회를 주기 위해 매주 무작위로 새로운 그룹을 만듭니다). 그런 다음 일반적으로 “생각-짝-공유” 연습 (Lyman 1981)을 따라 대부분의 연습 문제를 먼저 스스로 해결하고, 그룹과 비교한 다음, 마지막으로 선택된 답변을 수업과 공유하도록 합니다.\n시기와 범위 측면에서, 파트 I “기초”가 다루어지는 한, 나머지 장들은 상당히 독립적입니다. 첫 번째 논문은 특히 중요하며, 학생들이 미래의 논문에 대한 교훈을 통합할 수 있도록 학생들에게 신속하게 돌려주어야 합니다.\n각 장에는 다음과 같은 특징이 있습니다:\n\n해당 장을 읽기 전에 살펴보아야 할 필수 자료 목록. 명확히 말하면, 먼저 해당 자료를 읽은 다음 이 책으로 돌아와야 합니다. 각 장에는 광범위한 참고 자료도 포함되어 있습니다. 해당 주제에 특히 관심이 있다면, 이를 추가 탐색을 위한 시작점으로 사용해야 합니다.\n해당 장에서 개발되는 핵심 개념 및 기술 요약. 기술 장에는 추가적으로 해당 장에서 사용되는 소프트웨어 및 패키지 목록이 포함되어 있습니다. 이러한 기능들의 조합은 학습 체크리스트 역할을 하며, 장을 완료한 후 다시 확인해야 합니다.\n“연습”은 작은 시나리오를 제공하고 이 책에서 옹호하는 워크플로우를 통해 작업하도록 요청합니다. 이 작업은 아마 15-30분 정도 걸릴 것입니다. 미국 바이올리니스트 힐러리 한은 거의 매일 바이올린 연습(종종 스케일 또는 유사한 연습)을 공개적으로 기록합니다. 여러분도 비슷한 것을 하도록 권장하며, 이것들은 그것을 가능하게 하도록 설계되었습니다.\n필수 자료를 살펴본 후, 그러나 장을 진행하기 전에 지식을 테스트하기 위해 완료해야 하는 몇 가지 “퀴즈” 질문. 장을 완료한 후에는 각 측면을 이해했는지 확인하기 위해 질문을 다시 살펴보아야 합니다. 답변 가이드는 요청 시 제공됩니다.\n자료에 적극적으로 참여하도록 더욱 장려하는 “과제”. 이 질문에 대한 답변을 논의하기 위해 소그룹을 구성하는 것을 고려할 수 있습니다.\n\n일부 장에는 추가적으로 다음이 포함됩니다:\n\n“오, 우리가 그 데이터에 대해 좋은 데이터를 가지고 있다고 생각하는군요!”라는 섹션은 흠잡을 데 없고 명확한 데이터가 있다고 종종 가정되지만 현실은 그와는 거리가 먼 특정 상황에 초점을 맞춥니다.\n“거인의 어깨”라는 섹션은 우리가 구축하는 지적 기반을 만든 사람들 중 일부에 초점을 맞춥니다.",
    "crumbs": [
      "서문"
    ]
  },
  {
    "objectID": "index.html#소프트웨어-정보-및-규칙",
    "href": "index.html#소프트웨어-정보-및-규칙",
    "title": "데이터로 이야기하기",
    "section": "소프트웨어 정보 및 규칙",
    "text": "소프트웨어 정보 및 규칙\n이 책에서 시작하는 소프트웨어는 R (R Core Team 2024)입니다. 이 언어는 오픈 소스이고, 널리 사용되며, 전체 워크플로우를 다룰 만큼 충분히 일반적이지만, 잘 개발된 기능이 많을 만큼 충분히 구체적이기 때문에 선택되었습니다. 저는 여러분이 이전에 R을 사용해 본 적이 있다고 가정하지 않으며, 이 책에 R을 선택한 또 다른 이유는 R 사용자 커뮤니티 때문입니다. 이 커뮤니티는 초보자에게 특히 환영하며, 보완적인 초보자 친화적인 자료가 많이 있습니다.\n프로그래밍 언어가 없다면 R은 시작하기에 좋은 언어입니다. Online Appendix A — R 필수 사항를 꼭 살펴보세요.\nR에 익숙해졌다면, 또 다른 오픈 소스 프로그래밍 언어인 Python도 배우는 것을 권장합니다. Python은 R보다 초보자가 시작하기에 약간 덜 쉽지만, 산업계에서 널리 사용됩니다. 저는 Python을 사용하는 것이 합리적인 경우에 사용할 것이며, 이는 여러분에게 R과 Python 모두에 대한 친숙함을 제공할 것입니다.\n\nR과 RStudio를 자신의 컴퓨터에 다운로드하세요. R은 여기에서 무료로 다운로드할 수 있으며, RStudio Desktop은 여기에서 무료로 다운로드할 수 있습니다. 그리고 Quarto는 여기에서 다운로드하세요.\nPosit Cloud 여기에서 계정을 만드세요. 이를 통해 클라우드에서 R을 실행할 수 있습니다.\nVS Code를 여기에서 무료로 다운로드하세요.\nGoogle Colab 여기에서 무료 계정을 만드세요.\n\n패키지는 tidyverse와 같이 타자기 텍스트로 표시되며, 함수는 filter()와 같이 타자기 텍스트로 표시되지만 괄호가 포함됩니다.",
    "crumbs": [
      "서문"
    ]
  },
  {
    "objectID": "index.html#저자-소개",
    "href": "index.html#저자-소개",
    "title": "데이터로 이야기하기",
    "section": "저자 소개",
    "text": "저자 소개\n저는 토론토 대학교의 정보학부와 통계학과에 공동 임용된 조교수입니다. 또한 캐나다 통계 과학 연구소(CANSSI) 온타리오의 부소장, 매시 칼리지의 선임 연구원, 슈워츠 라이스만 기술 및 사회 연구소의 교수 연구원, 데이터 과학 연구소 재현성 주제 프로그램의 공동 책임자입니다. 저는 호주 국립 대학교에서 경제학 박사 학위를 받았으며, 경제사에 중점을 두었고 존 탕(의장), 마르틴 마리오티, 팀 해튼, 자크 워드의 지도를 받았습니다.\n제 연구는 데이터 과학의 신뢰성을 향상시키는 워크플로우를 개발하는 방법을 탐구합니다. 저는 특히 데이터 과학에서 테스트의 역할에 관심이 있습니다.\n저는 가르치는 것을 즐기며, 다양한 배경을 가진 학생들이 데이터를 사용하여 설득력 있는 이야기를 전달하는 방법을 배우도록 돕는 것을 목표로 합니다. 저는 다양한 분야에서 통계적 방법을 사용하는 데 능숙할 뿐만 아니라 그 한계를 이해하고 작업의 더 넓은 맥락에 대해 깊이 생각하는 학생들을 양성하려고 노력합니다. 저는 토론토 대학교의 정보학부와 통계학과에서 학부 및 대학원 수준 모두에서 가르칩니다. 저는 RStudio 공인 Tidyverse 트레이너입니다.\n저는 모니카 알렉산더와 결혼했으며 두 자녀가 있습니다. 저는 아마도 책에 너무 많은 돈을 쓰고, 도서관에서 너무 많은 시간을 보낼 것입니다. 여러분만의 책 추천이 있다면 듣고 싶습니다.",
    "crumbs": [
      "서문"
    ]
  },
  {
    "objectID": "index.html#토지-인정",
    "href": "index.html#토지-인정",
    "title": "데이터로 이야기하기",
    "section": "토지 인정",
    "text": "토지 인정\n이 책은 주로 크레딧 미시사가, 휴런-웬다트, 세네카의 전통적인 땅에서 작성되었습니다. 데이터는 오랫동안 억압과 해를 끼치는 데 사용되어 왔으며, 이 땅의 역사를 인정하는 것은 데이터 사용에 있어 더 나아지기 위한 필요성을 상기시켜 줍니다.",
    "crumbs": [
      "서문"
    ]
  },
  {
    "objectID": "index.html#감사-인사",
    "href": "index.html#감사-인사",
    "title": "데이터로 이야기하기",
    "section": "감사 인사",
    "text": "감사 인사\n많은 분들이 이 책을 개발하는 데 도움이 된 코드, 데이터, 예제, 지도, 기회, 생각 및 시간을 아낌없이 제공해 주셨습니다.\n감사합니다. 데이비드 그럽스, 커티스 힐, 로빈 로이드-스타크스, 그리고 Taylor & Francis 팀이 이 책을 편집하고 출판하며 귀중한 지도와 지원을 제공해 주셨습니다. 이 책을 철저히 편집해 주신 에리카 올로프에게 감사드립니다. 이 책의 초기 초고를 철저히 검토하고 개선을 위한 상세한 피드백을 제공해 주신 이사벨라 게멘트에게 감사드립니다.\n이 책의 모든 단어를 살펴보고 많은 단어를 개선하고 이 책에서 다루는 많은 내용에 대한 제 생각을 날카롭게 하는 데 도움을 준 애니 콜린스에게 감사드립니다. 가르치는 즐거움 중 하나는 애니와 같이 재능 있는 사람들이 경력을 시작할 때 함께 일할 수 있는 기회입니다.\n책의 초기 계획에 대한 자세한 의견을 제공해 주신 에밀리 리더러에게 감사드립니다. 그런 다음 초고가 완성된 후 원고로 돌아와 세세한 부분까지 살펴보셨습니다. 그녀의 사려 깊은 의견은 책을 크게 개선했습니다. 더 넓게는 그녀의 작업은 이 책의 많은 내용에 대한 제 생각을 바꾸었습니다.\n저는 운 좋게도 전체 장, 때로는 두세 장 이상을 읽은 많은 검토자가 있었습니다. 그들은 정말로 그 이상을 해냈고 이 책을 개선하기 위한 훌륭한 제안을 제공했습니다. 이에 대해 저는 다음 분들께 감사드립니다. 알버트 랩, 알렉스 헤이즈, 알렉스 러스콤(경찰 폭력 “아, 그것에 대해 좋은 데이터가 있다고 생각하시는군요!” 항목도 제안), 아리엘 문도, 벤자민 하이베-카인스, 댄 라이언, 에릭 드라이스데일, 플로렌스 발레-뒤부아, 잭 베일리, 재 해트릭-심퍼스, 존 칸, 조나단 킨(파케이 전문 지식도 아낌없이 공유), 로렌 케네디(MRP에 대한 제 생각을 발전시키기 위해 코드, 데이터 및 전문 지식도 아낌없이 공유), 리암 웰시, 리자 볼튼(이 책을 어떻게 가르쳐야 하는지에 대한 제 아이디어를 발전시키는 데도 도움), 루이스 코레이아, 맷 라토, 마티아스 버거, 마이클 문, 로베르토 렌티니, 라이언 브릭스, 그리고 테일러 라이트.\n많은 분들이 상황을 크게 개선한 구체적인 제안을 해주셨습니다. 이 모든 분들은 이 책이 기반을 두고 있는 오픈 소스 프로그래밍 언어 커뮤니티를 특징짓는 관대함의 정신에 기여합니다. 모든 분들께 감사드립니다.\n\nA 마흐푸즈는 포아송 회귀를 다루는 것이 중요하다고 깨닫게 해주었습니다.\n애런 밀러는 FINER 프레임워크를 제안했습니다.\n앨리슨 프레스만스 힐은 워드뱅크를 제안했습니다.\n크리스 워쇼는 민주주의 기금 유권자 연구 그룹 설문 조사 데이터를 제안했습니다.\n크리스티나 웨이는 많은 코드 오류를 지적했습니다.\n클레어 배터실은 글쓰기에 관한 많은 책으로 저를 안내했습니다.\n엘라 케이는 Quarto로 전환할 것을 제안하고 당연히 주장했습니다.\n파리아 칸다커는 “R 필수 사항” 장이 된 것을 제안했습니다.\n하림 나비드는 업계 경험을 아낌없이 공유했습니다.\n히스 프리스턴은 토론토 노숙자 데이터에 대한 지원을 제공했습니다.\n제시카 그론스벨은 통계 실무에 대한 귀중한 제안을 했습니다.\n켈리 치우는 텍스트-데이터의 중요성을 강조했습니다.\n레슬리 루트는 “아, 그것에 대해 좋은 데이터가 있다고 생각하시는군요!”라는 아이디어를 생각해냈습니다.\n마이클 총은 EDA에 대한 제 접근 방식을 형성했습니다.\n마이클 도넬리, 피터 헵번, 레오 레이몽-벨질은 제가 알지 못했던 정치학, 사회학, 통계학 분야의 고전 논문에 대한 포인터를 제공했습니다.\n닉 호튼은 ?sec-exploratory-data-analysis에서 해들리 위컴 비디오를 제안했습니다.\n폴 호제츠는 R 패키지를 만드는 방법을 가르쳐주었고 이 책의 표지 아트를 만\n라두 크라이우는 표본 추출이 적절한 위치를 차지하도록 보장했습니다.\n샬라 겔판드는 제가 R을 사용하는 방법에 대해 주장하는 접근 방식입니다.\n토마스 윌리엄 로젠탈은 샤이니의 잠재력을 깨닫게 해주었습니다.\n톰 카르도소와 제인 슈워츠는 언론인들이 모은 훌륭한 데이터 출처였습니다.\n얀보 탕은 낸시 리드의 “거인의 어깨 위에 서서” 항목을 지원했습니다.\n마지막으로 크리스 매디슨과 마이아 발린트는 맺음 시를 제안했습니다.\n\n박사 과정 지도 위원회인 존 탕, 마틴 마리오티, 팀 해튼, 잭 워드에게 감사드립니다. 그들은 제가 관심 있는 지적 공간을 탐색할 자유, 그러한 관심사를 추구할 지원, 그리고 그 모든 것이 가시적인 결과로 이어지도록 하는 지도를 제공했습니다. 그 기간 동안 배운 것이 이 책의 기초가 되었습니다.\n이 책은 크리스 베일, 스콧 커닝햄, 앤드류 하이스(이 책이 나오기 훨씬 전에 이 책과 같은 이름의 과정을 독립적으로 가르쳤음), 리사 렌드웨이, 그랜트 맥더못, 네이선 마티아스, 데이비드 밈노, 에드 루빈 등 다른 사람들의 온라인에서 무료로 제공되는 노트와 교재로부터 큰 도움을 받았습니다. 이분들께 감사드립니다. 학자들이 자신의 자료를 온라인에서 무료로 제공하는 변화된 규범은 훌륭하며, 이 책의 무료 온라인 버전(여기)이 이에 기여하기를 바랍니다.\n일부 평가 항목 개발에 도움을 주신 사만다-조 카에타노에게 감사드립니다. 또한 일부 채점 기준을 적용할 수 있도록 허락해 주신 리사 롬키와 앨런 총에게도 감사드립니다. 4  연구 논문 작성 튜토리얼의 일부 측면에 대한 촉매제는 McPhee (2017, p. 186)와 첼시 팔렛-펠레리티였습니다. “대화형 커뮤니케이션” 튜토리얼의 아이디어는 마우리시오 바르가스 세풀베다(“파차”)와 앤드류 휘트비의 작업이었습니다.\n다음 분들의 수정에 감사드립니다. - 에이미 패로우, - 아르시 라칸팔, - 세자르 비야레알 구즈만, - 클로이 티어스타인, - 핀 코롤-오드와이어, - 플라비아 로페즈, - 그레고리 파워, - 홍시, - 정재든, - 존 헤이즈, - 조이스 쉬안, - 로라 클라인, - 로레나 알마라즈 데 라 가르자, - 매튜 로버트슨, - 미카엘라 드루이야르, - 무니카 타남, - 림 알라사디, - 롭 짐머만, - 타예자 치쿰비리케, - 위즈단 타리크, - 양우, - 한예원.\n켈리 라이언스는 지원, 지도, 멘토링 및 우정을 제공했습니다. 매일 그녀는 학자가 어떠해야 하는지, 더 넓게는 한 사람으로서 무엇을 지향해야 하는지를 보여줍니다.\n그렉 윌슨은 교육에 대해 생각할 수 있는 구조를 제공하고 “척도” 스타일 연습을 제안했습니다. 그는 이 책의 촉매제였으며 초고에 유용한 의견을 제공했습니다. 매일 그는 지적 공동체에 기여하는 방법을 보여줍니다.\n팬데믹 기간 동안 첫째 아이, 그리고 둘째 아이를 돌보며 이 책을 쓸 수 있게 해준 엘 코테에게 감사드립니다.\n2021년 크리스마스 현재 이 책은 부분적으로 완성된 노트의 이질적인 모음이었습니다. 모든 것을 버리고 두 달 동안 세계 반대편에서 와서 모든 것을 다시 쓰고 일관된 초고를 만들 기회를 주신 엄마와 아빠께 감사드립니다.\n캔버라에서 2주간의 “글쓰기 수련회”를 후원해 주신 마리야 타플라가와 ANU 호주 정치 연구 센터 정치 및 국제 관계 학교에 감사드립니다.\n마지막으로 모니카 알렉산더에게 감사드립니다. 당신이 없었다면 저는 책을 쓰지 못했을 것입니다. 가능하다고 생각조차 하지 못했을 것입니다. 이 책의 가장 좋은 아이디어 중 다수는 당신의 것이며, 그렇지 않은 아이디어는 당신이 모든 것을 여러 번 읽음으로써 더 좋게 만들었습니다. 이 책을 쓰는 데 헤아릴 수 없는 도움을 주시고, 그 기반을 제공해 주시고(도서관에서 R에서 특정 행을 가져오는 방법을 여러 번 보여준 것을 기억합니다!), 글을 쓸 시간을 주시고, 어제 완벽했던 것을 끝없이 다시 쓰는 것이 책을 쓰는 것이라는 사실이 밝혀졌을 때 격려해 주시고, 이 책의 모든 것을 여러 번 읽어주시고, 적절하게 커피나 칵테일을 만들어 주시고, 아이들을 돌봐주시고, 그 외에도 많은 것에 대해 감사드립니다.\nrohan.alexander@utoronto.ca로 저에게 연락할 수 있습니다.\n\n로한 알렉산더 토론토, 캐나다 2023년 5월\n\n\n\n\n\nDeWitt, Helen. 2000. The Last Samurai. 1st ed. United States: Talk Mirimax Books.\n\n\nHamming, Richard. (1997년) 2020. The Art of Doing Science and Engineering. 2nd ed. Stripe Press.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, 와/과 Robert Tibshirani. (2013년) 2021. An Introduction to Statistical Learning with Applications in R. 2nd ed. Springer. https://www.statlearning.com.\n\n\nLyman, Frank. 1981. “The responsive classroom discussion: The inclusion of all students”. Mainstreaming Digest 109: 109–13.\n\n\nMcElreath, Richard. (2015년) 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. 2nd ed. Chapman; Hall/CRC.\n\n\nMcPhee, John. 2017. Draft No. 4. 1st ed. Farrar, Straus; Giroux.\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nRobinson, Emily, 와/과 Jacqueline Nolis. 2020. Build a Career in Data Science. Shelter Island: Manning Publications. https://livebook.manning.com/book/build-a-career-in-data-science.\n\n\nTimbers, Tiffany, Trevor Campbell, 와/과 Melissa Lee. 2022. Data Science: A First Introduction. Chapman; Hall/CRC. https://datasciencebook.ca.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, 와/과 Garrett Grolemund. (2016년) 2023. R for Data Science. 2nd ed. O’Reilly Media. https://r4ds.hadley.nz.\n\n\nWiessner, Polly. 2014. “Embers of society: Firelight talk among the Ju/’hoansi Bushmen”. Proceedings of the National Academy of Sciences 111 (39): 14027–35. https://doi.org/10.1073/pnas.1404212111.",
    "crumbs": [
      "서문"
    ]
  },
  {
    "objectID": "00-errata_ko.html",
    "href": "00-errata_ko.html",
    "title": "오류 및 업데이트",
    "section": "",
    "text": "오류\n마지막 업데이트: 2024년 11월 21일.\n이 책은 Piotr Fryzlewicz가 The American Statistician(Fryzlewicz 2024)에, Nick Cox가 Amazon에 리뷰했습니다. 리뷰를 제공하는 데 많은 시간을 할애해 주시고 수정 및 제안을 해주신 점에 대해 감사드립니다.\n2023년 7월 이 책이 출판된 이후 세상에는 다양한 변화가 있었습니다. 생성형 AI의 등장은 사람들이 코딩하는 방식을 바꾸었고, Quarto 덕분에 Python을 R과 함께 통합하기가 더 쉬워졌으며, 패키지는 계속 업데이트되고 있습니다(새로운 학생들이 이 책을 공부하기 시작했다는 점은 말할 것도 없습니다). 온라인 버전의 장점 중 하나는 개선할 수 있다는 것입니다.\n다음 분들의 수정과 제안에 감사드립니다: Andrew Black, Clay Ford, Crystal Lewis, David Jankoski, Donna Mulkern, Emi Tanaka, Emily Su, Inessa De Angelis, James Wade, Julia Kim, Krishiv Jain, Seamus Ross, Tino Kanngiesser, and Zak Varty.\n다음 오류는 인쇄 버전에 있지만 온라인 버전에서는 업데이트되었습니다. 아래에 언급되지 않은 오류를 발견하면 문제를 제출하거나 rohan.alexander@utoronto.ca로 이메일을 보내주십시오.\ntoronto_shelters_clean &lt;-\n     clean_names(toronto_shelters) |&gt;\n     select(occupancy_date, id, occupied_beds)\n\nhead(toronto_shelters_clean)\ntoronto_shelters_clean &lt;-\n  clean_names(toronto_shelters) |&gt;\n  mutate(occupancy_date = ymd(occupancy_date)) |&gt;\n  select(occupancy_date, occupied_beds)\n\nhead(toronto_shelters_clean)",
    "crumbs": [
      "오류 및 업데이트"
    ]
  },
  {
    "objectID": "00-errata_ko.html#오류",
    "href": "00-errata_ko.html#오류",
    "title": "오류 및 업데이트",
    "section": "",
    "text": "xxi: 감사의 말에 Alex Hayes를 추가합니다.\n\np. 20: “use the tidyverse and janitor packages.”에 “packages”를 추가합니다.\np. 34: \"daily-shelter-overnight-service-occupancy-capacity-2021\"은 \"daily-shelter-overnight-service-occupancy-capacity-2021.csv\"여야 합니다(“.csv” 추가됨).\np. 34: 첫 번째 코드 청크를 두 번째 코드로 바꿉니다.\n\n\n\n\np. 38: “이 시점에서 2019년 캐나다 연방 선거에서 각 정당이 획득한 의석 수에 대한 멋진 그래프를 만들 수 있습니다.”는 2021년 선거를 참조해야 합니다.\np. 41: 불필요한 “:::”를 제거합니다.\np. 66: “New Project$dots”는 “New Project…”여야 합니다.\np. 138: scale_color_brewer(palette = \"Set1\")은 불필요하며 제거해야 합니다.\np. 138: 그림 캡션은 실업률이 아닌 인플레이션을 참조해야 합니다.\np. 154: Q9는 코드 청크 다음, “if” 앞에 “work”가 빠져 있습니다.\np. 188: “Leonhard Euler”는 “Carl Friedrich Gauss”여야 합니다.\np. 279: “detonated”는 “denoted”여야 합니다.\np. 342: Q5 옵션 b가 옵션 c에 반복됩니다.\np. 347: R for Data Science의 “탐색적 데이터 분석” 장은 12가 아니라 11입니다.\np. 353: “the the”를 수정합니다.\np. 355: “…결과는 5,814로 추정되며 둘 다 너무 낮습니다.”는 “…결과는 11,197로 추정되며 전자는 너무 낮고 후자는 너무 높습니다.”여야 합니다.\np. 371: 그림 11.11a를 참조하는 문장이 혼란스러웠으며 그림을 더 명확하게 참조해야 합니다.\np. 587: 링크는 https://fivethirtyeight.com/features/police-misconduct-costs-cities-millions-every-year-but-thats-where-the-accountability-ends/ 여야 합니다.\n\n\n\n\n\nFryzlewicz, Piotr. 2024. “Telling Stories with Data: With Applications in R”. The American Statistician, 4월, 1–5. https://doi.org/10.1080/00031305.2024.2339562.",
    "crumbs": [
      "오류 및 업데이트"
    ]
  },
  {
    "objectID": "06-farm_ko.html",
    "href": "06-farm_ko.html",
    "title": "6  측정, 인구 조사 및 표본 추출",
    "section": "",
    "text": "6.1 소개\n선수 지식\n주요 개념 및 기술\n주요 패키지 및 함수\n우리가 세상을 생각하고 그것에 대한 이야기를 할 때, 가장 어려운 측면 중 하나는 세상의 아름다운 복잡성을 우리가 사용할 수 있는 데이터셋으로 축소하는 것입니다. 우리는 이 작업을 할 때 무엇을 포기하는지 알아야 합니다. 그리고 신중하고 사려 깊게 진행해야 합니다. 일부 데이터셋은 너무 커서 특정 데이터 포인트 하나가 중요하지 않습니다. 다른 데이터 포인트로 바꿔도 아무런 영향이 없을 수 있습니다 (Crawford 2021 p. 94). 그러나 이것이 항상 합리적인 것은 아닙니다. 다른 어머니가 있었다면 당신의 삶은 얼마나 달라졌을까요?\n우리는 종종 일부 데이터셋의 의미를 이해하는 데 관심이 있습니다. 이를 기반으로 예측을 하거나 해당 데이터셋을 사용하여 더 넓은 현상에 대한 주장을 합니다. 우리가 세상을 데이터로 바꾸는 방법에 관계없이, 우리는 일반적으로 필요한 데이터의 표본만 갖게 될 것입니다. 통계는 이러한 문제를 염두에 두고 그 의미를 이해하는 데 사용하는 공식적인 접근 방식을 제공합니다. 그러나 수집된 데이터로부터 누가 이익을 얻고 누구의 권력을 반영하는지와 같은 더 넓은 문제에 대한 명확한 지침을 제공하지는 않습니다.\n이 장에서는 먼저 측정과 그것이 가져오는 몇 가지 우려 사항에 대해 논의합니다. 그런 다음 일반적으로 전체 인구에 대한 데이터를 얻으려고 시도하는 인구 조사로 넘어갑니다. 또한 다른 정부 공식 통계와 오래된 설문 조사에 대해서도 논의합니다. 우리는 이러한 유형의 데이터셋을 “양식 데이터”라고 설명합니다.  양식 데이터셋은 일반적으로 잘 구성되어 있고, 철저하게 문서화되어 있으며, 이러한 데이터셋을 수집, 준비 및 정리하는 작업은 대부분 우리를 위해 수행됩니다. 또한 일반적으로 알려진 출시 주기에 따라 수행됩니다. 예를 들어, 많은 국가에서 실업률 및 인플레이션 데이터셋을 매월, GDP를 분기별로, 인구 조사를 5~10년마다 발표합니다.\n그런 다음 우리는 지속적으로 돌아올 기초를 제공하기 위해 표본 추출에 대한 통계적 개념을 소개합니다. 지난 100년 동안 통계학자들은 표본에 대해 생각하는 데 상당한 정교함을 발전시켰고 많은 논란을 다루었습니다 (Brewer 2013). 이 장에서는 확률 및 비확률 표본 추출을 고려하고 특정 주요 용어와 개념을 소개합니다.\n이 장은 우리를 위해 제공되는 데이터에 관한 것입니다. 데이터는 중립적이지 않습니다. 예를 들어, 기록 보관 담당자는 이제 기록 보관소를 사실의 원천으로만 간주하는 것이 아니라, 특히 국가에 의해 구성된 특정 맥락 내에서 발생한 사실 생산의 일부로 간주하는 데 신중을 기합니다 (Stoler 2002). 데이터셋에 누가 포함되고 누가 체계적으로 제외되는지 명확하게 생각하는 것이 중요합니다. 데이터를 이해하고, 캡처하고, 분류하고, 이름을 지정하는 것은 세상을 구축하는 행위이며 사회적, 역사적, 재정적 또는 법적 권력을 반영합니다 (Crawford 2021, p. 121).\n예를 들어, 설문 조사 연구에서 성별과 젠더의 역할을 고려할 수 있습니다. 성별은 생물학적 속성을 기반으로 하며 출생 시 지정되는 반면, 젠더는 사회적으로 구성되며 생물학적 및 문화적 측면을 모두 가지고 있습니다 (Lips 2020, p. 7). 우리는 성별보다는 젠더와 일부 결과 간의 관계에 관심이 있을 수 있습니다. 그러나 공식 통계에서 미묘한 젠더 개념으로의 전환은 최근에야 일어났습니다. 성별과 동일한 이분법적 젠더 변수를 주장하는 설문 조사는 그렇게 식별하지 않는 응답자를 반영하지 않습니다. (kennedy2020using은?) 젠더 응답으로 무엇을 할지 결정할 때 고려해야 할 다양한 측면을 제공하며, 여기에는 윤리, 정확성, 실용성 및 유연성이 포함됩니다. 그러나 보편적으로 최상의 해결책은 없습니다. 설문 조사 응답자에 대한 존중을 보장하는 것이 최우선 순위여야 합니다 (Kennedy 기타 2022 p. 16).\n그러한 우려를 야기한다면 왜 분류와 그룹화가 필요한가요? (scott1998seeing은?) 이 대부분을 국가가 자체 목적으로 사회를 읽기 쉽게 만들고 싶어하는 결과로 보고 이것을 현대 국가의 결정적인 특징으로 간주합니다. 예를 들어, (scott1998seeing은?) 성의 사용이 조세, 재산 소유권, 징병 및 인구 조사를 위해 읽기 쉬운 목록을 원하는 국가의 욕구에서 비롯되었다고 봅니다. 국가의 가독성에 대한 욕구는 또한 측정에 일관성을 부과해야 했습니다. “측정이 어떻게 이루어지고 데이터가 어떻게 비교되는지에 대한 연구”인 현대 계측학의 형태 (Plant 와/과 Hanisch 2020)는 프랑스 혁명에서 다양한 측정이 표준화되면서 시작되었습니다. 이것은 나중에 나폴레옹 국가 건설의 일부로 더욱 발전했습니다 (Scott 1998 p. 30). Prévost 와/과 Beaud (2015, p. 154)는 변화의 본질을 지식이 “단수적이고, 지역적이며, 특이하며\\(\\dots\\) 종종 문학적 형태로 표현되는” 것에서 일반화되고, 표준화되고, 수치화되는 것으로 묘사합니다. 그렇긴 하지만, 분류 가능하고 측정 가능한 척도 없이는 데이터를 수집하기 어려울 것입니다. 또 다른 우려는 이러한 측정이 구성되어야 한다는 것을 잊어버리는 구체화입니다.\n모든 데이터셋에는 단점이 있습니다. 이 장에서는 “양식 데이터”에 대한 편안함을 개발합니다. 우리는 이 용어를 데이터로 사용될 목적으로 특별히 개발된 데이터셋을 지칭하는 데 사용합니다.\n이러한 양식 데이터셋은 우리가 사용하도록 구성되어 있고 일반적으로 쉽게 얻을 수 있지만, 그럼에도 불구하고 우리가 그 구성을 철저히 이해하는 것이 특히 중요합니다. 19세기 스코틀랜드 작가인 제임스 밀은 인도에 발을 들여놓지 않고 영국령 인도의 역사를 썼습니다. 그는 다음과 같이 주장했습니다.\n그가 전문가로 간주되고 그의 견해가 영향력을 가졌다는 것은 놀랍게 보일 수 있습니다. 그러나 오늘날 많은 사람들은, 예를 들어, 몇 가지 가격을 추적하려고 시도하지 않고 인플레이션 통계를 사용하고, 응답자에게 질문을 한 번도 해보지 않고 정치 설문 조사의 응답을 사용하거나, 일부 이미지를 직접 레이블링하는 경험 없이 ImageNet을 사용할 것입니다. 우리는 항상 데이터의 세부 사항에 몰두해야 합니다.",
    "crumbs": [
      "Acquisition",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>측정, 인구 조사 및 표본 추출</span>"
    ]
  },
  {
    "objectID": "06-farm_ko.html#소개",
    "href": "06-farm_ko.html#소개",
    "title": "6  측정, 인구 조사 및 표본 추출",
    "section": "",
    "text": "인도에서 보거나 들을 가치가 있는 모든 것은 글로 표현될 수 있습니다. 중요한 모든 것이 글로 표현되자마자, 정식으로 자격을 갖춘 사람은 영국에 있는 자신의 서재에서 1년 안에 인도에 대한 더 많은 지식을 얻을 수 있으며, 이는 인도에서 눈과 귀를 사용하여 가장 긴 생애 동안 얻을 수 있는 것보다 더 많습니다.\nMill (1817, p. xv)",
    "crumbs": [
      "Acquisition",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>측정, 인구 조사 및 표본 추출</span>"
    ]
  },
  {
    "objectID": "06-farm_ko.html#측정",
    "href": "06-farm_ko.html#측정",
    "title": "6  측정, 인구 조사 및 표본 추출",
    "section": "6.2 측정",
    "text": "6.2 측정\n측정은 오래된 관심사입니다. 아리스토텔레스조차도 양과 질을 구별했습니다 (Tal 2020). 측정, 특히 측정의 비교는 모든 정량 분석의 기초가 됩니다. 그러나 무엇을 측정하고 어떻게 할 것인지를 결정하는 것은 어렵습니다.\n측정은 보기보다 까다롭습니다. 예를 들어, 음악에서 아이오와 주립 대학교 정치학 교수인 데이비드 피터슨은 원히트 원더를 정의하는 것이 얼마나 어려운지 분명히 합니다. 즉시 떠오르는 놀라운 수의 아티스트들이 차트에 오르는 데 상당히 성공한 다른 한두 곡을 가지고 있는 것으로 나타났습니다 (Molanphy 2012). 모든 단임 정부에 대한 분석에 임기를 다 채우지 못한 정부를 포함해야 할까요? 한 달, 심지어 일주일만 지속된 정부는 어떻습니까? 이러한 많은 부분이 현물 급여일 때 정부 이전의 범위를 어떻게 측정하기 시작할 수 있습니까 (Garfinkel, Rainwater, 와/과 Smeeding 2006)? 그것이 근본적인 관심사임에도 불구하고 민주주의에서 사람이 얼마나 잘 대표되는지 어떻게 측정할 수 있습니까 (Achen 1978)? 그리고 세계보건기구(WHO)가 사용하는 임신 관련 및 산모 사망의 표준 정의가 추정치에 상당한 영향을 미칠 때 출산, 중절 또는 낙태 후 42일 이내에 발생하는 것만 포함해야 하는 이유는 무엇입니까 (Gazeley 기타 2022)?\n철학은 측정의 정의에 더 많은 뉘앙스와 깊이를 더하지만 (Tal 2020), International Organization Of Legal Metrology (2007, p. 44)는 측정을 “양에 합리적으로 귀속될 수 있는 하나 이상의 양 값을 실험적으로 얻는 과정”으로 정의하며, 여기서 양은 “숫자와 참조를 함께” 의미합니다. 이는 “실체의 계산을 포함한 양의 비교”를 의미하며, “측정 결과의 의도된 사용, 측정 절차 및 보정된 측정 시스템에 상응하는 양의 설명을 전제로 합니다\\(\\dots\\)”. 이 측정의 정의는 우리가 계측 및 단위를 포함한 다양한 우려 사항을 가지고 있으며, 유효하고 신뢰할 수 있는 측정에 관심이 있음을 분명히 합니다.\n계측은 우리가 측정을 수행하는 데 사용하는 것을 의미합니다. 계측에 대한 철저한 고려는 우리가 무엇을 측정할 수 있는지를 결정하기 때문에 중요합니다. 예를 들어, Morange (2016, p. 63)는 16세기에 현미경의 발명이 1661년 마르첼로 말피기에 의한 모세혈관 관찰, 1665년 로버트 훅에 의한 세포 관찰, 1677년 안토니 판 레이우엔훅에 의한 박테리아 관찰로 이어졌다고 설명합니다 (Lane 2015). 그리고 시간 측정을 고려해 보십시오. 다시 우리는 계측과 측정 사이의 상호 작용을 봅니다. 해시계로는 경과 시간을 한 시간 정도로 구체적으로 측정하기 어려웠습니다. 그러나 더 정확한 시간 측정 기구의 점진적인 개발은 결국 일부 스포츠에서 경쟁자를 천분의 일초까지 구별할 수 있게 하고, GPS를 통해 미터 이내의 정확한 항해를 가능하게 할 것입니다.\n\n\n\n\n\n\n아, 우리가 그것에 대해 좋은 데이터를 가지고 있다고 생각하는군요!\n\n\n\n시간을 아는 것은 중요한 측정입니다. 예를 들어, 포뮬러 1은 랩 타임을 천분의 일초까지 측정합니다. 그리고 미국 수영 선수인 마이클 펠프스는 베이징 올림픽에서 단 100분의 1초 차이로 금메달을 땄습니다. 타이밍은 이벤트가 동시에 발생하지 않더라도 결과를 구별할 수 있게 해줍니다. 예를 들어, (chambliss1989mundanity의?) 수영 선수에 대한 논의를 다시 생각해 보면, 각 이벤트가 각 수영 선수에게 얼마나 걸렸는지 알지 못하면 이것이 불가능했을 것입니다. 타이밍은 또한 자산이 판매 가능한지 여부에 대해 시장 참여자들이 동의해야 하는 금융에서도 중요합니다. 그러나 “지금 몇 시입니까?”라는 질문에 대한 답은 대답하기 어려울 수 있습니다. 일부 개인에 따른 시간은 다른 소스로 설정될 수 있으므로 누구에게 묻느냐에 따라 달라집니다. 1970년대 이후로 명확한 답은 원자 시간을 사용하는 것이었습니다. 세슘 1초는 “세슘 133의 바닥 상태에서 초미세 전이 주파수의 9192 631 770 주기”로 정의됩니다 (Levine, Tavella, 와/과 Milton 2022, p. 4). 그러나 시계 동기화 문제—모든 비원자 시계가 원자 시간과 서로 일치하도록 하는 방법—는 여전히 남아 있습니다. (hopper2022는?) (mills1991internet의?) 네트워크 시간 프로토콜(NTP)이 시계 동기화를 가능하게 하는 방법과 컴퓨터 네트워크가 원자 시간을 발견하는 데 존재하는 몇 가지 어려움에 대한 개요를 제공합니다. 시간의 또 다른 척도는 지구의 자전을 기반으로 하는 천문 시간입니다. 그러나 지구가 불규칙하게 회전하고 다른 문제로 인해 원자 시간과 천문 시간이 일치하도록 조정되었습니다. 이로 인해 양의 윤초가 포함되었고 음의 윤초 가능성이 생겨 문제가 발생했습니다 (Levine, Tavella, 와/과 Milton 2022). 결과적으로 미래의 어느 시점에는 천문 시간과 원자 시간이 분기되도록 허용될 것입니다 (Gibney 2022; Mitchell 2022b).\n\n\n일반적인 측정 도구는 설문 조사이며, 이에 대해서는 ?sec-hunt-data에서 자세히 설명합니다. 또 다른 일반적으로 사용되는 도구는 센서입니다. 예를 들어, 기후 과학자들은 온도, 습도 또는 압력에 관심이 있을 수 있습니다. (LeosBarajas2016과?) 같은 동물 이동에 대한 많은 분석은 가속도계를 사용합니다. 위성에 배치된 센서는 특히 이미지에 관심이 있을 수 있으며, 이러한 데이터는 Landsat 프로그램에서 사용할 수 있습니다. 물리학자들은 측정에 매우 관심이 많으며, 계측뿐만 아니라 저장 용량에 의해서도 제약을 받을 수 있습니다. 예를 들어, CERN의 ATLAS 검출기는 입자 충돌에 초점을 맞추고 있지만, 초당 80TB가 발생하기 때문에 모든 측정을 저장할 수는 없습니다 (Colombo 기타 2016)! 그리고 ?sec-hunt-data에서 논의하는 A/B 테스트의 경우 쿠키, 비콘, 시스템 설정 및 행동 패턴이 광범위하게 사용됩니다. 계측의 또 다른 측면은 전달입니다. 예를 들어, 설문 조사를 사용하는 경우 우편으로 보내야 할까요, 아니면 온라인으로 보내야 할까요? 응답자가 직접 작성해야 할까요, 아니면 조사원이 작성해야 할까요?\n계측학에서 제공하는 측정의 정의는 두 번째 기본 관심사가 참조, 즉 단위라는 것을 분명히 합니다. 단위의 선택은 관심 있는 연구 질문과 사용 가능한 계측 모두와 관련이 있습니다. 예를 들어, ?sec-introduction의 튜토리얼에서 우리는 식물 성장을 측정하는 데 관심이 있었습니다. 킬로미터나 마일을 단위로 사용하는 것은 이 목적에 적합하지 않을 것입니다. 자를 사용했다면 밀리미터를 측정할 수 있었겠지만, 캘리퍼스를 사용했다면 수십 마이크로미터를 고려할 수 있었을 것입니다.\n\n6.2.1 측정의 속성\n유효한 측정은 우리가 측정하는 양이 관심 있는 추정량 및 연구 질문과 관련이 있는 측정입니다. 이는 적절성을 말합니다. ?sec-on-writing에서 추정량은 흡연이 기대 수명에 미치는 (알 수 없는) 실제 효과와 같은 실제 효과라는 것을 기억하십시오. 추정량을 실제로 관심 있는 것으로 생각하는 것이 유용할 수 있습니다. 이는 우리가 개인의 관련 측면을 측정하고 있는지 확인해야 함을 의미합니다. 예를 들어, 흡연에 대한 의견보다는 그들이 피운 담배 수와 그들이 산 햇수입니다.\n미터나 초와 같은 일부 단위의 경우 명확한 정의가 있습니다. 그리고 그 정의가 발전할 때 널리 동의됩니다 (Mitchell 2022a). 그러나 우리가 측정하고자 하는 다른 측면의 경우 덜 명확하므로 측정의 타당성이 중요해집니다. 14세기의 어느 시점에는 은혜와 미덕을 측정하려는 시도가 있었습니다 (Crosby 1997, p. 14)! 더 최근에는 지능이나 대학의 질을 측정하려고 합니다. 그것이 다른 사람들보다 더 많거나 적은 은혜, 미덕, 지능을 가진 사람들이 없다는 것을 말하는 것이 아니며, 확실히 더 좋고 나쁜 대학이 있습니다. 그러나 이것들의 측정은 어렵습니다.\nU.S. 뉴스 앤 월드 리포트는 수업 규모, 박사 학위를 가진 교수 수, 전임 교수 수와 같은 측면을 기반으로 대학의 질을 정량화하려고 합니다. 그러나 특히 사회적 환경에서 이러한 구성된 척도의 문제는 측정되는 사람들의 인센티브를 변경한다는 것입니다. 예를 들어, 컬럼비아 대학교는 1988년 18위에서 2022년 2위로 상승했습니다. 그러나 컬럼비아 대학교 수학 교수인 마이클 타데우스는 컬럼비아가 U.S. 뉴스 앤 월드 리포트에 보고한 것과 다른 출처를 통해 얻을 수 있는 것 사이에 컬럼비아에 유리한 차이가 있음을 보여주었습니다 (Hartocollis 2022).\n이러한 우려는 많은 기본 개념에 대한 명확한 척도가 없기 때문에 심리학에서 특히 중요합니다. (Fried2022는?) 우울증 측정을 검토하고 타당성과 신뢰성 부족을 포함한 많은 우려 사항을 발견합니다. 이것이 우리가 그러한 것을 측정하려고 시도해서는 안 된다는 것을 말하는 것이 아니라, 측정 결정에 대한 투명성을 보장해야 한다는 것입니다. 예를 들어, (Flake2020은?) 측정을 구성해야 할 때마다 다양한 명확화 질문에 답할 것을 권장합니다. 여기에는 관심 있는 기본 구성, 측정으로 이어진 결정 과정, 고려된 대안, 정량화 과정 및 척도에 대한 질문이 포함됩니다. 이러한 질문은 척도가 다른 곳에서 채택되는 것이 아니라 특정 목적을 위해 구성될 때 특히 중요합니다. 이는 척도가 미리 정해진 결과를 제공하는 방식으로 구성될 것이라는 우려 때문입니다.\n신뢰성은 “실험적으로 얻는 과정\\(\\dots\\)”이라는 측정 정의의 일부에서 비롯됩니다. 이는 어느 정도의 일관성을 의미하며, 특정 측면의 여러 측정이 특정 시간에 본질적으로 동일해야 함을 의미합니다. 두 명의 조사원이 거리의 상점 수를 세면, 우리는 그들의 수가 같기를 바랍니다. 그리고 만약 다르다면, 우리는 그 차이의 이유를 이해할 수 있기를 바랍니다. 예를 들어, 아마도 한 조사원이 지침을 오해하고 열려 있는 상점만 잘못 세었을 수 있습니다. 또 다른 예를 고려해 보면, 인구 통계학자들은 종종 국가 간의 인구 이동에 관심이 있고, 경제학자들은 종종 국제 무역에 관심이 있습니다. A국의 B국으로부터의 이민 또는 수입 데이터가 B국의 A국으로의 이민 또는 수출 데이터와 일치하지 않는 횟수가 우려됩니다.\n\n\n\n\n\n\n아, 우리가 그것에 대해 좋은 데이터를 가지고 있다고 생각하는군요!\n\n\n\n비행기 조종사가 승객에게 고도를 알리는 것은 일반적입니다. 그러나 고도의 개념과 측정은 기만적으로 복잡하며, 측정이 더 넓은 맥락에서 발생한다는 사실을 강조합니다 (Vanhoenacker 2015). 예를 들어, 비행기와 지상 사이의 미터 수에 관심이 있다면, 조종사가 앉아 있는 곳과 지상 사이의 차이를 측정해야 할까요? 이는 발표에 가장 관련이 있을 것입니다. 아니면 바퀴 바닥까지 측정해야 할까요? 이는 착륙에 가장 관련이 있을 것입니다. 산 위를 지나가면 어떻게 될까요? 비행기가 하강하지 않았더라도, 비행기와 지상 사이의 미터 수라는 그러한 측정은 고도 감소를 주장하고 여러 비행기를 수직으로 분리하기 어렵게 만들 것입니다. 우리는 해수면과의 비교에 관심이 있을 수 있습니다. 그러나 해수면은 조수 때문에 변하고, 다른 위치에서 다릅니다. 따라서 일반적인 고도 측정은 기압의 양에 의해 결정되는 비행 수준입니다. 그리고 기압은 날씨, 계절 및 위치에 영향을 받기 때문에, 한 비행 수준은 비행 과정에서 지상까지의 매우 다른 미터 수와 관련될 수 있습니다. 비행기에서 사용되는 고도 측정은 비교적 안전한 항공 여행을 가능하게 하는 목적을 달성합니다.\n\n\n\n\n6.2.2 측정 오차\n측정 오차는 우리가 관찰하는 값과 실제 값의 차이입니다. 때로는 특정 응답을 확인할 수 있습니다. 우리가 확인할 수 있는 응답과 확인할 수 없는 응답 사이의 차이가 일관되면, 우리는 전체 측정 오차의 정도를 추정할 수 있습니다. 예를 들어, (Sakshaug2010은?) 대학 동문 설문 조사를 고려하고 응답자의 성적에 대한 답변을 대학 기록과 비교했습니다. 그들은 설문 조사 방식—인간이 실시하는 전화 인터뷰, 컴퓨터가 실시하는 전화 인터뷰 또는 인터넷 설문 조사—이 측정 오차의 정도에 영향을 미친다는 것을 발견했습니다.\n이러한 오류는 조사원이 응답자를 대신하여 설문지를 작성할 때 특히 만연할 수 있습니다. 이것은 인종과 관련하여 특히 우려됩니다. 예를 들어, Davis (1997, p. 177)은 미국의 흑인들이 백인 면접관에게 자신의 정치적, 인종적 신념을 설명하는 정도를 제한할 수 있다고 설명합니다.\n또 다른 예는 검열된 데이터로, 실제 값에 대한 부분적인 지식만 있는 경우입니다. 오른쪽 검열된 데이터는 실제 값이 관찰된 값보다 높다는 것을 알지만 얼마나 높은지는 모르는 경우입니다. 예를 들어, 1986년 체르노빌 재해 직후, 방사선을 측정할 수 있는 유일한 기구는 특정 최대 한계가 있었습니다. 방사선이 그 (최대) 수준으로 측정되었지만, 실제 값은 훨씬 더 높다는 것을 의미했습니다.\n오른쪽 검열 데이터는 의료 연구에서 종종 볼 수 있습니다. 예를 들어, 어떤 실험이 수행되고 환자들이 10년 동안 추적 관찰된다고 가정해 봅시다. 그 10년 기간이 끝나면 우리가 아는 것은 환자가 최소 10년을 살았는지 여부이지, 그들의 정확한 수명은 아닙니다. 왼쪽 검열 데이터는 반대 상황입니다. 예를 들어, 영하까지만 내려가는 온도계를 생각해 보십시오. 실제 온도가 더 낮더라도 온도계는 여전히 영하로 등록할 것입니다.\n검열된 데이터의 약간의 변형은 윈저화된 데이터입니다. 이것은 우리가 실제 값을 관찰하지만, 덜 극단적인 값으로 변경할 때 발생합니다. 예를 들어, 나이를 고려하고 있다면 100세 이상인 사람의 나이를 100세로 변경할 수 있습니다. 너무 큰 값이 너무 큰 영향을 미칠까 걱정된다면 이렇게 할 수 있습니다.\n절단된 데이터는 우리가 해당 값을 기록조차 하지 않는 약간 다른 상황입니다. 예를 들어, 어린이의 나이와 키 사이의 관계에 관심이 있는 상황을 생각해 보십시오. 우리의 첫 번째 질문은 “나이가 어떻게 되세요?”일 수 있으며, 응답자가 성인으로 판명되면 키를 계속 묻지 않을 것입니다. 절단된 데이터는 특히 선택 편향과 밀접한 관련이 있습니다. 예를 들어, 과정을 중도에 포기하는 학생을 생각해 보십시오. 그들의 의견은 과정 평가에서 측정되지 않습니다.\n이러한 개념 간의 차이를 설명하기 위해, 신생아 체중의 실제 분포가 3.5kg를 중심으로 하는 정규 분포를 갖는 상황을 고려해 보겠습니다. 저울에 어떤 결함이 있어서 2.75kg 이하의 값은 2.75kg으로 할당된다고 상상해 보십시오. 그리고 4.25kg 이상으로 예상되는 아기는 다른 병원으로 옮겨져 태어나는 규칙이 있다고 상상해 보십시오. 이 세 가지 시나리오는 ?fig-babyweights에 설명되어 있습니다. 우리는 또한 평균 체중을 고려하는 데 관심이 있을 수 있으며, 이는 편향을 강조합니다(표 6.1).\n\nset.seed(853)\n\nnewborn_weight &lt;-\n  tibble(\n    weight = rep(\n      x = rnorm(n = 1000, mean = 3.5, sd = 0.5), \n      times = 3),\n    measurement = rep(\n      x = c(\"Actual\", \"Censored\", \"Truncated\"),\n      each = 1000)\n    )\n\nnewborn_weight &lt;-\n  newborn_weight |&gt;\n  mutate(\n    weight = case_when(\n      weight &lt;= 2.75 & measurement == \"Censored\" ~ 2.75,\n      weight &gt;= 4.25 & measurement == \"Truncated\" ~ NA_real_,\n      TRUE ~ weight\n    )\n  )\n\nnewborn_weight |&gt;\n  ggplot(aes(x = weight)) +\n  geom_histogram(bins = 50) +\n  facet_wrap(vars(measurement)) +\n  theme_minimal()\n\n\n\n\n\n\n\n그림 6.1: 실제 체중과 검열 및 절단된 체중 비교\n\n\n\n\n\n\nnewborn_weight |&gt;\n  summarise(mean = mean(weight, na.rm = TRUE),\n            .by = measurement) |&gt; \n  tt() |&gt; \n  style_tt(j = 1:2, align = \"lr\") |&gt; \n  format_tt(digits = 3, num_fmt = \"decimal\") |&gt; \n  setNames(c(\"Measurement\", \"Mean\"))\n\n\n\n표 6.1: 다른 시나리오의 평균을 비교하여 편향을 식별합니다\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Measurement\n                Mean\n              \n        \n        \n        \n                \n                  Actual\n                  3.521\n                \n                \n                  Censored\n                  3.53\n                \n                \n                  Truncated\n                  3.455\n                \n        \n      \n    \n\n\n\n\n\n\n\n\n6.2.3 결측 데이터\n데이터 수집 프로세스가 아무리 좋아도 결측 데이터가 있을 것입니다. 즉, 우리가 가지고 있지 않다는 것을 아는 관측치입니다. 그러나 변수는 결측되기 위해 측정되거나 적어도 생각하고 고려되어야 합니다. 충분한 고려가 없으면 변수가 전혀 고려되지 않았기 때문에 우리가 결측되었다는 사실조차 모르는 결측 데이터의 위험이 있습니다. 그들은 “짖지 않은 개”라는 의미에서 결측되었습니다. 이것이 상황에 대해 생각하고, 스케치하고, 시뮬레이션하고, 주제 전문가와 협력하는 것이 매우 중요한 이유입니다.\n무응답은 측정 오차의 변형으로 간주될 수 있습니다 여기서 실제 값이 있어야 함에도 불구하고 null을 관찰합니다. 그러나 일반적으로 자체적으로 고려됩니다. 그리고 무응답에는 설문 조사에 응답하는 것조차 거부하는 것부터 한 질문만 누락하는 것까지 다양한 정도가 있습니다. 무응답은 특히 비확률 표본에서 핵심적인 문제인데, 응답하지 않는 사람들이 응답하는 사람들과 체계적으로 다르다고 생각할 만한 충분한 이유가 있기 때문입니다. 그리고 이것은 설문 조사가 설문 조사 응답자 이상을 말하는 데 사용될 수 있는 정도를 제한하는 역할을 합니다. (gelman2016mythical은?) 선거를 앞두고 보고되는 여론의 많은 변화가 사람들이 마음을 바꾸는 것이 아니라 차등적 무응답이라고까지 말합니다. 즉, 개인이 어떤 설문 조사 응답을 선택할지 선택하는 것뿐만 아니라 상황에 따라 설문 조사에 응답할지 여부를 선택하는 것입니다. 사전 통지 및 알림을 사용하면 일부 상황에서 무응답을 해결하는 데 도움이 될 수 있습니다 (Koitsalu 기타 2018; Frandell 기타 2021).\n무응답 편향이 특히 만연했을 수 있는 한 가지 상황은 시험 응시입니다. 국제 학생 평가 프로그램(PISA)은 다양한 표준화된 시험에서 15세 학생들의 성과를 국가 간에 비교하는 것입니다. 캐나다는 일반적으로 높은 순위를 차지합니다. 그러나 (Anders2020은?) 2015년에 캐나다 15세 학생의 절반만이 표본에 포함된 반면, 다른 일부 국가에서는 90% 이상이 포함되었다는 것을 보여줍니다. 비교 가능한 표본을 사용하면 캐나다가 과도하게 성과를 내는 것으로 보이지 않을 것이라고 그들은 발견했습니다.\n데이터는 응답자가 특정 질문, 관련된 질문 모음 또는 전체 설문 조사에 응답하고 싶지 않았기 때문에 누락될 수 있지만, 이들은 상호 배타적이거나 집합적으로 완전하지는 않습니다 (Newman 2014). 이상적인 상황에서는 데이터가 완전히 무작위로 누락됩니다(MCAR). 이것은 거의 발생하지 않지만, 만약 발생한다면 추론은 여전히 더 넓은 인구를 반영해야 합니다. 데이터가 무작위로 누락(MAR)되거나 무작위가 아닌 누락(MNAR)될 가능성이 더 높습니다. 우리가 그것에 대해 걱정해야 하는 정도는 다릅니다. 예를 들어, 우리가 성별이 정치적 지지에 미치는 영향에 관심이 있다면, 남성이 설문 조사에 응답할 가능성이 적을 수 있지만, 이것은 그들이 누구를 지지할지와 관련이 없습니다. 그 차등적 응답이 남성이라는 이유만으로 발생하고 정치적 지지와 관련이 없다면, 회귀에 성별을 포함하거나 성별에 따라 사후 층화할 수 있다면 계속 진행할 수 있습니다. 그렇긴 하지만, 이 독립성이 유지될 가능성은 낮으며, (gelman2016mythical에서와?) 같이 설문 조사에 응답하는 것과 정치적 지지 사이에 관계가 있을 가능성이 더 높습니다. 그 더 가능성 있는 경우, 우리는 더 중요한 문제를 가질 수 있습니다. 한 가지 접근 방식은 추가적인 설명 변수를 고려하는 것입니다. 불완전한 사례를 삭제하고 싶지만, 이것은 표본을 더욱 편향시킬 수 있으며, 정당화와 시뮬레이션의 지원이 필요합니다. 데이터 대체를 고려할 수 있지만, 다시 표본을 편향시킬 수 있습니다. 이상적으로는 데이터 수집 프로세스를 재고하고 개선할 수 있습니다.\n?sec-exploratory-data-analysis에서 결측 데이터로 돌아갑니다.",
    "crumbs": [
      "Acquisition",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>측정, 인구 조사 및 표본 추출</span>"
    ]
  },
  {
    "objectID": "06-farm_ko.html#인구-조사-및-기타-정부-데이터",
    "href": "06-farm_ko.html#인구-조사-및-기타-정부-데이터",
    "title": "6  측정, 인구 조사 및 표본 추출",
    "section": "6.3 인구 조사 및 기타 정부 데이터",
    "text": "6.3 인구 조사 및 기타 정부 데이터\n데이터셋으로 사용될 목적으로 생산된 다양한 데이터 소스가 있습니다. 여기서는 특히 인구 조사를 생각합니다. Whitby (2020, p. 30-31)는 우리가 서면 기록을 가지고 있는 가장 오래된 인구 조사가 중국의 황하 계곡에서 비롯되었다고 설명합니다. 인구 조사의 한 가지 동기는 조세였으며, (jones1953census는?) 새로운 조세 제도를 가능하게 한 서기 3세기 후반 또는 4세기 초의 인구 조사 기록을 설명합니다. 인구 조사와 같은 상세한 기록은 또한 남용되었습니다. 예를 들어, Luebke 와/과 Milton (1994, p. 25)는 나치가 인구 조사 및 경찰 등록 데이터셋을 사용하여 “결국 추방 및 사망 예정인 그룹을 찾는” 방법을 설명합니다. 그리고 Bowen (2022, p. 17)은 미국 인구 조사국이 일본계 미국인의 인턴십에 기여한 정보를 제공했다고 설명합니다. 클린턴 대통령은 1990년대에 이에 대해 사과했습니다.\n데이터셋으로 의도적으로 구성된 또 다른 데이터 소스에는 실업률, 인플레이션 및 GDP와 같은 경제 상황에 대한 설문 조사와 같은 공식 통계가 포함됩니다. 흥미롭게도 (rockoff2019controversies는?) 이러한 경제 통계가 연방 정부에 의해 실제로 개발되지 않았다고 설명하지만, 정부가 결국 그 역할을 맡게 되었습니다. 인구 조사 및 기타 정부 운영 설문 조사는 국가의 권력과 재정 자원을 배경으로 하므로 다른 데이터셋이 할 수 없는 방식으로 철저할 수 있습니다. 예를 들어, 2020년 미국 인구 조사는 156억 달러의 비용이 들 것으로 추정됩니다 (Hawes 2020). 그러나 이것은 유사하게 특정 관점을 가져옵니다. 인구 조사 데이터는 모든 데이터와 마찬가지로 비난할 수 없습니다. 일반적인 오류에는 과소 및 과대 열거뿐만 아니라 오보도 포함됩니다 (Steckel 1991). 품질을 평가하는 데 사용되는 다양한 측정 및 접근 방식이 있습니다 (Statistics Canada 2023).\n\n\n\n\n\n\n아, 우리가 그것에 대해 좋은 데이터를 가지고 있다고 생각하는군요!\n\n\n\n인구 조사는 중요하지만 비난할 수 없습니다. (anderson1999counts는?) 미국 인구 조사의 역사가 과소 집계의 역사이며, 조지 워싱턴조차도 1790년대에 이에 대해 불평했다고 설명합니다. 과소 집계의 정도는 제2차 세계 대전에서 징병에 사용된 선택적 복무 등록 시스템으로 인해 추정되었습니다. 해당 기록은 인구 조사 기록과 비교되었으며, 징병 목적으로 기록된 남성이 인구 조사보다 약 50만 명 더 많은 것으로 나타났습니다. 이것은 인종에 따라 다르며, 평균 과소 집계는 약 3%였지만, 징병 연령의 흑인 남성의 과소 집계는 약 13%였습니다 (Anderson 와/과 Fienberg 1999, p. 29). 이것은 1960년대에 정치적 문제가 되었고, 인종 및 민족 관련 질문은 1990년대에 특히 우려되었습니다. Nobles (2002, p. 47)은 인종별 계산이 먼저 인종이 존재해야 하지만, 이것이 생물학적으로 확립하기 어려울 수 있다고 논의합니다. 미국 인구 조사에서 인종이 얼마나 근본적인지에 관계없이, 그것은 “고정”되고 “객관적인” 것이 아니라 계급, 사회, 법률, 구조 및 정치적 측면의 영향을 받습니다 (Nobles 2002, p. 48).\n\n\n\n\n\n\n\n\n거인의 어깨 위에 서서\n\n\n\n마고 앤더슨은 위스콘신-밀워키 대학교의 역사 및 도시 연구 분야의 저명한 교수입니다. 1978년 럿거스 대학교에서 역사학 박사 학위를 취득한 후 위스콘신-밀워키 대학교에 부임하여 1987년에 교수로 승진했습니다. Anderson 와/과 Fienberg (1999) 외에도 그녀가 쓴 또 다른 중요한 책은 (anderson2015census입니다?). 그녀는 1998년에 미국 통계 협회 펠로우로 임명되었습니다.\n\n\n또 다른 유사하게 크고 확립된 데이터 소스는 장기적인 대규모 설문 조사에서 비롯됩니다. 이들은 정기적으로 수행되며, 일반적으로 정부가 직접 수행하지는 않지만, 어떤 식으로든 정부의 자금 지원을 받습니다. 예를 들어, 여기서는 1965년 이후 모든 연방 선거와 관련하여 실시된 캐나다 선거 연구와 1964년 이후 모든 총선과 관련하여 실시된 영국 선거 연구와 같은 선거 설문 조사를 생각합니다.\n최근에는 정부의 데이터 공개에 대한 큰 움직임이 있었습니다. 정부가 가지고 있는 데이터를 공개해야 한다는 기본 원칙은 부인할 수 없습니다. 그러나 이 용어는 실제로 어떻게 발생했는지 때문에 약간 논란의 여지가 있습니다. 정부는 제공하고 싶은 데이터만 제공합니다. 때로는 정부의 서술에 맞게 데이터를 조작하는 것을 볼 수도 있습니다 (Kalgin 2014; Zhang 기타 2019; Berdine, Geloso, 와/과 Powell 2018). 정부가 가지고 있지만 반드시 제공하고 싶지 않은 데이터를 얻는 한 가지 방법은 정보 자유(FOI) 요청을 제출하는 것입니다 (Walby 와/과 Luscombe 2019). 예를 들어, (biasbehindbars는?) FOI의 데이터를 사용하여 캐나다 교도소 시스템의 체계적인 인종 차별의 증거를 찾습니다.\n양식 데이터셋은 항상 유용했지만, 많은 분석이 프로그래밍 언어를 사용하지 않고 수행되던 시기에 개발되었습니다. 이러한 데이터셋을 R로 쉽게 가져올 수 있도록 많은 R 패키지가 개발되었습니다. 여기서는 특히 유용한 몇 가지를 다룹니다.\n\n6.3.1 캐나다\n캐나다 최초의 인구 조사는 1666년에 실시되었습니다. 이것은 또한 모든 개인이 이름으로 기록된 최초의 현대 인구 조사였지만, 원주민은 포함하지 않습니다 (Godfrey 1918, p. 179). 3,215명의 주민이 집계되었으며, 인구 조사는 연령, 성별, 결혼 상태 및 직업에 대해 질문했습니다 (Statistics Canada 2023). 캐나다 연방과 관련하여 1867년에는 새로운 의회에 정치 대표를 할당할 수 있도록 10년마다 인구 조사가 요구되었습니다. 그 이후로 정기적인 인구 조사가 실시되었습니다.\ncanlang을 사용하여 2016년 인구 조사에서 캐나다에서 사용되는 언어에 대한 일부 데이터를 탐색할 수 있습니다. 이 패키지는 CRAN에 없지만 install.packages(\"devtools\") 다음 devtools::install_github(\"ttimbers/canlang\")를 사용하여 GitHub에서 설치할 수 있습니다.\ncanlang을 로드한 후 can_lang 데이터셋을 사용할 수 있습니다. 이것은 214개 언어 각각을 사용하는 캐나다인의 수를 제공합니다.\n\ncan_lang\n\n# A tibble: 214 × 6\n   category          language mother_tongue most_at_home most_at_work lang_known\n   &lt;chr&gt;             &lt;chr&gt;            &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n 1 Aboriginal langu… Aborigi…           590          235           30        665\n 2 Non-Official & N… Afrikaa…         10260         4785           85      23415\n 3 Non-Official & N… Afro-As…          1150          445           10       2775\n 4 Non-Official & N… Akan (T…         13460         5985           25      22150\n 5 Non-Official & N… Albanian         26895        13135          345      31930\n 6 Aboriginal langu… Algonqu…            45           10            0        120\n 7 Aboriginal langu… Algonqu…          1260          370           40       2480\n 8 Non-Official & N… America…          2685         3020         1145      21930\n 9 Non-Official & N… Amharic          22465        12785          200      33670\n10 Non-Official & N… Arabic          419890       223535         5585     629055\n# ℹ 204 more rows\n\n\n모국어로 가장 흔한 상위 10개 언어를 빠르게 볼 수 있습니다.\n\ncan_lang |&gt;\n  slice_max(mother_tongue, n = 10) |&gt;\n  select(language, mother_tongue)\n\n# A tibble: 10 × 2\n   language                     mother_tongue\n   &lt;chr&gt;                                &lt;dbl&gt;\n 1 English                           19460850\n 2 French                             7166700\n 3 Mandarin                            592040\n 4 Cantonese                           565270\n 5 Punjabi (Panjabi)                   501680\n 6 Spanish                             458850\n 7 Tagalog (Pilipino, Filipino)        431385\n 8 Arabic                              419890\n 9 German                              384040\n10 Italian                             375635\n\n\nregion_lang과 region_data라는 두 데이터셋을 결합하여 가장 큰 지역인 토론토와 가장 작은 지역인 벨빌 간에 가장 흔한 5개 언어가 다른지 확인할 수 있습니다.\n\nregion_lang |&gt;\n  left_join(region_data, by = \"region\") |&gt;\n  slice_max(c(population)) |&gt;\n  slice_max(mother_tongue, n = 5) |&gt;\n  select(region, language, mother_tongue, population) |&gt;\n  mutate(prop = mother_tongue / population)\n\n# A tibble: 5 × 5\n  region  language          mother_tongue population   prop\n  &lt;chr&gt;   &lt;chr&gt;                     &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n1 Toronto English                 3061820    5928040 0.516 \n2 Toronto Cantonese                247710    5928040 0.0418\n3 Toronto Mandarin                 227085    5928040 0.0383\n4 Toronto Punjabi (Panjabi)        171225    5928040 0.0289\n5 Toronto Italian                  151415    5928040 0.0255\n\nregion_lang |&gt;\n  left_join(region_data, by = \"region\") |&gt;\n  slice_min(c(population)) |&gt;\n  slice_max(mother_tongue, n = 5) |&gt;\n  select(region, language, mother_tongue, population) |&gt;\n  mutate(prop = mother_tongue / population)\n\n# A tibble: 5 × 5\n  region     language mother_tongue population    prop\n  &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n1 Belleville English          93655     103472 0.905  \n2 Belleville French            2675     103472 0.0259 \n3 Belleville German             635     103472 0.00614\n4 Belleville Dutch              600     103472 0.00580\n5 Belleville Spanish            350     103472 0.00338\n\n\n토론토 거주자의 50% 이상이 영어를 모국어로 사용하는 반면, 벨빌 거주자의 약 90%가 영어를 모국어로 사용하는 등 비율에 상당한 차이가 있음을 알 수 있습니다.\n일반적으로 캐나다 인구 조사 데이터는 다른 국가만큼 관련 정부 기관을 통해 쉽게 구할 수 없지만, 나중에 논의할 통합 공공 사용 마이크로데이터 시리즈(IPUMS)는 일부에 대한 액세스를 제공합니다. 인구 조사 및 기타 공식 통계를 담당하는 정부 기관인 캐나다 통계청은 2016년 인구 조사의 “개인 파일”을 공공 사용 마이크로데이터 파일(PUMF)로 무료로 제공하지만 요청에 응답하는 경우에만 제공합니다. 그리고 2016년 인구 조사의 2.7% 표본이지만, 이 PUMF는 제한된 세부 정보를 제공합니다.\n캐나다 인구 조사 데이터에 액세스하는 또 다른 방법은 cancensus를 사용하는 것입니다. API 키가 필요하며, 계정을 만들고 “프로필 편집”으로 이동하여 요청할 수 있습니다. 이 패키지에는 API 키를 “.Renviron” 파일에 더 쉽게 추가할 수 있는 도우미 함수가 있으며, 이에 대해서는 ?sec-gather-data에서 자세히 설명합니다.\ncancensus를 설치하고 로드한 후 get_census()를 사용하여 인구 조사 데이터를 얻을 수 있습니다. 관심 있는 인구 조사와 다양한 기타 인수를 지정해야 합니다. 예를 들어, 인구 기준으로 캐나다에서 가장 큰 주인 온타리오에 대한 2016년 인구 조사 데이터를 얻을 수 있습니다.\n\nset_api_key(\"ADD_YOUR_API_KEY_HERE\", install = TRUE)\n\nontario_population &lt;-\n  get_census(\n    dataset = \"CA16\",\n    level = \"Regions\",\n    vectors = \"v_CA16_1\",\n    regions = list(PR = c(\"35\"))\n  )\n\nontario_population\n\n\n\n# A tibble: 1 × 9\n  GeoUID Type  `Region Name` `Area (sq km)` Population Dwellings Households\n  &lt;chr&gt;  &lt;fct&gt; &lt;fct&gt;                  &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 35     PR    Ontario              986722.   13448494   5598391    5169174\n# ℹ 2 more variables: C_UID &lt;chr&gt;, `v_CA16_1: Age Stats` &lt;dbl&gt;\n\n\n1996년 이후의 인구 조사 데이터를 사용할 수 있으며, list_census_datasets()는 이러한 데이터에 액세스하기 위해 get_census()에 제공해야 하는 메타데이터를 제공합니다. 데이터는 다양한 지역을 기반으로 사용할 수 있으며, list_census_regions()는 필요한 메타데이터를 제공합니다. 마지막으로, list_census_vectors()는 사용 가능한 변수에 대한 메타데이터를 제공합니다.\n\n\n6.3.2 미국\n\n6.3.2.1 인구 조사\n인구 조사의 요구 사항은 미국 헌법에 포함되어 있지만, 1639년 초에 매사추세츠가 된 곳에서는 출생과 사망이 법적으로 등록되어야 했습니다 (Gutman 1958). 설치하고 로드한 후 tidycensus를 사용하여 미국 인구 조사 데이터에 액세스할 수 있습니다. cancensus와 마찬가지로 먼저 인구 조사국 API에서 API 키를 얻고 도우미 함수를 사용하여 로컬에 저장해야 합니다.\n설정이 완료되면 get_decennial()을 사용하여 관심 변수에 대한 데이터를 얻을 수 있습니다. 예를 들어, 2010년 전체 및 소유자 또는 임차인별 특정 주에 대한 평균 가구 규모에 대한 데이터를 수집할 수 있습니다(그림 6.2).\n\ncensus_api_key(\"ADD_YOUR_API_KEY_HERE\")\n\nus_ave_household_size_2010 &lt;-\n  get_decennial(\n    geography = \"state\",\n    variables = c(\"H012001\", \"H012002\", \"H012003\"),\n    year = 2010\n  )\n\nus_ave_household_size_2010 |&gt;\n  filter(NAME %in% c(\"District of Columbia\", \"Utah\", \"Massachusetts\")) |&gt;\n  ggplot(aes(y = NAME, x = value, color = variable)) +\n  geom_point() +\n  theme_minimal() +\n  labs(\n    x = \"Average household size\", y = \"State\", color = \"Household type\"\n    ) +\n  scale_color_brewer(\n    palette = \"Set1\", labels = c(\"Total\", \"Owner occupied\", \"Renter occupied\")\n    )\n\n\n\n\n\n\n\n\n\n그림 6.2: DC, 유타, 매사추세츠의 가구 유형별 평균 가구 규모 비교\n\n\n\n\n\n(walker2022는?) R로 미국 인구 조사 데이터를 분석하는 방법에 대해 자세히 설명합니다.\n\n\n6.3.2.2 미국 지역사회 조사\n미국은 일반적으로 인구 조사를 사용하는 것보다 더 나은 접근 방식이 있고 정부 통계 기관 웹사이트를 사용하는 것보다 더 나은 방법이 있는 부러운 상황에 있습니다. IPUMS는 국제 인구 조사 마이크로데이터를 포함한 광범위한 데이터셋에 대한 액세스를 제공합니다. 미국의 특정 경우, 미국 지역사회 조사(ACS)는 많은 인구 조사에서 질문하는 내용과 비교할 수 있는 내용의 설문 조사이지만, 데이터가 사용 가능해질 때까지 상당히 오래될 수 있는 인구 조사와 비교하여 연간 단위로 제공됩니다. 매년 수백만 건의 응답으로 끝납니다. ACS는 인구 조사보다 작지만, 더 시기적절하게 사용할 수 있다는 장점이 있습니다. 우리는 IPUMS를 통해 ACS에 액세스합니다.\n\n\n\n\n\n\n거인의 어깨 위에 서서\n\n\n\n스티븐 러글스는 미네소타 대학교의 역사 및 인구 연구 분야의 리전트 교수이며 IPUMS를 책임지고 있습니다. 1984년 펜실베이니아 대학교에서 역사 인구 통계학 박사 학위를 취득한 후 미네소타 대학교의 조교수로 임명되었고 1995년에 정교수로 승진했습니다. 최초의 IPUMS 데이터 공개는 1993년이었습니다 (Sobek 와/과 Ruggles 1999). 그 이후로 성장하여 현재는 많은 국가의 사회 및 경제 데이터를 포함합니다. 러글스는 2022년에 맥아더 재단 펠로우십을 수상했습니다.\n\n\nIPUMS로 이동한 다음 “IPUMS USA”로 이동하여 “데이터 가져오기”를 클릭합니다. 우리는 표본에 관심이 있으므로 “표본 선택”으로 이동합니다. “각 연도의 기본 표본”을 선택 취소하고 대신 “2019 ACS”를 선택한 다음 “표본 선택 제출”을 클릭합니다(그림 6.3 (a)).\n\n\n\n\n\n\n\n\n\n\n\n(a) IPUMS USA에서 표본을 선택하고 2019 ACS에 대한 관심 지정\n\n\n\n\n\n\n\n\n\n\n\n(b) 주에 관심이 있음을 지정\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) 장바구니에 STATEICP 추가\n\n\n\n\n\n\n\n\n\n\n\n(d) 결제 프로세스 시작\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e) .dta 파일에 관심이 있음을 지정\n\n\n\n\n\n\n\n\n\n\n\n(f) 표본 크기를 300만 응답에서 50만으로 줄이기\n\n\n\n\n\n\n\n그림 6.3: IPUMS에서 데이터를 얻는 단계 개요\n\n\n\n우리는 주별 데이터에 관심이 있을 수 있습니다. “HOUSEHOLD” 변수를 보고 “GEOGRAPHIC”을 선택하는 것으로 시작합니다(그림 6.3 (b)). 더하기를 클릭하여 “STATEICP”를 “장바구니”에 추가하면 더하기가 체크 표시로 바뀝니다(그림 6.3 (c)). 그런 다음 “PERSON” 기준의 데이터, 예를 들어 “AGE”와 같은 “DEMOGRAPHIC” 변수에 관심이 있을 수 있으며, 이를 장바구니에 추가해야 합니다. 또한 “SEX”와 “EDUC”(둘 다 “PERSON”에 있음)도 원합니다.\n완료되면 “장바구니 보기”를 클릭한 다음 “데이터 추출 생성”을 클릭합니다(그림 6.3 (d)). 이 시점에서 변경하고 싶은 두 가지 측면이 있습니다.\n\n“데이터 형식”을 “.dat”에서 “.dta”로 변경합니다(그림 6.3 (e)).\n300만 개의 응답이 필요하지 않을 가능성이 높으므로 표본 크기를 사용자 지정하고, 예를 들어 500,000개로 변경할 수 있습니다(그림 6.3 (f)).\n\n요청의 차원을 간략하게 확인하십시오. 약 40MB를 훨씬 넘지 않아야 합니다. 그렇다면 필요하지 않은 변수가 우연히 선택되었는지 확인하거나 관측치 수를 더 줄이십시오.\n마지막으로, 추출에 대한 설명적인 이름을 포함하고 싶습니다. 예를 들어, “2023-05-15: 주, 연령, 성별, 교육”과 같이 추출한 날짜와 추출에 포함된 내용을 지정합니다. 그 후 “추출 제출”을 클릭할 수 있습니다.\n로그인하거나 계정을 만들라는 요청을 받게 되며, 그 후에 요청을 제출할 수 있습니다. IPUMS는 추출이 가능해지면 이메일을 보내며, 그 후에 다운로드하여 일반적인 방법으로 R로 읽어들일 수 있습니다. 데이터셋이 로컬에 “usa_00015.dta”로 저장되었다고 가정합니다(데이터셋의 파일 이름이 약간 다를 수 있습니다).\n이 데이터셋을 사용할 때 인용하는 것이 중요합니다. 예를 들어, (ipumsusa에?) 대해 다음 BibTeX 항목을 사용할 수 있습니다.\n@misc{ipumsusa,\n  author       = {Ruggles,  Steven and Flood,  Sarah and Foster,  Sophia and Goeken,  Ronald and Pacas,  Jose and Schouweiler,  Megan and Sobek,  Matthew},\n  year         = 2021,\n  title        = {IPUMS USA: Version 11.0},\n  publisher    = {Minneapolis,  MN: IPUMS},\n  doi          = {10.18128/d010.v11.0},\n  url          = {https://usa.ipums.org},\n  language     = {en},\n}\n?sec-multilevel-regression-with-post-stratification에서 사용할 것이므로 이 데이터셋을 간략하게 정리하고 준비하겠습니다. 우리 코드는 (greatstudentwork를?) 기반으로 합니다.\n\nipums_extract &lt;- read_dta(\"usa_00015.dta\")\n\nipums_extract &lt;- \n  ipums_extract |&gt;\n  select(stateicp, sex, age, educd) |&gt;\n  to_factor()\n\nipums_extract\n\n\n\n# A tibble: 500,221 × 4\n   stateicp sex    age   educd                                       \n * &lt;fct&gt;    &lt;fct&gt;  &lt;fct&gt; &lt;fct&gt;                                       \n 1 alabama  male   77    grade 9                                     \n 2 alabama  male   62    1 or more years of college credit, no degree\n 3 alabama  male   25    ged or alternative credential               \n 4 alabama  female 20    1 or more years of college credit, no degree\n 5 alabama  male   37    1 or more years of college credit, no degree\n 6 alabama  female 19    regular high school diploma                 \n 7 alabama  female 67    regular high school diploma                 \n 8 alabama  female 20    1 or more years of college credit, no degree\n 9 alabama  male   66    grade 8                                     \n10 alabama  male   58    regular high school diploma                 \n# ℹ 500,211 more rows\n\n\n\ncleaned_ipums &lt;-\n  ipums_extract |&gt;\n  mutate(age = as.numeric(age)) |&gt;\n  filter(age &gt;= 18) |&gt;\n  rename(gender = sex) |&gt;\n  mutate(\n    age_group = case_when(\n      age &lt;= 29 ~ \"18-29\",\n      age &lt;= 44 ~ \"30-44\",\n      age &lt;= 59 ~ \"45-59\",\n      age &gt;= 60 ~ \"60+\",\n      TRUE ~ \"Trouble\"\n    ),\n    education_level = case_when(\n      educd %in% c(\n        \"nursery school, preschool\", \"kindergarten\", \"grade 1\",\n        \"grade 2\", \"grade 3\", \"grade 4\", \"grade 5\", \"grade 6\",\n        \"grade 7\", \"grade 8\", \"grade 9\", \"grade 10\", \"grade 11\",\n        \"12th grade, no diploma\", \"regular high school diploma\",\n        \"ged or alternative credential\", \"no schooling completed\"\n      ) ~ \"High school or less\",\n      educd %in% c(\n        \"some college, but less than 1 year\",\n        \"1 or more years of college credit, no degree\"\n      ) ~ \"Some post sec\",\n      educd  %in% c(\"associate's degree, type not specified\",\n                    \"bachelor's degree\") ~ \"Post sec +\",\n      educd %in% c(\n        \"master's degree\",\n        \"professional degree beyond a bachelor's degree\",\n        \"doctoral degree\"\n      ) ~ \"Grad degree\",\n      TRUE ~ \"Trouble\"\n    )\n  ) |&gt;\n  select(gender, age_group, education_level, stateicp) |&gt;\n  mutate(across(c(\n    gender, stateicp, education_level, age_group),\n    as_factor)) |&gt;\n  mutate(age_group =\n           factor(age_group, levels = c(\"18-29\", \"30-44\", \"45-59\", \"60+\")))\n\ncleaned_ipums\n\n# A tibble: 407,354 × 4\n   gender age_group education_level     stateicp\n   &lt;fct&gt;  &lt;fct&gt;     &lt;fct&gt;               &lt;fct&gt;   \n 1 male   60+       High school or less alabama \n 2 male   60+       Some post sec       alabama \n 3 male   18-29     High school or less alabama \n 4 female 18-29     Some post sec       alabama \n 5 male   30-44     Some post sec       alabama \n 6 female 18-29     High school or less alabama \n 7 female 60+       High school or less alabama \n 8 female 18-29     Some post sec       alabama \n 9 male   60+       High school or less alabama \n10 male   45-59     High school or less alabama \n# ℹ 407,344 more rows\n\n\n?sec-multilevel-regression-with-post-stratification에서 이 데이터셋을 사용할 것이므로 저장하겠습니다.\n\nwrite_csv(x = cleaned_ipums,\n          file = \"cleaned_ipums.csv\")\n\n일부 변수도 살펴볼 수 있습니다(그림 6.4).\n\ncleaned_ipums |&gt;\n  ggplot(mapping = aes(x = age_group, fill = gender)) +\n  geom_bar(position = \"dodge2\") +\n  theme_minimal() +\n  labs(\n    x = \"Age-group of respondent\",\n    y = \"Number of respondents\",\n    fill = \"Education\"\n  ) +\n  facet_wrap(vars(education_level)) +\n  guides(x = guide_axis(angle = 90)) +\n  theme(legend.position = \"bottom\") +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n그림 6.4: 연령, 성별 및 교육별 IPUMS ACS 표본 수 검토\n\n\n\n\n\n전체 집계 데이터—즉, 전체 인구 조사—는 1890년을 제외하고 1850년에서 1940년 사이에 실시된 미국 인구 조사에 대해 IPUMS를 통해 사용할 수 있습니다. 1890년 인구 조사 기록의 대부분은 1921년 화재로 소실되었습니다. 1990년까지의 모든 인구 조사에 대해 1% 표본을 사용할 수 있습니다. ACS 데이터는 2000년부터 사용할 수 있습니다.",
    "crumbs": [
      "Acquisition",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>측정, 인구 조사 및 표본 추출</span>"
    ]
  },
  {
    "objectID": "06-farm_ko.html#표본-추출-필수-사항",
    "href": "06-farm_ko.html#표본-추출-필수-사항",
    "title": "6  측정, 인구 조사 및 표본 추출",
    "section": "6.4 표본 추출 필수 사항",
    "text": "6.4 표본 추출 필수 사항\n통계는 우리가 원하는 모든 데이터를 얻는 것이 거의 불가능하기 때문에 데이터로 이야기를 하는 데 핵심적인 역할을 합니다. 통계학자들은 다양한 데이터 표본이 가질 속성과 그것이 더 넓은 인구에 대한 의미를 말하는 데 어떻게 도움이 되는지에 대해 상당한 시간과 노력을 기울였습니다.\n데이터가 있다고 가정해 봅시다. 예를 들어, 특정 유아는 매일 저녁 6시에 잠자리에 듭니다. 우리는 그 취침 시간이 모든 유아에게 일반적인지, 아니면 특이한 유아를 가지고 있는지 알고 싶을 수 있습니다. 유아가 한 명뿐이라면 그들의 취침 시간을 사용하여 모든 유아에 대해 말하는 능력은 제한될 것입니다.\n한 가지 접근 방식은 유아를 둔 친구들과 이야기하는 것입니다. 그런 다음 친구의 친구들과 이야기합니다. 유아 취침 시간의 근본적인 진실에 대해 말할 수 있다고 편안하게 느끼기 시작하기 전에 얼마나 많은 친구와 친구의 친구들에게 물어봐야 할까요?\nWu 와/과 Thompson (2020, p. 3)은 통계를 “데이터를 수집하고 분석하고 미지의 인구에 대한 진술과 결론을 도출하는 방법에 대한 과학”이라고 설명합니다. 여기서 “인구”는 통계적 의미로 사용되며, 우리가 정확히 알 수는 없지만 무작위 변수의 확률 분포를 사용하여 특성을 설명할 수 있는 무한한 그룹을 의미합니다. 우리는 ?sec-its-just-a-linear-model에서 확률 분포에 대해 더 자세히 논의합니다. Fisher ([1925년] 1928, p. 41)는 더 나아가 다음과 같이 말합니다:\n\n하나 이상의 특성에 대해 빈도 분포로 분포된 무한한 인구의 아이디어는 모든 통계 작업의 기본입니다. 제한된 경험에서\\(\\dots\\) 우리는 표본이 추출된 무한한 가상 인구에 대한 아이디어를 얻을 수 있으며, 따라서 우리의 결론이 적용될 미래 표본의 가능한 성격에 대한 아이디어를 얻을 수 있습니다.\n\n이것을 다른 말로 표현하면, 통계는 일부 데이터를 얻고 모든 데이터를 가질 수 없음에도 불구하고 그것을 기반으로 합리적인 것을 말하려고 시도하는 것을 포함합니다.\n세 가지 중요한 용어는 다음과 같습니다.\n\n“목표 모집단”: 우리가 말하고 싶은 모든 항목의 모음입니다.\n“표본 추출 프레임”: 우리가 데이터를 얻을 수 있는 목표 모집단의 모든 항목 목록입니다.\n“표본”: 우리가 데이터를 얻는 표본 추출 프레임의 항목입니다.\n\n목표 모집단은 크기가 \\(N\\)인 레이블이 지정된 항목의 유한 집합입니다. 예를 들어, 이론적으로 우리는 세상의 모든 책에 “책 1”, “책 2”, “책 3”, \\(\\dots\\), “책 \\(N\\)”과 같은 레이블을 추가할 수 있습니다. 여기서 인구라는 용어의 사용과 일상적인 사용법 사이에는 차이가 있습니다. 예를 들어, 인구 조사 데이터로 작업하는 사람들은 국가의 전체 인구를 가지고 있기 때문에 표본 추출에 대해 걱정할 필요가 없다고 말하는 것을 때때로 듣습니다. 이것은 용어의 혼동이며, 그들이 가지고 있는 것은 국가 인구의 인구 조사에 의해 수집된 표본입니다. 인구 조사의 목표는 모든 단위를 얻는 것이지만(그리고 이것이 달성되면 표본 추출 오차는 덜 문제가 될 것입니다), 여전히 다른 많은 문제가 있을 것입니다. 인구 조사가 완벽하게 수행되고 목표 모집단의 모든 단위에 대한 데이터를 얻더라도, 예를 들어 측정 오차로 인해 여전히 문제가 있으며, 특정 시점의 표본입니다. (totalsurveyerror는?) 총 설문 조사 오차의 진화에 대한 논의를 제공합니다.\n측정할 대상을 정의하는 것이 얼마나 어려운지 본 것처럼, 목표 모집단을 정의하는 것도 어려울 수 있습니다. 예를 들어, 대학생의 소비 습관에 대해 알아보라는 요청을 받았다고 가정해 봅시다. 그 목표 모집단을 어떻게 정의할 수 있을까요? 학생이지만 정규직으로 일하는 사람은 모집단에 속할까요? 다른 책임을 가질 수 있는 성인 학생은 어떻습니까? 우리가 관심이 있을 수 있는 일부 측면은 항상 일반적으로 실현되는 정도까지 공식적으로 정의됩니다. 예를 들어, 지역이 도시 또는 농촌으로 분류되는지 여부는 종종 국가의 통계 기관에 의해 공식적으로 정의됩니다. 그러나 다른 측면은 덜 명확합니다. Gelman, Hill, 와/과 Vehtari (2020, p. 24)는 누군가를 “흡연자”로 분류하는 방법의 어려움에 대해 논의합니다. 15세가 평생 100개의 담배를 피웠다면, 우리는 그들을 담배를 피우지 않은 사람과 다르게 대해야 합니다. 그러나 90세가 평생 100개의 담배를 피웠다면, 그들은 담배를 피우지 않은 90세와 다를 가능성이 있을까요? 어떤 나이와 담배 수에서 이러한 대답이 바뀔까요?\n지금까지 쓰여진 모든 책의 제목에 대해 이야기하고 싶다고 생각해 보십시오. 우리의 목표 모집단은 지금까지 쓰여진 모든 책입니다. 그러나 19세기에 쓰여졌지만 저자가 책상에 잠그고 아무에게도 말하지 않은 책의 제목에 대한 정보를 얻을 수 있다고 상상하기는 거의 불가능합니다. 한 가지 표본 추출 프레임은 의회 도서관 온라인 카탈로그의 모든 책일 수 있고, 다른 하나는 Google이 디지털화한 2,500만 권의 책일 수 있습니다 (Somers 2017). 우리의 표본은 나중에 장에서 사용할 구텐베르크 프로젝트를 통해 사용할 수 있는 수만 권의 책일 수 있습니다.\n또 다른 예를 고려해 보면, 독일에 거주하는 모든 브라질인의 태도에 대해 이야기하고 싶다고 가정해 봅시다. 목표 모집단은 독일에 거주하는 모든 브라질인입니다. 한 가지 가능한 정보 출처는 페이스북일 것이므로, 이 경우 표본 추출 프레임은 페이스북을 사용하는 독일에 거주하는 모든 브라질인일 수 있습니다. 그리고 우리의 표본은 우리가 데이터를 수집할 수 있는 페이스북을 사용하는 독일에 거주하는 모든 브라질인일 수 있습니다. 독일에 거주하는 모든 브라질인이 페이스북을 사용하지는 않기 때문에 목표 모집단과 표본 추출 프레임은 다를 것입니다. 그리고 우리가 페이스북을 사용하고 독일에 거주하는 모든 브라질인에 대한 데이터를 수집할 수 없을 가능성이 높기 때문에 표본 추출 프레임은 표본과 다를 것입니다.\n\n6.4.1 더블린과 리딩에서의 표본 추출\n더 명확하게 하기 위해, 우리는 두 가지 예를 고려합니다: 1798년 아일랜드 더블린의 주민 수 조사 (Whitelaw 1805)와 1912년 영국 리딩의 노동 계급 가구 수 조사 (Bowley 1913).\n\n6.4.1.1 1798년 더블린 조사\n1798년 제임스 휘틀로 목사는 아일랜드 더블린의 인구를 세기 위해 조사를 실시했습니다. (whitelaw1805는?) 당시 인구 추정치가 상당히 다양했다고 설명합니다. 예를 들어, 당시 런던의 추정 인구는 128,570명에서 300,000명에 달했습니다. 휘틀로는 더블린 시장이 각 가구의 책임자에게 해당 가구의 주민 목록을 문에 부착하도록 강요할 수 있고, 그러면 휘틀로는 단순히 이것을 사용할 수 있을 것이라고 예상했습니다.\n대신, 그는 목록이 “자주 읽을 수 없고, 일반적으로 실제 수보다 3분의 1, 심지어 절반이 부족하다”는 것을 발견했습니다. 그래서 대신 그는 조수를 모집했고, 그들은 집집마다 다니며 직접 수를 세었습니다. 그 결과 추정치는 특히 유익합니다(그림 6.5). 1798년 더블린의 총 인구는 182,370명으로 추정되었습니다.\n\n\n\n\n\n\n그림 6.5: 1798년 휘틀로가 발견한 결과 발췌\n\n\n\n주목할 만한 한 가지 측면은 휘틀로가 계급에 대한 정보를 포함한다는 것입니다. 그것이 어떻게 결정되었는지 알기는 어렵지만, 데이터 수집에 큰 역할을 했습니다. 휘틀로는 “중상류층의 집에는 항상 [목록을 만드는] 작업에 능숙한 개인이 있었다”고 설명합니다. 그러나 “이 도시 인구의 대부분을 차지하는 하층 계급 사이에서는 상황이 매우 달랐다”고 합니다. 휘틀로가 상류층과 하류층의 집에 들어가지 않고는 그것을 어떻게 알 수 있었는지 알기 어렵습니다. 그러나 휘틀로가 상류층의 집에 들어가서 그 수를 세는 것을 상상하기도 어렵습니다. 다른 접근 방식이 필요했을 수 있습니다.\n휘틀로는 자신의 선택을 안내하기 위해 통계적 기계를 거의 사용하지 않고 더블린 주민의 전체 표본을 구성하려고 시도했습니다. 이제 우리는 1912년에 실시된 두 번째 예를 고려할 것이며, 그들은 오늘날에도 여전히 사용하는 표본 추출 접근 방식을 사용하기 시작할 수 있었습니다.\n\n\n6.4.1.2 1912년 리딩의 노동 계급 가구 조사\nWhitelaw (1805) 이후 100년이 조금 지난 후, (bowley1913working은?) 영국 리딩의 노동 계급 가구 수를 세는 데 관심이 있었습니다. 볼리는 다음 절차를 사용하여 표본을 선택했습니다 (Bowley 1913, p. 672):\n\n지역 디렉토리 전체에서 거리의 알파벳 순서로 10개 건물 중 하나를 표시하여 총 1,950개를 만들었습니다. 그 중 약 300개는 상점, 공장, 기관 및 비주거용 건물로 표시되었고, 약 300개는 주요 거주자 중에서 색인되어 그렇게 표시되었습니다. 나머지 1,350개는 노동 계급 주택이었습니다\\(\\dots\\) [나는] 중간 10분의 1에 대한 불완전한 정보를 거부하고 20개 주택 중 하나만 취하기로 결정했습니다. 방문객들은 정보를 얻기가 아무리 어렵거나 주택 유형이 어떻든 간에 표시된 주택을 다른 주택으로 대체하지 말라고 지시받았습니다.\n\n(bowley1913working은?) 622개의 노동 계급 가구에 대한 정보를 얻을 수 있었다고 말합니다. 예를 들어, 그들은 매주 지불되는 임대료를 추정할 수 있었습니다(그림 6.6).\n\n\n\n\n\n\n그림 6.6: 영국 리딩의 노동 계급이 지불한 임대료에 대해 볼리가 발견한 결과 발췌\n\n\n\n그런 다음, 인구 조사에서 리딩에 약 18,000가구가 있다고 판단한 후, (bowley1913working은?) 표본에 21의 승수를 적용하여 리딩 전체에 대한 추정치를 산출했습니다. 결과 추정치가 합리적임을 보장하는 핵심 측면은 표본 추출이 무작위 방식으로 이루어졌다는 것입니다. 이것이 (bowley1913working이?) 방문객들이 선택된 실제 집에 가고 다른 집으로 대체하지 않도록 그렇게 주장한 이유입니다.\n\n\n\n6.4.2 확률적 표본 추출\n목표 모집단과 표본 추출 프레임을 확인한 후, 우리는 확률적 표본 추출과 비확률적 표본 추출을 구별해야 합니다.\n\n“확률적 표본 추출”: 표본 추출 프레임의 모든 단위는 표본 추출될 확률이 알려져 있으며, 특정 표본은 이러한 확률에 따라 무작위로 얻어집니다. 표본 추출될 확률은 각 단위에 대해 반드시 동일할 필요는 없습니다.\n“비확률적 표본 추출”: 표본 추출 프레임의 단위는 편의, 할당, 판단 또는 기타 비무작위 프로세스를 기반으로 표본 추출됩니다.\n\n종종 확률적 표본 추출과 비확률적 표본 추출의 차이는 정도의 차이입니다. 예를 들어, 우리는 일반적으로 강제로 데이터를 얻을 수 없으므로 응답자 측에서는 거의 항상 자원 봉사의 측면이 있습니다. 많은 국가에서 인구 조사 양식을 작성하지 않으면 처벌이 있는 경우에도, 정부조차도 사람들이 그것을 완전히 또는 진실하게 작성하도록 강요하기는 어렵습니다. 유명하게도 2001년 뉴질랜드 인구 조사에서는 인구의 1% 이상이 자신의 종교를 “제다이”라고 기재했습니다 (Taylor 2015). 확률적 표본 추출에서 가장 중요한 측면은 불확실성의 역할입니다. 이를 통해 우리는 알려진 양의 오차로 표본을 기반으로 인구에 대한 주장을 할 수 있습니다. 그 대가로 확률적 표본 추출은 종종 비싸고 어렵습니다.\n우리는 네 가지 유형의 확률적 표본 추출을 고려할 것입니다.\n\n단순 무작위;\n체계적;\n층화; 그리고\n군집.\n\n우리의 논의에 더 구체성을 더하기 위해, Lohr ([1999년] 2022, p. 27)가 사용하는 방식과 마찬가지로, 1부터 100까지의 숫자를 목표 모집단으로 간주하는 것이 도움이 될 수 있습니다. 단순 무작위 표본 추출에서는 모든 단위가 포함될 확률이 동일합니다. 이 경우 20%라고 가정해 봅시다. 이는 표본에 약 20개의 단위가 있을 것으로 예상하거나, 목표 모집단과 비교하여 5개 중 1개 정도가 될 것으로 예상한다는 의미입니다.\n\nset.seed(853)\n\nillustrative_sampling &lt;- tibble(\n  unit = 1:100,\n  simple_random_sampling =\n    sample(x = c(\"In\", \"Out\"), \n           size = 100, \n           replace = TRUE, \n           prob = c(0.2, 0.8))\n  )\n\nillustrative_sampling |&gt;\n  count(simple_random_sampling)\n\n# A tibble: 2 × 2\n  simple_random_sampling     n\n  &lt;chr&gt;                  &lt;int&gt;\n1 In                        14\n2 Out                       86\n\n\n체계적 표본 추출에서는 (bowley1913working에서?) 사용된 것처럼, 어떤 값을 선택하고 20% 표본을 얻기 위해 5번째 단위마다 표본을 추출합니다. 시작하려면 1부터 5까지의 단위에서 시작점을 무작위로 선택합니다. 예를 들어 3이라고 가정해 봅시다. 그러면 5번째 단위마다 표본을 추출한다는 것은 3번째, 8번째, 13번째 등을 살펴보는 것을 의미합니다.\n\nset.seed(853)\n\nstarting_point &lt;- sample(x = c(1:5), size = 1)\n\nillustrative_sampling &lt;-\n  illustrative_sampling |&gt;\n  mutate(\n    systematic_sampling =\n      if_else(unit %in% seq.int(from = starting_point, to = 100, by = 5), \n              \"In\", \n              \"Out\"\n              )\n    )\n\nillustrative_sampling |&gt;\n  count(systematic_sampling)\n\n# A tibble: 2 × 2\n  systematic_sampling     n\n  &lt;chr&gt;               &lt;int&gt;\n1 In                     20\n2 Out                    80\n\n\n우리가 인구를 고려할 때, 일반적으로 어떤 그룹화가 있을 것입니다. 이것은 국가가 주, 도, 군 또는 통계 구역을 갖는 것, 대학이 학부와 학과를 갖는 것, 그리고 인간이 연령 그룹을 갖는 것처럼 간단할 수 있습니다. 층화 구조는 인구를 상호 배타적이고 집합적으로 완전한 하위 인구인 “층”으로 나눌 수 있는 구조입니다.\n우리는 표본 추출의 효율성이나 설문 조사의 균형을 돕기 위해 층화를 사용합니다. 예를 들어, 미국 인구는 약 3억 3,500만 명이며, 캘리포니아에는 약 4,000만 명, 와이오밍에는 약 50만 명이 있습니다. 10,000명의 응답을 받은 설문 조사조차도 와이오밍에서 15명의 응답만 기대할 수 있으며, 이는 와이오밍에 대한 추론을 어렵게 만들 수 있습니다. 우리는 층화를 사용하여 각 주에서 200명의 응답이 있도록 보장할 수 있습니다. 각 주 내에서 무작위 표본 추출을 사용하여 데이터를 수집할 사람을 선택할 수 있습니다.\n우리의 경우, 우리는 층이 10의 배수, 즉 1에서 10이 한 층, 11에서 20이 다른 층 등이라고 생각하여 그림을 층화할 것입니다. 우리는 이러한 층 내에서 단순 무작위 표본 추출을 사용하여 각각에서 두 단위를 선택할 것입니다.\n\nset.seed(853)\n\npicked_in_strata &lt;-\n  illustrative_sampling |&gt;\n  mutate(strata = (unit - 1) %/% 10) |&gt;\n  slice_sample(n = 2, by = strata) |&gt;\n  pull(unit)\n\nillustrative_sampling &lt;-\n  illustrative_sampling |&gt;\n  mutate(stratified_sampling = \n           if_else(unit %in% picked_in_strata, \"In\", \"Out\"))\n\nillustrative_sampling |&gt;\n  count(stratified_sampling)\n\n# A tibble: 2 × 2\n  stratified_sampling     n\n  &lt;chr&gt;               &lt;int&gt;\n1 In                     20\n2 Out                    80\n\n\n그리고 마지막으로, 데이터셋에 존재할 수 있는 일부 군집을 활용할 수도 있습니다. 층과 마찬가지로 군집은 집합적으로 완전하고 상호 배타적입니다. 이전의 주, 부서 및 연령 그룹의 예는 군집으로서 유효합니다. 그러나 이러한 그룹에 대한 우리의 의도는 다릅니다. 구체적으로, 군집 표본 추출에서는 모든 군집에서 데이터를 수집할 의도가 없는 반면, 층화 표본 추출에서는 그렇게 합니다. 층화 표본 추출에서는 모든 층을 살펴보고 각 층 내에서 단순 무작위 표본 추출을 수행하여 표본을 선택합니다. 군집 표본 추출에서는 관심 있는 군집을 선택합니다. 그런 다음 선택된 군집의 모든 단위를 표본 추출하거나, 선택된 군집 내에서 단순 무작위 표본 추출을 사용하여 단위를 선택할 수 있습니다. 그렇긴 하지만, 이 차이는 실제로, 특히 사후에 덜 명확해질 수 있습니다. (rose2006comparison은?) 2005년 수단 북다르푸르의 사망률 데이터를 수집합니다. 그들은 군집 및 체계적 표본 추출 모두 유사한 결과를 제공한다는 것을 발견했으며, 체계적 표본 추출은 설문 조사 팀의 훈련이 덜 필요하다고 지적합니다. 일반적으로 군집 표본 추출은 지리적으로 가까운 위치에 집중하기 때문에 더 저렴할 수 있습니다.\n우리의 경우, 우리는 다시 10의 배수를 기준으로 그림을 군집화할 것입니다. 우리는 단순 무작위 표본 추출을 사용하여 전체 군집을 사용할 두 개의 군집을 선택할 것입니다.\n\nset.seed(853)\n\npicked_clusters &lt;-\n  sample(x = c(0:9), size = 2)\n\nillustrative_sampling &lt;-\n  illustrative_sampling |&gt;\n  mutate(\n    cluster = (unit - 1) %/% 10,\n    cluster_sampling = if_else(cluster %in% picked_clusters, \"In\", \"Out\")\n    ) |&gt;\n  select(-cluster)\n\nillustrative_sampling |&gt;\n  count(cluster_sampling)\n\n# A tibble: 2 × 2\n  cluster_sampling     n\n  &lt;chr&gt;            &lt;int&gt;\n1 In                  20\n2 Out                 80\n\n\n이 시점에서 우리는 우리의 접근 방식 간의 차이점을 설명할 수 있습니다(그림 6.7). 우리는 또한 세계의 다른 지역에서 다른 방법을 사용하여 무작위로 표본을 추출하는 척하여 시각적으로 고려할 수도 있습니다(그림 6.8). \n\nnew_labels &lt;- c(\nsimple_random_sampling = \"Simple random sampling\",\nsystematic_sampling = \"Systematic sampling\",\nstratified_sampling = \"Stratified sampling\",\ncluster_sampling = \"Cluster sampling\"\n)\n\nillustrative_sampling_long &lt;-\n  illustrative_sampling |&gt;\n  pivot_longer(\n    cols = names(new_labels), names_to = \"sampling_method\",\n    values_to = \"in_sample\"\n    ) |&gt;\n  mutate(sampling_method = \n           factor(sampling_method,levels = names(new_labels)))\n\nillustrative_sampling_long |&gt;\n  filter(in_sample == \"In\") |&gt;\n  ggplot(aes(x = unit, y = in_sample)) +\n  geom_point() +\n  facet_wrap(vars(sampling_method), dir = \"v\", ncol = 1, \n             labeller = labeller(sampling_method = new_labels)\n             ) +\n  theme_minimal() +\n  labs(x = \"Unit\", y = \"Is included in sample\") +\n  theme(axis.text.y = element_blank())\n\n\n\n\n\n\n\n그림 6.7: 1부터 100까지의 숫자에 대한 단순 무작위 표본 추출, 체계적 표본 추출, 층화 표본 추출 및 군집 표본 추출의 예시\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) 세계\n\n\n\n\n\n\n\n\n\n\n\n(b) 체계적 표본 추출\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) 층화 표본 추출\n\n\n\n\n\n\n\n\n\n\n\n(d) 군집 표본 추출\n\n\n\n\n\n\n\n그림 6.8: 세계 여러 지역에 걸친 단순 무작위 표본 추출, 체계적 표본 추출, 층화 표본 추출 및 군집 표본 추출의 예시\n\n\n\n?fig-samplingexamples와 ?fig-samplingexamplesworld는 다른 방법 간의 절충안과 그것들이 다르게 적절할 수 있는 방식을 보여줍니다. 예를 들어, 체계적 표본 추출은 ?fig-samplingexamplesworld에서 세계의 유용한 그림을 제공하지만, 만약 우리가 육지에만 관심이 있다면, 여전히 유익하지 않은 많은 표본이 남을 것입니다. 층화 표본 추출과 군집 표본 추출은 관심 있는 측면에 집중할 수 있게 해주지만, 더 전체적인 그림을 희생해야 합니다.\n이러한 접근 방식 간의 차이점을 이해하는 좋은 방법은 실제로 고려하는 것입니다. (thatrandyaupersonagain은?) 여러 가지 예를 제공합니다. 특히 랩터 수를 세는 맥락에서 (raptors는?) 단순 무작위 표본 추출, 층화 표본 추출, 체계적 표본 추출 및 군집 표본 추출뿐만 아니라 추가적인 고려 사항을 비교합니다.\n\n6.4.2.1 확률 표본에 대한 추론\n표본을 확립한 후, 우리는 일반적으로 그것을 사용하여 인구에 대한 주장을 합니다. Neyman (1934, p. 561)는 더 나아가 “\\(\\dots\\)대표 방법의 문제는 탁월하게 통계적 추정의 문제입니다. 우리는 특정 인구, 예를 들어 \\(\\pi\\)의 특성에 관심이 있으며, 이를 자세히 연구하는 것이 불가능하거나 적어도 매우 어렵기 때문에 표본을 기반으로 이러한 특성을 추정하려고 합니다.”라고 말합니다.\n특히, 우리는 일반적으로 모집단 평균과 분산을 추정하는 데 관심이 있을 것입니다. 우리는 ?sec-on-writing에서 추정량, 추정량 및 추정치의 개념을 소개했습니다. 우리는 모집단 평균과 분산을 추정하기 위해 추정량을 구성할 수 있습니다. 예를 들어, 크기가 \\(n\\)인 표본으로 단순 무작위 표본 추출을 사용하고 있다면, 표본 평균과 분산(?sec-its-just-a-linear-model에서 다시 다룸)을 구성하여 모집단 평균과 분산의 추정치를 생성할 수 있습니다.\n\\[\n\\begin{aligned}\n\\hat{\\mu} &= \\frac{1}{n} \\times \\sum_{i = 1}^{n}x_i\\\\\n\\hat{\\sigma}^2 &= \\frac{1}{n-1} \\times \\sum_{i = 1}^{n}\\left(x_i - \\hat{\\mu}\\right)^2\n\\end{aligned}\n\\]\n지금까지 사용한 접근 방식을 사용하여 다양한 유형의 설문 조사를 시뮬레이션할 수 있습니다. DeclareDesign (Blair 기타 2019) 및 survey (Lumley 2020)를 포함하여 도움이 될 수 있는 패키지도 있습니다.\n추정치를 확장하는 것은 표본의 개수를 사용하여 목표 모집단의 총 개수를 암시하는 데 관심이 있을 때 사용할 수 있습니다. 우리는 (bowley1913working에서?) 이것을 보았는데, 표본의 가구 수와 인구 조사에서 알려진 가구 수의 비율이 21이었고 이 정보는 표본을 확장하는 데 사용되었습니다.\n예를 들어, 1부터 100까지의 숫자 합에 관심이 있었다고 가정해 봅시다. 이러한 숫자에서 표본을 추출하는 다른 방법을 설명하는 예로 돌아가서, 표본 크기가 20이므로 5배로 확장해야 한다는 것을 알고 있습니다(표 6.2).\n\n\n\n\n표 6.2: 각 표본의 숫자 합계 및 암시된 모집단 합계\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Sampling method\n                Sum of sample\n                Implied population sum\n              \n        \n        \n        \n                \n                  Systematic sampling\n                  970\n                  4,850\n                \n                \n                  Stratified sampling\n                  979\n                  4,895\n                \n                \n                  Cluster sampling\n                  910\n                  4,550\n                \n                \n                  Simple random sampling\n                  840\n                  4,200\n                \n        \n      \n    \n\n\n\n\n\n\n인구의 실제 합계는 5,050입니다.1 세부 사항은 이 표본에 고유하지만, 확장을 기반으로 한 인구 합계의 추정치는 드러납니다. 가장 가까운 것은 층화 표본이며, 체계적 표본 추출이 그 뒤를 잇습니다. 군집 표본 추출은 10% 이상 차이가 나고, 단순 무작위 표본 추출은 조금 더 멀리 떨어져 있습니다. 실제 합계에 가까워지려면 표본 추출 방법이 가능한 한 많은 높은 값을 얻는 것이 중요합니다. 따라서 더 큰 숫자에서 결과를 얻도록 보장한 층화 및 체계적 표본 추출이 특히 잘 수행되었습니다. 군집 및 단순 무작위 표본 추출의 성능은 선택된 특정 군집 및 단위에 따라 달라집니다. 이 경우 층화 및 체계적 표본 추출은 인구 합계의 추정치가 실제 인구 합계에서 너무 멀리 떨어져 있지 않도록 보장했습니다. 여기서 우리는 GDP 및 합산되는 기타 구성과 같은 측정의 구성 및 평가에 대한 의미와 크기에 따른 다른 층의 총계에 미치는 영향을 생각할 수 있습니다.\n이 접근 방식은 오랜 역사를 가지고 있습니다. 예를 들어, Stigler (1986, p. 163)는 1826년까지 19세기 천문학자인 아돌프 케틀레가 인구 조사를 계획하고 있던 통계국에 관여하게 되었다고 설명합니다. 케틀레는 출생과 사망은 잘 알려져 있지만 이주는 그렇지 않다고 주장했습니다. 그는 특정 지역의 수를 기반으로 한 접근 방식을 제안했으며, 이를 전국으로 확장할 수 있었습니다. 계획에 대한 비판은 적절한 지역을 선택하는 어려움에 초점을 맞추었으며, 이는 군집 표본 추출의 예에서도 보았습니다. 비판은 합리적이었고, 오늘날에도 약 200년이 지난 지금도 우리가 염두에 두어야 할 것입니다 (Stigler 1986):\n\n그[케틀레]는 자신이 측정하고자 하는 양에 영향을 미칠 수 있는 무한한 수의 요인을 예리하게 인식하고 있었고, 어떤 것이 실제로 중요한지, 그리고 그 효과가 어떻게 느껴질 수 있는지 알려줄 정보가 부족했습니다. 그는\\(\\dots\\) 동질적이라고 믿을 이유가 없는 데이터를 함께 묶는 것을 꺼렸습니다\\(\\dots\\) 어떤 것이 진정으로 중요한지, 그리고 그 효과가 어떻게 느껴질 수 있는지 알지 못한 채 무수한 잠재적으로 중요한 요인을 인식하는 것은 종종 최악을 두려워하는 것입니다\\(\\dots\\) 그[케틀레]는 넓은 지역을 동질적으로 취급할 수 없었고, [그래서] 단일 비율이 넓은 지역에 적용된다고 생각할 수 없었습니다.\n\n모집단 총계를 알 때 이 확장을 할 수 있지만, 그것을 모르거나 그 접근 방식의 정밀도에 대한 우려가 있는 경우 비율 추정량을 사용할 수 있습니다.\n비율 추정량은 1802년 피에르-시몽 라플라스가 프랑스의 총 인구를 추정하는 데 사용되었으며, 이는 전국적으로 알려진 등록된 출생 수와 특정 코뮌에서만 알려진 주민 수의 비율을 기반으로 했습니다. 그는 세 코뮌에 대해 이 비율을 계산한 다음, 전국적으로 출생 수를 알고 있는 것을 기반으로 프랑스 인구의 추정치를 생성하기 위해 확장했습니다 (Lohr [1999년] 2022).\n\n\n\n\n\n\n\n\n일부 모집단 매개변수의 비율 추정량은 두 평균의 비율입니다. 예를 들어, 30일 동안 유아가 잠을 잔 총 시간을 알고 있고, 같은 기간 동안 부모가 잠을 잔 시간을 알고 싶다고 상상해 보십시오. 30일 동안 유아가 밤에 잠을 자는 시간, \\(x\\), 그리고 부모가 밤에 잠을 자는 시간, \\(y\\)에 대한 정보가 있을 수 있습니다.\n\nset.seed(853)\n\nsleep &lt;-\n  tibble(\n    toddler_sleep = sample(x = c(2:14), size = 30, replace = TRUE),\n    difference = sample(x = c(0:3), size = 30, replace = TRUE),\n    parent_sleep = toddler_sleep - difference\n  ) |&gt;\n  select(toddler_sleep, parent_sleep, difference)\n\nsleep\n\n# A tibble: 30 × 3\n   toddler_sleep parent_sleep difference\n           &lt;int&gt;        &lt;int&gt;      &lt;int&gt;\n 1            10            9          1\n 2            11           11          0\n 3            14           12          2\n 4             2            0          2\n 5             6            5          1\n 6            14           12          2\n 7             3            3          0\n 8             5            3          2\n 9             4            1          3\n10             4            3          1\n# ℹ 20 more rows\n\n\n각각의 평균은 다릅니다(표 6.3).\n\n\n\n\n표 6.3: 평균 유아 수면과 평균 부모 수면 비교\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Toddler sleep average\n                Parent sleep average\n              \n        \n        \n        \n                \n                  6.17\n                  4.9\n                \n        \n      \n    \n\n\n\n\n\n\n부모가 유아에 비해 잠을 자는 비율은 다음과 같습니다.\n\\[\\hat{B} = \\frac{\\bar{y}}{\\bar{x}} = \\frac{4.9}{6.17} \\approx 0.8.\\]\n유아가 30일 동안 185시간을 잤다는 점을 감안할 때, 부모가 잠을 잔 시간에 대한 우리의 추정치는 \\(185 \\times 0.8 = 148\\)입니다. 이것은 합계가 147이므로 거의 정확한 것으로 나타났습니다. 이 예에서는 데이터를 합산할 수 있었기 때문에 추정치가 필요하지 않았지만, 다른 부모 집합이 유아가 잠을 잔 시간만 기록하고 얼마나 오래 잤는지는 기록하지 않았다면, 이것을 사용하여 그들이 얼마나 잤는지 추정할 수 있습니다.\n일반적으로 사용되는 비율 추정량의 한 가지 변형은 포획 및 재포획이며, 이는 데이터 수집의 최고 보석 중 하나입니다. 모든 동물에 대한 데이터를 수집할 수 없다는 것을 알고 있는 생태학에서 일반적으로 사용됩니다. 대신, 표본을 포획하고, 표시하고, 방출합니다. 연구자들은 일정 시간이 지난 후 다른 표본을 포획하기 위해 돌아옵니다. 처음에 포획된 동물이 인구에 다시 통합될 충분한 시간이 지났지만, 출생, 사망 및 이주에 대한 극복할 수 없는 우려가 있을 만큼 많은 시간이 지나지 않았다고 가정하면, 이러한 값을 사용하여 인구 크기를 추정할 수 있습니다. 핵심은 이 두 번째 표본에서 재포획된 비율입니다. 이 비율은 전체 인구의 크기를 추정하는 데 사용될 수 있습니다. 흥미롭게도 1990년대에는 방법론에 대한 우려로 인해 1990년 미국 인구 조사를 조정하기 위해 포획-재포획 모델을 사용할지 여부에 대한 상당한 논쟁이 있었습니다. (Breiman1994와?) (newwoyktimesgleickcensus의?) 조합은 당시의 우려, 더 일반적으로 인구 조사의 우려, 그리고 포획 및 재포획 방법에 대한 유용한 배경에 대한 개요를 제공합니다. 더 최근에는 ?sec-gather-data에서 고려하는 웹 스크래핑과 포획 및 재포획을 결합하여 설문 조사 프레임을 구성하는 것을 보았습니다 (Hyman, Sartore, 와/과 Young 2021).\n\n\n\n6.4.3 비확률 표본\n스펙트럼이라는 것을 인정하면서도, 많은 통계는 확률적 표본 추출을 기반으로 개발되었습니다. 그러나 상당한 양의 현대 표본 추출은 비확률적 표본 추출을 사용하여 수행됩니다. 한 가지 접근 방식은 소셜 미디어 및 기타 광고를 사용하여 응답자 패널을 모집하는 것이며, 보상을 받을 수도 있습니다. 이 패널은 필요에 따라 다양한 설문 조사를 받는 그룹입니다. 그러나 잠시 이 의미에 대해 생각해 보십시오. 예를 들어, 어떤 유형의 사람들이 그러한 광고에 응답할 가능성이 있습니까? 세계에서 가장 부유한 사람이 응답할 가능성이 있습니까? 특히 젊거나 특히 나이가 많은 사람들이 응답할 가능성이 있습니까? 어떤 경우에는 인구 조사를 할 수 있습니다. 정부는 일반적으로 5~10년마다 한 번씩 실시합니다. 그러나 일반적으로 정부가 그렇게 하는 데에는 이유가 있습니다. 비싸고, 시간이 많이 걸리며, 놀랍게도, 그들이 얼마나 일반적이어야 하는지 때문에 우리가 희망하는 것만큼 정확하지 않을 때가 있습니다.\n비확률 표본은 일반적으로 확률 표본보다 저렴하고 빠르게 얻을 수 있기 때문에 중요한 역할을 합니다. (beaumont2020probability는?) 확률 표본에 대한 응답률 감소 및 실시간 통계에 대한 수요 증가를 포함하여 비확률 표본에 유리한 다양한 요인을 설명합니다. 또한, 우리가 논의했듯이, 확률 표본과 비확률 표본의 차이는 때때로 이분법보다는 정도의 차이입니다. 비확률 표본은 절충안에 대해 명확하고 투명성을 보장하는 한 일부 작업에 대해 합법적이고 적절합니다 (Baker 기타 2013). 낮은 응답률은 진정한 확률 표본이 드물다는 것을 의미하므로 비확률 표본 추출의 의미를 다루는 것이 중요합니다.\n편의 표본 추출은 접근하기 쉬운 표본에서 데이터를 수집하는 것을 포함합니다. 예를 들어, 광범위한 배포 전에 설문 조사를 테스트하는 방법으로 친구와 가족에게 설문 조사를 작성하도록 요청하는 경우가 많습니다. 그러한 표본을 분석한다면, 우리는 편의 표본 추출을 사용할 가능성이 높습니다.\n편의 표본 추출의 주요 관심사는 더 넓은 인구에 대해 말할 수 있는지 여부입니다. 또한 까다로운 윤리적 고려 사항이 있으며, 일반적으로 익명성이 부족하여 결과를 더욱 편향시킬 수 있습니다. 반면에, 상황을 저렴하게 빠르게 파악하는 데 유용할 수 있습니다.\n할당 표본 추출은 층이 있지만, 해당 층 내에서 단위를 선택하기 위해 무작위 표본 추출을 사용하지 않을 때 발생합니다. 예를 들어, 우리가 다시 주를 기준으로 미국을 층화했지만, 와이오밍의 모든 사람이 해당 층에 대해 선택될 기회를 보장하는 대신, 잭슨 홀에서 사람들을 선택했다면 말입니다. 이 접근 방식에는 특히 속도와 비용 측면에서 몇 가지 장점이 있지만, 결과 표본은 다양한 방식으로 편향될 수 있습니다. 그렇다고 해서 장점이 없는 것은 아닙니다. 예를 들어, 캐나다 은행은 상품 및 서비스 지불 방법에 초점을 맞춘 비확률 설문 조사를 실시합니다. 그들은 할당 표본 추출과 다양한 조정 방법을 사용합니다. 이러한 비확률 표본 추출의 사용은 인구의 도달하기 어려운 측면에 의도적으로 집중할 수 있게 해줍니다 (H. Chen, Felt, 와/과 Henry 2018).\n속담처럼, 같은 깃털의 새는 함께 모입니다. 그리고 우리는 표본 추출에서 그것을 활용할 수 있습니다. (handcock2011comment가?) 이전에 다양한 용도를 설명하고 다학문적 작업에서 귀속을 정의하기가 악명 높게 어렵지만, 눈덩이 표본 추출은 (goodman1961snowball에?) 의해 잘 정의됩니다. (goodman1961snowball에?) 따르면, 눈덩이 표본 추출을 수행하기 위해, 우리는 먼저 표본 추출 프레임에서 무작위 표본을 추출합니다. 이들 각각에게 표본 모집단에 있지만 초기 추출에는 없는 다른 \\(k\\)명을 지명하도록 요청하고, 이들이 “1단계”를 형성합니다. 1단계의 각 개인은 유사하게 표본 모집단에 있지만 무작위 추출이나 1단계에는 없는 다른 \\(k\\)명을 지명하도록 요청받고, 이들이 “2단계”를 형성합니다. 우리는 단계 수, \\(s\\)와 \\(k\\)를 미리 지정해야 합니다.\n응답자 주도 표본 추출은 (heckathorn1997respondent에?) 의해 숨겨진 인구에 초점을 맞추기 위해 개발되었으며, 여기서는 다음과 같습니다.\n\n표본 추출 프레임이 없습니다. 그리고\n표본 모집단에 있는 것으로 알려지면 부정적인 영향을 미칠 수 있습니다.\n\n예를 들어, 게이 인구나 낙태를 한 사람들과 같이 표본을 추출하기 어려운 여러 국가를 상상할 수 있습니다. 응답자 주도 표본 추출은 눈덩이 표본 추출과 두 가지 면에서 다릅니다.\n\n눈덩이 표본 추출의 경우와 마찬가지로 자신의 응답에 대한 보상 외에도, 응답자 주도 표본 추출은 일반적으로 다른 사람을 모집하는 데 대한 보상도 포함합니다.\n응답자는 조사관에게 다른 사람에 대한 정보를 제공하도록 요청받는 것이 아니라, 연구에 그들을 모집합니다. 표본 선택은 표본 추출 프레임에서가 아니라, 이미 표본에 있는 사람들의 네트워크에서 발생합니다 (Salganik 와/과 Heckathorn 2004).",
    "crumbs": [
      "Acquisition",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>측정, 인구 조사 및 표본 추출</span>"
    ]
  },
  {
    "objectID": "06-farm_ko.html#연습-문제",
    "href": "06-farm_ko.html#연습-문제",
    "title": "6  측정, 인구 조사 및 표본 추출",
    "section": "6.5 연습 문제",
    "text": "6.5 연습 문제\n\n연습\n\n(계획) 다음 시나리오를 고려하십시오: 1년 동안 매일 두 사람—마크와 로렌—이 그들이 출신인 두 다른 주에서 그날 내린 눈의 양을 기록합니다. 데이터셋이 어떻게 생겼을지 스케치한 다음, 모든 관측치를 보여주기 위해 만들 수 있는 그래프를 스케치하십시오.\n(시뮬레이션) 설명된 시나리오를 더 고려하고 모든 변수가 서로 독립적인 상황을 시뮬레이션하십시오. 그런 다음 시뮬레이션된 데이터를 기반으로 5개의 테스트를 작성하십시오.\n(수집) 강설량에 대한 실제 데이터를 얻고 이 실제 데이터에 대한 시뮬레이션된 테스트를 업데이트하는 스크립트를 추가하십시오.\n(탐색) 실제 데이터를 사용하여 그래프와 표를 만드십시오.\n(전달) 그래프와 표에 첨부할 텍스트를 작성하십시오. 코드를 R 파일과 Quarto 문서로 적절하게 분리하십시오. 고품질 GitHub 리포지토리 링크를 제출하십시오.\n\n\n\n퀴즈\n\n세상의 일부 현상을 데이터셋으로 변환할 때의 과제는 무엇입니까(하나 선택)?\n\n데이터 저장 솔루션의 높은 비용.\n편향되지 않은 데이터의 과잉.\n사용 가능한 데이터 수집 도구의 부족.\n무엇을 측정하고 어떻게 적절하게 측정할지 결정하는 것.\n\n(statsoftheworld를?) 참조하여 GDP와 인구 수가 발명되었는지 발견되었는지 논의하십시오.\n계측학에 따르면, 다음 중 측정을 가장 잘 정의하는 것은 무엇입니까(하나 선택)?\n\n예측 모델을 사용한 미지 변수 추정.\n데이터 분석에서 통계적 유의성 계산.\n객체에 임의로 숫자 할당.\n현상, 신체 또는 물질의 속성에 귀속될 수 있는 양 값을 실험적으로 얻는 과정.\n\n최소 두 단락으로, 자신의 말로 측정 오차를 정의하고 자신의 경험에서 예를 들어보십시오.\n(Gargiulo2022를?) 참조하여 실제 세계에서 측정의 과제를 논의하십시오.\n측정의 타당성은 무엇을 의미합니까(하나 선택)?\n\n측정 결과의 통계적 유의성.\n측정이 측정하려는 개념을 정확하게 반영하는 정도.\n데이터를 수집할 수 있는 속도.\n측정을 복제할 수 있는 정밀도.\n\n(kennedy2020using은?) 윤리를 어떻게 정의합니까(하나 선택)?\n\n개별 설문 조사 응답자의 관점과 존엄성을 존중합니다.\n일반 인구 및 관심 있는 하위 인구에 대한 추정치를 생성합니다.\n유용한 기능을 제공할 때만 더 복잡한 절차를 사용합니다.\n\n다음 중 측정 오차를 가장 잘 설명하는 것은 무엇입니까(하나 선택)?\n\n관찰된 값과 측정되는 것의 실제 값의 차이.\n데이터가 정규 분포가 아닐 때만 발생하는 오류.\n더 나은 기기로 제거할 수 있는 오류 유형.\n분석을 오도하기 위한 데이터의 의도적인 변경.\n\n검열된 데이터란 무엇입니까(하나 선택)?\n\n손상되어 읽을 수 없는 데이터.\n개인 정보 보호 문제로 인해 의도적으로 생략된 데이터.\n관측 값이 부분적으로만 알려진 데이터.\n승인되지 않은 출처에서 수집된 데이터.\n\n절단된 데이터는 검열된 데이터와 어떻게 다릅니까(하나 선택)?\n\n절단된 데이터는 과대평가를 다루고, 검열된 데이터는 과소평가를 다룹니다.\n절단된 데이터에서는 특정 값이 데이터셋에서 생략되는 반면, 검열된 데이터에서는 값이 부분적으로 알려져 있지만 불완전합니다.\n절단된 데이터는 검열된 데이터보다 정확도가 낮습니다.\n\n완전히 무작위로 누락(MCAR)이란 무엇을 의미합니까(하나 선택)?\n\n통계 모델을 사용하여 쉽게 예측할 수 있는 결측 데이터.\n결측 가능성이 관찰되지 않은 데이터와 관련된 결측 데이터.\n결측 가능성이 관찰된 데이터와 관련된 결측 데이터.\n결측이 완전히 무작위이며 관찰되거나 관찰되지 않은 데이터와 관련이 없는 결측 데이터.\n\n인구 조사가 중요한 데이터셋으로 간주되는 이유는 무엇입니까(하나 선택)?\n\n드물게 수행되므로 새로운 정보를 가지고 있습니다.\n인구의 모든 단위에 대한 데이터를 수집하여 분석을 위해 설계된 포괄적인 데이터셋을 제공하는 것을 목표로 합니다.\n독점적인 통찰력을 제공하는 개인 데이터셋입니다.\n경제 계획에 필수적인 농업 데이터에만 집중합니다.\n\n(statcanhistory에서?) 인구 조사 데이터의 품질이 평가되는 이유는 무엇입니까(하나 선택)?\n\n향후 인구 조사의 비용을 줄이기 위해.\n데이터 보급을 제한하기 위해.\n데이터 개인 정보 보호를 개선하기 위해.\n인구 조사 데이터가 신뢰할 수 있고 사용자 요구를 충족하는지 확인하기 위해.\n\n(statcanhistory에서?) 표본 추출 오차의 주요 원인은 무엇입니까(하나 선택)?\n\n개인의 무응답.\n인구가 아닌 표본 사용.\n처리 중 데이터 캡처 오류.\n주택의 오분류.\n\n(statcanhistory에서?) 사람이나 주택이 누락되거나 두 번 이상 계산될 때 발생하는 오류는 무엇입니까(하나 선택)?\n\n무응답 오류.\n포괄 범위 오류.\n표본 추출 오류.\n처리 오류.\n\n(statcanhistory에서?) 응답자 또는 조사원의 오해 또는 오보와 관련된 오류 유형은 무엇입니까(하나 선택)?\n\n표본 추출 오류.\n응답 오류.\n처리 오류.\n포괄 범위 오류.\n\n(statcanhistory에서?) 주택 분류 조사(DCS)의 목적은 무엇입니까(하나 선택)?\n\n장기 설문지를 개선하기 위해.\n새로운 주택 개발을 분류하기 위해.\n가구 소득에 대한 데이터를 수집하기 위해.\n주택의 분류 오류를 연구하기 위해.\n\n(statcanhistory에서?) 인구 조사 과소 집계 연구(CUS)는 무엇을 추정하도록 설계되었습니까(하나 선택)?\n\n인구 조사에서 누락된 사람 수.\n표본 추출 오차의 분산.\n결측 데이터에 대한 대체율.\n장기 설문지에 대한 무응답률.\n\n(statcanhistory에서?) 인구 조사 과대 집계 연구(COS)는 무엇을 식별합니까(하나 선택)?\n\n잘못 분류된 주택.\n개인이 두 번 이상 계산된 경우.\n인구 조사에서 누락된 사람.\n처리 중 잘못된 데이터 입력.\n\n(statcanhistory에서?) 인구 조사의 총 무응답(TNR)률은 어떻게 정의됩니까(하나 선택)?\n\n인구 조사 데이터의 대체된 값 수.\n인구 조사의 부정확한 응답 비율.\n부분 응답이 있는 설문지의 비율.\n설문지가 최소 내용을 충족하지 않는 주택의 비율.\n\nW. Chen 기타 (2019) 및 (martinez2019much를?) 참조하여 정부 통계를 어느 정도 신뢰할 수 있다고 생각하십니까? 최소 한 페이지를 작성하고 답변에서 최소 두 정부를 비교하십시오.\n2021년 캐나다 인구 조사는 먼저 “이 사람의 출생 시 성별은 무엇이었습니까? 성별은 출생 시 지정된 성별을 의미합니다. 남성/여성”이라고 물었고, 그런 다음 “이 사람의 성별은 무엇입니까? 출생 시 지정된 성별과 다를 수 있고 법적 문서에 표시된 것과 다를 수 있는 현재 성별을 의미합니다. 남성/여성/또는 이 사람의 성별을 지정하십시오(입력 또는 손으로 쓴 답변을 위한 공간)”라고 물었습니다. (statcan2020을?) 참조하여 인구 조사가 이와 같이 진행된 것이 어느 정도 적절하다고 생각하는지 논의하십시오. 더 익숙하다면 다른 국가의 경우를 논의해도 좋습니다.\nIPUMS를 사용하여 2020 ACS에 액세스하십시오. 코드북을 사용하여 캘리포니아(STATEICP)에서 박사 학위를 최고 학력(EDUC)으로 가진 응답자는 몇 명이었습니까(하나 선택)?\n\n2,007\n732\n5,765\n4,684\n\nIPUMS를 사용하여 1940년 1% 표본에 액세스하십시오. 코드북을 사용하여 캘리포니아(STATEICP)에서 5년 이상의 대학 교육을 최고 학력(EDUC)으로 가진 응답자는 몇 명이었습니까(하나 선택)?\n\n532\n1,056\n904\n1,789\n\n(Dean2022를?) 참조하여 확률적 표본 추출과 비확률적 표본 추출의 차이점을 논의하십시오.\n목표 모집단이란 무엇입니까(하나 선택)?\n\n결론을 도출하고자 하는 전체 그룹.\n표본을 추출할 수 있는 목표 모집단의 모든 단위 목록.\n표본 추출을 위해 쉽게 접근할 수 있는 인구의 하위 집합.\n연구에 참여하기로 동의한 개인 목록.\n\n표본 추출 프레임이란 무엇입니까(하나 선택)?\n\n응답자로부터 데이터를 수집하는 데 사용되는 방법.\n표본을 추출할 수 있는 목표 모집단의 모든 단위 목록.\n데이터 수집이 발생하는 기간.\n연구자가 결론을 도출하고자 하는 전체 그룹.\n\n확률적 표본 추출과 비확률적 표본 추출의 차이점은 무엇입니까(하나 선택)?\n\n확률적 표본 추출에는 표본 추출 프레임이 필요하지 않습니다.\n확률적 표본 추출은 항상 비확률적 표본 추출보다 비용 효율적입니다.\n확률적 표본 추출에서는 모든 단위가 선택될 확률이 알려져 있는 반면, 비확률적 표본 추출에서는 선택이 확률에 기반하지 않습니다.\n비확률적 표본 추출 방법은 확률적 표본 추출 방법보다 더 정확합니다.\n\n(beaumont2020probability를?) 참조하여 확률 설문 조사가 사라질 것이라고 생각하십니까? 그리고 그 이유는 무엇입니까(한두 단락으로 작성하십시오)?\n표본 추출 프레임의 모든 관측치가 선택될 확률이 동일하도록 단위를 선택하는 표본 추출 방법은 무엇입니까(하나 선택)?\n\n체계적 표본 추출.\n층화 표본 추출.\n단순 무작위 표본 추출.\n군집 표본 추출.\n\n첫 번째 단위는 무작위로 선택되고 후속 단위는 일정한 간격으로 선택되는 표본 추출 방법은 무엇입니까(하나 선택)?\n\n체계적 표본 추출.\n편의 표본 추출.\n층화 표본 추출.\n군집 표본 추출.\n\n층화 표본 추출의 특징은 무엇입니까(하나 선택)?\n\n인구를 하위 그룹으로 나누고 각 하위 그룹에서 무작위 표본을 추출합니다.\n참가자가 네트워크를 통해 다른 참가자를 모집합니다.\n전체 군집이 무작위로 선택되고 그 안의 모든 단위가 표본 추출됩니다.\n전체 그룹 또는 군집을 무작위로 선택한 다음 그 안의 모든 또는 일부 단위를 표본 추출합니다.\n\n군집 표본 추출에서 단위는 어떻게 선택됩니까(하나 선택)?\n\n인구를 하위 그룹으로 나누고 각 하위 그룹에서 무작위 표본을 추출합니다.\n전체 그룹 또는 군집을 무작위로 선택한 다음 그 안의 모든 또는 일부 단위를 표본 추출합니다.\n목록에서 n번째 단위를 선택합니다.\n특정 할당량을 기반으로 단위를 선택합니다.\n\n군집 표본 추출을 사용하려는 몇 가지 이유를 들어보십시오(해당하는 모든 항목 선택)?\n\n하위 인구 측면에서 응답의 균형.\n관리상의 편의.\n비용 측면에서의 효율성.\n\n정수 [1:100]을 고려하십시오. 10개의 표본만 사용하여 중앙값을 추정하는 데 관심이 있다면 어떤 접근 방식을 선택하시겠습니까(하나 선택)?\n\n단순 무작위 표본 추출.\n체계적 표본 추출.\n군집 표본 추출.\n층화 표본 추출.\n\n1부터 100까지의 숫자를 고려하고, 20개의 숫자로 구성된 군집 표본을 기반으로 평균을 추정하는 R 코드를 작성하십시오. 이 코드를 100번 다시 실행하고 매번 평균의 추정치를 기록한 다음 히스토그램을 플롯하십시오. 그래프에 대해 무엇을 알 수 있습니까? 설명과 토론의 단락을 추가하십시오.\n(bowley1913working에서?) 리딩 가구 연구를 위한 표본은 어떻게 선택되었습니까(하나 선택)?\n\n거리의 무작위 선택에 의해.\n소득을 기준으로 가구를 선택하여.\n지역 디렉토리에서 10개 건물 중 하나를 표시하여.\n5번째 가구마다 인터뷰하여.\n\n(bowley1913working에서?) 사용된 표본 추출 접근 방식은 무엇입니까(하나 선택)?\n\n군집 표본 추출.\n단순 무작위 표본 추출.\n층화 표본 추출.\n체계적 표본 추출.\n\n(bowley1913working에서?) 표본 데이터를 기반으로 리딩 전체에 대한 추정치를 제공하기 위해 사용된 방법은 무엇입니까(하나 선택)?\n\n비례 인구 조사 데이터 사용.\n중앙값 소득 수준 계산.\n승수 적용.\n\n(bowley1913working에서?) 리딩의 노동 계급 가구로부터 임대료 및 소득 데이터를 수집하는 방법은 무엇이었습니까(하나 선택)?\n\n집주인 인터뷰를 통해.\n인구 조사 기록을 검사하여.\n세금 기록에서 데이터를 수집하여.\n가구와의 자원 봉사 인터뷰를 통해.\n\nBowley (1913, p. 673)의 다음 진술에 대해 논의하십시오. “표본 추출 과정에 익숙하지 않은 사람들에게는 21분의 1의 비율이 어떤 결론을 내리기에는 너무 작고, 어떤 경우에도 막연한 확률 이상을 얻을 수 없다고 생각될 수 있습니다. … [그러나] 표본의 정밀도는 전체에 대한 비율이 아니라 자체 크기에 따라 달라지며, 무작위 표본 추출의 조건이 확보되면, 이 조사에서 그랬다고 믿어집니다.”\n(neyman1934two에서?) 층화 표본 추출의 주요 목표는 무엇입니까(하나 선택)?\n\n무작위화의 필요성을 줄이기 위해.\n인구의 모든 층이 대표되도록 보장하기 위해.\n표본 선택의 편향을 증가시키기 위해.\n\n(neyman1934two의?) 주요 초점은 무엇이었습니까(하나 선택)?\n\n단순 무작위 표본의 도입.\n할당 표본 추출 기법의 개발.\n표본 선택의 모든 편향 제거.\n층화 표본 추출과 비확률 표본 추출의 구별.\n\n(neyman1934two에서?) 단순 무작위 표본 추출에 비해 층화 표본 추출의 한 가지 장점은 무엇입니까(하나 선택)?\n\n표본 추출 프레임이 필요하지 않습니다.\n더 적은 표본이 필요합니다.\n더 저렴합니다.\n추정치의 정밀도를 높일 수 있습니다.\n\nNeyman (1934, 섹션 V)에서 인구의 속성에 관계없이 인구의 평균 집단적 특성에 대한 일관된 추정치를 허용하는 접근 방식은 무엇입니까(하나 선택)?\n\n무작위 표본 추출.\n의도적 표본 추출.\n\n다음 중 편의 표본 추출을 가장 잘 설명하는 것은 무엇입니까(하나 선택)?\n\n모든 하위 그룹이 비례적으로 대표되도록 보장하는 표본 추출.\n무작위 숫자를 기반으로 참가자 선택.\n가장 접근하기 쉬운 참가자 선택.\n표본을 선택하기 위해 알고리즘 사용.\n\n눈덩이 표본 추출은 일반적으로 무엇에 사용됩니까(하나 선택)?\n\n포괄적인 표본 추출 프레임이 있는 잘 정의된 인구 연구.\n기존 참가자가 미래 참가자를 모집하여 숨겨지거나 접근하기 어려운 인구 연구.\n다른 인구 통계 그룹에 걸쳐 동등한 대표성 보장.\n포획-재포획 방법을 사용하여 총 인구 크기 추정.\n\n응답자 주도 표본 추출이란 무엇입니까(하나 선택)?\n\n응답자를 선택하기 위해 자동화된 시스템을 사용하는 표본 추출 기법.\n응답자가 다른 응답자를 추천하는 비확률 표본 추출의 한 형태로, 종종 숨겨진 인구에 사용되며 모집에 대한 인센티브를 포함합니다.\n응답자가 확률 메커니즘에 따라 선택되는 무작위 표본 추출 유형.\n모집 노력 없이 자원 봉사하는 응답자에 의존하는 표본 추출 방법.\n\n캐나다의 모든 사람을 대상으로 연령, 성별 및 젠더를 묻는 설문 조사를 실시했다고 가정해 봅시다. 친구가 “우리는 전체 인구를 가지고 있기 때문에 불확실성에 대해 걱정할 필요가 없다”고 주장합니다. 친구가 맞습니까, 틀립니까? 그리고 그 이유는 무엇입니까?\n(meng2018statistical을?) 참조하여 “백만 개의 응답이 있으면 무작위화에 대해 걱정할 필요가 없다”는 주장에 대해 논의하십시오.\n은행에서 일하게 되었고 그들이 이미 사용할 데이터셋을 가지고 있다고 상상해 보십시오. 해당 데이터가 유용할지 결정할 때 탐색해야 할 몇 가지 질문은 무엇입니까?\n\n\n\n수업 활동\n\nusethis::git_vaccinate()는 무엇을 합니까?\n?sec-r-essentials의 아미아 스리니바산의 인용문을 되돌아보고 지능 측정과 관련하여 논의하십시오.\n시뮬레이션할 때 set.seed()를 사용하는 것이 왜 중요한가요?\n행복 측정이 어느 정도 타당한지 논의하고, 토론의 일부로 용어를 정의하십시오.\n아름다움 측정이 어느 정도 신뢰할 수 있는지 논의하고, 토론의 일부로 용어를 정의하십시오.\nA국의 B국으로부터의 특정 상품 수입 측정이 B국의 A국으로의 동일 상품 수출 측정과 어떻게 같지 않을 수 있습니까? 이것이 의문을 제기하는 다른 측정에 대해 자세히 논의하십시오.\n결측 데이터와 관련하여 ?tbl-bandmembers에 대해 논의하십시오.\n\n#| echo: false\n#| label: tbl-bandmembers\n#| tbl-cap: \"다양한 밴드의 멤버와 관련 출생 연도\"\n\ntibble(\n  band = c(rep(\"Beatles\", 3), rep(\"Spice Girls\", 3), NA_character_, rep(\"Girls' Generation\", 8)),\n  name = c(\n    \"Ringo Starr\", \"John Lennon\", \"George Harrison\",\n    \"Victoria Beckham\", \"Mel B\", NA_character_, \"Mel C\",\n    \"Taeyeon\", \"Sunny\", NA_character_, \"Hyoyeon\", \"Yuri\", \"Sooyoung\", \"Yoona\", NA_character_\n  ),\n  birth_year = c(1940, 1940, NA_integer_,\n                 1975, 1975, 1976, 1974,\n                 1989, 1989, NA_integer_, NA_integer_, 1989, 1990, 1990, 1991)\n  ) |&gt; \n  tt() |&gt; \n  style_tt(j = 1:3, align = \"lrr\") |&gt; \n  setNames(c(\"Band\", \"Person\", \"Year of birth\"))\n\n올 소울스 칼리지의 펠로우인 캐서린 런델이 “해방”이라는 단어가 존 던에서 유래했는지에 대한 질문에 대한 응답과 관련하여 표본 추출 편향에 대해 논의하십시오.\n\n\n그렇습니다. 물론, OED가 항상 정식 작가에서 첫 사용을 발견했다는 경고를 제공하지 않으면 안 될 것이라고 생각합니다. 부분적으로는 그들이 불에서 살아남은 사람들이기 때문입니다. 그래서 물론, 그는 발명가가 아니라 일반적인 용어로 단어를 기록했을 수도 있습니다.\n캐서린 런델 (Cowen 2023)\n\n\n학장은 학부 학생들의 평균 통계 능력을 이해하고 싶어합니다. 그녀는 인구 조사를 할 수 있을 만큼은 아니지만 약간의 자금을 제공하는 데 동의하고 일주일 안에 보고하라고 요청합니다. 목표 모집단, 표본 추출 프레임 및 표본을 정의하십시오. 또한 확률적 또는 비확률적 표본 추출을 사용할지 여부와 그 안에서 어떤 접근 방식을 사용할지 논의하십시오. 접근 방식의 장단점을 논의하십시오.\n확률적 표본 추출 접근 방식을 사용하여 수업의 평균 키를 결정하십시오.\n대규모 언어 모델을 사용하여 ’캐나다의 모든 사람을 대상으로 연령, 성별 및 젠더를 묻는 설문 조사를 실시했다고 가정해 봅시다. 친구가 “우리는 전체 인구를 가지고 있기 때문에 불확실성에 대해 걱정할 필요가 없다”고 주장합니다. 친구가 맞습니까, 틀립니까? 그리고 그 이유는 무엇입니까?’에 대한 초기 응답을 생성하십시오. 그런 다음 수업에서 다룬 내용, 자신의 연구를 사용하여 수정하고 확장하십시오. 특히 맥락, 뉘앙스 및 인용뿐만 아니라 다른 측면을 추가해야 합니다. 제출: 1) 프롬프트, 2) 초기 LLM 응답, 3) 증강된 응답.2\n지금 캠퍼스에 있는 사람 수를 추정하는 데 관심이 있습니다. 1-3명의 소그룹을 구성하고 15분이 주어지면 실행할 수 있는 표본 추출을 기반으로 한 계획을 개발하십시오. 측정, 계산, 오차, 계측 및 표본 추출 방법론에 대해 반드시 언급하십시오. 계획을 수업과 공유하고 피드백을 기반으로 개선한 다음 실행하십시오. 지금 캠퍼스에 몇 명이 있는지 추정치를 알려주십시오.\n“라보이스의 실수”: 표본이 관심 있는 일부 질문에 대한 전체 인구로 구성되어 있다고 상상해 보십시오. 편향에 대해 걱정해야 합니까? (힌트: 어떤 해에 어떤 나라의 모든 사람이 병원에 갔는지 여부와 사망했는지 여부에 대한 데이터셋을 가지고 있고, 그것이 병원이 사망에 미치는 영향에 대해 어떤 결론을 내리게 할 수 있는지 생각해 보십시오.) 더 나은 품질의 데이터를 위해 관측치 수를 어느 정도 포기할 의향이 있습니까(강력한 답변은 Meng (2018) 및 (Bradley2021을?) 참조할 것입니다)? 어떤 데이터가 필요합니까(강력한 답변은 추정량을 설정할 것입니다)?\n논문 검토: (Bradley2021을?) 읽고 익숙한 예를 들어 최소 한 페이지의 검토를 작성하십시오.\n\n\n\n과제 I\n이 과제의 목적은 다음에 대한 편안함을 개발하는 것입니다.\n\n더 큰 데이터셋 처리,\n비율 추정량 이해, 그리고\n표본 추출.\n\nIPUMS를 사용하여 2022 ACS에 액세스하십시오. 코드북을 사용하여 각 주(STATEICP)에서 박사 학위를 최고 학력(EDUC)으로 가진 응답자는 몇 명이었습니까? (힌트: 이것을 티블의 열로 만드십시오.)\n캘리포니아(STATEICP)에 모든 교육 수준에 걸쳐 391,171명의 응답자가 있었다면, 라플라스의 비율 추정량 접근 방식을 사용하여 각 주의 총 응답자 수를 추정할 수 있습니까? 즉, 캘리포니아에 대해 계산한 비율을 나머지 주에 적용하십시오. (힌트: 이제 한 주의 박사 학위 소지 응답자 수와 한 주의 응답자 수 사이의 비율을 계산한 다음 해당 비율을 각 주의 박사 학위 소지 응답자 수 열에 적용할 수 있습니다.) 각 주의 실제 응답자 수와 비교하십시오.\nQuarto를 사용하여 간략한 논문을 작성하고 과정의 일반적인 기대를 충족하는 GitHub 리포지토리(그룹당 하나의 리포지토리) 링크를 제출하십시오. 루브릭의 관련 구성 요소는 “R/Python이 인용됨”, “데이터가 적절하게 인용됨”, “수업 논문”, “LLM 사용이 문서화됨”, “제목”, “저자, 날짜 및 리포지토리”, “초록”, “소개”, “데이터”, “결과”, “토론”, “산문”, “상호 참조”, “캡션”, “그래프 및 표”, “참조”, “커밋”, “스케치”, “시뮬레이션”, “테스트” 및 “재현 가능한 워크플로”입니다.\n논문은 최소한 다음을 다루어야 합니다.\n\n데이터 획득 방법에 대한 지침(부록).\n비율 추정량 접근 방식에 대한 간략한 개요.\n추정치와 실제 응답자 수.\n왜 다르다고 생각하는지에 대한 설명, 즉 비율 추정량 사용의 장단점.\nACS에서 사용되는 표본 추출 접근 방식의 장단점에 대한 논의. (힌트: 한 가지 재미있는 것은 한 주의 실제 인구를 찾은 다음, 해당 주와 모든 주의 응답자 수를 감안할 때 비율 추정량 접근 방식을 사용하여 모든 주의 인구를 추정하고 실제 인구와 비교하는 것입니다.)\n\n데이터를 인용하는 것을 잊지 마십시오(단, 원시 데이터를 GitHub에 업로드하지 마십시오).\n참고:\n\n압축 파일을 여는 데 문제가 있는 경우 터미널을 열고 폴더로 이동한 다음 다음을 사용하십시오. gunzip usa_00004.csv.gz (파일 이름을 자신의 것으로 변경하십시오).\n박사 학위에 집중하려면 EDUC 대신 EDUCD가 필요합니다(단, 데이터를 다운로드한 후 선택할 수 있습니다).\n원시 데이터를 GitHub에 업로드하지 마십시오. 너무 크고 IPUMS에서도 그렇게 하지 말라고 요청합니다.\n\n\n\n과제 II\n이 과제의 목적은 다음과 같습니다.\n\n단순 무작위 표본 추출과 군집 표본 추출 비교.\n확률적 표본 추출과 비확률적 표본 추출의 차이점 고려.\n\n바이오뱅크에는 비식별화된 생의학 데이터가 포함되어 있습니다. 예를 들어, UK 바이오뱅크에는 50만 명의 영국 참가자의 표본이 포함되어 있습니다. 한 가지 용도는 응답자의 전체 게놈 시퀀싱 데이터를 보고 유전적 결정 요인의 정도를 더 잘 이해하기 위해 그들이 가진 다양한 조건과 관련시키는 것입니다.\nUK 바이오뱅크 데이터셋을 시뮬레이션하십시오. 전체 게놈은 약 30억 길이의 A, T, C, G 문자의 시퀀스입니다. 각 개인에 대해 총 12개의 길이만 시뮬레이션하고, 3개의 문자로 구성된 4개의 그룹으로 구성하십시오. 그런 다음 결장암, 낭포성 섬유증, 파킨슨병 및 피부암의 네 가지 조건을 고려하십시오. 시뮬레이션된 게놈 시퀀스의 다른 세 문자 조합과 사람이 해당 조건을 가질지 여부 사이에 일부 확률적 연관성을 만드십시오. 예를 들어, 처음 세 위치에 AAA가 있으면 결장암 발병률이 5% 더 높을 수 있습니다. UK 바이오뱅크가 단순 무작위 표본 추출을 사용한다고 가정하십시오. 시드를 설정하는 것을 잊지 마십시오.\n(Davies2024는?) UK 바이오뱅크가 가족 기반 표본 추출을 사용해야 한다고 주장합니다. 즉, 가족 수준에서 군집화할 것입니다. 우리는 가족이 유사한(반드시 동일하지는 않음) 시퀀스를 가질 것으로 예상합니다. 1~5인 가족을 가정하여 시뮬레이션을 다시 수행하십시오.\n이제 시뮬레이션을 분석하고 세 문자 위치와 다양한 조건 간의 관계를 찾는 능력을 비교하십시오. 이것을 표본 무작위 표본 추출과 군집 표본 추출의 차이점과 관련하여 논의하십시오.\n주목해야 할 다른 측면은 가족 수준에서의 표본 추출이 수집가에게 더 쉬울 수 있다는 것입니다. 왜냐하면 한 가족 구성원에 대한 데이터를 수집하는 경우 해당 가족의 다른 구성원에 대한 데이터를 수집하는 것이 약간 더 편리할 수 있기 때문입니다. 확률적 표본 추출과 비확률적 표본 추출의 차이점과 구별의 뉘앙스에 대해 논의하십시오.\nQuarto를 사용하여 간략한 논문을 작성하고 과정의 일반적인 기대를 충족하는 GitHub 리포지토리(그룹당 하나의 리포지토리) 링크를 제출하십시오. 루브릭의 관련 구성 요소는 “R/Python이 인용됨”, “수업 논문”, “LLM 사용이 문서화됨”, “제목”, “저자, 날짜 및 리포지토리”, “초록”, “소개”, “데이터”, “결과”, “토론”, “산문”, “상호 참조”, “캡션”, “그래프 및 표”, “참조”, “커밋”, “스케치”, “시뮬레이션”, “테스트” 및 “재현 가능한 워크플로”입니다.\n\n\n\n\nAchen, Christopher. 1978. “Measuring Representation”. American Journal of Political Science 22 (3): 475–510. https://doi.org/10.2307/2110458.\n\n\nAnderson, Margo, 와/과 Stephen Fienberg. 1999. Who Counts?: The Politics of Census-Taking in Contemporary America. Russell Sage Foundation. http://www.jstor.org/stable/10.7758/9781610440059.\n\n\nArel-Bundock, Vincent. 2024. tinytable: Simple and Configurable Tables in “HTML”, “LaTeX”, “Markdown”, “Word”, “PNG”, “PDF”, and “Typst” Formats. https://vincentarelbundock.github.io/tinytable/.\n\n\nBaker, Reg, Michael Brick, Nancy Bates, Mike Battaglia, Mick Couper, Jill Dever, Krista Gile, 와/과 Roger Tourangeau. 2013. “Summary Report of the AAPOR Task Force on Non-Probability Sampling”. Journal of Survey Statistics and Methodology 1 (2): 90–143. https://doi.org/10.1093/jssam/smt008.\n\n\nBecker, Richard, Allan Wilks, Ray Brownrigg, Thomas Minka, 와/과 Alex Deckmyn. 2022. maps: Draw Geographical Maps. https://CRAN.R-project.org/package=maps.\n\n\nBerdine, Gilbert, Vincent Geloso, 와/과 Benjamin Powell. 2018. “Cuban infant mortality and longevity: health care or repression?” Health Policy and Planning 33 (6): 755–57. https://doi.org/10.1093/heapol/czy033.\n\n\nBlair, Graeme, Jasper Cooper, Alexander Coppock, 와/과 Macartan Humphreys. 2019. “Declaring and Diagnosing Research Designs”. American Political Science Review 113 (3): 838–59. https://doi.org/10.1017/S0003055419000194.\n\n\nBowen, Claire McKay. 2022. Protecting Your Privacy in a Data-Driven World. 1st ed. Chapman; Hall/CRC. https://doi.org/10.1201/9781003122043.\n\n\nBowley, Arthur Lyon. 1913. “Working-class households in Reading”. Journal of the Royal Statistical Society 76 (7): 672–701. https://doi.org/10.2307/2339708.\n\n\nBrewer, Ken. 2013. “Three controversies in the history of survey sampling”. Survey Methodology 39 (2): 249–63.\n\n\nChen, Heng, Marie-Hélène Felt, 와/과 Christopher Henry. 2018. “2017 Methods-of-Payment Survey: Sample Calibration and Variance Estimation”. Bank of Canada. https://doi.org/10.34989/tr-114.\n\n\nChen, Wei, Xilu Chen, Chang-Tai Hsieh, 와/과 Zheng Song. 2019. “A Forensic Examination of China’s National Accounts”. Brookings Papers on Economic Activity, 77–127. https://www.jstor.org/stable/26798817.\n\n\nColombo, Tommaso, Holger Fröning, Pedro Javier Garcı̀a, 와/과 Wainer Vandelli. 2016. “Optimizing the data-collection time of a large-scale data-acquisition system through a simulation framework”. The Journal of Supercomputing 72 (12): 4546–72. https://doi.org/10.1007/s11227-016-1764-1.\n\n\nCowen, Tyler. 2023. “Episode 168: Katherine Rundell on the Art of Words”. Conversations with Tyler, 1월. https://conversationswithtyler.com/episodes/katherine-rundell/.\n\n\nCrawford, Kate. 2021. Atlas of AI. 1st ed. New Haven: Yale University Press.\n\n\nCrosby, Alfred. 1997. The measure of reality: Quantification in Western Europe, 1250-1600. Cambridge: Cambridge University Press.\n\n\nDavis, Darren. 1997. “Nonrandom measurement error and race of interviewer effects among African Americans”. The Public Opinion Quarterly 61 (1): 183–207. https://doi.org/10.1086/297792.\n\n\nFisher, Ronald. (1925년) 1928. Statistical Methods for Research Workers. 2nd ed. London: Oliver; Boyd.\n\n\nFrandell, Ashlee, Mary Feeney, Timothy Johnson, Eric Welch, Lesley Michalegko, 와/과 Heyjie Jung. 2021. “The effects of electronic alert letters for internet surveys of academic scientists”. Scientometrics 126 (8): 7167–81. https://doi.org/10.1007/s11192-021-04029-3.\n\n\nGarfinkel, Irwin, Lee Rainwater, 와/과 Timothy Smeeding. 2006. “A re-examination of welfare states and inequality in rich nations: How in-kind transfers and indirect taxes change the story”. Journal of Policy Analysis and Management 25 (4): 897–919. https://doi.org/10.1002/pam.20213.\n\n\nGazeley, Ursula, Georges Reniers, Hallie Eilerts-Spinelli, Julio Romero Prieto, Momodou Jasseh, Sammy Khagayi, 와/과 Veronique Filippi. 2022. “Women’s risk of death beyond 42 days post partum: a pooled analysis of longitudinal Health and Demographic Surveillance System data in sub-Saharan Africa”. The Lancet Global Health 10 (11): e1582–89. https://doi.org/10.1016/s2214-109x(22)00339-4.\n\n\nGelman, Andrew, Jennifer Hill, 와/과 Aki Vehtari. 2020. Regression and Other Stories. Cambridge University Press. https://avehtari.github.io/ROS-Examples/.\n\n\nGibney, Elizabeth. 2022. “The leap second’s time is up: world votes to stop pausing clocks”. Nature 612 (7938): 18–18. https://doi.org/10.1038/d41586-022-03783-5.\n\n\nGodfrey, Ernest. 1918. “History and development of statistics in Canada”. In The History of Statistics–Their Development and Progress in Many Countries. New York: Macmillan, 편집자： John Koren, 179–98. Macmillan Company of New York.\n\n\nGutman, Robert. 1958. “Birth and death registration in Massachusetts: II. The inauguration of a modern system, 1800-1849”. The Milbank Memorial Fund Quarterly 36 (4): 373–402.\n\n\nHartocollis, Anemona. 2022. “U.S. News Ranked Columbia No. 2, but a Math Professor Has His Doubts”. The New York Times, 3월. https://www.nytimes.com/2022/03/17/us/columbia-university-rank.html.\n\n\nHawes, Michael. 2020. “Implementing Differential Privacy: Seven Lessons From the 2020 United States Census”. Harvard Data Science Review 2 (2). https://doi.org/10.1162/99608f92.353c6f99.\n\n\nHyman, Michael, Luca Sartore, 와/과 Linda J Young. 2021. “Capture-Recapture Estimation of Characteristics of U.S. Local Food Farms Using a Web-Scraped List Frame”. Journal of Survey Statistics and Methodology 10 (4): 979–1004. https://doi.org/10.1093/jssam/smab008.\n\n\nInternational Organization Of Legal Metrology. 2007. International Vocabulary of Metrology – Basic and General Concepts and Associated Terms. 3rd ed. https://www.oiml.org/en/files/pdf%5Fv/v002-200-e07.pdf.\n\n\nKalgin, Alexander. 2014. “Implementation of Performance Management in Regional Government in Russia: Evidence of Data Manipulation”. Public Management Review 18 (1): 110–38. https://doi.org/10.1080/14719037.2014.965271.\n\n\nKennedy, Lauren, Katharine Khanna, Daniel Simpson, Andrew Gelman, Yajun Jia, 와/과 Julien Teitler. 2022. “He, she, they: Using sex and gender in survey adjustment”. https://arxiv.org/abs/2009.14401.\n\n\nKoitsalu, Marie, Martin Eklund, Jan Adolfsson, Henrik Grönberg, 와/과 Yvonne Brandberg. 2018. “Effects of pre-notification, invitation length, questionnaire length and reminder on participation rate: a quasi-randomised controlled trial”. BMC Medical Research Methodology 18 (3): 1–5. https://doi.org/10.1186/s12874-017-0467-5.\n\n\nLane, Nick. 2015. “The unseen world: reflections on Leeuwenhoek (1677) ‘Concerning little animals’”. Philosophical Transactions of the Royal Society B: Biological Sciences 370 (1666): 20140344. https://doi.org/10.1098/rstb.2014.0344.\n\n\nLevine, Judah, Patrizia Tavella, 와/과 Martin Milton. 2022. “Towards a consensus on a continuous coordinated universal time”. Metrologia 60 (1): 014001. https://doi.org/10.1088/1681-7575/ac9da5.\n\n\nLips, Hilary. 2020. Sex and Gender: An Introduction. 7th ed. Illinois: Waveland Press.\n\n\nLohr, Sharon. (1999년) 2022. Sampling: Design and Analysis. 3rd ed. Chapman; Hall/CRC.\n\n\nLuebke, David Martin, 와/과 Sybil Milton. 1994. “Locating the victim: An overview of census-taking, tabulation technology, and persecution in Nazi Germany”. IEEE Annals of the History of Computing 16 (3): 25–39. https://doi.org/10.1109/MAHC.1994.298418.\n\n\nLumley, Thomas. 2020. “survey: analysis of complex survey samples”. https://cran.r-project.org/web/packages/survey/index.html.\n\n\nMeng, Xiao-Li. 2018. “Statistical paradises and paradoxes in big data (I): Law of large populations, big data paradox, and the 2016 US presidential election”. The Annals of Applied Statistics 12 (2): 685–726. https://doi.org/10.1214/18-AOAS1161SF.\n\n\nMill, James. 1817. The History of British India. 1st ed. https://books.google.ca/books?id=Orw_AAAAcAAJ.\n\n\nMitchell, Alanna. 2022a. “Get Ready for the New, Improved Second”. The New York Times, 4월. https://www.nytimes.com/2022/04/25/science/time-second-measurement.html.\n\n\n———. 2022b. “Time Has Run Out for the Leap Second”. The New York Times, 11월. https://www.nytimes.com/2022/11/14/science/time-leap-second.html.\n\n\nMolanphy, Chris. 2012. “100 & Single: Three Rules To Define The Term ‘One-Hit Wonder’ In 2012”. The Village Voice, 9월. https://www.villagevoice.com/2012/09/10/100-single-three-rules-to-define-the-term-one-hit-wonder-in-2012/.\n\n\nMorange, Michel. 2016. A History of Biology. New Jersey: Princeton University Press.\n\n\nNewman, Daniel. 2014. “Missing Data: Five Practical Guidelines”. Organizational Research Methods 17 (4): 372–411. https://doi.org/10.1177/1094428114548590.\n\n\nNeyman, Jerzy. 1934. “On the Two Different Aspects of the Representative Method: The Method of Stratified Sampling and the Method of Purposive Selection”. Journal of the Royal Statistical Society 97 (4): 558–625. https://doi.org/10.2307/2342192.\n\n\nNobles, Melissa. 2002. “Racial categorization and censuses”. In Census and identity: The politics of race, ethnicity, and language in national censuses, 편집자： David Kertzer 와/과 Dominique Arel, 43–70. Cambridge: Cambridge University Press. https://doi.org/10.1017/CBO9780511606045.003.\n\n\nPlant, Anne, 와/과 Robert Hanisch. 2020. “Reproducibility in Science: A Metrology Perspective”. Harvard Data Science Review 2 (4). https://doi.org/10.1162/99608f92.eb6ddee4.\n\n\nPrévost, Jean-Guy, 와/과 Jean-Pierre Beaud. 2015. Statistics, public debate and the state, 1800–1945: A social, political and intellectual history of numbers. Routledge.\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nRegister, Yim. 2020. “Introduction to Sampling and Randomization”. YouTube, 11월. https://youtu.be/U272FFxG8LE.\n\n\nSalganik, Matthew, 와/과 Douglas Heckathorn. 2004. “Sampling and estimation in hidden populations using respondent-driven sampling”. Sociological methodology 34 (1): 193–240. https://doi.org/10.1111/j.0081-1750.2004.00152.x.\n\n\nScott, James. 1998. Seeing Like a State. Yale University Press.\n\n\nSobek, Matthew, 와/과 Steven Ruggles. 1999. “The IPUMS project: An update”. Historical Methods: A Journal of Quantitative and Interdisciplinary History 32 (3): 102–10. https://doi.org/10.1080/01615449909598930.\n\n\nSomers, James. 2017. “Torching the modern-day library of Alexandria”. The Atlantic, 4월. https://www.theatlantic.com/technology/archive/2017/04/the-tragedy-of-google-books/523320/.\n\n\nStatistics Canada. 2023. “Guide to the Census of Population, 2021”. Statistics Canada. https://www12.statcan.gc.ca/census-recensement/2021/ref/98-304/98-304-x2021001-eng.pdf.\n\n\nSteckel, Richard. 1991. “The quality of census data for historical inquiry: A research agenda”. Social Science History 15 (4): 579–99. https://doi.org/10.2307/1171470.\n\n\nStigler, Stephen. 1986. The History of Statistics. Massachusetts: Belknap Harvard.\n\n\nStoler, Ann Laura. 2002. “Colonial archives and the arts of governance”. Archival Science 2 (3월): 87–109. https://doi.org/10.1007/bf02435632.\n\n\nTal, Eran. 2020. “Measurement in Science”. In The Stanford Encyclopedia of Philosophy, 편집자： Edward Zalta, Fall 2020. https://plato.stanford.edu/archives/fall2020/entries/measurement-science/; Metaphysics Research Lab, Stanford University.\n\n\nTaylor, Adam. 2015. “New Zealand says no to Jedis”. The Washington Post, 9월. https://www.washingtonpost.com/news/worldviews/wp/2015/09/29/new-zealand-says-no-to-jedis/.\n\n\nTimbers, Tiffany. 2020. canlang: Canadian Census language data. https://ttimbers.github.io/canlang/.\n\n\nVanhoenacker, Mark. 2015. Skyfaring: A Journey with a Pilot. 1st ed. Alfred A. Knopf.\n\n\nvon Bergmann, Jens, Dmitry Shkolnik, 와/과 Aaron Jacobs. 2021. cancensus: R package to access, retrieve, and work with Canadian Census data and geography. https://mountainmath.github.io/cancensus/.\n\n\nWalby, Kevin, 와/과 Alex Luscombe. 2019. Freedom of information and social science research design. Routledge.\n\n\nWalker, Kyle, 와/과 Matt Herman. 2022. tidycensus: Load US Census Boundary and Attribute Data as “tidyverse” and “sf”-Ready Data Frames. https://CRAN.R-project.org/package=tidycensus.\n\n\nWhitby, Andrew. 2020. The Sum of the People. New York: Basic Books.\n\n\nWhitelaw, James. 1805. An essay on the population of Dublin. Being the result of an actual survey taken in 1798, with great care and precision, and arranged in a manner entirely new. Graisberry; Campbell.\n\n\nWickham, Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, 기타. 2019. “Welcome to the Tidyverse”. Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWu, Changbao, 와/과 Mary Thompson. 2020. Sampling Theory and Practice. Springer.\n\n\nZhang, Ping, XunPeng Shi, YongPing Sun, Jingbo Cui, 와/과 Shuai Shao. 2019. “Have China’s provinces achieved their targets of energy intensity reduction? Reassessment based on nighttime lighting data”. Energy Policy 128 (5월): 276–83. https://doi.org/10.1016/j.enpol.2019.01.014.",
    "crumbs": [
      "Acquisition",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>측정, 인구 조사 및 표본 추출</span>"
    ]
  },
  {
    "objectID": "06-farm_ko.html#footnotes",
    "href": "06-farm_ko.html#footnotes",
    "title": "6  측정, 인구 조사 및 표본 추출",
    "section": "",
    "text": "18세기 수학자인 칼 프리드리히 가우스에게 귀속된 트릭을 사용하여 이를 얻을 수 있습니다. 그는 1부터 임의의 숫자까지의 합은 중간 숫자를 찾은 다음 그 숫자에 1을 더한 값을 곱하여 빠르게 얻을 수 있다는 것을 발견했습니다. 이 경우 \\(50 \\times 101\\)입니다. 또는 R을 사용할 수 있습니다: sum(1:100).↩︎\n데이미언 패트릭 윌리엄스가 이 질문의 아이디어를 냈습니다.↩︎",
    "crumbs": [
      "Acquisition",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>측정, 인구 조사 및 표본 추출</span>"
    ]
  },
  {
    "objectID": "07-gather_ko.html",
    "href": "07-gather_ko.html",
    "title": "7  API, 스크래핑 및 파싱",
    "section": "",
    "text": "7.1 소개\n선수 지식\n주요 개념 및 기술\n소프트웨어 및 패키지\n이 장에서는 우리가 직접 수집해야 하는 데이터를 고려합니다. 이는 관측치가 존재하지만, 우리가 고려할 데이터셋을 얻기 위해 파싱, 가져오기, 정리 및 준비해야 함을 의미합니다. ?sec-farm-data에서 논의된 양식 데이터와 대조적으로, 종종 이러한 관측치는 분석 목적으로 제공되지 않습니다. 이는 우리가 문서화, 포함 및 제외 결정, 결측 데이터 및 윤리적 행동에 특히 관심을 가져야 함을 의미합니다.\n이러한 데이터셋의 예로, 1892년에서 1992년 사이 영국의 개인 수준 유언 검인 기록을 사용하여 데이터셋을 생성한 (Cummins2022를?) 고려해 보십시오. 그들은 “엘리트” 상속의 약 3분의 1이 은폐된다는 것을 발견했습니다. 유사하게, (Taflaga2019는?) 호주 장관 전화번호부를 기반으로 직무 책임에 대한 체계적인 데이터셋을 구성합니다. 그들은 성별에 따라 상당한 차이를 발견합니다. 유언장이나 전화번호부 모두 데이터셋에 포함될 목적으로 만들어지지 않았습니다. 그러나 존중하는 접근 방식을 통해 다른 방법으로는 얻을 수 없는 통찰력을 얻을 수 있습니다. 우리는 이것을 “데이터 수집”이라고 부릅니다. 데이터는 존재하지만 우리가 그것을 얻어야 합니다.\n프로젝트 시작 시 프로젝트가 가질 가치에 대해 결정해야 합니다. 예를 들어, (huggingfaceethics는?) 투명성, 재현성, 공정성, 자기 비판 및 공로 인정을 중요하게 생각합니다. 그것이 프로젝트에 어떤 영향을 미칠까요? “공로 인정”을 중요하게 생각한다는 것은 귀속 및 라이선스에 대해 특히 열성적이라는 것을 의미할 수 있습니다. 수집된 데이터의 경우 원본, 편집되지 않은 데이터가 우리 것이 아닐 수 있으므로 이에 대해 특별히 생각해야 합니다.\n데이터 과학 워크플로의 결과는 기본 데이터보다 나을 수 없습니다 (Bailey 2008). 가장 정교한 통계 분석조차도 제대로 수집되지 않은 데이터를 조정하는 데 어려움을 겪을 것입니다. 이는 팀에서 작업할 때 데이터 수집은 팀의 선임 구성원이 감독하고 적어도 부분적으로 수행해야 함을 의미합니다. 그리고 혼자 작업할 때는 이 단계에 특별한 고려와 주의를 기울이십시오.\n이 장에서는 데이터를 수집하는 다양한 접근 방식을 살펴봅니다. 우리는 JSON 및 XML과 같은 반정형 데이터와 API 사용으로 시작합니다. API를 사용하는 것은 일반적으로 데이터 제공자가 액세스를 제공하는 데 편안한 조건을 지정한 상황입니다. API를 사용하면 코드를 작성하여 데이터를 수집할 수 있습니다. 이것은 효율적이고 잘 확장되기 때문에 가치가 있습니다. API를 통해 데이터를 수집하는 데 익숙해지면 흥미로운 데이터셋에 액세스할 수 있습니다. 예를 들어, (facebookapitrump는?) Facebook 정치 광고 API를 사용하여 트럼프 2020 캠페인 광고 218,100개를 수집하여 캠페인을 더 잘 이해합니다.\n그런 다음 웹사이트에 데이터가 있을 때 사용할 수 있는 웹 스크래핑으로 넘어갑니다. 이러한 데이터는 일반적으로 데이터셋으로 구성되지 않았기 때문에 프로젝트에 대한 의도적이고 명확한 가치를 갖는 것이 특히 중요합니다. 스크래핑은 데이터 제공자의 우선 순위가 API를 구현하지 않았다는 것을 의미하는 많은 데이터 소스가 있기 때문에 데이터 수집의 중요한 부분입니다. 예를 들어, 팬데믹 초기에 COVID-19 대시보드를 만드는 데 웹 스크래핑이 상당히 많이 사용되었습니다 (Eisenstein 2022).\n마지막으로, PDF에서 데이터를 수집하는 것을 고려합니다. 이를 통해 특히 정부 보고서 및 오래된 책에 포함된 흥미로운 데이터셋을 구성할 수 있습니다. 실제로, 많은 국가에 정보 자유법이 존재하고 정부가 데이터를 공개하도록 요구하지만, 이들은 너무 자주 스프레드시트가 처음에는 CSV였음에도 불구하고 PDF로 공유되는 결과를 낳습니다.\n데이터를 수집하는 것은 양식 데이터를 사용하는 것보다 더 많은 것을 요구할 수 있지만, 그렇지 않으면 할 수 없는 데이터셋을 탐색하고 질문에 답할 수 있게 해줍니다. 세계에서 가장 흥미로운 작업 중 일부는 수집된 데이터를 사용하지만, 우리가 존중하는 마음으로 접근하는 것이 특히 중요합니다.",
    "crumbs": [
      "획득",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>API, 스크래핑 및 파싱</span>"
    ]
  },
  {
    "objectID": "07-gather_ko.html#api",
    "href": "07-gather_ko.html#api",
    "title": "7  API, 스크래핑 및 파싱",
    "section": "7.2 API",
    "text": "7.2 API\n일상 언어와 우리의 목적을 위해, 응용 프로그래밍 인터페이스(API)는 누군가가 자신의 컴퓨터에 특정 파일을 설정하여 우리가 그들의 지침에 따라 그것들을 얻을 수 있는 상황입니다. 예를 들어, 우리가 Slack에서 gif를 사용할 때, 백그라운드에서 작동할 수 있는 한 가지 방법은 Slack이 Giphy의 서버에 적절한 gif를 요청하고, Giphy의 서버가 그 gif를 Slack에 주고, 그런 다음 Slack이 그것을 채팅에 삽입하는 것입니다. Slack과 Giphy가 상호 작용하는 방식은 Giphy의 API에 의해 결정됩니다. 더 엄밀히 말하면, API는 HTTP 프로토콜을 사용하여 액세스하는 서버에서 실행되는 응용 프로그램입니다.\n여기서는 데이터 수집을 위해 API를 사용하는 데 중점을 둡니다. 그 맥락에서 API는 사람이 아닌 다른 컴퓨터가 액세스할 수 있도록 설정된 웹사이트입니다. 예를 들어, Google 지도로 이동할 수 있습니다. 그리고 우리는 지도를 스크롤하고 클릭하고 드래그하여 호주 캔버라에 지도를 중앙에 맞출 수 있습니다. 또는 브라우저에 이 링크를 붙여넣을 수 있습니다. 탐색하는 대신 해당 링크를 붙여넣음으로써 우리는 API를 사용하는 방법을 모방했습니다. URL을 제공하고 무언가를 돌려받는 것입니다. 이 경우 결과는 ?fig-focuson2020과 같은 지도여야 합니다.\n\n\n\n\n\n\n그림 7.1: 2023년 2월 12일 기준 Google 지도 API 응답 예시\n\n\n\nAPI 사용의 장점은 데이터 제공자가 일반적으로 제공하려는 데이터와 제공할 조건을 지정한다는 것입니다. 이러한 조건에는 속도 제한(즉, 데이터를 얼마나 자주 요청할 수 있는지) 및 데이터로 무엇을 할 수 있는지(예: 상업적 목적으로 사용하거나 재게시할 수 없음)와 같은 측면이 포함될 수 있습니다. API는 우리가 사용하도록 특별히 제공되므로 예기치 않은 변경이나 법적 문제의 대상이 될 가능성이 적습니다. 이 때문에 API를 사용할 수 있을 때 웹 스크래핑 대신 사용해야 한다는 것이 분명합니다.\n이제 API 사용에 대한 몇 가지 사례 연구를 살펴보겠습니다. 첫 번째에서는 httr을 사용하여 API를 직접 다룹니다. 그런 다음 spotifyr을 사용하여 Spotify에서 데이터에 액세스합니다.\n\n7.2.1 arXiv, NASA 및 Dataverse\nhttr을 설치하고 로드한 후 GET()을 사용하여 API에서 직접 데이터를 얻습니다. 이것은 일부 특정 데이터를 얻으려고 시도하며 주요 인수는 “url”입니다. 이것은 우리가 관심 있는 특정 정보가 지도였던 ?fig-focuson2020의 Google 지도 예시와 유사합니다.\n\n7.2.1.1 arXiv\n이 사례 연구에서는 arXiv에서 제공하는 API를 사용할 것입니다. arXiv는 동료 심사를 거치기 전의 학술 논문을 위한 온라인 저장소입니다. 이러한 논문은 일반적으로 “사전 인쇄물”이라고 합니다. 우리는 URL을 제공하여 사전 인쇄물에 대한 일부 정보를 얻기 위해 arXiv에 요청하기 위해 GET()을 사용합니다.\n\narxiv &lt;- GET(\"http://export.arxiv.org/api/query?id_list=2310.01402\")\n\nstatus_code(arxiv)\n\nstatus_code()를 사용하여 응답을 확인할 수 있습니다. 예를 들어, 200은 성공을 의미하고 400은 서버에서 오류를 수신했음을 의미합니다. 서버에서 무언가를 받았다고 가정하면 content()를 사용하여 표시할 수 있습니다. 이 경우 XML 형식의 데이터를 받았습니다. XML은 태그로 식별되는 항목이 있는 마크업 언어이며, 다른 태그 내에 중첩될 수 있습니다. xml2를 설치하고 로드한 후 read_xml()을 사용하여 XML을 읽을 수 있습니다. XML은 반정형 구조이며, html_structure()를 사용하여 살펴보는 것이 유용할 수 있습니다.\n\ncontent(arxiv) |&gt;\n  read_xml() |&gt;\n  html_structure()\n\n이 XML 트리의 다양한 측면을 추출하여 데이터셋을 만들고 싶을 수 있습니다. 예를 들어, 8번째 항목인 “entry”를 보고, 특히 “entry” 내의 4번째 및 9번째 항목인 “title”과 “URL”을 얻을 수 있습니다.\n\ndata_from_arxiv &lt;-\n  tibble(\n    title = content(arxiv) |&gt;\n      read_xml() |&gt;\n      xml_child(search = 8) |&gt;\n      xml_child(search = 4) |&gt;\n      xml_text(),\n    link = content(arxiv) |&gt;\n      read_xml() |&gt;\n      xml_child(search = 8) |&gt;\n      xml_child(search = 9) |&gt;\n      xml_attr(\"href\")\n  )\ndata_from_arxiv\n\n\n\n7.2.1.2 NASA 오늘의 천문 사진\n또 다른 예를 고려해 보면, 매일 NASA는 APOD API를 통해 오늘의 천문 사진(APOD)을 제공합니다. 우리는 GET()을 사용하여 특정 날짜의 사진에 대한 URL을 얻은 다음 표시할 수 있습니다.\n\nNASA_APOD_20190719 &lt;-\n  GET(\"https://api.nasa.gov/planetary/apod?api_key=DEMO_KEY&date=2019-07-19\")\n\ncontent()를 사용하여 반환된 데이터를 검사하면 날짜, 제목, 설명 및 URL과 같은 다양한 필드가 제공됨을 알 수 있습니다.\n\n# APOD 2019년 7월 19일\ncontent(NASA_APOD_20190719)$date\n\n[1] \"2019-07-19\"\n\ncontent(NASA_APOD_20190719)$title\n\n[1] \"Tranquility Base Panorama\"\n\ncontent(NASA_APOD_20190719)$explanation\n\n[1] \"On July 20, 1969 the Apollo 11 lunar module Eagle safely touched down on the Moon. It landed near the southwestern corner of the Moon's Mare Tranquillitatis at a landing site dubbed Tranquility Base. This panoramic view of Tranquility Base was constructed from the historic photos taken from the lunar surface. On the far left astronaut Neil Armstrong casts a long shadow with Sun is at his back and the Eagle resting about 60 meters away ( AS11-40-5961). He stands near the rim of 30 meter-diameter Little West crater seen here to the right ( AS11-40-5954). Also visible in the foreground is the top of the camera intended for taking stereo close-ups of the lunar surface.\"\n\ncontent(NASA_APOD_20190719)$url\n\n[1] \"https://apod.nasa.gov/apod/image/1907/apollo11TranquilitybasePan600h.jpg\"\n\n\nknitr의 include_graphics()에 해당 URL을 제공하여 표시할 수 있습니다(그림 7.2).\n\n\n\n\n\n\n\n\n\n(a) 고요의 기지 파노라마 (이미지 출처: 닐 암스트롱, 아폴로 11호, NASA)\n\n\n\n\n\n그림 7.2: NASA APOD API에서 얻은 이미지\n\n\n\n\n\n7.2.1.3 Dataverse\n마지막으로, 반정형 형식의 또 다른 일반적인 API 응답은 JSON입니다. JSON은 기계가 파싱할 수 있는 사람이 읽을 수 있는 데이터 저장 방식입니다. 행과 열에 익숙한 CSV와 달리 JSON은 키-값 쌍을 사용합니다.\n\n{\n  \"firstName\": \"Rohan\",\n  \"lastName\": \"Alexander\",\n  \"age\": 36,\n  \"favFoods\": {\n    \"first\": \"Pizza\",\n    \"second\": \"Bagels\",\n    \"third\": null\n  }\n}\n\n\njsonlite로 JSON을 파싱할 수 있습니다. 구체적인 예를 고려하기 위해, 데이터셋 공유를 더 쉽게 해주는 웹 애플리케이션인 “Dataverse”를 사용합니다. API를 사용하여 데모 데이터버스를 쿼리할 수 있습니다. 예를 들어, 정치와 관련된 데이터셋에 관심이 있을 수 있습니다.\n\npolitics_datasets &lt;-\n  fromJSON(\"https://demo.dataverse.org/api/search?q=politics\")\n\npolitics_datasets\n\n$status\n[1] \"OK\"\n\n$data\n$data$q\n[1] \"politics\"\n\n$data$total_count\n[1] 7793\n\n$data$start\n[1] 0\n\n$data$spelling_alternatives\nnamed list()\n\n$data$items\n                                                              name      type\n1                                             CAP - United Kingdom dataverse\n2                                          China Archive Dataverse dataverse\n3                               Isabel Causadias Domingo Dataverse dataverse\n4                                                   Dataset-Omer-1   dataset\n5                                                  CAP - Australia dataverse\n6  Czech Household Panel Survey wave 3 - time-use diaries (adults)   dataset\n7                                     Politics Now Poll: July 1996   dataset\n8                                Politics Now Poll: September 1996   dataset\n9                                Politics Now Poll: September 1996   dataset\n10                                 Politics Now Poll: October 1996   dataset\n                                                           url\n1                  https://demo.dataverse.org/dataverse/CAP_UK\n2           https://demo.dataverse.org/dataverse/china-archive\n3  https://demo.dataverse.org/dataverse/icausadias_artresearch\n4                          https://doi.org/10.70122/FK2/RXCK22\n5           https://demo.dataverse.org/dataverse/CAP_Australia\n6                          https://doi.org/10.70122/FK2/BSEQZI\n7                      https://doi.org/10.25940/ROPER-31106695\n8                      https://doi.org/10.25940/ROPER-31106705\n9                      https://doi.org/10.25940/ROPER-31106707\n10                     https://doi.org/10.25940/ROPER-31106688\n                                                   image_url\n1  https://demo.dataverse.org/api/access/dvCardImage/2058461\n2                                                       &lt;NA&gt;\n3                                                       &lt;NA&gt;\n4                                                       &lt;NA&gt;\n5  https://demo.dataverse.org/api/access/dvCardImage/2058462\n6                                                       &lt;NA&gt;\n7                                                       &lt;NA&gt;\n8                                                       &lt;NA&gt;\n9                                                       &lt;NA&gt;\n10                                                      &lt;NA&gt;\n               identifier\n1                  CAP_UK\n2           china-archive\n3  icausadias_artresearch\n4                    &lt;NA&gt;\n5           CAP_Australia\n6                    &lt;NA&gt;\n7                    &lt;NA&gt;\n8                    &lt;NA&gt;\n9                    &lt;NA&gt;\n10                   &lt;NA&gt;\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      description\n1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             The UK Policy Agendas Project seeks to develop systematic measures of the policy agenda of British government and politics over time. It applies the policy content coding system of the original Policy Agendas Project in the United States, founded by Frank Baumgartner and Bryan Jones, with the aim of creating a consistent record of the issues that are attended to at different points in time, across many of the main venues of British public policy and politics – namely in parliament, the media and public opinion. The reliability of these measures of policy attention are ensured through adherence to clearly defined coding rules and standards, which give us confidence that changes in the priorities of government can be tracked consistently over time and in different arenas of politics. Location: University of Edinburgh; University of Southampton Downloadable Data Series: 12 Time Span: 1910-2015 Total Observations: 125,539\n2  Introduction The China Archive is a data archive dedicated to support of scholarly, empirical research by anthropologists, economists, historians, political scientists, sociologists, and others in the fields of business, agriculture, and engineering. The goal of the Archive is to enable case research on Chinese domestic matters and China–U.S. relations, as well as to facilitate the inclusion of China in more broadly comparative studies. To that end, the Archive’s mission includes: acquiring and maintaining extant data sets and data sources on an ongoing basis, facilitating production of quantitative data from textual information when such is desirable and feasible, making all of the Archive’s data available on a user-friendly basis to scholars at and visiting Texas A&M University, and establishing web-based links to searchable electronic sources of information on China. As long-term goals, The China Archive is especially dedicated to: locating and acquiring difficult-to-obtain Chinese data such as public opinion data, making as many of the holdings as possible available online or via other computer media, and providing capability for converting textual data to numerical data suitable for quantitative analysis. In keeping with these goals, the Archive includes data sets collected by individuals and research centers/institutes as well as by government agencies. The Archive was planned by a faculty committee in 2002–2003, and is now a project of the Texas A&M University Libraries. A faculty committee continues to function as an advisory committee to the Libraries on matters related to the upkeep and expansion of The China Archive. The faculty committee is, in turn, advised by an External Advisory Panel, composed of China scholars from other academic institutions in the United States. Faculty Planning Committee Bob Harmel, Archive Planning Committee Chair; Political Science; Director, Program in the Cross-National Study of Politics Stephen Atkins, Sterling Evans Library Ben Crouch, Associate Dean, College of Liberal Arts Qi Li, Department of Economics Xinsheng Liu, Institute for Science, Technology and Public Policy, Bush School of Government and Public Service; School of Government, Peking University Rick Nader, Director, Institute for Pacific Asia Dudley Poston, Professor of Sociology and Abell Professor of Liberal Arts Raghavan (Srini) Srinivasan, Director, Spatial Sciences Laboratory, Texas Agriculture Experiment Station Di Wang, Assistant Professor of History Ben Wu, Associate Professor, Rangeland Ecology and Management Wei Zhao, Professor, Computer Science; Associate Vice President for Research Dianzhi (Dan) Sui, Department of Geography\n3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    This Dataverse explores the diverse contexts in which artistic practice and collective identity develop, examining the intersections of cultural critique, market dynamics, and technological advancement. The datasets available in this Dataverse include a wide-ranging investigation into how artist collectives shape collective identities that transcend individual authorship, often reflecting cultural, social, and political themes. Additionally, research on the art market sheds light on the systems governing art distribution and valuation, while studies on the influence of technology in art education explore how digital tools and AI are reshaping creative processes and academic environments. This Dataverse offers resources for researchers, students, and professionals in the fields of art history, cultural studies, and social sciences, aiming to deepen understanding of the structures, markets, and technologies that inform contemporary art and its societal impact.\n4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  This should be bold. This should be a new line. This should be a blockquote. This should be code This should be emphasized This should be strong There should be a horizontal rule below this sentence. This should be h1 size. This should be h2 This should be h3. This should be first item in a list This should be second item in a list This should be a paragraph break This should be subscript. This should be superscript This should be striked out\n5                                                                                                                                                                                                                                          The Australian Policy Agendas Project collects and organizes data on Australian legislation, executive speeches, opposition questions, public opinion, media coverage, and High Court decisions. Some details are listed below. Data is forthcoming. Decisions of the High Court of Australia This dataset contains information on every case decided by the High Court of Australia between the years 1970 and 2015. Cases serve as the unit of analysis. Each case was coded in terms of its policy content and several other variables controlling for the nature of the case and the nature of the court. In coding for policy content, we utilized the Comparative Agendas Project’s topics coding scheme, where each case was assigned both a major topic and a sub topic depending on its policy content. A full description of these categories and their corresponding codes may be found in the codebook. Sydney Morning Herald - Front Page Articles This dataset contains information on each article published on the Sydney Morning Herald's front page for each day from 1990 through 2015. Front page articles serve as the unit of analysis. Each article was coded in terms of its policy content and other variables of interest controlling for location, political context, and key actors. In coding for policy content, we utilized the Comparative Agendas Project’s major topics coding scheme, where each article was assigned a major topic code. A full description of the policy content categories and their corresponding codes may be found in the major topics codebook. Dr. Keith Dowding (ANU), Dr. Aaron Martin (Melbourne), and Dr. Rhonda Evans (UT-Austin) lead the Australian Policy Agendas Project. Dr. Dowding and Dr. Martin coded legislation, executive speeches, opposition questions, public opinion, and media data, and Dr. Evans collected data on decisions of the High Court of Australia as well as additional media data. Data is forthcoming. Principal Investigator: Dr. Keith Dowding, Dr. Aaron Martin, Dr. Rhonda Evans Location: Australian National University, University of Melbourne, The University of Texas at Austin Downloadable Data Series: 1 Time Span: 1970-2015 Total Observations: 2,548 Sponsoring Institutions Dr. Dowding and Dr. Martin’s research was funded by the Australian Research Council Discovery Award DP 110102622. Dr. Evans’ research is funded by the Edward A. Clark Center for Australian and New Zealand Studies at The University of Texas at Austin.\n6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         The dataset contains information from the time-use diaries distributed to adults (18 years and more) in the respective wave of the Czech Household Panel Survey. The dataset does not include other information on time-use diary respondents - these need to be appended from the main data file from the respective wave. The Czech Household Panel Survey (CHPS) is a sample survey that repeatedly interviews a representative sample of the sample households living in the Czech Republic. Between 2015 and 2019, five waves with annual intervals were fielded. In 2020 and 2021, two smaller data collections focused on the covid-19 pandemic occurred. From 2023 to 2024, the next (sixth) wave was undertaken. CHPS is an interdisciplinary study using insights from sociology, economics, and political science. From 2015 to 2018, the study focused on five main thematic areas: Family life, time use, health; Education and the labour market; Social stratification; Housing; Political participation and civil society. The first wave's sample of households was constructed using probabilistic (random) sampling to be representative of the Czech Republic's household population. In the next waves, households that took part in the previous year were asked to participate. All household members in the target age range were invited to complete the questionnaires. New households were not introduced into the sample from the second to the fifth wave.\n7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            &lt;NA&gt;\n8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            &lt;NA&gt;\n9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            &lt;NA&gt;\n10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           &lt;NA&gt;\n           published_at publicationStatuses                 affiliation\n1  2023-06-06T17:18:53Z           Published Comparative Agendas Project\n2  2016-12-09T20:13:22Z           Published      Texas A & M University\n3  2024-10-27T21:01:11Z           Published    Universitat de Barcelona\n4  2024-11-18T15:32:02Z           Published                        &lt;NA&gt;\n5  2023-06-06T17:18:42Z           Published Comparative Agendas Project\n6  2024-11-25T15:55:51Z           Published                        &lt;NA&gt;\n7  2020-12-22T18:47:48Z           Published                        &lt;NA&gt;\n8  2020-12-22T19:09:20Z           Published                        &lt;NA&gt;\n9  2020-12-22T19:11:02Z           Published                        &lt;NA&gt;\n10 2020-12-22T19:16:27Z           Published                        &lt;NA&gt;\n           parentDataverseName parentDataverseIdentifier\n1  Demo testing request access         requestaccessdemo\n2               Demo Dataverse                      demo\n3               Demo Dataverse                      demo\n4                         &lt;NA&gt;                      &lt;NA&gt;\n5  Demo testing request access         requestaccessdemo\n6                         &lt;NA&gt;                      &lt;NA&gt;\n7                         &lt;NA&gt;                      &lt;NA&gt;\n8                         &lt;NA&gt;                      &lt;NA&gt;\n9                         &lt;NA&gt;                      &lt;NA&gt;\n10                        &lt;NA&gt;                      &lt;NA&gt;\n                     global_id\n1                         &lt;NA&gt;\n2                         &lt;NA&gt;\n3                         &lt;NA&gt;\n4      doi:10.70122/FK2/RXCK22\n5                         &lt;NA&gt;\n6      doi:10.70122/FK2/BSEQZI\n7  doi:10.25940/ROPER-31106695\n8  doi:10.25940/ROPER-31106705\n9  doi:10.25940/ROPER-31106707\n10 doi:10.25940/ROPER-31106688\n                                                  publisher\n1                                                      &lt;NA&gt;\n2                                                      &lt;NA&gt;\n3                                                      &lt;NA&gt;\n4                                            Demo Dataverse\n5                                                      &lt;NA&gt;\n6        Czech Social Science Data Archive - TEST DATAVERSE\n7  Harvesting Roper Center Datasets (this is an experiment)\n8  Harvesting Roper Center Datasets (this is an experiment)\n9  Harvesting Roper Center Datasets (this is an experiment)\n10 Harvesting Roper Center Datasets (this is an experiment)\n                                                                                                                                                                                                                                                                                                              citationHtml\n1                                                                                                                                                                                                                                                                                                                     &lt;NA&gt;\n2                                                                                                                                                                                                                                                                                                                     &lt;NA&gt;\n3                                                                                                                                                                                                                                                                                                                     &lt;NA&gt;\n4                                                                                                                                                           Fahim, Omer, 2024, \"Dataset-Omer-1\", &lt;a href=\"https://doi.org/10.70122/FK2/RXCK22\" target=\"_blank\"&gt;https://doi.org/10.70122/FK2/RXCK22&lt;/a&gt;, Demo Dataverse, V2\n5                                                                                                                                                                                                                                                                                                                     &lt;NA&gt;\n6  Lyons, Pat; Hamplov&aacute;, Dana; R&ouml;schov&aacute;, Michaela; Kudrn&aacute;č, Ale&scaron;; Hrub&aacute;, Lucie, 2024, \"Czech Household Panel Survey wave 3 - time-use diaries (adults)\", &lt;a href=\"https://doi.org/10.70122/FK2/BSEQZI\" target=\"_blank\"&gt;https://doi.org/10.70122/FK2/BSEQZI&lt;/a&gt;, Demo Dataverse, V1\n7                                                                                                              Politics Now, 2025, \"Politics Now Poll: July 1996\", &lt;a href=\"https://doi.org/10.25940/ROPER-31106695\" target=\"_blank\"&gt;https://doi.org/10.25940/ROPER-31106695&lt;/a&gt;, Roper Center for Public Opinion Research\n8                                                                                                         Politics Now, 2025, \"Politics Now Poll: September 1996\", &lt;a href=\"https://doi.org/10.25940/ROPER-31106705\" target=\"_blank\"&gt;https://doi.org/10.25940/ROPER-31106705&lt;/a&gt;, Roper Center for Public Opinion Research\n9                                                                                                         Politics Now, 2025, \"Politics Now Poll: September 1996\", &lt;a href=\"https://doi.org/10.25940/ROPER-31106707\" target=\"_blank\"&gt;https://doi.org/10.25940/ROPER-31106707&lt;/a&gt;, Roper Center for Public Opinion Research\n10                                                                                                          Politics Now, 2025, \"Politics Now Poll: October 1996\", &lt;a href=\"https://doi.org/10.25940/ROPER-31106688\" target=\"_blank\"&gt;https://doi.org/10.25940/ROPER-31106688&lt;/a&gt;, Roper Center for Public Opinion Research\n   identifier_of_dataverse\n1                     &lt;NA&gt;\n2                     &lt;NA&gt;\n3                     &lt;NA&gt;\n4                     demo\n5                     &lt;NA&gt;\n6                CSDA_TEST\n7           roperharvested\n8           roperharvested\n9           roperharvested\n10          roperharvested\n                                          name_of_dataverse\n1                                                      &lt;NA&gt;\n2                                                      &lt;NA&gt;\n3                                                      &lt;NA&gt;\n4                                            Demo Dataverse\n5                                                      &lt;NA&gt;\n6        Czech Social Science Data Archive - TEST DATAVERSE\n7  Harvesting Roper Center Datasets (this is an experiment)\n8  Harvesting Roper Center Datasets (this is an experiment)\n9  Harvesting Roper Center Datasets (this is an experiment)\n10 Harvesting Roper Center Datasets (this is an experiment)\n                                                                                                                                                                                                        citation\n1                                                                                                                                                                                                           &lt;NA&gt;\n2                                                                                                                                                                                                           &lt;NA&gt;\n3                                                                                                                                                                                                           &lt;NA&gt;\n4                                                                                                                   Fahim, Omer, 2024, \"Dataset-Omer-1\", https://doi.org/10.70122/FK2/RXCK22, Demo Dataverse, V2\n5                                                                                                                                                                                                           &lt;NA&gt;\n6  Lyons, Pat; Hamplová, Dana; Röschová, Michaela; Kudrnáč, Aleš; Hrubá, Lucie, 2024, \"Czech Household Panel Survey wave 3 - time-use diaries (adults)\", https://doi.org/10.70122/FK2/BSEQZI, Demo Dataverse, V1\n7                                                                          Politics Now, 2025, \"Politics Now Poll: July 1996\", https://doi.org/10.25940/ROPER-31106695, Roper Center for Public Opinion Research\n8                                                                     Politics Now, 2025, \"Politics Now Poll: September 1996\", https://doi.org/10.25940/ROPER-31106705, Roper Center for Public Opinion Research\n9                                                                     Politics Now, 2025, \"Politics Now Poll: September 1996\", https://doi.org/10.25940/ROPER-31106707, Roper Center for Public Opinion Research\n10                                                                      Politics Now, 2025, \"Politics Now Poll: October 1996\", https://doi.org/10.25940/ROPER-31106688, Roper Center for Public Opinion Research\n              storageIdentifier        subjects fileCount versionId\n1                          &lt;NA&gt;            NULL        NA        NA\n2                          &lt;NA&gt;            NULL        NA        NA\n3                          &lt;NA&gt;            NULL        NA        NA\n4      s3://10.70122/FK2/RXCK22 Social Sciences         0    273564\n5                          &lt;NA&gt;            NULL        NA        NA\n6      s3://10.70122/FK2/BSEQZI Social Sciences         0    274306\n7  s3://10.25940/roper-31106695                         0    321192\n8  s3://10.25940/roper-31106705                         0    321773\n9  s3://10.25940/roper-31106707                         0    321819\n10 s3://10.25940/roper-31106688                         0    321973\n   versionState majorVersion minorVersion            createdAt\n1          &lt;NA&gt;           NA           NA                 &lt;NA&gt;\n2          &lt;NA&gt;           NA           NA                 &lt;NA&gt;\n3          &lt;NA&gt;           NA           NA                 &lt;NA&gt;\n4      RELEASED            2            0 2024-11-05T18:57:22Z\n5          &lt;NA&gt;           NA           NA                 &lt;NA&gt;\n6      RELEASED            1            1 2024-11-11T13:37:31Z\n7      RELEASED           NA           NA 2025-07-01T20:25:31Z\n8      RELEASED           NA           NA 2025-07-01T20:26:26Z\n9      RELEASED           NA           NA 2025-07-01T20:26:30Z\n10     RELEASED           NA           NA 2025-07-01T20:26:45Z\n              updatedAt\n1                  &lt;NA&gt;\n2                  &lt;NA&gt;\n3                  &lt;NA&gt;\n4  2024-11-18T15:32:02Z\n5                  &lt;NA&gt;\n6  2024-12-22T13:18:13Z\n7  2025-07-01T20:25:31Z\n8  2025-07-01T20:26:26Z\n9  2025-07-01T20:26:30Z\n10 2025-07-01T20:26:45Z\n                                                                      contacts\n1                                                                         NULL\n2                                                                         NULL\n3                                                                         NULL\n4                                              Fahim, Omer, Harvard University\n5                                                                         NULL\n6  Röschová, Michaela, Institute of Sociology of the Czech Academy of Sciences\n7                                                                           , \n8                                                                           , \n9                                                                           , \n10                                                                          , \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  publications\n1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         NULL\n2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         NULL\n3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         NULL\n4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             McGhee, Eric, Seth Masket, Boris Shor, Steven Rogers, and Nolan McCarty. 2014. “A Primary Cause of Partisanship? Nomination Systems and Legislator Ideology.” &lt;i&gt;American Journal of Political Science&lt;/i&gt; 58 (2): 337–51., http://dx.doi.org/10.1111/ajps.12070\n5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         NULL\n6  Kudrnáč, A., Eger. A. M., Hjerm, M. 2023. Scapegoating Immigrants in Times of Personal and Collective Crises: Results from a Czech Panel Study. International Migration Review 0(0)1-20, Kudrnáčová M, Kudrnáč A (2023) Better sleep, better life? testing the role of sleep on quality of life. PLoS ONE 18(3): e0282085. https://doi.org/10.1371/journal.pone.0282085, Sládek, M., Klusáček, J., Hamplová, D., &amp; Sumová, A. 2023. Population-representative study reveals cardiovascular and metabolic disease biomarkers associated with misaligned sleep schedules. Sleep, https://doi.org/10.1093/sleep/zsad037, Raudenská, P., D. Hamplová. 2022. The Effect of Parents' Education and Income on Children's School Performance: the Mediating Role of the Family Environment and Children's Characteristics, and Gender Differences. Polish Sociological Review 218: 247-271, https://doi.org/10.26412/psr218.06, Kudrnáčová, M., D. Hamplová. 2022. Social Jetlag in the Context of Work and Family. Sociológia / Slovak Sociological Review: 54(4): 295-324. 0049-1225. DOI: https://doi.org/10.31577/sociologia.2022.54.4.11, Kudrnáč, A., J. Klusáček. 2022. Dočasný nárůst důvěry ve vládu a dodržování protiepidemických opatření na počátku koronavirové krize. Sociologický časopis / Czech Sociological Review 58(2):119-150, https://doi.org/10.13060/csr.2022.016., Hamplová, D., Raudenská, P. 2021. Gender Differences in the Link between Family Scholarly Culture and Parental Educational Aspirations. Sociológia - Slovak Sociological Review 53 (5): 435-462, https://doi.org/10.31577/sociologia.2021.53.5.16, Raudenská, P., K. Bašná. 2021. Individual’s cultural capital: intergenerational transmission, partner effect, or individual merit? Poetics, https://doi.org/10.1016/j.poetic.2021.101575, Kudrnáč, A. 2021. A study of the effects of obesity and poor health on the relationship between distance to the polling station and the probability to vote. Party Politics 27(3):540-551, https://doi.org/10.1177/1354068819867414, Sládek, M., Kudrnáčová Röschová, M., Adámková, V., Hamplová, D., &amp; Sumová, A. 2020. Chronotype assessment via a large scale sociodemographic survey favours yearlong Standard time over Daylight Saving Time in central Europe. Scientific reports, 10(1), 1419, https://doi.org/10.1038/s41598-020-58413-9, Nývlt, Ondřej. 2018. sociodemografické determinanty studijních výsledků a začátku pracovní kariéry v České republice. Demografie 60 (2):111-123, Lux, M., P. Sunega, L. Kážmér. 2018. Intergenerational financial transfers and indirect reciprocity: determinants of the reproduction of homeownership in the postsocialist Czech Republic. Housing Studies, https://doi.org/10.1080/02673037.2018.1541441, Slepičková, Lenka, Petr Fučík. 2018. Využívání předškolního vzdělávání v České republice: Komu chybí místa ve školkách? Sociologický časopis / Czech Sociological Review 54 (1): 35-62, https://doi.org/10.13060/00380288.2018.54.1.395, Hrubá, L. 2017. Sociální determinanty vysokých vzdělanostních očekávání rodičů. Sociológia - Slovak Sociological Review 49 (5): 463-481, https://journals.sagepub.com/doi/10.1177/01979183231177971, https://doi.org/10.1371/journal.pone.0282085, https://doi.org/10.1093/sleep/zsad037, https://doi.org/10.26412/psr218.06, https://doi.org/10.31577/sociologia.2022.54.4.11, https://doi.org/10.13060/csr.2022.016, https://www.sav.sk/index.php?lang=sk&amp;doc=journal-list&amp;part=article_response_page&amp;journal_article_no=26724, https://www.sciencedirect.com/science/article/pii/S0304422X21000590?via%3Dihub, https://journals.sagepub.com/doi/10.1177/1354068819867414, https://doi.org/10.1038/s41598-020-58413-9, https://www.czechdemography.cz/aktuality/demografie-c-2-2018/, https://doi.org/10.1080/02673037.2018.1541441, https://doi.org/10.13060/00380288.2018.54.1.395, https://www.ceeol.com/search/article-detail?id=583362\n7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         NULL\n8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         NULL\n9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         NULL\n10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        NULL\n                                                                       authors\n1                                                                         NULL\n2                                                                         NULL\n3                                                                         NULL\n4                                                                  Fahim, Omer\n5                                                                         NULL\n6  Lyons, Pat, Hamplová, Dana, Röschová, Michaela, Kudrnáč, Aleš, Hrubá, Lucie\n7                                                                 Politics Now\n8                                                                 Politics Now\n9                                                                 Politics Now\n10                                                                Politics Now\n       keywords\n1          NULL\n2          NULL\n3          NULL\n4          NULL\n5          NULL\n6  time budgets\n7          NULL\n8          NULL\n9          NULL\n10         NULL\n                                                                                                          producers\n1                                                                                                              NULL\n2                                                                                                              NULL\n3                                                                                                              NULL\n4                                                                                                              NULL\n5                                                                                                              NULL\n6  Institute of Sociology of the Czech Academy of Sciences, CERGE-EI, Masaryk University, Faculty of Social Studies\n7                                                                                                              NULL\n8                                                                                                              NULL\n9                                                                                                              NULL\n10                                                                                                             NULL\n   geographicCoverage\n1                NULL\n2                NULL\n3                NULL\n4                NULL\n5                NULL\n6      Czech Republic\n7                NULL\n8                NULL\n9                NULL\n10               NULL\n\n$data$count_in_response\n[1] 10\n\n\nView(politics_datasets)를 사용하여 데이터셋을 볼 수 있으며, 이를 통해 관심 있는 것을 기반으로 트리를 확장할 수 있습니다. 항목 위에 마우스를 올린 다음 녹색 화살표가 있는 아이콘을 클릭하여 다른 측면에 집중하는 데 필요한 코드를 얻을 수도 있습니다(그림 7.3).\n\n\n\n\n\n\n그림 7.3: JSON 요소 “items” 위에 마우스를 올린 예시, 녹색 화살표가 있는 아이콘을 클릭하여 해당 요소에 집중하는 코드를 얻을 수 있음\n\n\n\n이것은 관심 있는 데이터셋을 얻는 방법을 알려줍니다.\n\nas_tibble(politics_datasets[[\"data\"]][[\"items\"]])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.2.2 스포티파이\n때로는 API를 중심으로 구축된 R 패키지가 있으며, 이전에 본 것과 유사한 방식으로 상호 작용할 수 있습니다. 예를 들어, spotifyr는 스포티파이 API의 래퍼입니다. API를 사용할 때, 이 경우 spotifyr와 같이 R 패키지로 래핑된 경우에도 액세스가 제공되는 조건을 읽는 것이 중요합니다.\n스포티파이 API에 액세스하려면 스포티파이 개발자 계정이 필요합니다. 이것은 무료이지만 스포티파이 계정으로 로그인한 다음 개발자 약관에 동의해야 합니다(그림 7.4).\n\n\n\n\n\n\n그림 7.4: 스포티파이 개발자 계정 약관 동의 페이지\n\n\n\n등록 절차를 계속 진행하면, 우리의 경우, 우리가 무엇을 만들고 있는지 “알지 못하므로” 스포티파이는 비상업적 계약을 사용하도록 요구하며 이는 괜찮습니다. 스포티파이 API를 사용하려면 “클라이언트 ID”와 “클라이언트 비밀”이 필요합니다. 이것들은 우리가 우리 자신에게만 간직하고 싶은 것들입니다. 그렇지 않으면 세부 정보를 가진 사람이 우리 개발자 계정을 마치 자신인 것처럼 사용할 수 있기 때문입니다. 이러한 세부 정보를 최소한의 번거로움으로 비밀로 유지하는 한 가지 방법은 “시스템 환경”에 보관하는 것입니다. 이렇게 하면 GitHub에 푸시할 때 포함되지 않아야 합니다. 이렇게 하려면 usethis를 로드하고 사용하여 시스템 환경을 수정합니다. 특히, “.Renviron”이라는 파일이 있으며, 이 파일을 열고 “클라이언트 ID”와 “클라이언트 비밀”을 추가합니다.\n\nedit_r_environ()\n\nedit_r_environ()을 실행하면 “.Renviron” 파일이 열리고 “Spotify 클라이언트 ID”와 “클라이언트 비밀”을 추가할 수 있습니다. spotifyr는 해당 특정 이름을 가진 키를 환경에서 찾기 때문에 동일한 이름을 사용하십시오. 이 책에서는 일반적으로 큰따옴표를 사용하지만 여기서는 작은따옴표를 사용하는 것이 중요합니다.\n\nSPOTIFY_CLIENT_ID = 'PUT_YOUR_CLIENT_ID_HERE'\nSPOTIFY_CLIENT_SECRET = 'PUT_YOUR_SECRET_HERE'\n\n“.Renviron” 파일을 저장한 다음 R을 다시 시작합니다: “Session” \\(\\rightarrow\\) “Restart R”. 이제 필요에 따라 “Spotify 클라이언트 ID”와 “클라이언트 비밀”을 사용할 수 있습니다. 그리고 해당 세부 정보가 인수로 필요한 함수는 다시 명시적으로 지정하지 않아도 작동합니다.\n이를 시험해 보기 위해 spotifyr를 설치하고 로드합니다. get_artist_audio_features()를 사용하여 영국 록 밴드인 라디오헤드에 대한 일부 정보를 가져와 저장합니다. 필수 인수 중 하나는 authorization이지만, 기본적으로 “.Renviron” 파일을 보도록 설정되어 있으므로 여기서는 지정할 필요가 없습니다.\n\nradiohead &lt;- get_artist_audio_features(\"radiohead\")\nsaveRDS(radiohead, \"radiohead.rds\")\n\n\nradiohead &lt;- readRDS(\"radiohead.rds\")\n\n노래를 기반으로 다양한 정보를 사용할 수 있습니다. 시간이 지남에 따라 노래가 길어지고 있는지 확인하는 데 관심이 있을 수 있습니다(그림 7.5). ?sec-static-communication의 지침에 따라, 동시에 앨범별 요약 통계를 전달하기 위해 상자 그림을 추가로 사용하는 좋은 기회입니다.\n\nradiohead &lt;- as_tibble(radiohead)\n\nradiohead |&gt;\n  mutate(album_release_date = ymd(album_release_date)) |&gt;\n  ggplot(aes(\n    x = album_release_date,\n    y = duration_ms,\n    group = album_release_date\n  )) +\n  geom_boxplot() +\n  geom_jitter(alpha = 0.5, width = 0.3, height = 0) +\n  theme_minimal() +\n  labs(\n    x = \"Album release date\",\n    y = \"Duration of song (ms)\"\n  )\n\n\n\n\n\n\n\n그림 7.5: 스포티파이에서 수집한 라디오헤드 각 노래의 길이, 시간에 따른 변화\n\n\n\n\n\n스포티파이가 각 노래에 대해 제공하는 흥미로운 변수 중 하나는 “valence”입니다. 스포티파이 문서는 이것을 0과 1 사이의 측정값으로 설명하며, 값이 높을수록 더 긍정적인 트랙의 “음악적 긍정성”을 나타냅니다. 라디오헤드, 미국 록 밴드 더 내셔널, 미국 가수 테일러 스위프트와 같은 몇몇 아티스트 간의 시간에 따른 valence를 비교하는 데 관심이 있을 수 있습니다.\n먼저, 데이터를 수집해야 합니다.\n\ntaylor_swift &lt;- get_artist_audio_features(\"taylor swift\")\nthe_national &lt;- get_artist_audio_features(\"the national\")\n\nsaveRDS(taylor_swift, \"taylor_swift.rds\")\nsaveRDS(the_national, \"the_national.rds\")\n\n그런 다음 그것들을 하나로 모아 그래프를 만듭니다(그림 7.6). 이것은 테일러 스위프트와 라디오헤드가 시간이 지남에 따라 대체로 그들의 valence 수준을 유지한 반면, 더 내셔널은 그들의 valence 수준을 감소시켰음을 보여주는 것 같습니다.\n\nrbind(taylor_swift, the_national, radiohead) |&gt;\n  select(artist_name, album_release_date, valence) |&gt;\n  mutate(album_release_date = ymd(album_release_date)) |&gt; \n  ggplot(aes( x = album_release_date, y = valence, color = artist_name)) +\n  geom_point(alpha = 0.3) +\n  geom_smooth() +\n  theme_minimal() +\n  facet_wrap(facets = vars(artist_name), dir = \"v\") +\n  labs(\n    x = \"Album release date\",\n    y = \"Valence\",\n    color = \"Artist\"\n  ) +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n그림 7.6: 라디오헤드, 테일러 스위프트, 더 내셔널의 시간에 따른 valence 비교\n\n\n\n\n\n이 모든 정보가 거의 노력이나 비용 없이 사용할 수 있는 세상에 살고 있다는 것이 얼마나 놀라운 일입니까! 그리고 데이터를 수집한 후에는 할 수 있는 일이 많습니다. 예를 들어, (kaylinpavlik는?) 확장된 데이터셋을 사용하여 음악 장르를 분류하고 (theeconomistonspotify는?) 언어가 스포티파이에서 음악 스트리밍과 어떻게 관련되는지 살펴봅니다. 이러한 데이터를 수집하는 우리의 능력은 과거에 실험적으로 고려해야 했던 질문에 답할 수 있게 해줍니다. 예를 들어, (salganik2006experimental은?) 히트곡을 만드는 사회적 측면을 분석하기 위해 실험 데이터를 사용해야 했지만, 이제는 관찰 데이터를 사용할 수 있습니다.\n그렇긴 하지만, valence가 무엇을 측정하려고 하는지 생각해 볼 가치가 있습니다. 스포티파이 문서에는 그것이 어떻게 만들어졌는지에 대한 정보가 거의 없습니다. 한 숫자가 노래가 얼마나 긍정적인지를 완전히 나타낼 수 있다는 것은 의심스럽습니다. 그리고 스포티파이에 없거나 공개되지 않은 이 아티스트들의 노래는 어떻습니까? 이것은 측정과 표본 추출이 데이터로 이야기를 하는 모든 측면에 어떻게 스며드는지에 대한 좋은 예입니다.",
    "crumbs": [
      "획득",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>API, 스크래핑 및 파싱</span>"
    ]
  },
  {
    "objectID": "07-gather_ko.html#웹-스크래핑",
    "href": "07-gather_ko.html#웹-스크래핑",
    "title": "7  API, 스크래핑 및 파싱",
    "section": "7.3 웹 스크래핑",
    "text": "7.3 웹 스크래핑\n\n7.3.1 원칙\n웹 스크래핑은 웹사이트에서 데이터를 얻는 방법입니다. 브라우저를 사용하여 웹사이트로 이동한 다음 사본을 저장하는 대신, 우리를 위해 그렇게 하는 코드를 작성합니다. 이것은 우리에게 상당한 데이터를 열어주지만, 다른 한편으로는 이러한 목적으로 제공되는 데이터가 일반적으로 아닙니다. 이는 존중하는 것이 특히 중요하다는 것을 의미합니다. 일반적으로 불법은 아니지만, 웹 스크래핑의 합법성에 대한 구체적인 내용은 관할권과 우리가 무엇을 하고 있는지에 따라 다르므로 유념하는 것도 중요합니다. 우리의 사용이 상업적으로 경쟁적이지 않더라도, 특히 우려되는 것은 데이터 재게시를 허용하지 않을 수 있는 서비스 약관을 존중해야 하는 필요성과 우리 작업이 재현 가능해야 하는 필요성 사이의 갈등입니다 (Luscombe, Dick, 와/과 Walby 2021).\n개인 정보 보호는 종종 재현성보다 우선합니다. 또한 웹사이트에서 공개적으로 사용할 수 있는 데이터와 스크랩, 정리 및 준비되어 공개적으로 공개되는 데이터셋 사이에는 상당한 차이가 있습니다. 예를 들어, (kirkegaard2016okcupid는?) 공개적으로 사용 가능한 OKCupid 프로필을 스크랩한 다음 결과 데이터셋을 쉽게 사용할 수 있도록 했습니다 (Hackett 2016). (zimmer2018addressing은?) “피해 최소화”, “정보에 입각한 동의” 및 데이터셋에 있는 사람들이 “개인 정보 보호 및 기밀 유지”를 보장하는 것을 포함하여 간과된 몇 가지 중요한 고려 사항을 자세히 설명합니다. OKCupid가 데이터를 공개했다고 말하는 것은 정확하지만, 그들은 특정 맥락에서 그렇게 했고, 그들의 데이터가 스크랩되었을 때 그 맥락이 변경되었습니다.\n\n\n\n\n\n\n아, 우리가 그것에 대해 좋은 데이터를 가지고 있다고 생각하는군요!\n\n\n\n경찰과 사회 간의 신뢰가 필요하기 때문에 경찰 폭력은 특히 우려됩니다. 좋은 데이터 없이는 경찰서를 책임지게 하거나 문제가 있는지 알기 어렵지만, 데이터를 얻는 것은 어렵습니다 (Thomson-DeVeaux, Bronner, 와/과 Sharma 2021). 근본적인 문제는 폭력을 초래하는 만남을 데이터셋으로 쉽게 단순화할 방법이 없다는 것입니다. 두 가지 인기 있는 데이터셋은 웹 스크래핑을 사용합니다.\n\n“경찰 폭력 매핑”; 그리고\n“치명적인 무력 데이터베이스”.\n\n(Bor2018은?) “경찰 폭력 매핑”을 사용하여 흑인 미국인, 특히 비무장 상태에서의 경찰 살해를 조사하고 흑인 미국인의 정신 건강에 상당한 영향을 미친다는 것을 발견했습니다. (Nix2020과?) 같은 논문에 대한 응답은 데이터셋의 코딩에 특별한 우려를 가지고 있으며, 재코딩 후 다른 결론을 도출합니다. 코딩 차이의 예는 장난감 총기로 살해된 개인을 “무장” 또는 “비무장”으로 코딩해야 하는지 여부와 같이 맥락과 사용법에 따라 달라지기 때문에 대답할 수 없는 질문입니다. 별도의 범주를 원할 수 있지만, 정량적 데이터셋을 구성하려면 일부 단순화가 필요합니다. 워싱턴 포스트는 “치명적인 무력 데이터베이스”를 사용하여 많은 기사를 작성합니다 (The Washington Post 2023). (washpostfatalforcemethods는?) 그들의 방법론과 표준화의 과제를 설명합니다. (Comer2022는?) 데이터셋을 비교하고 유사점을 발견하지만, 데이터셋이 다른 방식을 문서화합니다.\n\n\n웹 스크래핑은 귀중한 데이터 소스입니다. 그러나 일반적으로 다른 목표를 달성하려는 사람의 부산물로 생성될 수 있는 데이터셋입니다. 그리고 웹 스크래핑은 웹사이트 호스트에게 비용을 부과하므로 가능한 한 이를 줄여야 합니다. 예를 들어, 소매업체는 제품과 가격이 있는 웹사이트를 가질 수 있습니다. 그것은 데이터 소스로 의도적으로 만들어진 것이 아니지만, 우리는 그것을 스크랩하여 데이터셋을 만들 수 있습니다. 다음 원칙은 웹 스크래핑을 안내하는 데 유용할 수 있습니다.\n\n피하십시오. 가능하면 API를 사용하십시오.\n그들의 욕구를 따르십시오. 일부 웹사이트에는 스크레이퍼가 무엇을 하는 데 편안한지에 대한 정보가 포함된 “robots.txt” 파일이 있습니다. 일반적으로, 존재하는 경우 “robots.txt” 파일은 기본 URL에 “robots.txt”를 추가하여 액세스할 수 있습니다. 예를 들어, https://www.google.com의 “robots.txt” 파일은 https://www.google.com/robots.txt에서 액세스할 수 있습니다. “Disallow:”에 대해 나열된 폴더가 있는지 확인하십시오. 이것들은 웹사이트가 스크랩되기를 원하지 않는 폴더입니다. 그리고 “Crawl-delay:”의 모든 인스턴스도 확인하십시오. 이것은 웹사이트가 방문 사이에 기다리기를 원하는 초 수입니다.\n영향을 줄이십시오.\n\n스크레이퍼의 속도를 늦추십시오. 예를 들어, 매초 웹사이트를 방문하게 하는 대신 sys.sleep()을 사용하여 속도를 늦추십시오. 수백 개의 파일만 필요하다면, 밤새 백그라운드에서 실행하면서 분당 몇 번만 웹사이트를 방문하게 하는 것은 어떻습니까?\n스크레이퍼를 실행하는 시기를 고려하십시오. 예를 들어, 소매업체를 스크랩하는 경우, 더 적은 고객이 사이트를 사용할 가능성이 있는 밤 10시부터 아침까지 스크립트를 실행하도록 설정할 수 있습니다. 마찬가지로, 정부 웹사이트이고 정기적인 월간 릴리스가 있는 경우, 그날을 피하는 것이 예의일 수 있습니다.\n\n필요한 것만 가져가십시오. 예를 들어, 크로아티아에서 가장 큰 10개 도시의 이름만 필요하다면 위키피디아 전체를 스크랩할 필요가 없습니다. 이것은 웹사이트에 미치는 영향을 줄이고, 우리의 행동을 더 쉽게 정당화할 수 있게 해줍니다.\n한 번만 스크랩하십시오. 이것은 스크레이퍼가 어느 시점에서 필연적으로 실패할 때 데이터를 다시 수집할 필요가 없도록 진행하면서 모든 것을 저장해야 함을 의미합니다. 예를 들어, 일반적으로 한 페이지에서 스크레이퍼가 작동하도록 상당한 시간을 할애하지만, 일반적으로 페이지 구조가 어느 시점에서 변경되고 스크레이퍼를 업데이트해야 합니다. 데이터를 얻으면, 수정된 데이터와 별도로 원본, 편집되지 않은 데이터를 저장해야 합니다. 시간이 지남에 따라 데이터가 필요한 경우 다시 돌아가야 하지만, 이것은 불필요하게 페이지를 다시 스크랩하는 것과는 다릅니다.\n스크랩한 페이지를 재게시하지 마십시오(이것은 생성한 데이터셋과 대조됩니다).\n소유권을 갖고 가능하면 허가를 요청하십시오. 최소한 모든 스크립트에는 연락처 정보가 있어야 합니다. 상황에 따라 스크랩하기 전에 허가를 요청하는 것이 가치가 있을 수 있습니다.\n\n\n\n7.3.2 HTML/CSS 필수 사항\n웹 스크래핑은 웹페이지의 기본 구조를 활용하여 가능합니다. 우리는 원하는 데이터를 얻기 위해 HTML/CSS의 패턴을 사용합니다. 기본 HTML/CSS를 보려면 다음 중 하나를 수행할 수 있습니다.\n\n브라우저를 열고 마우스 오른쪽 버튼을 클릭한 다음 “검사”와 같은 것을 선택합니다. 또는\n웹사이트를 저장한 다음 브라우저가 아닌 텍스트 편집기로 엽니다.\n\nHTML/CSS는 일치하는 태그를 기반으로 하는 마크업 언어입니다. 텍스트를 굵게 표시하려면 다음과 같은 것을 사용합니다.\n&lt;b&gt;내 굵은 텍스트&lt;/b&gt;\n마찬가지로, 목록을 원하면 목록을 시작하고 끝내고 각 항목을 표시합니다.\n&lt;ul&gt;\n  &lt;li&gt;웹 스크래핑 배우기&lt;/li&gt;\n  &lt;li&gt;데이터 과학 하기&lt;/li&gt;\n  &lt;li&gt;수익 창출&lt;/li&gt;\n&lt;/ul&gt;\n스크래핑할 때 이러한 태그를 검색합니다.\n시작하려면, 웹사이트에서 일부 HTML을 얻었고, 거기서 이름을 얻고 싶다고 가정해 봅시다. 이름이 굵게 표시되어 있으므로 해당 기능에 집중하여 추출하고 싶습니다.\n\nwebsite_extract &lt;- \"&lt;p&gt;안녕하세요, 저는 &lt;b&gt;로한&lt;/b&gt; 알렉산더입니다.&lt;/p&gt;\"\n\nrvest는 tidyverse의 일부이므로 설치할 필요는 없지만 핵심 부분은 아니므로 로드해야 합니다. 그런 다음 read_html()을 사용하여 데이터를 읽어들입니다.\n\nrohans_data &lt;- read_html(website_extract)\n\nrohans_data\n\n{html_document}\n&lt;html&gt;\n[1] &lt;body&gt;&lt;p&gt;안녕하세요, 저는 &lt;b&gt;로한&lt;/b&gt; 알렉산더입니다.&lt;/p&gt;&lt;/body&gt;\n\n\nrvest가 태그를 찾는 데 사용하는 언어는 “node”이므로 굵은 노드에 집중합니다. 기본적으로 html_elements()는 태그도 반환합니다. html_text()로 텍스트를 추출합니다.\n\nrohans_data |&gt;\n  html_elements(\"b\")\n\n{xml_nodeset (1)}\n[1] &lt;b&gt;로한&lt;/b&gt;\n\nrohans_data |&gt;\n  html_elements(\"b\") |&gt;\n  html_text()\n\n[1] \"로한\"\n\n\n웹 스크래핑은 흥미로운 데이터 소스이며, 이제 몇 가지 예를 살펴보겠습니다. 그러나 이러한 예와 대조적으로, 정보는 일반적으로 한 페이지에 모두 있지 않습니다. 웹 스크래핑은 빠르게 연습이 필요한 어려운 예술 형식이 됩니다. 예를 들어, 인덱스 스크랩과 콘텐츠 스크랩을 구별합니다. 전자는 원하는 콘텐츠가 있는 URL 목록을 작성하기 위한 스크래핑이고, 후자는 해당 URL에서 콘텐츠를 얻기 위한 것입니다. (luscombe2022jumpstarting에서?) 예시를 제공합니다. 웹 스크래핑을 많이 하게 되면 polite (Perepolkin 2022)가 워크플로를 더 잘 최적화하는 데 도움이 될 수 있습니다. 그리고 GitHub Actions를 사용하여 시간이 지남에 따라 더 크고 느린 스크랩을 허용합니다.\n\n\n7.3.3 도서 정보\n이 사례 연구에서는 여기에서 사용할 수 있는 책 목록을 스크랩합니다. 그런 다음 데이터를 정리하고 저자 성의 첫 글자 분포를 살펴봅니다. 위의 예보다 약간 더 복잡하지만 기본 워크플로는 동일합니다. 웹사이트를 다운로드하고, 관심 있는 노드를 찾고, 정보를 추출하고, 정리합니다.\nrvest를 사용하여 웹사이트를 다운로드한 다음 HTML을 탐색하여 관심 있는 측면을 찾습니다. 그리고 tidyverse를 사용하여 데이터셋을 정리합니다. 먼저 웹사이트로 이동한 다음 로컬 사본을 저장해야 합니다.\n\nbooks_data &lt;- read_html(\"https://rohansbooks.com\")\n\nwrite_html(books_data, \"raw_data.html\")\n\n원하는 측면을 얻기 위해 HTML을 탐색해야 합니다. 그런 다음 가능한 한 빨리 데이터를 티블로 가져오려고 노력해야 합니다. 이렇게 하면 dplyr 동사와 tidyverse의 다른 함수를 더 쉽게 사용할 수 있기 때문입니다.\n이것이 익숙하지 않다면 ?sec-r-essentials를 참조하십시오.\n\nbooks_data &lt;- read_html(\"raw_data.html\")\n\n\nbooks_data\n\n{html_document}\n&lt;html&gt;\n[1] &lt;head&gt;\\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] &lt;body&gt;\\n    &lt;h1&gt;Books&lt;/h1&gt;\\n\\n    &lt;p&gt;\\n      This is a list of books that ...\n\n\n데이터를 티블로 가져오려면 먼저 HTML 태그를 사용하여 관심 있는 데이터를 식별해야 합니다. 웹사이트를 보면 목록 항목에 집중해야 한다는 것을 알 수 있습니다(그림 7.7 (a)). 그리고 소스를 보고, 특히 목록을 찾는 데 집중할 수 있습니다(그림 7.7 (b)).\n\n\n\n\n\n\n\n\n\n\n\n(a) 표시된 책 웹사이트\n\n\n\n\n\n\n\n\n\n\n\n(b) 책 웹사이트 상단 및 책 목록에 대한 HTML\n\n\n\n\n\n\n\n그림 7.7: 2022년 6월 16일 기준 책 웹사이트의 화면 캡처\n\n\n\n목록 항목의 태그는 “li”이므로 이를 사용하여 목록에 집중할 수 있습니다.\n\ntext_data &lt;-\n  books_data |&gt;\n  html_elements(\"li\") |&gt;\n  html_text()\n\nall_books &lt;-\n  tibble(books = text_data)\n\nhead(all_books)\n\n# A tibble: 6 × 1\n  books                                                                         \n  &lt;chr&gt;                                                                         \n1 \"\\n        Agassi, Andre, 2009, Open\\n      \"                                 \n2 \"\\n        Cramer, Richard Ben, 1992, What It Takes: The Way to the White Hou…\n3 \"\\n        DeWitt, Helen, 2000, The Last Samurai\\n      \"                     \n4 \"\\n        Gelman, Andrew and Jennifer Hill, 2007, Data Analysis Using Regres…\n5 \"\\n        Halberstam, David, 1972, The Best and the Brightest\\n      \"       \n6 \"\\n        Ignatieff, Michael, 2013, Fire and Ashes: Success and Failure in P…\n\n\n이제 데이터를 정리해야 합니다. 먼저 separate()를 사용하여 제목과 저자를 분리한 다음 저자 및 제목 열을 정리하고 싶습니다. 연도가 있다는 사실을 활용하여 이를 기반으로 분리할 수 있습니다.\n\nall_books &lt;-\n  all_books |&gt;\n  mutate(books = str_squish(books)) |&gt;\n  separate(books, into = c(\"author\", \"title\"), sep = \"\\\\, [[:digit:]]{4}\\\\, \")\n\nhead(all_books)\n\n# A tibble: 6 × 2\n  author                           title                                        \n  &lt;chr&gt;                            &lt;chr&gt;                                        \n1 Agassi, Andre                    Open                                         \n2 Cramer, Richard Ben              What It Takes: The Way to the White House    \n3 DeWitt, Helen                    The Last Samurai                             \n4 Gelman, Andrew and Jennifer Hill Data Analysis Using Regression and Multileve…\n5 Halberstam, David                The Best and the Brightest                   \n6 Ignatieff, Michael               Fire and Ashes: Success and Failure in Polit…\n\n\n마지막으로, 이름의 첫 글자 분포 표를 만들 수 있습니다(표 7.1).\n\nall_books |&gt;\n  mutate(\n    first_letter = str_sub(author, 1, 1)\n  ) |&gt;\n  count(.by = first_letter) |&gt;\n  tt() |&gt; \n  style_tt(j = 1:2, align = \"lr\") |&gt; \n  setNames(c(\"First letter\", \"Number of times\"))\n\n\n\n표 7.1: 책 모음의 저자 이름 첫 글자 분포\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                First letter\n                Number of times\n              \n        \n        \n        \n                \n                  A\n                  1\n                \n                \n                  C\n                  1\n                \n                \n                  D\n                  1\n                \n                \n                  G\n                  1\n                \n                \n                  H\n                  1\n                \n                \n                  I\n                  1\n                \n                \n                  L\n                  1\n                \n                \n                  M\n                  1\n                \n                \n                  P\n                  3\n                \n                \n                  R\n                  1\n                \n                \n                  V\n                  2\n                \n                \n                  W\n                  4\n                \n                \n                  Y\n                  1\n                \n        \n      \n    \n\n\n\n\n\n\n\n\n7.3.4 영국 총리\n이 사례 연구에서는 영국 총리가 태어난 해를 기준으로 얼마나 오래 살았는지에 관심이 있습니다. rvest를 사용하여 위키피디아에서 데이터를 스크랩하고, 정리하고, 그래프를 만들 것입니다. 때때로 웹사이트가 변경됩니다. 이로 인해 이전 프로젝트에서 일부 코드를 빌릴 수 있더라도 많은 스크랩이 대체로 맞춤형이 됩니다. 때때로 좌절감을 느끼는 것은 정상입니다. 끝을 염두에 두고 시작하는 것이 도움이 됩니다.\n이를 위해 시뮬레이션된 데이터를 생성하는 것으로 시작할 수 있습니다. 이상적으로는 각 총리에 대한 행, 이름에 대한 열, 출생 및 사망 연도에 대한 열이 있는 표를 원합니다. 아직 살아 있다면 사망 연도는 비워 둘 수 있습니다. 출생 및 사망 연도는 1700년에서 1990년 사이여야 하고, 사망 연도는 출생 연도보다 커야 한다는 것을 알고 있습니다. 마지막으로, 연도는 정수여야 하고 이름은 문자여야 한다는 것도 알고 있습니다. 대략 다음과 같은 것을 원합니다.\n\nset.seed(853)\n\nsimulated_dataset &lt;-\n  tibble(\n    prime_minister = babynames |&gt;\n      filter(prop &gt; 0.01) |&gt;\n      distinct(name) |&gt;\n      unlist() |&gt;\n      sample(size = 10, replace = FALSE),\n    birth_year = sample(1700:1990, size = 10, replace = TRUE),\n    years_lived = sample(50:100, size = 10, replace = TRUE),\n    death_year = birth_year + years_lived\n  ) |&gt;\n  select(prime_minister, birth_year, death_year, years_lived) |&gt;\n  arrange(birth_year)\n\nsimulated_dataset\n\n# A tibble: 10 × 4\n   prime_minister birth_year death_year years_lived\n   &lt;chr&gt;               &lt;int&gt;      &lt;int&gt;       &lt;int&gt;\n 1 Kevin                1813       1908          95\n 2 Karen                1832       1896          64\n 3 Robert               1839       1899          60\n 4 Bertha               1846       1915          69\n 5 Jennifer             1867       1943          76\n 6 Arthur               1892       1984          92\n 7 Donna                1907       2006          99\n 8 Emma                 1957       2031          74\n 9 Ryan                 1959       2053          94\n10 Tyler                1990       2062          72\n\n\n시뮬레이션된 데이터셋을 생성하는 장점 중 하나는 그룹으로 작업하는 경우 한 사람은 시뮬레이션된 데이터셋을 사용하여 그래프를 만들기 시작할 수 있고, 다른 사람은 데이터를 수집할 수 있다는 것입니다. 그래프 측면에서 우리는 ?fig-pmsgraphexample과 같은 것을 목표로 하고 있습니다.\n\n\n\n\n\n\n그림 7.8: 영국 총리가 얼마나 오래 살았는지 보여주는 계획된 그래프 스케치\n\n\n\n우리는 영국 총리 각자가 얼마나 오래 살았는지에 대한 관심 있는 질문으로 시작합니다. 따라서 데이터 소스를 식별해야 합니다. 각 총리의 출생과 사망에 대한 데이터 소스는 많지만, 우리가 신뢰할 수 있는 것을 원하고, 스크래핑을 할 것이기 때문에 어떤 구조를 가진 것을 원합니다. 영국 총리에 대한 위키피디아 페이지는 이 두 기준을 모두 충족합니다. 인기 있는 페이지이므로 정보가 정확할 가능성이 높고, 데이터는 표로 제공됩니다.\nrvest를 로드한 다음 read_html()을 사용하여 페이지를 다운로드합니다. 로컬에 저장하면 웹사이트가 변경될 경우 재현성을 위해 필요한 사본을 제공하고, 웹사이트를 계속 방문할 필요가 없음을 의미합니다. 그러나 그것은 우리 것이 아니므로 일반적으로 공개적으로 재배포해서는 안 됩니다.\n\nraw_data &lt;-\n  read_html(\n    \"https://en.wikipedia.org/wiki/List_of_prime_ministers_of_the_United_Kingdom\"\n  )\nwrite_html(raw_data, \"pms.html\")\n\n이전 사례 연구와 마찬가지로, 우리가 원하는 데이터에 더 가까이 다가가는 데 도움이 될 수 있는 HTML의 패턴을 찾고 있습니다. 이것은 반복적인 과정이며 시행착오를 포함합니다. 간단한 예라도 시간이 걸립니다.\n한 가지 도움이 될 수 있는 도구는 SelectorGadget입니다. 이를 통해 원하는 요소를 선택하고 선택한 다음 html_element()에 대한 입력을 제공할 수 있습니다(그림 7.9). 기본적으로 SelectorGadget은 CSS 선택기를 사용합니다. 이것들은 원하는 정보의 위치를 지정하는 유일한 방법은 아니며, XPath와 같은 대안을 사용하는 것은 고려할 만한 유용한 옵션이 될 수 있습니다.\n\n\n\n\n\n\n그림 7.9: 2023년 2월 12일 기준 태그를 식별하기 위해 선택기 가젯 사용\n\n\n\n\nraw_data &lt;- read_html(\"pms.html\")\n\n\nparse_data_selector_gadget &lt;-\n  raw_data |&gt;\n  html_element(\".wikitable\") |&gt;\n  html_table()\n\nhead(parse_data_selector_gadget)\n\n# A tibble: 6 × 11\n  Portrait Portrait   Prime ministerOffice(L…¹ `Term of office` `Term of office`\n  &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt;                    &lt;chr&gt;            &lt;chr&gt;           \n1 Portrait \"Portrait\" Prime ministerOffice(Li… start            end             \n2 ​         \"\"         Robert Walpole[27]MP fo… 3 April1721      11 February1742 \n3 ​         \"\"         Robert Walpole[27]MP fo… 3 April1721      11 February1742 \n4 ​         \"\"         Robert Walpole[27]MP fo… 3 April1721      11 February1742 \n5 ​         \"\"         Robert Walpole[27]MP fo… 3 April1721      11 February1742 \n6 ​         \"\"         Spencer Compton[28]1st … 16 February1742  2 July1743      \n# ℹ abbreviated name: ¹​`Prime ministerOffice(Lifespan)`\n# ℹ 6 more variables: `Term of office` &lt;chr&gt;, `Mandate[a]` &lt;chr&gt;,\n#   `Ministerial offices held as prime minister` &lt;chr&gt;, Party &lt;chr&gt;,\n#   Government &lt;chr&gt;, MonarchReign &lt;chr&gt;\n\n\n이 경우 필요하지 않은 열이 많고 일부 중복된 행이 있습니다.\n\nparsed_data &lt;-\n  parse_data_selector_gadget |&gt; \n  clean_names() |&gt; \n  rename(raw_text = prime_minister_office_lifespan) |&gt; \n  select(raw_text) |&gt; \n  filter(raw_text != \"Prime ministerOffice(Lifespan)\") |&gt; \n  distinct() \n\nhead(parsed_data)\n\n# A tibble: 6 × 1\n  raw_text                                                \n  &lt;chr&gt;                                                   \n1 Robert Walpole[27]MP for King's Lynn(1676–1745)         \n2 Spencer Compton[28]1st Earl of Wilmington(1673–1743)    \n3 Henry Pelham[29]MP for Sussex(1694–1754)                \n4 Thomas Pelham-Holles[30]1st Duke of Newcastle(1693–1768)\n5 William Cavendish[31]4th Duke of Devonshire(1720–1764)  \n6 Thomas Pelham-Holles[32]1st Duke of Newcastle(1693–1768)\n\n\n이제 파싱된 데이터가 있으므로 원하는 것과 일치하도록 정리해야 합니다. 이름 열과 출생 연도 및 사망 연도 열이 필요합니다. 이름과 날짜가 괄호로 구분되어 있다는 사실을 활용하기 위해 separate()를 사용합니다. str_extract()의 인수는 정규식입니다. 4자리 숫자가 연속으로 나오고, 대시가 나오고, 다시 4자리 숫자가 연속으로 나오는 것을 찾습니다. 아직 살아있는 총리의 경우 약간 다른 정규식을 사용합니다.\n\ninitial_clean &lt;-\n  parsed_data |&gt;\n  separate(\n    raw_text, into = c(\"name\", \"not_name\"), sep = \"\\\\[\", extra = \"merge\",\n  ) |&gt; \n  mutate(date = str_extract(not_name, \"[[:digit:]]{4}–[[:digit:]]{4}\"),\n         born = str_extract(not_name, \"born[[:space:]][[:digit:]]{4}\")\n         ) |&gt;\n  select(name, date, born)\n  \nhead(initial_clean)\n\n# A tibble: 6 × 3\n  name                 date      born \n  &lt;chr&gt;                &lt;chr&gt;     &lt;chr&gt;\n1 Robert Walpole       1676–1745 &lt;NA&gt; \n2 Spencer Compton      1673–1743 &lt;NA&gt; \n3 Henry Pelham         1694–1754 &lt;NA&gt; \n4 Thomas Pelham-Holles 1693–1768 &lt;NA&gt; \n5 William Cavendish    1720–1764 &lt;NA&gt; \n6 Thomas Pelham-Holles 1693–1768 &lt;NA&gt; \n\n\n마지막으로 열을 정리해야 합니다.\n\ncleaned_data &lt;-\n  initial_clean |&gt;\n  separate(date, into = c(\"birth\", \"died\"), \n           sep = \"–\") |&gt;   # 사망한 총리는 출생 및 사망 연도가\n  # 하이픈으로 구분되지만, 하이픈이 약간\n  # 이상한 유형의 하이픈인 것 같으므로 복사/붙여넣기해야 합니다.\n  mutate(\n    born = str_remove_all(born, \"born[[:space:]]\"),\n    birth = if_else(!is.na(born), born, birth)\n  ) |&gt; # 살아있는 총리는 약간 다른 형식을 가집니다.\n  select(-born) |&gt; \n  rename(born = birth) |&gt; \n  mutate(across(c(born, died), as.integer)) |&gt; \n  mutate(Age_at_Death = died - born) |&gt; \n  distinct() # 일부 총리는 두 번 재임했습니다.\n\nhead(cleaned_data)\n\n# A tibble: 6 × 4\n  name                  born  died Age_at_Death\n  &lt;chr&gt;                &lt;int&gt; &lt;int&gt;        &lt;int&gt;\n1 Robert Walpole        1676  1745           69\n2 Spencer Compton       1673  1743           70\n3 Henry Pelham          1694  1754           60\n4 Thomas Pelham-Holles  1693  1768           75\n5 William Cavendish     1720  1764           44\n6 John Stuart           1713  1792           79\n\n\n우리 데이터셋은 처음에 원했던 것과 비슷해 보입니다(표 7.2).\n\ncleaned_data |&gt;\n  head() |&gt;\n  tt() |&gt; \n  style_tt(j = 1:4, align = \"lrrr\") |&gt; \n  setNames(c(\"Prime Minister\", \"Birth year\", \"Death year\", \"Age at death\"))\n\n\n\n표 7.2: 사망 당시 나이별 영국 총리\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Prime Minister\n                Birth year\n                Death year\n                Age at death\n              \n        \n        \n        \n                \n                  Robert Walpole\n                  1676\n                  1745\n                  69\n                \n                \n                  Spencer Compton\n                  1673\n                  1743\n                  70\n                \n                \n                  Henry Pelham\n                  1694\n                  1754\n                  60\n                \n                \n                  Thomas Pelham-Holles\n                  1693\n                  1768\n                  75\n                \n                \n                  William Cavendish\n                  1720\n                  1764\n                  44\n                \n                \n                  John Stuart\n                  1713\n                  1792\n                  79\n                \n        \n      \n    \n\n\n\n\n\n\n이 시점에서 우리는 각 총리가 얼마나 오래 살았는지 보여주는 그래프를 만들고 싶습니다(그림 7.10). 아직 살아 있다면 끝까지 실행되기를 원하지만, 다르게 색칠하고 싶습니다.\n\ncleaned_data |&gt;\n  mutate(\n    still_alive = if_else(is.na(died), \"Yes\", \"No\"),\n    died = if_else(is.na(died), as.integer(2023), died)\n  ) |&gt;\n  mutate(name = as_factor(name)) |&gt;\n  ggplot(\n    aes(x = born, xend = died, y = name, yend = name, color = still_alive)\n    ) +\n  geom_segment() +\n  labs(\n    x = \"Year of birth\", y = \"Prime minister\", color = \"PM is currently alive\"\n    ) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n그림 7.10: 영국 각 총리가 얼마나 오래 살았는지\n\n\n\n\n\n\n\n7.3.5 반복\n텍스트를 데이터로 간주하는 것은 흥미롭고 많은 다른 연구 질문을 탐색할 수 있게 해줍니다. ?sec-text-as-data에서 이를 활용할 것입니다. 많은 가이드에서는 이미 멋지게 서식이 지정된 텍스트 데이터셋이 있다고 가정하지만, 실제로는 거의 그렇지 않습니다. 이 사례 연구에서는 몇 개의 다른 페이지에서 파일을 다운로드합니다. 이미 웹 스크래핑의 두 가지 예를 보았지만, 그것들은 한 페이지에만 집중되어 있었지만, 종종 많은 페이지가 필요합니다. 여기서는 이 반복에 집중할 것입니다. 다운로드를 위해 download.file()을 사용하고, 여러 사이트에 이 함수를 적용하기 위해 purrr를 사용할 것입니다. 해당 패키지는 핵심 tidyverse의 일부이므로 tidyverse를 로드할 때 로드되므로 설치하거나 로드할 필요가 없습니다.\n호주 준비은행(RBA)은 호주의 중앙은행입니다. 은행 간 대출에 사용되는 이자율인 현금 금리를 설정할 책임이 있습니다. 이 이자율은 특히 중요한 이자율이며 경제의 다른 이자율에 큰 영향을 미칩니다. 1년에 네 번—2월, 5월, 8월, 11월—RBA는 통화 정책에 대한 성명을 발표하며, 이는 PDF로 제공됩니다. 이 예에서는 2023년에 발표된 두 개의 성명을 다운로드합니다.\n먼저 필요한 정보가 있는 티블을 설정합니다. URL 구조의 공통점을 활용할 것입니다. 각 상태에 대한 URL과 로컬 파일 이름을 모두 지정해야 합니다.\n\nfirst_bit &lt;- \"https://www.rba.gov.au/publications/smp/2023/\"\nlast_bit &lt;- \"/pdf/overview.pdf\"\n\nstatements_of_interest &lt;-\n  tibble(\n    address =\n      c(\n        paste0(first_bit, \"feb\", last_bit),\n        paste0(first_bit, \"may\", last_bit)\n      ),\n    local_save_name = c(\"2023-02.pdf\", \"2023-05.pdf\")\n    )\n\n\nstatements_of_interest\n\n# A tibble: 2 × 2\n  address                                                        local_save_name\n  &lt;chr&gt;                                                          &lt;chr&gt;          \n1 https://www.rba.gov.au/publications/smp/2023/feb/pdf/overview… 2023-02.pdf    \n2 https://www.rba.gov.au/publications/smp/2023/may/pdf/overview… 2023-05.pdf    \n\n\ndownload.files() 함수를 이 두 문장에 적용하고 싶습니다. 이를 위해 파일을 다운로드하고, 다운로드되었음을 알리고, 정중한 시간 동안 기다린 다음, 다음 파일을 가져오는 함수를 작성합니다.\n\nvisit_download_and_wait &lt;-\n  function(address_to_visit,\n           where_to_save_it_locally) {\n    download.file(url = address_to_visit,\n                  destfile = where_to_save_it_locally)\n    \n    print(paste(\"Done with\", address_to_visit, \"at\", Sys.time()))\n    \n    Sys.sleep(sample(5:10, 1))\n  }\n\n이제 walk2() 함수를 사용하여 URL 및 저장 이름의 티블에 해당 함수를 적용합니다.\n\nwalk2(\n  statements_of_interest$address,\n  statements_of_interest$local_save_name,\n  ~ visit_download_and_wait(.x, .y)\n)\n\n결과적으로 이러한 PDF를 다운로드하여 컴퓨터에 저장했습니다. 이러한 함수를 직접 작성하는 대신 heapsofpapers (Alexander 와/과 Mahfouz 2021)를 사용할 수 있습니다. 여기에는 특히 PDF, CSV 및 txt 파일과 같은 파일 목록을 다운로드하는 데 유용한 다양한 옵션이 포함되어 있습니다. 예를 들어, (collinsalexander는?) 이를 사용하여 수천 개의 PDF를 얻고 COVID-19 연구가 어느 정도 재현 가능한지 추정합니다. 다음 섹션에서는 이를 기반으로 PDF에서 정보를 얻는 방법에 대해 논의합니다.",
    "crumbs": [
      "획득",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>API, 스크래핑 및 파싱</span>"
    ]
  },
  {
    "objectID": "07-gather_ko.html#pdf",
    "href": "07-gather_ko.html#pdf",
    "title": "7  API, 스크래핑 및 파싱",
    "section": "7.4 PDF",
    "text": "7.4 PDF\nPDF 파일은 1990년대에 기술 회사인 Adobe에서 개발했습니다. 문서를 만든 환경이나 문서를 보는 환경에 관계없이 일관된 방식으로 표시되도록 되어 있기 때문에 문서에 유용합니다. iPhone에서 본 PDF는 Android 휴대폰에서 본 것과 동일하게 보여야 하며, Linux 데스크톱에서도 마찬가지입니다. PDF의 한 가지 특징은 텍스트, 사진, 그림 등 다양한 개체를 포함할 수 있다는 것입니다. 그러나 이러한 다양성은 PDF가 데이터로 직접 사용될 수 있는 용량을 제한할 수 있습니다. 데이터는 먼저 PDF에서 추출해야 합니다.\nPDF에서 데이터를 복사하여 붙여넣는 것이 종종 가능합니다. 이것은 PDF에 텍스트나 일반 표만 포함된 경우에 더 가능성이 높습니다. 특히, PDF가 Microsoft Word와 같은 응용 프로그램이나 다른 문서 또는 양식 생성 시스템에 의해 생성된 경우, 텍스트 데이터는 실제로 PDF 내에 텍스트로 저장되기 때문에 종종 이러한 방식으로 추출될 수 있습니다. 우리는 그 경우부터 시작합니다. 그러나 텍스트가 이미지로 저장된 다음 PDF의 일부가 된 경우에는 그렇게 쉽지 않습니다. 이것은 물리적 문서의 스캔이나 사진, 그리고 일부 오래된 문서 준비 소프트웨어를 통해 생성된 PDF의 경우일 수 있습니다. 나중에 그 경우를 살펴보겠습니다.\nAPI와 대조적으로, PDF는 일반적으로 컴퓨터 소비보다는 인간 소비를 위해 생성됩니다. PDF의 좋은 점은 정적이고 일정하다는 것입니다. 그리고 데이터가 사용 가능하다는 것은 좋습니다. 그러나 절충안은 다음과 같습니다.\n\n대규모 데이터를 수행하는 데 그다지 유용하지 않습니다.\nPDF가 어떻게 구성되었는지 모르기 때문에 신뢰할 수 있는지 알 수 없습니다.\n관심 있는 결과를 얻기 위해 데이터를 조작할 수 없습니다.\n\nPDF에서 데이터를 추출할 때 명심해야 할 두 가지 중요한 측면이 있습니다.\n\n끝을 염두에 두고 시작하십시오. 시간 낭비를 제한하기 위해 최종 데이터셋/그래프/논문에서 원하는 것을 계획하고 스케치하십시오.\n간단하게 시작한 다음 반복하십시오. 복잡해야 하는 것을 만드는 가장 빠른 방법은 종종 먼저 간단한 버전을 만든 다음 추가하는 것입니다. PDF의 한 페이지만 작동하도록 시도하거나 심지어 한 줄만 작동하도록 시도하는 것으로 시작하십시오. 그런 다음 거기서부터 반복하십시오.\n\n몇 가지 예를 살펴본 다음, 미국 총 출산율을 주별로 수집하는 사례 연구를 살펴보겠습니다.\n\n7.4.1 제인 에어\n?fig-firstpdfexample는 구텐베르크 프로젝트 (Brontë 1847)에서 가져온 샬럿 브론테의 소설 제인 에어의 첫 문장만으로 구성된 PDF입니다. 여기에서 얻을 수 있습니다. “first_example.pdf”로 저장되었다고 가정하면, pdftools를 설치하고 로드한 후 이 한 페이지 PDF에서 R로 텍스트를 가져옵니다.\n\n\n\n\n\n\n그림 7.11: 제인 에어의 첫 문장\n\n\n\n\nfirst_example &lt;- pdf_text(\"first_example.pdf\")\n\nfirst_example\n\nclass(first_example)\n\n\n\n[1] \"There was no possibility of taking a walk that day.\\n\"\n\n\n[1] \"character\"\n\n\nPDF가 문자 벡터로 올바르게 읽혔음을 알 수 있습니다.\n이제 제인 에어의 처음 몇 단락으로 구성된 약간 더 복잡한 예를 시도해 보겠습니다(그림 7.12). 이제 장 제목도 있습니다.\n\n\n\n\n\n\n그림 7.12: 제인 에어의 처음 몇 단락\n\n\n\n이전과 동일한 함수를 사용합니다.\n\nsecond_example &lt;- pdf_text(\"second_example.pdf\")\nclass(second_example)\nsecond_example\n\n\n\n[1] \"character\"\n\n\n[1] \"CHAPTER I\\nThere was no possibility of taking a walk that day. We had been wandering, indeed, in the\\nleafless shrubbery an hour in the morning; but since dinner (Mrs. Reed, when there was no\\ncompany, dined early) the cold winter wind had brought with it clouds so sombre, and a rain so\\npenetrating, that further out-door exercise was now out of the question.\\n\\nI was glad of it: I never liked long walks, especially on chilly afternoons: dreadful to me was the\\ncoming home in the raw twilight, with nipped fingers and toes, and a heart saddened by the\\nchidings of Bessie, the nurse, and humbled by the consciousness of my physical inferiority to\\nEliza, John, and Georgiana Reed.\\n\\nThe said Eliza, John, and Georgiana were now clustered round their mama in the drawing-room:\\nshe lay reclined on a sofa by the fireside, and with her darlings about her (for the time neither\\nquarrelling nor crying) looked perfectly happy. Me, she had dispensed from joining the group;\\nsaying, “She regretted to be under the necessity of keeping me at a distance; but that until she\\nheard from Bessie, and could discover by her own observation, that I was endeavouring in good\\nearnest to acquire a more sociable and childlike disposition, a more attractive and sprightly\\nmanner—something lighter, franker, more natural, as it were—she really must exclude me from\\nprivileges intended only for contented, happy, little children.”\\n\\n“What does Bessie say I have done?” I asked.\\n\\n“Jane, I don’t like cavillers or questioners; besides, there is something truly forbidding in a child\\ntaking up her elders in that manner. Be seated somewhere; and until you can speak pleasantly,\\nremain silent.”\\n\\nA breakfast-room adjoined the drawing-room, I slipped in there. It contained a bookcase: I soon\\npossessed myself of a volume, taking care that it should be one stored with pictures. I mounted\\ninto the window-seat: gathering up my feet, I sat cross-legged, like a Turk; and, having drawn the\\nred moreen curtain nearly close, I was shrined in double retirement.\\n\\nFolds of scarlet drapery shut in my view to the right hand; to the left were the clear panes of\\nglass, protecting, but not separating me from the drear November day. At intervals, while\\nturning over the leaves of my book, I studied the aspect of that winter afternoon. Afar, it offered\\na pale blank of mist and cloud; near a scene of wet lawn and storm-beat shrub, with ceaseless\\nrain sweeping away wildly before a long and lamentable blast.\\n\"\n\n\n다시, 우리는 문자 벡터를 가지고 있습니다. 각 줄의 끝은 “\\n”으로 표시되지만, 그 외에는 꽤 좋아 보입니다. 마지막으로, 처음 두 페이지를 고려합니다.\n\nthird_example &lt;- pdf_text(\"third_example.pdf\")\nclass(third_example)\nthird_example\n\n\n\n[1] \"character\"\n\n\n[1] \"CHAPTER I\\nThere was no possibility of taking a walk that day. We had been wandering, indeed, in the\\nleafless shrubbery an hour in the morning; but since dinner (Mrs. Reed, when there was no\\ncompany, dined early) the cold winter wind had brought with it clouds so sombre, and a rain so\\npenetrating, that further out-door exercise was now out of the question.\\n\\nI was glad of it: I never liked long walks, especially on chilly afternoons: dreadful to me was the\\ncoming home in the raw twilight, with nipped fingers and toes, and a heart saddened by the\\nchidings of Bessie, the nurse, and humbled by the consciousness of my physical inferiority to\\nEliza, John, and Georgiana Reed.\\n\\nThe said Eliza, John, and Georgiana were now clustered round their mama in the drawing-room:\\nshe lay reclined on a sofa by the fireside, and with her darlings about her (for the time neither\\nquarrelling nor crying) looked perfectly happy. Me, she had dispensed from joining the group;\\nsaying, “She regretted to be under the necessity of keeping me at a distance; but that until she\\nheard from Bessie, and could discover by her own observation, that I was endeavouring in good\\nearnest to acquire a more sociable and childlike disposition, a more attractive and sprightly\\nmanner—something lighter, franker, more natural, as it were—she really must exclude me from\\nprivileges intended only for contented, happy, little children.”\\n\\n“What does Bessie say I have done?” I asked.\\n\\n“Jane, I don’t like cavillers or questioners; besides, there is something truly forbidding in a child\\ntaking up her elders in that manner. Be seated somewhere; and until you can speak pleasantly,\\nremain silent.”\\n\\nA breakfast-room adjoined the drawing-room, I slipped in there. It contained a bookcase: I soon\\npossessed myself of a volume, taking care that it should be one stored with pictures. I mounted\\ninto the window-seat: gathering up my feet, I sat cross-legged, like a Turk; and, having drawn the\\nred moreen curtain nearly close, I was shrined in double retirement.\\n\\nFolds of scarlet drapery shut in my view to the right hand; to the left were the clear panes of\\nglass, protecting, but not separating me from the drear November day. At intervals, while\\nturning over the leaves of my book, I studied the aspect of that winter afternoon. Afar, it offered\\na pale blank of mist and cloud; near a scene of wet lawn and storm-beat shrub, with ceaseless\\nrain sweeping away wildly before a long and lamentable blast.\\n\\nI returned to my book—Bewick’s History of British Birds: the letterpress thereof I cared little\\nfor, generally speaking; and yet there were certain introductory pages that, child as I was, I could\\nnot pass quite as a blank. They were those which treat of the haunts of sea-fowl; of “the solitary\\nrocks and promontories” by them only inhabited; of the coast of Norway, studded with isles from\\nits southern extremity, the Lindeness, or Naze, to the North Cape—\\n\\n“Where the Northern Ocean, in vast whirls,\\nBoils round the naked, melancholy isles\\n\"\n[2] \"Of farthest Thule; and the Atlantic surge\\nPours in among the stormy Hebrides.”\\n\\nNor could I pass unnoticed the suggestion of the bleak shores of Lapland, Siberia, Spitzbergen,\\nNova Zembla, Iceland, Greenland, with “the vast sweep of the Arctic Zone, and those forlorn\\nregions of dreary space,—that reservoir of frost and snow, where firm fields of ice, the\\naccumulation of centuries of winters, glazed in Alpine heights above heights, surround the pole,\\nand concentre the multiplied rigours of extreme cold.” Of these death-white realms I formed an\\nidea of my own: shadowy, like all the half-comprehended notions that float dim through\\nchildren’s brains, but strangely impressive. The words in these introductory pages connected\\nthemselves with the succeeding vignettes, and gave significance to the rock standing up alone in\\na sea of billow and spray; to the broken boat stranded on a desolate coast; to the cold and ghastly\\nmoon glancing through bars of cloud at a wreck just sinking.\\n\\nI cannot tell what sentiment haunted the quite solitary churchyard, with its inscribed headstone;\\nits gate, its two trees, its low horizon, girdled by a broken wall, and its newly-risen crescent,\\nattesting the hour of eventide.\\n\\nThe two ships becalmed on a torpid sea, I believed to be marine phantoms.\\n\\nThe fiend pinning down the thief’s pack behind him, I passed over quickly: it was an object of\\nterror.\\n\\nSo was the black horned thing seated aloof on a rock, surveying a distant crowd surrounding a\\ngallows.\\n\\nEach picture told a story; mysterious often to my undeveloped understanding and imperfect\\nfeelings, yet ever profoundly interesting: as interesting as the tales Bessie sometimes narrated on\\nwinter evenings, when she chanced to be in good humour; and when, having brought her ironing-\\ntable to the nursery hearth, she allowed us to sit about it, and while she got up Mrs. Reed’s lace\\nfrills, and crimped her nightcap borders, fed our eager attention with passages of love and\\nadventure taken from old fairy tales and other ballads; or (as at a later period I discovered) from\\nthe pages of Pamela, and Henry, Earl of Moreland.\\n\\nWith Bewick on my knee, I was then happy: happy at least in my way. I feared nothing but\\ninterruption, and that came too soon. The breakfast-room door opened.\\n\\n“Boh! Madam Mope!” cried the voice of John Reed; then he paused: he found the room\\napparently empty.\\n\\n“Where the dickens is she!” he continued. “Lizzy! Georgy! (calling to his sisters) Joan is not\\nhere: tell mama she is run out into the rain—bad animal!”\\n\\n“It is well I drew the curtain,” thought I; and I wished fervently he might not discover my hiding-\\nplace: nor would John Reed have found it out himself; he was not quick either of vision or\\nconception; but Eliza just put her head in at the door, and said at once—\\n\"                                                                                                                                                                                                            \n\n\n첫 번째 페이지는 문자 벡터의 첫 번째 요소이고, 두 번째 페이지는 두 번째 요소라는 점에 유의하십시오. 우리는 직사각형 데이터에 가장 익숙하므로 가능한 한 빨리 해당 형식으로 변환하려고 합니다. 그런 다음 tidyverse의 함수를 사용하여 처리할 수 있습니다.\n먼저 문자 벡터를 티블로 변환하고 싶습니다. 이 시점에서 페이지 번호도 추가하고 싶을 수 있습니다.\n\njane_eyre &lt;- tibble(\n  raw_text = third_example,\n  page_number = c(1:2)\n)\n\n그런 다음 각 줄이 관측치가 되도록 줄을 분리하고 싶습니다. 백슬래시가 특수 문자이므로 이스케이프해야 한다는 것을 기억하면서 “\\n”을 찾아 그렇게 할 수 있습니다.\n\njane_eyre &lt;-\n  separate_rows(jane_eyre, raw_text, sep = \"\\\\n\", convert = FALSE)\n\njane_eyre\n\n# A tibble: 93 × 2\n   raw_text                                                          page_number\n   &lt;chr&gt;                                                                   &lt;int&gt;\n 1 \"CHAPTER I\"                                                                 1\n 2 \"There was no possibility of taking a walk that day. We had been…           1\n 3 \"leafless shrubbery an hour in the morning; but since dinner (Mr…           1\n 4 \"company, dined early) the cold winter wind had brought with it …           1\n 5 \"penetrating, that further out-door exercise was now out of the …           1\n 6 \"\"                                                                          1\n 7 \"I was glad of it: I never liked long walks, especially on chill…           1\n 8 \"coming home in the raw twilight, with nipped fingers and toes, …           1\n 9 \"chidings of Bessie, the nurse, and humbled by the consciousness…           1\n10 \"Eliza, John, and Georgiana Reed.\"                                          1\n# ℹ 83 more rows\n\n\n\n\n7.4.2 미국의 총 출산율\n미국 보건복지부 생명 통계 보고서는 각 주의 총 출산율(TFR)에 대한 정보를 제공합니다. 여성이 생식 기간 동안 현재의 연령별 출산율을 경험할 경우 여성 1인당 평균 출생아 수입니다. 데이터는 PDF로 제공됩니다. 위의 접근 방식을 사용하여 데이터를 데이터셋으로 가져올 수 있습니다.\n관심 있는 표는 여기 또는 여기에서 사용할 수 있는 PDF의 40페이지에 있습니다. 관심 있는 열은 “총 출산율”로 표시되어 있습니다(그림 7.13).\n\n\n\n\n\n\n그림 7.13: 2000년의 예시 생명 통계 보고서\n\n\n\nPDF에서 데이터를 가져오는 첫 번째 단계는 결국 원하는 것을 스케치하는 것입니다. PDF에는 일반적으로 상당한 양의 정보가 포함되어 있으므로 필요한 것이 무엇인지 명확히 해야 합니다. 이것은 집중력을 유지하는 데 도움이 되고 범위 확장을 방지하지만, 데이터 확인을 생각할 때도 도움이 됩니다. 우리는 말 그대로 우리가 염두에 두고 있는 것을 종이에 적습니다. 이 경우 필요한 것은 주, 연도 및 총 출산율(TFR)에 대한 열이 있는 표입니다(그림 7.14).\n\n\n\n\n\n\n그림 7.14: 각 미국 주에 대한 TFR의 계획된 데이터셋\n\n\n\n이 PDF의 특정 표에 있는 특정 열에 관심이 있습니다. 불행히도, 다음에 올 것에 대해 마법 같은 것은 없습니다. 이 첫 번째 단계는 온라인에서 PDF를 찾고, 각각에 대한 링크를 알아내고, 관심 있는 페이지와 열 이름을 검색하는 것이 필요합니다. 필요한 세부 정보가 있는 CSV를 만들었고 그것을 읽어들일 수 있습니다.\n\nsummary_tfr_dataset &lt;- read_csv(\n  paste0(\"https://raw.githubusercontent.com/RohanAlexander/\",\n         \"telling_stories/main/inputs/tfr_tables_info.csv\")\n  )\n\n\n\n\n\n표 7.3: TFR 표에 대한 연도 및 관련 데이터\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Year\n                Page\n                Table\n                Column\n                URL\n              \n        \n        \n        \n                \n                  2000\n                  40\n                  10\n                  Total fertility rate\n                  https://www.cdc.gov/nchs/data/nvsr/nvsr50/nvsr50_05.pdf\n                \n        \n      \n    \n\n\n\n\n\n\n먼저 download.file()을 사용하여 PDF를 다운로드하고 저장합니다.\n\ndownload.file(\n  url = summary_tfr_dataset$url[1],\n  destfile = \"year_2000.pdf\"\n)\n\n그런 다음 pdftools의 pdf_text()를 사용하여 PDF를 문자 벡터로 읽어들입니다. 그런 다음 익숙한 동사를 사용할 수 있도록 티블로 변환합니다.\n\ndhs_2000 &lt;- pdf_text(\"year_2000.pdf\")\n\n\ndhs_2000_tibble &lt;- tibble(raw_data = dhs_2000)\n\nhead(dhs_2000_tibble)\n\n# A tibble: 6 × 1\n  raw_data                                                                      \n  &lt;chr&gt;                                                                         \n1 \"Volume 50, Number 5                                                         …\n2 \"2   National Vital Statistics Report, Vol. 50, No. 5, February 12, 2002\\n\\n\\…\n3 \"                                                                            …\n4 \"4   National Vital Statistics Report, Vol. 50, No. 5, February 12, 2002\\n\\n\\…\n5 \"                                                                            …\n6 \"6   National Vital Statistics Report, Vol. 50, No. 5, February 12, 2002\\n\\n …\n\n\n관심 있는 페이지를 가져옵니다(각 페이지는 문자 벡터의 요소이므로 티블의 행임을 기억하십시오).\n\ndhs_2000_relevant_page &lt;-\n  dhs_2000_tibble |&gt;\n  slice(summary_tfr_dataset$page[1])\n\nhead(dhs_2000_relevant_page)\n\n# A tibble: 1 × 1\n  raw_data                                                                      \n  &lt;chr&gt;                                                                         \n1 \"40 National Vital Statistics Report, Vol. 50, No. 5, Revised May 15, 20022\\n…\n\n\n행을 분리하고 핵심 tidyverse의 일부인 tidyr의 separate_rows()를 사용하고 싶습니다.\n\ndhs_2000_separate_rows &lt;-\n  dhs_2000_relevant_page |&gt;\n  separate_rows(raw_data, sep = \"\\\\n\", convert = FALSE)\n\nhead(dhs_2000_separate_rows)\n\n# A tibble: 6 × 1\n  raw_data                                                                      \n  &lt;chr&gt;                                                                         \n1 \"40 National Vital Statistics Report, Vol. 50, No. 5, Revised May 15, 20022\"  \n2 \"\"                                                                            \n3 \"Table 10. Number of births, birth rates, fertility rates, total fertility ra…\n4 \"United States, each State and territory, 2000\"                               \n5 \"[By place of residence. Birth rates are live births per 1,000 estimated popu…\n6 \"estimated in each area; total fertility rates are sums of birth rates for 5-…\n\n\n우리는 사용할 수 있는 패턴을 찾고 있습니다. 콘텐츠의 처음 10줄을 살펴보겠습니다(페이지 상단의 제목 및 페이지 번호와 같은 측면은 무시합니다).\n\ndhs_2000_separate_rows[13:22, ] |&gt;\n  mutate(raw_data = str_remove(raw_data, \"\\\\.{40}\"))\n\n# A tibble: 10 × 1\n   raw_data                                                                     \n   &lt;chr&gt;                                                                        \n 1 \"                                  State                                    …\n 2 \"                                                                           …\n 3 \"                                                                           …\n 4 \"\"                                                                           \n 5 \"\"                                                                           \n 6 \"United States 1 ..............          4,058,814   14.7      67.5      2,1…\n 7 \"\"                                                                           \n 8 \"Alabama .......................           63,299    14.4      65.0      2,0…\n 9 \"Alaska ...........................         9,974    16.0      74.6      2,4…\n10 \"Arizona .........................         85,273    17.5      84.4      2,6…\n\n\n그리고 이제 한 줄만.\n\ndhs_2000_separate_rows[20, ] |&gt;\n  mutate(raw_data = str_remove(raw_data, \"\\\\.{40}\"))\n\n# A tibble: 1 × 1\n  raw_data                                                                      \n  &lt;chr&gt;                                                                         \n1 Alabama .......................           63,299    14.4      65.0      2,021…\n\n\n이보다 더 좋을 수는 없습니다.\n\n주와 데이터를 구분하는 점이 있습니다.\n각 열 사이에 공백이 있습니다.\n\n이제 이것을 열로 분리할 수 있습니다. 먼저, 점이 두 개 이상 있을 때 일치시키고 싶습니다(점은 특수 문자이므로 이스케이프해야 한다는 것을 기억하십시오).\n\ndhs_2000_separate_columns &lt;-\n  dhs_2000_separate_rows |&gt;\n  separate(\n    col = raw_data,\n    into = c(\"state\", \"data\"),\n    sep = \"\\\\.{2,}\",\n    remove = FALSE,\n    fill = \"right\"\n  )\n\ndhs_2000_separate_columns[18:28, ] |&gt;\n  select(state, data)\n\n# A tibble: 11 × 2\n   state                   data                                                 \n   &lt;chr&gt;                   &lt;chr&gt;                                                \n 1 \"United States 1 \"      \"          4,058,814   14.7      67.5      2,130.0  …\n 2 \"\"                       &lt;NA&gt;                                                \n 3 \"Alabama \"              \"           63,299    14.4      65.0      2,021.0   …\n 4 \"Alaska \"               \"         9,974    16.0      74.6      2,437.0      …\n 5 \"Arizona \"              \"         85,273    17.5      84.4      2,652.5     …\n 6 \"Arkansas \"             \"          37,783    14.7      69.1      2,140.0    …\n 7 \"California \"           \"        531,959    15.8      70.7      2,186.0     …\n 8 \"Colorado \"             \"          65,438    15.8      73.1      2,356.5    …\n 9 \"Connecticut \"          \"           43,026    13.0      61.2      1,931.5   …\n10 \"Delaware \"             \"           11,051    14.5      63.5      2,014.0   …\n11 \"District of Columbia \" \"                7,666    14.8      63.0      1,975.…\n\n\n그런 다음 공백을 기준으로 데이터를 분리합니다. 공백 수가 일정하지 않으므로 먼저 stringr의 str_squish()를 사용하여 둘 이상의 공백이 있는 예를 하나로 줄입니다.\n\ndhs_2000_separate_data &lt;-\n  dhs_2000_separate_columns |&gt;\n  mutate(data = str_squish(data)) |&gt;\n  separate(\n    col = data,\n    into = c(\n      \"number_of_births\",\n      \"birth_rate\",\n      \"fertility_rate\",\n      \"TFR\",\n      \"teen_births_all\",\n      \"teen_births_15_17\",\n      \"teen_births_18_19\"\n    ),\n    sep = \"\\\\s\",\n    remove = FALSE\n  )\n\ndhs_2000_separate_data[18:28, ] |&gt;\n  select(-raw_data, -data)\n\n# A tibble: 11 × 8\n   state        number_of_births birth_rate fertility_rate TFR   teen_births_all\n   &lt;chr&gt;        &lt;chr&gt;            &lt;chr&gt;      &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt;          \n 1 \"United Sta… 4,058,814        14.7       67.5           2,13… 48.5           \n 2 \"\"           &lt;NA&gt;             &lt;NA&gt;       &lt;NA&gt;           &lt;NA&gt;  &lt;NA&gt;           \n 3 \"Alabama \"   63,299           14.4       65.0           2,02… 62.9           \n 4 \"Alaska \"    9,974            16.0       74.6           2,43… 42.4           \n 5 \"Arizona \"   85,273           17.5       84.4           2,65… 69.1           \n 6 \"Arkansas \"  37,783           14.7       69.1           2,14… 68.5           \n 7 \"California… 531,959          15.8       70.7           2,18… 48.5           \n 8 \"Colorado \"  65,438           15.8       73.1           2,35… 49.2           \n 9 \"Connecticu… 43,026           13.0       61.2           1,93… 31.9           \n10 \"Delaware \"  11,051           14.5       63.5           2,01… 51.6           \n11 \"District o… 7,666            14.8       63.0           1,97… 80.7           \n# ℹ 2 more variables: teen_births_15_17 &lt;chr&gt;, teen_births_18_19 &lt;chr&gt;\n\n\n이 모든 것이 상당히 좋아 보입니다. 남은 것은 정리하는 것뿐입니다.\n\ndhs_2000_cleaned &lt;-\n  dhs_2000_separate_data |&gt;\n  select(state, TFR) |&gt;\n  slice(18:74) |&gt;\n  drop_na() |&gt; \n  mutate(\n    TFR = str_remove_all(TFR, \",\"),\n    TFR = as.numeric(TFR),\n    state = str_trim(state),\n    state = if_else(state == \"United States 1\", \"Total\", state)\n  )\n\n그리고 모든 주가 있는지와 같은 몇 가지 확인을 실행합니다.\n\nall(state.name %in% dhs_2000_cleaned$state)\n\n[1] TRUE\n\n\n그리고 끝났습니다(표 7.4). 미국 주별 TFR 분포가 상당히 넓다는 것을 알 수 있습니다(그림 7.15). 유타가 가장 높고 버몬트가 가장 낮습니다.\n\ndhs_2000_cleaned |&gt;\n  slice(1:10) |&gt;\n  tt() |&gt; \n  style_tt(j = 1:2, align = \"lr\") |&gt; \n  format_tt(digits = 0, num_mark_big = \",\", num_fmt = \"decimal\") |&gt; \n  setNames(c(\"State\", \"TFR\"))\n\n\n\n표 7.4: 2000-2019년 미국 주별 TFR 데이터셋의 처음 10개 행\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                State\n                TFR\n              \n        \n        \n        \n                \n                  Total\n                  2,130\n                \n                \n                  Alabama\n                  2,021\n                \n                \n                  Alaska\n                  2,437\n                \n                \n                  Arizona\n                  2,652\n                \n                \n                  Arkansas\n                  2,140\n                \n                \n                  California\n                  2,186\n                \n                \n                  Colorado\n                  2,356\n                \n                \n                  Connecticut\n                  1,932\n                \n                \n                  Delaware\n                  2,014\n                \n                \n                  District of Columbia\n                  1,976\n                \n        \n      \n    \n\n\n\n\n\n\n\ndhs_2000_cleaned |&gt; \n  filter(state != \"Total\") |&gt; \n  ggplot(aes(x = TFR, y = fct_reorder(state, TFR))) +\n  geom_point() +\n  theme_classic() +\n  labs(y = \"State\", x = \"Total Fertility Rate\")\n\n\n\n\n\n\n\n그림 7.15: 2000년 미국 주별 TFR 분포\n\n\n\n\n\n(kieransparsing은?) 다른 맥락에서 이 접근 방식을 사용하는 또 다른 예를 제공합니다.\n\n\n7.4.3 광학 문자 인식\n위의 모든 것은 이미 “디지털화”된 PDF를 가지고 있다는 것을 전제로 합니다. 그러나 스캔 결과와 같이 이미지로 만들어진 경우는 어떻습니까? 이러한 PDF에는 종종 비정형 데이터가 포함되어 있으며, 이는 데이터가 태그가 지정되거나 규칙적인 방식으로 구성되지 않았음을 의미합니다. 광학 문자 인식(OCR)은 텍스트 이미지를 실제 텍스트로 변환하는 프로세스입니다. OCR 전후에 PDF를 읽는 사람에게는 큰 차이가 없을 수 있지만, PDF는 기계가 읽을 수 있게 되어 스크립트를 사용할 수 있게 됩니다 (Cheriet 기타 2007). OCR은 1950년대부터 문자 이미지를 파싱하는 데 사용되었으며, 처음에는 수동 접근 방식을 사용했습니다. 수동 접근 방식이 여전히 황금 표준으로 남아 있지만, 비용 효율성 때문에 이것은 대체로 통계 모델로 대체되었습니다.\n이 예에서는 tesseract를 사용하여 문서를 OCR합니다. 이것은 Tesseract 오픈 소스 OCR 엔진을 위한 R 래퍼입니다. Tesseract는 처음에 1980년대에 HP에서 개발되었으며, 현재는 주로 Google에서 개발합니다. tesseract를 설치하고 로드한 후 ocr()을 사용할 수 있습니다.\n제인 에어의 첫 페이지 스캔으로 예를 들어 보겠습니다(그림 7.16).\n\n\n\n\n\n\n그림 7.16: 제인 에어 첫 페이지 스캔\n\n\n\n\ntext &lt;- ocr(\n  here(\"jane_scan.png\"),\n  engine = tesseract(\"eng\")\n)\ncat(text)\n\n일반적으로 결과는 나쁘지 않습니다. OCR은 유용한 도구이지만 완벽하지 않으며 결과 데이터는 정리 측면에서 추가적인 주의가 필요할 수 있습니다. 예를 들어, ?fig-janescan의 OCR 결과에서는 수정해야 할 불규칙성을 볼 수 있습니다. 관심 있는 특정 데이터에 집중하고 대비를 높이는 것과 같은 다양한 옵션이 도움이 될 수 있습니다. 다른 인기 있는 OCR 엔진으로는 Amazon Textract, Google Vision API 및 ABBYY가 있습니다.",
    "crumbs": [
      "획득",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>API, 스크래핑 및 파싱</span>"
    ]
  },
  {
    "objectID": "07-gather_ko.html#연습-문제",
    "href": "07-gather_ko.html#연습-문제",
    "title": "7  API, 스크래핑 및 파싱",
    "section": "7.5 연습 문제",
    "text": "7.5 연습 문제\n\n연습\n\n(계획) 다음 시나리오를 고려하십시오: 5명의 학부생 그룹—맷, 애쉬, 재키, 롤, 마이크—이 각각 100일 동안 매일 책에서 일정 페이지를 읽습니다. 학부생 중 두 명은 커플이므로 페이지 수가 양의 상관 관계를 갖지만, 다른 모든 학생은 독립적입니다. 데이터셋이 어떻게 생겼을지 스케치한 다음, 모든 관측치를 보여주기 위해 만들 수 있는 그래프를 스케치하십시오.\n(시뮬레이션) 설명된 시나리오를 더 고려하고 상황을 시뮬레이션하십시오(일부 변수 간의 관계에 유의하십시오). 그런 다음 시뮬레이션된 데이터를 기반으로 5개의 테스트를 작성하십시오.\n(수집) 시나리오와 유사한 실제 데이터를 얻고, 이 실제 데이터에 대한 시뮬레이션된 테스트를 업데이트하는 스크립트를 추가하십시오.\n(탐색) 실제 데이터를 사용하여 그래프와 표를 만드십시오.\n(전달) 그래프와 표에 첨부할 텍스트를 작성하십시오. 코드를 R 파일과 Quarto 문서로 적절하게 분리하십시오. 고품질 GitHub 리포지토리 링크를 제출하십시오.\n\n\n\n퀴즈\n\n데이터 수집의 맥락에서 API란 무엇입니까(하나 선택)?\n\n데이터를 로컬에서 처리하기 위한 표준화된 함수 집합.\n데이터를 구조화하기 위한 마크업 언어.\n웹 브라우저가 HTML 콘텐츠를 렌더링하기 위한 프로토콜.\n다른 사람이 코드를 사용하여 데이터를 요청할 수 있도록 서버에서 제공하는 인터페이스.\n\n데이터 수집을 위해 API를 사용할 때 다음 중 인증에 사용할 수 있는 것은 무엇입니까(하나 선택)?\n\n요청에 API 키 또는 토큰 제공.\n브라우저에 저장된 쿠키 사용.\nSSL 확인 비활성화.\n클라이언트 시스템의 호스트 파일 수정.\n\ngh를 사용하여 GitHub API에 액세스하는 다음 코드를 고려하십시오. heapsofpapers의 리포지토리는 언제 생성되었습니까(하나 선택)?\n\n2021-02-23\n2021-03-06\n2021-05-25\n2021-04-27\n\n\n\n# Tyler Bradley와 Monica Alexander 기반\nrepos &lt;- gh(\"/users/RohanAlexander/repos\", per_page = 100)\nrepo_info &lt;- tibble(\n  name = map_chr(repos, \"name\"),\n  created = map_chr(repos, \"created_at\"),\n  full_name = map_chr(repos, \"full_name\"),\n)\n\n\nUN의 데이터 API와 (schmertmannunapi의?) 사용 방법에 대한 소개 노트를 고려하십시오. 아르헨티나의 위치 코드는 32입니다. 다음 코드를 수정하여 1995년 20세의 아르헨티나 단일 연령 출산율이 얼마였는지 확인하십시오(하나 선택)?\n\n147.679\n172.988\n204.124\n128.665\n\n\n\nmy_indicator &lt;- 68\nmy_location &lt;- 50\nmy_startyr &lt;- 1996\nmy_endyr &lt;- 1999\n\nurl &lt;- paste0(\n  \"https://population.un.org/dataportalapi/api/v1\",\n  \"/data/indicators/\", my_indicator, \"/locations/\",\n  my_location, \"/start/\", my_startyr, \"/end/\",\n  my_endyr, \"/?format=csv\"\n)\n\nun_data &lt;- read_delim(file = url, delim = \"|\", skip = 1)\n\nun_data |&gt;\n  filter(AgeLabel == 25 & TimeLabel == 1996) |&gt;\n  select(Value)\n\n\nhttr의 GET()에 대한 주요 인수는 무엇입니까(하나 선택)?\n\n“url”\n“website”\n“domain”\n“location”\n\n웹 스크래핑에서 robots.txt를 존중하는 목적은 무엇입니까(하나 선택)?\n\n스크랩한 데이터가 정확한지 확인하기 위해.\n사이트의 크롤링 지침을 따라 웹사이트의 서비스 약관을 위반하지 않기 위해.\n스크래핑 프로세스의 속도를 높이기 위해.\n인증 자격 증명을 얻기 위해.\n\n코드를 파싱할 때 일반적으로 웹사이트의 어떤 기능을 활용합니까(하나 선택)?\n\nHTML/CSS 마크업.\n쿠키.\n페이스북 비콘.\n코드 주석.\n\n스크래핑할 때 따라야 할 몇 가지 원칙은 무엇입니까(해당하는 모든 항목 선택)?\n\n가능하면 피하십시오.\n사이트의 지침을 따르십시오.\n속도를 늦추십시오.\n도끼가 아닌 메스를 사용하십시오.\n\n다음 중 웹 스크래핑을 수행할 때 권장되지 않는 원칙은 무엇입니까(하나 선택)?\n\n웹사이트의 서비스 약관을 준수하십시오.\n요청 속도를 늦춰 웹사이트 서버에 미치는 영향을 줄이십시오.\n필요에 관계없이 모든 데이터를 스크랩하십시오.\n스크랩한 페이지를 재게시하지 마십시오.\n\n다음 중 정규식의 일부로 사용될 때 마침표와 일치하는 것은 무엇입습니까(힌트: “문자열” 치트 시트 참조)(하나 선택)?\n\n“.”\n“\\.”\n“\\\\\\.”\n\n특정 연도에 한 국가의 출생아 수와 같은 인구 통계 데이터에 대해 사용하고 싶은 세 가지 확인 사항은 무엇입니까?\n다음 중 purrr 패키지의 함수는 무엇입니까(해당하는 모든 항목 선택)?\n\nmap()\nwalk()\nrun()\nsafely()\n\n목록의 항목에 대한 HTML 태그는 무엇입니까(하나 선택)?\n\nli\nbody\nb\nem\n\n“names”라는 열에 “rohan_alexander”라는 텍스트가 있고 밑줄을 기준으로 이름과 성으로 나누고 싶을 때 어떤 함수를 사용해야 합니까(하나 선택)?\n\nspacing()\nslice()\nseparate()\ntext_to_columns()\n\n광학 문자 인식(OCR)이란 무엇입니까(하나 선택)?\n\n손으로 쓴 메모를 입력된 텍스트로 변환하는 프로세스.\n텍스트 이미지를 기계가 읽을 수 있는 텍스트로 변환하는 방법.\nAPI에서 구조화된 데이터를 파싱하는 기술.\n더 빠른 실행을 위해 코드를 최적화하는 방법.\n\nR에서 웹 스크래핑 중 속도 제한을 존중하는 데 유용한 지정된 시간 동안 실행을 일시 중지하는 데 사용할 수 있는 함수는 무엇입니까(하나 선택)?\n\nsleep()\npause()\nsys.sleep()\nwait()\n\n다음 중 PDF에서 데이터를 추출할 때의 과제는 무엇입니까(하나 선택)?\n\nPDF는 어떤 프로그래밍 언어로도 읽을 수 없습니다.\nPDF는 데이터 추출이 아닌 일관된 인간 읽기를 위해 설계되었습니다.\nPDF는 항상 처리할 수 없는 비정형 데이터를 포함합니다.\nPDF는 암호화되어 암호 없이는 액세스할 수 없습니다.\n\n스캔한 문서에서 OCR을 수행할 때 텍스트 인식의 정확도에 영향을 미칠 수 있는 일반적인 문제는 무엇입니까(하나 선택)?\n\n이미지의 파일 크기.\n사용된 프로그래밍 언어.\n스캔한 이미지의 품질 및 해상도.\n문서의 페이지 수.\n\n(cirone에서?) 다음 중 역사 데이터로 작업할 때 추론에 대한 일반적인 위협이 아닌 것은 무엇입니까(하나 선택)?\n\n선택 편향.\n확증 편향.\n시간 붕괴.\n소외된 그룹의 과대 대표.\n\n(cirone에서?) 역사적 정치 경제학(그리고 더 일반적으로)에서 술 취한 사람의 검색 문제는 무엇입니까(하나 선택)?\n\n대표성을 고려하지 않고 가장 쉽게 접근할 수 있는 데이터를 선택하는 것.\n엘리트 출처에서만 데이터를 검색하는 것.\n연구를 위해 디지털 아카이브에 과도하게 의존하는 것.\n현대적 편견으로 인해 역사적 텍스트를 오해하는 것.\n\n(cirone에서?) DAG의 역할은 무엇입니까(하나 선택)?\n\n역사 데이터에 대한 OCR의 정확도를 향상시킵니다.\n역사적 출처에서 기계가 읽을 수 있는 텍스트를 생성합니다.\n연구자가 인과 관계를 시각화하고 해결하는 데 도움이 됩니다.\n역사적 아카이브를 구성하기 위한 메타데이터 역할을 합니다.\n\n(Johnson2021Two에서?) 미국 인구 조사국의 초기 교도소 데이터 수집의 초점은 무엇이었습니까(하나 선택)?\n\n건강 상태 문서화.\n선고에서 인종적 차이 조사.\n사회 경제적 배경 및 고용 기록.\n수감자 수 및 인구 통계 계산.\n\n(Johnson2021Two에서?) 지역 사회 출처의 교도소 데이터는 주 출처의 교도소 데이터와 어떻게 다릅니까(하나 선택)?\n\n지역 사회 데이터는 정부 공무원이 수집합니다.\n지역 사회 데이터는 생활 경험과 교도소 환경을 강조합니다.\n주 데이터는 지역 사회 데이터보다 신뢰성이 낮습니다.\n주 데이터는 지역 사회 데이터보다 신뢰성이 높습니다.\n\n(Johnson2021Two에서?) 다음 중 주 출처 데이터의 한계는 무엇입니까(하나 선택)?\n\n주 출처 데이터는 학술 연구보다 신뢰성이 낮습니다.\n교도소 인구를 과소 대표합니다.\n이전 데이터 수집의 편견과 가정을 재현할 수 있습니다.\n비폭력 범죄자에게만 집중합니다.\n\n(Johnson2021Two에서?) 교도소 데이터 수집을 볼 때 어떤 질문을 해야 합니까(하나 선택)?\n\n“누가 데이터 인프라를 구축했고 왜 그랬습니까?”.\n“경제적 요인이 교도소 관리에 어떤 영향을 미칩니까?”.\n“데이터가 공공 정책을 만드는 데 사용되고 있습니까?”.\n\n\n\n\n수업 활동\n\n스타터 폴더를 사용하고 새 리포지토리를 만듭니다. 오늘 NASA APOD를 API를 사용하여 얻은 다음 리포지토리의 Quarto 문서에 추가하십시오.\n스포티파이 API를 사용하여 어떤 비욘세 앨범이 평균 “춤추기 좋은 정도”가 가장 높은지 확인하십시오.\n2016년 12월 카밀라 카베요가 피프스 하모니를 탈퇴한 것이 스튜디오 앨범의 노래의 valence에 영향을 미쳤는지에 대한 질문에 답하기 위해 그래프를 만드십시오.1 일부 유용한 정리 코드는 다음과 같습니다.\n\n\nfifth_harmony |&gt;\n  filter(album_name %in% c(\"Reflection\", \"7/27 (Deluxe)\", \"Fifth Harmony\")) |&gt; \n  mutate(album_release_date = ymd(album_release_date)) |&gt;\n  filter(album_release_date != \"2017-10-29\") # 본질적으로 중복된 앨범이 있습니다.\n\n\n?fig-pmslives와 동등한 것을 만들되, 캐나다에 대해 만드십시오.\n논문 검토: (Kish1959를?) 읽고 익숙한 예를 들어 최소 한 페이지의 검토를 작성하십시오.\n\n\n\n과제\n웹 스크래핑 예제를 다시 수행하되, 다음 중 하나에 대해 수행하십시오: 호주, 캐나다, 인도 또는 뉴질랜드.\n데이터를 계획, 수집 및 정리한 다음 위에서 만든 것과 유사한 표를 만드는 데 사용하십시오. 결과에 대해 몇 단락을 작성하십시오. 그런 다음 데이터 소스, 수집한 내용 및 수행 방법에 대해 몇 단락을 작성하십시오. 예상보다 오래 걸린 것은 무엇입니까? 언제 재미있어졌습니까? 다음에 이 작업을 할 때 다르게 할 것은 무엇입니까? 제출물은 최소 두 페이지 이상이어야 하지만 더 많을 가능성이 높습니다.\nQuarto를 사용하고 적절한 제목, 저자, 날짜, GitHub 리포지토리 링크 및 인용을 포함하십시오. PDF를 제출하십시오.\n\n\n\n\nAlexander, Rohan, 와/과 A Mahfouz. 2021. heapsofpapers: Easily Download Heaps of PDF and CSV Files. https://CRAN.R-project.org/package=heapsofpapers.\n\n\nArel-Bundock, Vincent. 2024. tinytable: Simple and Configurable Tables in “HTML”, “LaTeX”, “Markdown”, “Word”, “PNG”, “PDF”, and “Typst” Formats. https://vincentarelbundock.github.io/tinytable/.\n\n\nBailey, Rosemary. 2008. Design of comparative experiments. Cambridge: Cambridge University Press. https://doi.org/10.1017/CBO9780511611483.\n\n\nBrontë, Charlotte. 1847. Jane Eyre. https://www.gutenberg.org/files/1260/1260-h/1260-h.htm.\n\n\nBryan, Jenny, 와/과 Hadley Wickham. 2021. gh: GitHub API. https://CRAN.R-project.org/package=gh.\n\n\nCheriet, Mohamed, Nawwaf Kharma, Cheng-Lin Liu, 와/과 Ching Suen. 2007. Character Recognition Systems: A Guide for Students and Practitioner. Wiley.\n\n\nCirone, Alexandra, 와/과 Arthur Spirling. 2021. “Turning History into Data: Data Collection, Measurement, and Inference in HPE”. Journal of Historical Political Economy 1 (1): 127–54. https://doi.org/10.1561/115.00000005.\n\n\nCrawford, Kate. 2021. Atlas of AI. 1st ed. New Haven: Yale University Press.\n\n\nEisenstein, Michael. 2022. “Need web data? Here’s how to harvest them”. Nature 607: 200–201. https://doi.org/10.1038/d41586-022-01830-9.\n\n\nFirke, Sam. 2023. janitor: Simple Tools for Examining and Cleaning Dirty Data. https://CRAN.R-project.org/package=janitor.\n\n\nGrolemund, Garrett, 와/과 Hadley Wickham. 2011. “Dates and Times Made Easy with lubridate”. Journal of Statistical Software 40 (3): 1–25. https://doi.org/10.18637/jss.v040.i03.\n\n\nHackett, Robert. 2016. “Researchers Caused an Uproar By Publishing Data From 70,000 OkCupid Users”. Fortune, 5월. https://fortune.com/2016/05/18/okcupid-data-research/.\n\n\nJohnson, Kaneesha. 2021. “Two Regimes of Prison Data Collection”. Harvard Data Science Review 3 (3). https://doi.org/10.1162/99608f92.72825001.\n\n\nLuscombe, Alex, Kevin Dick, 와/과 Kevin Walby. 2021. “Algorithmic thinking in the public interest: navigating technical, legal, and ethical hurdles to web scraping in the social sciences”. Quality & Quantity 56 (3): 1–22. https://doi.org/10.1007/s11135-021-01164-0.\n\n\nMüller, Kirill. 2020. here: A Simpler Way to Find Your Files. https://CRAN.R-project.org/package=here.\n\n\nOoms, Jeroen. 2014. “The jsonlite Package: A Practical and Consistent Mapping Between JSON Data and R Objects”. arXiv:1403.2805 [stat.CO]. https://arxiv.org/abs/1403.2805.\n\n\n———. 2022a. pdftools: Text Extraction, Rendering and Converting of PDF Documents. https://CRAN.R-project.org/package=pdftools.\n\n\n———. 2022b. tesseract: Open Source OCR Engine. https://CRAN.R-project.org/package=tesseract.\n\n\nPerepolkin, Dmytro. 2022. polite: Be Nice on the Web. https://CRAN.R-project.org/package=polite.\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nThe Washington Post. 2023. “Fatal Force Database”. https://github.com/washingtonpost/data-police-shootings.\n\n\nThompson, Charlie, Daniel Antal, Josiah Parry, Donal Phipps, 와/과 Tom Wolff. 2022. spotifyr: R Wrapper for the “Spotify” Web API. https://CRAN.R-project.org/package=spotifyr.\n\n\nThomson-DeVeaux, Amelia, Laura Bronner, 와/과 Damini Sharma. 2021. “Cities Spend Millions On Police Misconduct Every Year. Here’s Why It’s So Difficult to Hold Departments Accountable”. FiveThirtyEight, 2월. https://fivethirtyeight.com/features/police-misconduct-costs-cities-millions-every-year-but-thats-where-the-accountability-ends/.\n\n\nWickham, Hadley. 2021. babynames: US Baby Names 1880-2017. https://CRAN.R-project.org/package=babynames.\n\n\n———. 2022. rvest: Easily Harvest (Scrape) Web Pages. https://CRAN.R-project.org/package=rvest.\n\n\n———. 2023. httr: Tools for Working with URLs and HTTP. https://CRAN.R-project.org/package=httr.\n\n\nWickham, Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, 기타. 2019. “Welcome to the Tidyverse”. Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Jennifer Bryan, 와/과 Malcolm Barrett. 2022. usethis: Automate Package and Project Setup. https://CRAN.R-project.org/package=usethis.\n\n\nWickham, Hadley, 와/과 Lionel Henry. 2022. purrr: Functional Programming Tools. https://CRAN.R-project.org/package=purrr.\n\n\nWickham, Hadley, Jim Hester, 와/과 Jeroen Ooms. 2021. xml2: Parse XML. https://CRAN.R-project.org/package=xml2.",
    "crumbs": [
      "획득",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>API, 스크래핑 및 파싱</span>"
    ]
  },
  {
    "objectID": "07-gather_ko.html#footnotes",
    "href": "07-gather_ko.html#footnotes",
    "title": "7  API, 스크래핑 및 파싱",
    "section": "",
    "text": "빨리 끝내는 학생들은 2014년 9월 소녀시대에서 제시카가 탈퇴한 것을 유사하게 살펴본 다음 두 상황을 비교하려고 시도해야 합니다.↩︎",
    "crumbs": [
      "획득",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>API, 스크래핑 및 파싱</span>"
    ]
  },
  {
    "objectID": "08-hunt_ko.html",
    "href": "08-hunt_ko.html",
    "title": "8  실험 및 설문조사",
    "section": "",
    "text": "8.1 소개\n선수 지식\n주요 개념 및 기술\n소프트웨어 및 패키지\n이 장은 실험과 설문조사를 통해 데이터를 얻는 것에 관한 것입니다. 실험은 우리가 관심 있는 것을 명시적으로 제어하고 변화시킬 수 있는 상황입니다. 이것의 장점은 효과를 식별하고 추정하는 것이 명확해야 한다는 것입니다. 우리가 관심 있는 것에 노출되는 처리군과 그렇지 않은 대조군이 있습니다. 이들은 처리 전에 무작위로 나뉩니다. 따라서, 만약 그들이 다르게 끝난다면, 그것은 처리 때문이어야 합니다. 불행히도, 삶은 거의 그렇게 순탄하지 않습니다. 처리군과 대조군이 얼마나 유사했는지에 대한 논쟁은 무기한 계속되는 경향이 있습니다. 그리고 효과를 추정하기 전에, 우리는 우리가 관심 있는 것이 무엇이든 측정할 수 있어야 하며, 이는 종종 놀라울 정도로 어렵습니다.\n동기 부여를 위해, 2014년에 샌프란시스코로 이사한 사람의 상황을 생각해 보십시오. 그들이 이사하자마자 자이언츠가 월드 시리즈에서 우승했고 골든스테이트 워리어스가 역사적인 월드 챔피언십 연속 우승을 시작했습니다. 그런 다음 그들은 시카고로 이사했고, 즉시 컵스가 100년 만에 처음으로 월드 시리즈에서 우승했습니다. 그런 다음 그들은 매사추세츠로 이사했고, 패트리어츠가 다시, 그리고 다시, 그리고 다시 슈퍼볼에서 우승했습니다. 그리고 마지막으로, 그들은 토론토로 이사했고, 랩터스가 즉시 월드 챔피언십에서 우승했습니다. 도시가 그들을 이사하도록 돈을 지불해야 할까요, 아니면 시 자금을 다른 곳에 더 잘 사용할 수 있을까요?\n답을 얻는 한 가지 방법은 실험을 실행하는 것입니다. 주요 스포츠 팀이 있는 북미 도시 목록을 만드십시오. 그런 다음 주사위를 굴려 1년 동안 그곳에 살게 하고 스포츠 팀의 결과를 측정하십시오. 충분한 평생이 있다면, 우리는 그것을 알아낼 수 있을 것입니다. 우리가 한 도시에 살면서 동시에 살지 않을 수 없기 때문에 이것은 오랜 시간이 걸릴 것입니다. 이것이 인과 추론의 근본적인 문제입니다. 한 사람은 처리되고 처리되지 않을 수 없기 때문입니다. 실험과 무작위 통제 시험은 우리가 일부 처리를 무작위로 할당하여 다른 모든 것이 동일했다는(또는 적어도 무시할 수 있다는) 믿음을 갖기 위해 노력하는 상황입니다. 우리는 상황을 공식화하기 위해 네이만-루빈 잠재적 결과 프레임워크를 사용합니다 (Holland 1986).\n처리, \\(t\\)는 종종 이진 변수, 즉 0 또는 1이 될 것입니다. 사람 \\(i\\)가 처리되지 않은 경우, 즉 대조군에 있는 경우 0이고, 처리된 경우 1입니다. 우리는 일반적으로 해당 사람에 대한 관심 결과 \\(Y_i\\)를 가지며, 이는 이진, 범주형, 다항, 순서형, 연속형 또는 다른 유형의 변수일 수 있습니다. 예를 들어, 투표 선택일 수 있으며, 이 경우 우리는 그 사람이 “보수적”인지 “보수적이지 않은지”, 어떤 정당을 지지하는지, 예를 들어 “보수당”, “자유당”, “민주당”, “녹색당”인지, 또는 특정 지도자를 지지할 확률을 측정할 수 있습니다.\n처리의 효과는 \\((Y_i|t=0) \\neq (Y_i|t=1)\\)일 때 인과적입니다. 즉, 처리되지 않았을 때의 사람 \\(i\\)의 결과는 처리되었을 때의 결과와 다릅니다. 우리가 한 번에 한 개인을 처리하고 통제할 수 있다면, 결과의 변화를 일으킨 것은 처리뿐이라는 것을 알 수 있을 것입니다. 그것을 설명할 다른 요인은 없을 것입니다. 그러나 인과 추론의 근본적인 문제는 여전히 남아 있습니다. 우리는 한 번에 한 개인을 처리하고 통제할 수 없습니다. 따라서 처리의 효과를 알고 싶을 때, 우리는 그것을 반사실과 비교해야 합니다. ?sec-on-writing에서 소개된 반사실은 처리된 개인이 처리되지 않았다면 일어났을 일입니다. 결과적으로, 이것은 인과 추론을 결측 데이터 문제로 생각하는 한 가지 방법이며, 여기서 우리는 반사실을 놓치고 있습니다.\n한 개인에서 처리와 통제를 비교할 수 없습니다. 그래서 우리는 대신 두 그룹—처리된 그룹과 그렇지 않은 그룹—의 평균을 비교합니다. 우리는 개별 수준에서 불가능하기 때문에 그룹 수준에서 반사실을 추정하려고 합니다. 이 절충안을 만들면 앞으로 나아갈 수 있지만 확실성을 희생해야 합니다. 대신 우리는 무작위화, 확률 및 기대에 의존해야 합니다.\n우리는 일반적으로 효과가 없다는 것을 기본값으로 간주하고 마음을 바꿀 증거를 찾습니다. 우리는 그룹에서 무슨 일이 일어나고 있는지에 관심이 있으므로, 자신을 표현하기 위해 기대와 확률의 개념으로 전환합니다. 따라서 우리는 평균적으로 적용되는 주장을 할 것입니다. 재미있는 양말을 신는 것이 정말로 운 좋은 날을 보내게 할 수도 있지만, 그룹 전체적으로 평균적으로는 그렇지 않을 것입니다. 우리가 평균 효과에만 관심이 있는 것은 아니라는 점을 지적할 가치가 있습니다. 우리는 중앙값, 분산 또는 무엇이든 고려할 수 있습니다. 그럼에도 불구하고, 우리가 평균 효과에 관심이 있다면, 한 가지 방법은 다음과 같습니다.\n이것은 ?sec-on-writing에서 소개된 추정량이며, 관심 있는 것을 추측하는 방법입니다. 추정량은 관심 있는 것, 이 경우 평균 효과이며, 추정치는 우리의 추측이 무엇이든 간에 결과입니다. 우리는 상황을 설명하기 위해 데이터를 시뮬레이션할 수 있습니다.\n이 경우, 우리는 처리군과 대조군 각각에 대해 0 또는 1을 100번 추출하고, 처리되는 것의 평균 효과 추정치는 0.22입니다.\n더 넓게 말하면, 인과적 이야기를 하려면 이론과 우리가 관심 있는 것에 대한 상세한 지식을 결합해야 합니다 (Cunningham 2021, p. 4). ?sec-gather-data에서는 우리가 세상에 대해 관찰한 데이터 수집에 대해 논의했습니다. 이 장에서는 세상을 우리가 필요한 데이터로 바꾸는 데 더 적극적으로 참여할 것입니다. 연구자로서 우리는 무엇을 측정하고 어떻게 측정할지 결정하고, 우리가 관심 있는 것을 정의해야 합니다. 우리는 데이터 생성 과정에 적극적으로 참여할 것입니다. 즉, 이 데이터를 사용하려면 연구자로서 나가서 사냥해야 합니다.\n이 장에서는 실험, 특히 처리군과 대조군을 구성하고 그 결과를 적절하게 고려하는 것을 다룹니다. 설문 조사를 구현하는 과정을 거칩니다. 터스키기 매독 연구와 체외막산소공급(ECMO) 실험을 참조하여 실험에서의 윤리적 행동의 일부 측면을 논의하고 다양한 사례 연구를 거칩니다. 마지막으로, 업계에서 광범위하게 사용되는 A/B 테스트로 전환하고, 업워디 데이터를 기반으로 한 사례 연구를 고려합니다.\n20세기 통계학자인 로널드 피셔와 19세기 통계학자인 프랜시스 골턴은 이 장에서 다루는 많은 작업의 지적 할아버지입니다. 어떤 경우에는 직접적으로 그들의 작업이고, 다른 경우에는 그들의 기여를 기반으로 한 작업입니다. 두 사람 모두 우생학을 믿었으며, 그 외에도 일반적으로 비난받을 만한 것들을 믿었습니다. 미술사에서 카라바조를 살인자로 인정하면서도 그의 작품과 영향을 고려하는 것과 마찬가지로, 통계 및 데이터 과학도 더 나은 미래를 건설하려고 노력하는 동시에 이 과거에 대해 관심을 가져야 합니다.",
    "crumbs": [
      "획득",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>실험 및 설문조사</span>"
    ]
  },
  {
    "objectID": "08-hunt_ko.html#소개",
    "href": "08-hunt_ko.html#소개",
    "title": "8  실험 및 설문조사",
    "section": "",
    "text": "데이터셋을 두 개로 나눕니다—처리된 것과 처리되지 않은 것—그리고 이진 효과 변수를 가집니다—운 좋은 날 또는 그렇지 않은 날;\n변수를 합산한 다음 변수의 길이로 나눕니다. 그리고\n두 그룹 간에 이 값을 비교합니다.\n\n\n#| eval: true\n#| warning: false\n#| message: false\n\nset.seed(853)\n\ntreat_control &lt;-\n  tibble(\n    group = sample(x = c(\"Treatment\", \"Control\"), size = 100, replace = TRUE),\n    binary_effect = sample(x = c(0, 1), size = 100, replace = TRUE)\n    )\n\ntreat_control\n#| eval: true\n#| warning: false\n#| message: false\n\ntreat_control |&gt;\n  summarise(\n    treat_result = sum(binary_effect) / length(binary_effect),\n    .by = group\n  )",
    "crumbs": [
      "획득",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>실험 및 설문조사</span>"
    ]
  },
  {
    "objectID": "08-hunt_ko.html#현장-실험-및-무작위-통제-시험",
    "href": "08-hunt_ko.html#현장-실험-및-무작위-통제-시험",
    "title": "8  실험 및 설문조사",
    "section": "8.2 현장 실험 및 무작위 통제 시험",
    "text": "8.2 현장 실험 및 무작위 통제 시험\n\n8.2.1 무작위화\n어떤 환경에서는 상관 관계만으로도 충분할 수 있지만 (Hill 1965), 상황이 변하고 상황이 약간 다를 때 예측을 할 수 있으려면 인과 관계를 이해하려고 노력해야 합니다. 경제학은 2000년대에 신뢰성 혁명을 겪었습니다 (Angrist 와/과 Pischke 2010). 경제학자들은 이전 작업이 가능한 한 신뢰할 수 없다는 것을 깨달았습니다. 연구 설계 및 실험 사용에 대한 우려가 증가했습니다. 이것은 비슷한 시기에 정치학과 같은 다른 사회 과학에서도 일어났습니다 (Druckman 와/과 Green 2021).\n핵심은 반사실입니다. 치료가 없었다면 무슨 일이 일어났을까요? 이상적으로는 다른 모든 것을 일정하게 유지하고, 세상을 무작위로 두 그룹으로 나누고, 하나는 치료하고 다른 하나는 치료하지 않을 수 있습니다. 그러면 두 그룹 간의 차이가 치료 때문이라고 확신할 수 있습니다. 그 이유는 우리가 어떤 인구를 가지고 있고 그 인구에서 무작위로 두 그룹을 선택하면, 그 두 그룹(둘 다 충분히 크다면)은 인구와 동일한 특성을 가져야 하기 때문입니다. 무작위 통제 시험(RCT)과 A/B 테스트는 우리가 희망할 수 있는 이 “황금 표준”에 최대한 가깝게 우리를 데려가려고 시도합니다.\n우리와 (athey2017state와?) 같은 다른 사람들이 이러한 접근 방식을 지칭하기 위해 그러한 긍정적인 언어를 사용할 때, 우리는 그것들이 완벽하다는 것을 의미하지 않습니다. 단지 다른 대부분의 옵션보다 더 나을 수 있다는 것입니다. 예를 들어, ?sec-causality-from-observational-data에서는 관찰 데이터에서 인과 관계를 고려할 것이며, 이것이 때로는 우리가 할 수 있는 전부이지만, 둘 다 평가할 수 있는 상황에서는 관찰 데이터를 기반으로 한 접근 방식이 일반적으로 차선책이라는 것이 분명합니다 (Gordon 기타 2019; Gordon, Moakler, 와/과 Zettelmeyer 2022). RCT와 A/B 테스트는 또한 특정 질문에 초점을 맞추고 효과가 발생하는 메커니즘을 밝히려고 시도하는 연구를 설계할 기회와 같은 다른 이점을 가져옵니다 (Alsan 와/과 Finkelstein 2021). 그러나 그것들은 완벽하지 않으며, RCT의 수용은 만장일치가 아니었습니다 (Deaton 2010).\n실험 관행의 한 가지 기반은 눈가림이어야 한다는 것입니다. 즉, 참가자는 자신이 처리군에 있는지 대조군에 있는지 모릅니다. 특히 주관적인 결과에 대해 눈가림을 실패하는 것은 일부 분야에서 전체 실험을 기각할 근거가 됩니다 (Edwards 2017). 이상적으로 실험은 이중 눈가림이어야 합니다. 즉, 연구자조차도 모릅니다. (stolberg2006inventing은?) 1835년에 동종 요법 약물의 효과를 평가하기 위한 초기 무작위 이중 눈가림 시험의 예를 논의하는데, 여기서 참가자나 주최자 모두 누가 어느 그룹에 속해 있는지 몰랐습니다. 이것은 RCT와 A/B 테스트의 경우 거의 그렇지 않습니다. 다시 말하지만, 이것이 유용하지 않다는 것을 말하는 것이 아닙니다. 결국 1847년에 젬멜바이스는 눈가림 연구 없이 아기를 분만하기 전에 인턴이 손을 씻는 것의 이점을 확인했습니다 (Morange 2016, p. 121). 또 다른 주요 우려 사항은 RCT에서 발견된 결과가 해당 환경 외부로 일반화되는 정도입니다. 장기간에 걸쳐 수행된 RCT는 일반적으로 거의 없지만, 이것이 바뀌고 있을 가능성이 있으며 (Bouguen2019는?) 장기적인 효과를 평가하기 위해 추적할 수 있는 일부 RCT를 제공합니다. 마지막으로, 인과 관계에 대한 초점은 사회 과학에서 비용 없이 이루어지지 않았습니다. 일부는 인과 관계 중심 접근 방식이 다른 유형의 질문을 희생시키면서 답할 수 있는 유형의 질문에 주의를 집중시킨다고 주장합니다.\n\n\n8.2.2 시뮬레이션 예시: 고양이 또는 개\n우리는 치료를 제외하고는 동일한 치료군과 대조군을 설정하기를 희망합니다. 이것은 대조군을 만드는 것이 중요하며, 그렇게 할 때 반사실을 설정하기 때문입니다. 우리는 예를 들어, 이전-이후 비교의 한 가지 문제인 기본 추세나, 치료군에 대한 자가 선택을 허용할 때 발생할 수 있는 선택 편향에 대해 걱정할 수 있습니다. 이러한 문제 중 어느 것이든 편향된 추정치를 초래할 수 있습니다. 우리는 이러한 문제를 해결하기 위해 무작위화를 사용합니다.\n시작하려면, 우리는 인구를 시뮬레이션한 다음, 거기서 무작위로 표본을 추출합니다. 우리는 인구의 절반이 파란색을 좋아하고, 다른 절반은 흰색을 좋아하도록 설정할 것입니다. 그리고 더 나아가, 누군가가 파란색을 좋아하면 거의 확실히 개를 선호하지만, 흰색을 좋아하면 거의 확실히 고양이를 선호합니다. 시뮬레이션은 이 책에서 옹호하는 워크플로의 중요한 부분입니다. 이것은 우리가 시뮬레이션된 데이터 분석에서 결과가 어떠해야 하는지 알기 때문입니다. 반면에 실제 데이터를 바로 분석하면, 예기치 않은 결과가 우리 자신의 분석 오류 때문인지, 아니면 실제 결과 때문인지 알 수 없습니다. 시뮬레이션이라는 이 접근 방식을 취하는 또 다른 좋은 이유는 팀으로 작업할 때 데이터 수집 및 정리가 완료되기 전에 분석을 시작할 수 있다는 것입니다. 시뮬레이션은 또한 수집 및 정리 팀이 데이터에 대해 실행해야 할 테스트에 대해 생각하는 데 도움이 될 것입니다.\nset.seed(853)\n\nnum_people &lt;- 5000\n\npopulation &lt;- tibble(\n  person = 1:num_people,\n  favorite_color = sample(c(\"Blue\", \"White\"), size = num_people, replace = TRUE),\n  prefers_dogs = if_else(favorite_color == \"Blue\", \n                         rbinom(num_people, 1, 0.9), \n                         rbinom(num_people, 1, 0.1))\n  )\n\npopulation |&gt;\n  count(favorite_color, prefers_dogs)\n?sec-farm-data에서 소개된 용어와 개념을 바탕으로, 이제 목표 모집단의 약 80%를 포함하는 표본 추출 프레임을 구성합니다.\nset.seed(853)\n\nframe &lt;-\n  population |&gt;\n  mutate(in_frame = rbinom(n = num_people, 1, prob = 0.8)) |&gt; \n  filter(in_frame == 1)\n\nframe |&gt;\n  count(favorite_color, prefers_dogs)\n지금은 개나 고양이 선호도를 제쳐두고 좋아하는 색상만으로 처리군과 대조군을 만드는 데 집중하겠습니다.\nset.seed(853)\n\nsample &lt;-\n  frame |&gt;\n  select(-prefers_dogs) |&gt;\n  mutate(\n    group = \n      sample(x = c(\"Treatment\", \"Control\"), size = nrow(frame), replace = TRUE\n  ))\n두 그룹의 평균을 보면, 파란색이나 흰색을 선호하는 비율이 우리가 지정한 것과 매우 유사하다는 것을 알 수 있습니다(?tbl-dogsdtocats).\n#| label: tbl-dogsdtocats\n#| tbl-cap: \"파란색 또는 흰색을 선호하는 그룹의 비율\"\n\nsample |&gt;\n  count(group, favorite_color) |&gt;\n  mutate(prop = n / sum(n),\n         .by = group) |&gt;\n  tt() |&gt; \n  style_tt(j = 1:4, align = \"llrr\") |&gt; \n  format_tt(digits = 2, num_mark_big = \",\", num_fmt = \"decimal\") |&gt; \n  setNames(c(\"Group\", \"Prefers\", \"Number\", \"Proportion\"))\n우리는 좋아하는 색상만으로 무작위화했습니다. 그러나 우리는 동시에 개나 고양이 선호도를 함께 가져왔고, 개를 고양이보다 선호하는 사람들의 “대표적인” 몫을 가질 것이라는 것을 발견해야 합니다. 우리는 우리의 데이터셋을 볼 수 있습니다(?tbl-dogstocats).\n#| label: tbl-dogstocats\n#| tbl-cap: \"개를 고양이보다 선호하는 처리군과 대조군의 비율\"\n\nsample |&gt;\n  left_join(\n    frame |&gt; select(person, prefers_dogs),\n    by = \"person\"\n  ) |&gt;\n  count(group, prefers_dogs) |&gt;\n  mutate(prop = n / sum(n),\n         .by = group) |&gt; \n  tt() |&gt; \n  style_tt(j = 1:4, align = \"llrr\") |&gt; \n  format_tt(digits = 2, num_mark_big = \",\", num_fmt = \"decimal\") |&gt; \n  setNames(c(\n      \"Group\",\n      \"Prefers dogs to cats\",\n      \"Number\",\n      \"Proportion\"\n    ))\n“관찰 불가능한 것”에 대해 대표적인 몫을 갖는 것은 흥미롭습니다. (이 경우, 우리는 그것들을 “관찰”합니다—요점을 설명하기 위해—그러나 우리는 그것들을 선택하지 않았습니다). 변수들이 상관 관계가 있었기 때문에 이것을 얻습니다. 그러나 우리가 논의할 여러 가지 방식으로 무너질 것입니다. 또한 충분히 큰 그룹을 가정합니다. 예를 들어, 개를 하나의 실체로 생각하는 대신 특정 개 품종을 고려했다면, 우리는 이 상황에 처하지 않았을 수 있습니다. 두 그룹이 동일한지 확인하기 위해, 우리는 관찰 가능한 것, 이론, 경험 및 전문가 의견을 기반으로 두 그룹 간의 차이를 식별할 수 있는지 확인합니다. 이 경우 우리는 평균을 보았지만, 다른 측면도 볼 수 있습니다.\n이것은 전통적으로 우리를 분산 분석(ANOVA)으로 이끌 것입니다. ANOVA는 약 100년 전 피셔가 농업의 통계적 문제를 연구하는 동안 도입했습니다. ((Stolley1991은?) 피셔에 대한 추가 배경 정보를 제공합니다.) 이것은 역사적으로 농업 연구가 통계 혁신과 밀접하게 관련되어 있었기 때문에 예상보다 덜 놀랍습니다. 종종 통계적 방법은 “비료가 효과가 있는가?”와 같은 농업 질문에 답하기 위해 설계되었으며, 나중에 임상 시험에 적용되었습니다 (Yoshioka 1998). 밭을 “처리된” 것과 “처리되지 않은” 것으로 나누는 것은 비교적 쉬웠고, 효과의 크기는 클 가능성이 높았습니다. 그 맥락에 적절했지만, 종종 이러한 동일한 통계적 접근 방식은 오늘날에도 입문 자료에서 여전히 가르쳐지고 있으며, 설계된 상황과 다른 상황에 적용될 때도 있습니다. 무엇을 하고 있는지, 그리고 그것이 상황에 적절한지 한 걸음 물러서서 생각하는 것은 거의 항상 가치가 있습니다. 우리는 역사적으로 중요하기 때문에 여기서 ANOVA를 언급합니다. 올바른 환경에서는 아무런 문제가 없습니다. 그러나 그것이 최선의 선택인 현대적인 사용 사례의 수는 적은 경향이 있습니다. ANOVA의 기반이 되는 모델을 직접 구축하는 것이 더 나을 수 있으며, 이에 대해서는 ?sec-its-just-a-linear-model에서 다룹니다.\n\n\n8.2.3 처리 및 통제\n처리군과 대조군이 모든 면에서 동일하고 처리를 제외하고는 그렇게 유지된다면, 우리는 내부 타당성을 가지며, 이는 우리의 대조군이 반사실로 작동하고 우리의 결과가 해당 연구에서 그룹 간의 차이를 말할 수 있다는 것을 의미합니다. 내부 타당성은 처리 효과에 대한 우리의 추정치가 처리와 관련이 있고 다른 측면과는 관련이 없다는 것을 의미합니다. 이는 우리가 결과를 사용하여 실험에서 일어난 일에 대한 주장을 할 수 있음을 의미합니다.\n우리가 무작위화를 적용한 그룹이 더 넓은 인구를 대표하고 실험 설정이 외부 조건과 같다면, 우리는 더 나아가 외부 타당성을 가질 수 있습니다. 이는 우리가 발견한 차이가 우리 자신의 실험에만 적용되는 것이 아니라 더 넓은 인구에도 적용된다는 것을 의미합니다. 외부 타당성은 우리가 실험을 사용하여 실험 외부에서 일어날 일에 대한 주장을 할 수 있음을 의미합니다. 그것이 가능하게 한 것은 무작위화입니다. 실제로 우리는 한 번의 실험에만 의존하지 않고 대신 더 넓은 증거 수집 노력에 대한 기여로 간주할 것입니다 (Duflo 2020.1955).\n\n\n\n\n\n\n거인의 어깨 위에 서서\n\n\n\n에스더 뒤플로 박사는 MIT의 압둘 라티프 자밀 빈곤 완화 및 개발 경제학 교수입니다. 1999년 MIT에서 경제학 박사 학위를 취득한 후, 그녀는 MIT에서 조교수로 재직했으며, 2003년에 정교수로 승진했습니다. 그녀의 연구 분야 중 하나는 경제 개발이며, 빈곤을 해결하는 방법을 이해하기 위해 무작위 통제 시험을 사용합니다. 그녀의 가장 중요한 책 중 하나는 가난한 경제학 (Banerjee 와/과 Duflo 2011)입니다. 그녀의 가장 중요한 논문 중 하나는 소액 금융의 효과를 조사하기 위해 무작위화를 사용하는 (banerjee2015miracle입니다?). 그녀는 2019년에 알프레드 노벨을 기념하는 스웨덴 국립은행 경제학상을 수상했습니다.\n\n\n그러나 이것은 우리가 두 번의 무작위화가 필요하다는 것을 의미합니다. 첫째, 실험 대상 그룹으로, 그리고 둘째, 처리군과 대조군 사이입니다. 우리는 이 무작위화에 대해 어떻게 생각하며, 그것이 어느 정도 중요합니까?\n우리는 처리되는 것의 효과에 관심이 있습니다. 다른 가격을 부과할 수 있으며, 이는 연속적인 처리 변수가 될 것입니다. 또는 웹사이트에서 다른 색상을 비교할 수 있으며, 이는 이산적인 처리 변수가 될 것입니다. 어느 쪽이든, 우리는 그룹이 다른 면에서는 동일한지 확인해야 합니다. 이것을 어떻게 확신할 수 있을까요? 한 가지 방법은 처리 변수를 무시하고 다른 모든 변수를 검사하여 다른 변수를 기반으로 그룹 간의 차이를 감지할 수 있는지 확인하는 것입니다. 예를 들어, 웹사이트에서 실험을 수행하는 경우, 그룹은 예를 들어 다음과 같은 측면에서 대략적으로 유사합니까?\n\n마이크로소프트 및 애플 사용자?\n사파리, 크롬 및 파이어폭스 사용자?\n모바일 및 데스크톱 사용자?\n특정 위치의 사용자?\n\n또한, 그룹이 더 넓은 인구를 대표합니까? 이것들은 모두 우리 주장의 타당성에 대한 위협입니다. 예를 들어, 이 장의 뒷부분에서 고려할 네이션스케이프 설문 조사는 설문 조사를 완료한 파이어폭스 사용자의 수에 대해 우려했습니다. 결국 그들은 해당 응답자의 일부를 제외했습니다 (Vavreck 와/과 Tausanovitch 2021, p. 5).\n적절하게 수행되면, 즉 처리가 진정으로 독립적이라면, 우리는 평균 처리 효과(ATE)를 추정할 수 있습니다. 이진 처리 변수 설정에서 이것은 다음과 같습니다.\n\\[\\mbox{ATE} = \\mathbb{E}[Y|t=1] - \\mathbb{E}[Y|t=0].\\]\n{#eq-oregon}\n즉, 결과 \\(Y\\)의 기대값으로 측정했을 때 처리된 그룹 \\(t = 1\\)과 대조군 \\(t = 0\\) 간의 차이입니다. ATE는 두 조건부 기대값 간의 차이가 됩니다.\n이 개념을 설명하기 위해, 처리군과 대조군 사이에 평균 1의 차이를 보여주는 일부 데이터를 시뮬레이션합니다.\nset.seed(853)\n\nate_example &lt;- \n  tibble(person = 1:1000,\n         treated = sample(c(\"Yes\", \"No\"), size = 1000, replace = TRUE)) |&gt;\n  mutate(outcome = case_when(\n    treated == \"No\" ~ rnorm(n(), mean = 5, sd = 1),\n    treated == \"Yes\" ~ rnorm(n(), mean = 6, sd = 1),\n  ))\n?fig-exampleatefig에서 두 그룹 간의 차이(1로 시뮬레이션함)를 볼 수 있습니다. 그리고 그룹 간의 평균을 계산한 다음 차이를 계산하여 우리가 넣은 결과를 대략적으로 다시 얻는다는 것을 알 수 있습니다(?tbl-exampleatetable).\n#| fig-cap: \"처리군과 대조군 간의 차이를 보여주는 시뮬레이션된 데이터\"\n#| label: fig-exampleatefig\n\nate_example |&gt;\n  ggplot(aes(x = outcome, fill = treated)) +\n  geom_histogram(position = \"dodge2\", binwidth = 0.2) +\n  theme_minimal() +\n  labs(x = \"Outcome\", \n       y = \"Number of people\", \n       fill = \"Person was treated\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  theme(legend.position = \"bottom\")\n#| label: tbl-exampleatetable\n#| tbl-cap: \"평균 1의 차이를 갖도록 시뮬레이션된 데이터에 대한 처리군과 대조군 간의 평균 차이\"\n\nate_example |&gt;\n  summarise(mean = mean(outcome),\n            .by = treated) |&gt; \n  tt() |&gt; \n  style_tt(j = 1:2, align = \"lr\") |&gt; \n  format_tt(digits = 2, num_fmt = \"decimal\") |&gt; \n  setNames(c(\n      \"Was treated?\",\n      \"Average effect\"\n    ))\n불행히도, 시뮬레이션된 데이터와 현실 사이에는 종종 차이가 있습니다. 예를 들어, 실험이 너무 오래 실행되면 사람들이 여러 번 치료를 받거나 치료에 익숙해질 수 있습니다. 그러나 너무 짧으면 장기적인 결과를 측정할 수 없습니다. 인구의 모든 측면에 걸쳐 “대표적인” 표본을 가질 수 없지만, 그렇지 않으면 치료군과 대조군이 다를 수 있습니다. 실제적인 어려움으로 인해 특정 그룹을 추적하기 어려울 수 있으므로 편향된 수집으로 끝날 수 있습니다. 실제 실험 데이터로 작업할 때 탐색할 몇 가지 질문은 다음과 같습니다.\n\n참가자는 고려 대상 프레임으로 어떻게 선택되고 있습니까?\n그들은 치료를 위해 어떻게 선택되고 있습니까? 우리는 이것이 무작위로 이루어지기를 바라지만, 이 용어는 다양한 상황에 적용됩니다. 또한, 초기 “성공”은 특히 의료 환경에서 모든 사람을 치료해야 한다는 압력으로 이어질 수 있습니다.\n치료는 어떻게 평가되고 있습니까?\n무작위 할당은 어느 정도 윤리적이고 공정합니까? 일부는 부족함이 무작위 할당을 합리적으로 만든다고 주장하지만, 이는 혜택이 얼마나 선형적인지에 따라 달라질 수 있습니다. 정의를 확립하기 어려울 수도 있으며, 이러한 결정을 내리는 사람들과 치료받는 사람들의 권력 불균형을 고려해야 합니다.\n\n편향 및 기타 문제는 세상의 끝이 아닙니다. 그러나 우리는 그것들에 대해 신중하게 생각해야 합니다. ?sec-on-writing에서 소개된 선택 편향은 조정될 수 있지만, 인식될 경우에만 가능합니다. 예를 들어, 과정을 이수한 학생만 설문 조사하고 중도 탈락한 학생은 설문 조사하지 않으면 대학 과정의 난이도에 대한 설문 조사 결과가 어떻게 달라질까요? 우리는 데이터셋을 만들 때 가능한 한 대표적으로 만들기 위해 항상 노력해야 하지만, 사후에 일부 편향을 조정하기 위해 모델을 사용하는 것이 가능할 수 있습니다. 예를 들어, 이탈과 상관 관계가 있는 변수가 있다면, 그 자체로 또는 상호 작용으로 모델에 추가될 수 있습니다. 마찬가지로, 개인 간에 상관 관계가 있는 경우입니다. 예를 들어, 우리가 알지 못하는 일부 “숨겨진 변수”가 있어 일부 개인이 상관 관계가 있다면, 더 넓은 표준 오차를 사용할 수 있습니다. 이것은 신중하게 수행되어야 하며, ?sec-causality-from-observational-data에서 이에 대해 자세히 논의합니다. 그렇긴 하지만, 그러한 문제를 예상할 수 있다면 실험을 변경하는 것이 더 나을 수 있습니다. 예를 들어, 해당 변수로 층화하는 것이 가능할 수 있습니다.\n\n\n8.2.4 피셔의 티 파티\n영국인은 차에 관해서는 재미있습니다. 영국에서는 완벽한 “차 한 잔”을 만드는 방법에 대해 상당하고 지속적인 논쟁이 있으며, 조지 오웰부터 존 레논까지 모든 사람이 의견을 제시합니다. 어떤 사람들은 우유를 먼저 넣으라고 말합니다. 다른 사람들은 마지막에 넣으라고 말합니다. 여론 조사 회사인 유고브는 대부분의 응답자가 우유를 마지막에 넣는다는 것을 발견했습니다 (Smith 2018). 그러나 순서가 전혀 중요하지 않은지 궁금해할 수 있습니다.\n피셔는 우유를 먼저 넣었는지, 아니면 마지막에 넣었는지에 따라 사람이 차 한 잔을 구별할 수 있는지 확인하기 위해 고안된 실험을 소개했습니다. 우리는 8잔의 차를 준비하는 것으로 시작합니다. 4잔은 우유를 먼저 넣고, 다른 4잔은 우유를 마지막에 넣습니다. 그런 다음 8잔 모두의 순서를 무작위로 정합니다. 우리는 시음자에게, 그를 “이안”이라고 부를 것입니다, 실험 설정에 대해 알려줍니다. 8잔의 차가 있고, 각 유형이 4잔씩 있으며, 그는 무작위 순서로 차를 받을 것이고, 그의 임무는 그것들을 두 그룹으로 나누는 것입니다.\n이 실험의 좋은 점 중 하나는 우리가 직접 할 수 있다는 것입니다. 실제로 주의해야 할 몇 가지 사항이 있습니다. 여기에는 다음이 포함됩니다.\n\n우유와 차의 양이 일관되도록 하고,\n그룹이 시음자가 볼 수 없는 방식으로 표시되도록 하고,\n순서가 무작위화되도록 합니다.\n\n이 실험의 또 다른 좋은 점은 이안이 무작위로 그룹을 올바르게 맞출 확률을 계산할 수 있다는 것입니다. 그의 그룹화가 무작위로 발생했을 가능성이 있는지 결정하려면, 이것이 일어날 확률을 계산해야 합니다. 먼저, 선택된 4개 중 성공 횟수를 셉니다. \\({8 \\choose 4} = \\frac{8!}{4!(8-4)!}=70\\)개의 가능한 결과가 있습니다 (Fisher [1935년] 1949, p. 14). 이 표기법은 집합에 8개의 항목이 있고, 그 중 4개를 선택하고 있으며, 선택 순서는 중요하지 않을 때 사용됩니다.\n우리는 이안에게 컵을 그룹화하도록 요청하고 있으며, 어느 것이 어느 것인지 식별하도록 요청하는 것이 아니므로, 그가 완벽하게 정확할 수 있는 두 가지 방법이 있습니다. 그는 우유를 먼저 넣은 모든 것을 정확하게 식별하거나(70개 중 1개 결과) 또는 차를 먼저 넣은 모든 것을 정확하게 식별할 수 있습니다(70개 중 1개 결과). 이것은 이 사건의 확률이 \\(\\frac{2}{70}\\) 또는 약 3%임을 의미합니다.\nFisher ([1935년] 1949.15)가 분명히 하듯이, 이것은 이제 판단의 문제가 됩니다. 우리는 그룹화가 우연히 발생하지 않았고 이안이 자신이 무엇을 하고 있는지 알고 있었다는 것을 받아들이기 전에 필요한 증거의 무게를 고려해야 합니다. 우리는 우리를 설득하는 데 필요한 증거가 무엇인지 결정해야 합니다. 실험에 들어오면서 가졌던 견해, 예를 들어 우유를 먼저 넣는 것과 차를 먼저 넣는 것 사이에 차이가 없다는 견해에서 우리를 단념시킬 수 있는 가능한 증거가 없다면, 실험을 하는 요점은 무엇입니까? 우리는 이안이 그것을 완전히 맞혔다면, 합리적인 사람은 그가 차이를 구별할 수 있었다는 것을 받아들일 것이라고 기대합니다.\n그가 거의 완벽하다면 어떨까요? 우연히, 사람이 “하나 틀릴” 수 있는 16가지 방법이 있습니다. 이안이 차를 먼저 넣었을 때 우유를 먼저 넣었다고 생각하는 컵이 하나 있거나—이것이 일어날 수 있는 방법은 \\({4 \\choose 1} = 4\\)가지입니다—또는 그가 우유를 먼저 넣었을 때 차를 먼저 넣었다고 생각하는 컵이 하나 있습니다—다시, \\({4 \\choose 1}\\) = 4가지 방법이 있습니다. 이러한 결과는 독립적이므로 확률은 \\(\\frac{4\\times 4}{70}\\) 또는 약 23%입니다. 찻잔을 무작위로 그룹화하는 것만으로도 하나 틀릴 확률이 거의 23%라는 점을 감안할 때, 이 결과는 아마도 이안이 차를 먼저 넣는 것과 우유를 먼저 넣는 것의 차이를 구별할 수 있다는 것을 우리에게 확신시키지 못할 것입니다.\n실험적으로 입증 가능하다고 주장하기 위해 우리가 찾고 있는 것은, 그러한 결과가 신뢰성 있게 발견되는 실험의 특징을 알게 되었다는 것입니다 (Fisher [1935년] 1949 p. 16). 우리는 단지 하나의 실험이 아니라 증거의 무게가 필요합니다. 우리는 데이터와 실험을 철저히 심문하고, 우리가 사용하는 분석 방법에 대해 정확하게 생각하려고 합니다. 별자리에서 의미를 찾는 대신, 다른 사람들이 우리 작업을 재현하기 쉽게 만들고 싶습니다. 그렇게 함으로써 우리의 결론이 장기적으로 유지될 가능성이 더 높아집니다.\n\n\n8.2.5 윤리적 기초\n의료 환경에서 증거의 무게는 잃어버린 생명으로 측정될 수 있습니다. 의료 실험에서 윤리적 관행이 발전한 한 가지 이유는 불필요한 인명 손실을 방지하기 위함입니다. 이제 우리는 윤리적 관행의 기초를 확립하는 데 도움이 된, 인간의 생명이 불필요하게 손실되었을 수 있는 두 가지 사례를 자세히 설명합니다. 터스키기 매독 연구를 논의함으로써 정보에 입각한 동의를 얻을 필요성을 고려합니다. 그리고 ECMO 실험을 논의함으로써 실험이 필요하다는 것을 보장할 필요성을 고려합니다.\n\n8.2.5.1 터스키기 매독 연구\nBrandt (1978) 및 (tuskegeeandthehealthofblackmen에?) 따르면, 터스키기 매독 연구는 1932년에 시작된 악명 높은 의료 시험입니다. 이 실험의 일환으로, 매독에 걸린 400명의 흑인 미국인은 매독에 대한 표준 치료법이 확립되고 널리 사용 가능해진 후에도 적절한 치료를 받지 못했으며, 심지어 매독에 걸렸다는 사실조차 듣지 못했습니다. 매독이 없는 대조군도 효과 없는 약물을 투여받았습니다. 미국 남부의 이 재정적으로 가난한 흑인 미국인들은 최소한의 보상을 제공받았고 실험의 일부라는 사실을 듣지 못했습니다. 또한, 지역 의사와 지역 보건부에 편지를 쓰는 것을 포함하여 남성들이 어디에서도 치료를 받지 못하도록 광범위한 작업이 수행되었습니다. 일부 남성들이 징집되어 즉시 치료를 받으라는 지시를 받은 후에도, 징병 위원회는 남성들을 치료에서 제외해 달라는 요청에 따랐습니다. 1972년에 연구가 중단될 때까지 남성들의 절반 이상이 사망했으며, 많은 사망은 매독 관련 원인이었습니다.\n터스키기 매독 연구의 영향은 연구에 참여한 남성들뿐만 아니라 더 넓게 느껴졌습니다. (tuskegeeandthehealthofblackmen은?) 의료 불신과 의사와의 상호 작용 감소로 인해 앨라배마 중부에 위치한 흑인 남성의 45세 기대 수명이 최대 1.5년 감소하는 것과 관련이 있음을 발견했습니다. 이에 대응하여 미국은 기관 검토 위원회에 대한 요구 사항을 제정했으며 클린턴 대통령은 1997년에 공식 사과했습니다. Brandt (1978, p. 27)은 다음과 같이 말합니다.\n\n돌이켜보면 터스키기 연구는 매독의 병리학보다 인종 차별의 병리학을 더 많이 드러냈습니다. 질병 과정의 본질보다 과학적 탐구의 본질에 대해 더 많이 드러냈습니다\\(\\dots\\) [T]과학이 가치 없는 학문이라는 개념은 거부되어야 합니다. 사회적 가치와 태도가 전문적인 행동에 미치는 구체적인 방식을 평가하는 데 더 큰 경계가 필요하다는 것이 분명히 나타납니다.\n\n(hellertuskegee는?) 터스키기 매독 연구에 대한 추가 배경 정보를 제공합니다.\n\n\n\n\n\n\n거인의 어깨 위에 서서\n\n\n\n마르셀라 알산 박사는 하버드 대학교의 공공 정책 교수입니다. 그녀는 로욜라 대학교에서 의학 박사 학위를 받았고 2012년 하버드 대학교에서 경제학 박사 학위를 받았습니다. 그녀는 스탠포드의 조교수로 임명되었고, 2019년 하버드로 돌아와 정교수로 승진했습니다. 그녀의 연구 분야 중 하나는 건강 불평등이며, 특히 중요한 논문 중 하나는 위에서 논의한 (tuskegeeandthehealthofblackmen입니다?). 그녀는 2021년에 맥아더 재단 펠로우십을 수상했습니다.\n\n\n\n\n8.2.5.2 체외막산소공급(ECMO)\n체외막산소공급(ECMO) 평가로 넘어가서, (ware1989investigating는?) ECMO를 신생아의 지속적인 폐고혈압 치료 가능성으로 보았다고 설명합니다. 그들은 19명의 환자를 등록하고 그 중 10명에게는 기존의 의료 요법을, 9명에게는 ECMO를 사용했습니다. 대조군의 10명 중 6명이 생존한 반면, 치료군의 모든 환자가 생존한 것으로 나타났습니다. (ware1989investigating는?) 무작위 동의를 사용했는데, 여기서 ECMO로 치료하도록 무작위로 선택된 영아의 부모에게만 동의를 요청했습니다.\n우리는 “평형 상태”에 관심이 있으며, 이는 치료가 기존 절차보다 더 효과적인지에 대한 진정한 불확실성이 있는 상황을 의미합니다. 의료 환경에서는 초기에 평형 상태가 있더라도 연구 초기에 치료가 효과적인 것으로 밝혀지면 약화될 수 있습니다. (ware1989investigating는?) 처음 19명의 환자의 결과 이후 무작위화가 중단되고 ECMO만 사용되었다고 설명합니다. 모집자와 환자를 치료하는 사람들은 처음에 무작위화가 중단되었다는 사실을 듣지 못했습니다. 이 완전한 ECMO 할당은 “28번째 생존자 또는 4번째 사망이 관찰될 때까지” 계속하기로 결정되었습니다. 20명의 추가 환자 중 19명이 생존한 후 시험이 종료되었습니다. 실험은 사실상 두 단계로 나뉘었습니다. 첫 번째 단계에서는 ECMO를 무작위로 사용했고, 두 번째 단계에서는 ECMO만 사용했습니다.\n이러한 환경에서 한 가지 접근 방식은 (wei1978randomized에?) 따른 “무작위 승자 플레이” 규칙입니다. 치료는 여전히 무작위화되지만, 각 성공적인 치료마다 확률이 이동하여 치료 가능성이 높아지고, 어떤 중단 규칙이 있습니다. (berry1989investigating는?) 더 정교한 중단 규칙이 필요하기는커녕, 평형 상태가 결코 존재하지 않았기 때문에 이 ECMO 연구가 필요하지 않았다고 주장합니다. (berry1989investigating는?) (ware1989investigating가?) 언급한 문헌을 다시 방문하여 ECMO가 이미 효과적인 것으로 알려져 있다는 광범위한 증거를 발견합니다. (berry1989investigating는?) 거의 완전한 합의가 없으므로 상당한 증거의 무게에도 불구하고 부적절하게 평형 상태의 존재를 주장할 수 있다고 지적합니다. (berry1989investigating는?) 부모가 다른 선택 사항이 있다는 것을 알았다면 기존의 의료 요법을 받은 영아에게 다른 결과가 있었을 수 있다는 가능성 때문에 무작위 동의를 사용한 (ware1989investigating를?) 더욱 비판합니다.\n터스키기 매독 연구와 ECMO 실험은 현재 상황과 상당히 멀어 보일 수 있습니다. 요즘에는 이러한 정확한 연구를 하는 것이 불법일 수 있지만, 비윤리적인 연구가 여전히 일어나지 않는다는 것을 의미하지는 않습니다. 예를 들어, 건강 및 기타 분야의 기계 학습 응용 프로그램에서 볼 수 있습니다. 명시적으로 차별해서는 안 되고 동의를 얻어야 하지만, 소비자의 동의 없이 암묵적으로 차별할 수 없다는 것을 의미하지는 않습니다. 예를 들어, (obermeyer2019dissecting은?) 미국의 많은 의료 시스템이 환자가 얼마나 아픈지를 점수화하기 위해 알고리즘을 사용한다고 설명합니다. 그들은 동일한 점수에 대해 흑인 환자가 더 아프고, 흑인 환자가 백인 환자와 동일한 방식으로 점수화되면 훨씬 더 많은 치료를 받을 것이라는 것을 보여줍니다. 그들은 알고리즘이 질병이 아닌 의료 비용을 기반으로 하기 때문에 차별이 발생한다는 것을 발견합니다. 그러나 의료에 대한 접근이 흑인과 백인 환자 사이에 불평등하게 분배되기 때문에, 알고리즘은 의도치 않게 인종적 편견을 영속시킵니다.",
    "crumbs": [
      "획득",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>실험 및 설문조사</span>"
    ]
  },
  {
    "objectID": "08-hunt_ko.html#설문-조사",
    "href": "08-hunt_ko.html#설문-조사",
    "title": "8  실험 및 설문조사",
    "section": "8.3 설문 조사",
    "text": "8.3 설문 조사\n측정할 대상을 결정한 후, 값을 얻는 일반적인 방법 중 하나는 설문 조사를 사용하는 것입니다. 이것은 특히 어려운 일이며, 설문 조사 연구라는 전체 분야가 이에 집중하고 있습니다. (Edelman2021Interview는?) 여기에 새로운 문제가 없으며, 오늘날 우리가 직면한 과제는 과거에 직면했던 과제와 밀접한 관련이 있음을 분명히 합니다. 설문 조사를 구현하는 방법에는 여러 가지가 있으며, 이 결정은 중요합니다. 한동안 유일한 선택은 면접관이 응답자와 직접 설문 조사를 실시하는 대면 설문 조사였습니다. 결국 설문 조사는 다시 면접관에 의해 전화를 통해 실시되기 시작했습니다. 이 두 가지 환경 모두에서 한 가지 문제는 상당한 면접관 효과였습니다 (Elliott 기타 2022). 인터넷은 낮은 참여율을 특징으로 하는 설문 조사 연구의 세 번째 시대를 열었습니다 (Groves 2011). 설문 조사는 데이터를 얻는 인기 있고 귀중한 방법입니다. 대면 및 전화 설문 조사는 여전히 사용되고 중요한 역할을 하지만, 현재 많은 설문 조사는 인터넷 기반입니다.\n서베이몽키 및 퀄트릭스와 같이 대체로 인터넷 기반인 전용 설문 조사 플랫폼이 많이 있습니다. 무료이기 때문에 특히 일반적인 접근 방식 중 하나는 구글 설문지를 사용하는 것입니다. 일반적으로 이러한 플랫폼의 초점은 사용자가 설문 조사 양식을 구성하고 보낼 수 있도록 하는 것입니다. 일반적으로 사용자가 이미 일부 표본 추출 프레임에 대한 연락처 정보를 가지고 있다고 예상합니다.\n?sec-reproducible-workflows에서 언급된 아마존 메카니컬 터크 및 프로리픽과 같은 다른 플랫폼은 응답자를 제공하는 데 중점을 둡니다. 이러한 플랫폼을 사용할 때 우리는 해당 응답자가 누구이며 관심 있는 인구와 어떻게 다를 수 있는지 이해하려고 노력해야 합니다 (Levay, Freese, 와/과 Druckman 2016; Enns 와/과 Rothschild 2022).\n설문지는 더 넓은 연구의 맥락에서 그리고 응답자에 대한 특별한 관심과 함께 고려되어야 합니다. 설문지를 공개하기 전에 설문 조사를 테스트하십시오. Light, Singer, 와/과 Willett (1990, p. 213)는 고등 교육을 평가하기 위한 연구의 맥락에서, 파일럿 연구가 개선을 가져오지 않을 경우는 없으며, 거의 항상 그만한 가치가 있다고 말합니다. 설문 조사의 경우, 우리는 더 나아갑니다. 설문 조사를 테스트할 시간이나 예산이 없다면, 설문 조사를 해야 하는지 다시 고려하는 것이 더 나을 수 있습니다.\n설문 조사의 표현을 테스트하십시오 (Tourangeau, Rips, 와/과 Rasinski 2000, p. 23). 설문 조사를 설계할 때, 우리는 대화식이고 주제 내에서 그룹화된 다음으로 넘어가는 설문 질문이 필요합니다 (Elson 2018). 그러나 우리는 또한 응답자에게 부과하는 인지 부하를 고려하고 질문의 난이도를 다양하게 해야 합니다.\n설문 조사를 설계할 때 중요한 작업은 응답자를 최우선으로 생각하는 것입니다 (Dillman, Smyth, 와/과 Christian [1978년] 2014, p. 94). (surveydesign을?) 바탕으로, 모든 질문은 관련성이 있어야 하고 응답자가 대답할 수 있어야 합니다. 질문의 표현은 응답자가 편안하게 느낄 수 있는 것을 기반으로 해야 합니다. 다른 질문 유형 간의 결정은 오류와 응답자에게 부과하는 부담을 모두 최소화하는 데 달려 있습니다. 일반적으로 명확한 옵션이 적은 경우 객관식 질문이 적절합니다. 이 경우 응답은 일반적으로 상호 배타적이고 집합적으로 완전해야 합니다. 상호 배타적이지 않은 경우, 이는 질문의 텍스트에 표시되어야 합니다. 또한 단위가 지정되고 가능한 한 표준 개념이 사용되는 것이 중요합니다.\n잠재적인 답변이 많은 경우 개방형 텍스트 상자가 적절할 수 있습니다. 이것은 응답자가 설문 조사를 완료하는 데 걸리는 시간과 답변을 분석하는 데 걸리는 시간을 모두 증가시킵니다. 한 번에 한 가지 질문만 하고, 특정 응답으로 유도하지 않는 중립적인 방식으로 질문하려고 노력하십시오. 설문 조사를 테스트하면 응답자를 혼란스럽게 할 수 있는 모호하거나 이중적인 질문을 피하는 데 도움이 됩니다. 설문 조사의 주제는 또한 적절한 질문 유형의 선택에 영향을 미칩니다. 예를 들어, 잠재적으로 “위협적인” 주제는 개방형 질문으로 더 잘 고려될 수 있습니다 (Blair 기타 1977).\n모든 설문 조사에는 설문 조사 제목, 실시자, 연락처 정보 및 목적을 명시하는 소개가 있어야 합니다. 또한 마련된 기밀 유지 보호 조치 및 얻은 윤리 심의 위원회 승인에 대한 진술도 포함해야 합니다.\n설문 조사를 할 때, 올바른 사람에게 묻는 것이 중요합니다. 예를 들어, (Lichand2022는?) 아동 노동을 고려합니다. 아동 노동의 정도는 일반적으로 부모의 설문 조사를 기반으로 합니다. 어린이를 대상으로 설문 조사를 했을 때 부모에 의한 상당한 과소 보고가 발견되었습니다.\n특히 우려되는 한 가지 측면은 성적 지향 및 성 정체성에 대한 질문입니다. 이것은 진화하는 분야이지만, (whitehousebestpractice는?) 데이터가 어떻게 사용될지 고려하고 충분한 표본 크기를 보장하는 것과 같은 모범 사례에 대한 권장 사항을 제공합니다. 성적 지향에 대해 묻는 것과 관련하여 그들은 다음 질문을 권장합니다.\n\n“다음 중 자신을 어떻게 생각하는지 가장 잘 나타내는 것은 무엇입니까?”\n\n“게이 또는 레즈비언”\n“이성애자, 즉 게이 또는 레즈비언이 아님”\n“양성애자”\n“다른 용어를 사용합니다 [자유 텍스트]”\n“모르겠습니다”\n\n\n그리고 성별과 관련하여 그들은 다중 질문 접근 방식을 권장합니다.\n\n“출생 시, 원래 출생 증명서에 어떤 성별이 지정되었습니까?”\n\n“여성”\n“남성”\n\n“현재 자신을 어떻게 설명합니까 (해당하는 모든 항목에 표시)?”\n\n“여성”\n“남성”\n“트랜스젠더”\n“다른 용어를 사용합니다 [자유 텍스트]”\n\n\n다시 말하지만, 이것은 진화하는 분야이며 모범 사례는 변경될 가능성이 높습니다.\n마지막으로, 애초에 설문 조사를 하는 이유로 돌아가서, 이 모든 것을 하는 동안, 우리가 측정에 관심이 있는 것을 염두에 두는 것도 중요합니다. 설문 질문이 추정량과 관련이 있는지 확인하십시오.\n\n8.3.1 민주주의 기금 유권자 연구 그룹\n설문 데이터의 예로, 우리는 민주주의 기금 유권자 연구 그룹 네이션스케이프 데이터셋 (Tausanovitch 와/과 Vavreck 2021)을 고려할 것입니다. 이것은 2019년 7월부터 2021년 1월까지 실시된 대규모 설문 조사 시리즈입니다. 성별, 주요 인구 조사 지역, 인종, 히스패닉 민족, 가구 소득, 교육 및 연령을 포함한 여러 변수에 대해 가중치가 부여됩니다. (nationscape2021은?) 이를 ?sec-farm-data에서 소개된 편의 표본으로 설명하며, 인구 통계를 기반으로 합니다. 이 경우, (nationscape2021은?) 표본이 특정 인구 통계 할당량을 기반으로 설문 조사 응답자를 위한 온라인 플랫폼을 운영하는 Lucid에서 제공되었다고 자세히 설명합니다. (nationscape2021은?) 결과가 정부 및 상업 설문 조사와 유사하다는 것을 발견했습니다.\n데이터셋을 얻으려면 민주주의 기금 유권자 연구 그룹 웹사이트로 이동한 다음 “네이션스케이프”를 찾아 데이터에 대한 액세스를 요청하십시오. 하루나 이틀이 걸릴 수 있습니다. 액세스 권한을 얻은 후 “.dta” 파일에 집중하십시오. 네이션스케이프는 2020년 미국 선거를 앞두고 많은 설문 조사를 실시했으므로 많은 파일이 있습니다. 파일 이름은 참조 날짜이며, “ns20200625”는 2020년 6월 25일을 의미합니다. 여기서는 해당 파일을 사용하지만, 많은 파일이 유사합니다. 우리는 그것을 다운로드하여 “ns20200625.dta”로 저장합니다.\n?sec-r-essentials에서 소개된 바와 같이, haven 및 labelled을 설치한 후 “.dta” 파일을 가져올 수 있습니다. 설문 데이터셋을 가져오고 준비하는 데 사용하는 코드는 (greatstudentwork의?) 코드를 기반으로 합니다.\n#| eval: false\n#| echo: true\n\nraw_nationscape_data &lt;-\n  read_dta(\"ns20200625.dta\")\n#| eval: false\n#| echo: false\n\n# INTERNAL ONLY\n\nraw_nationscape_data &lt;-\n  read_dta(\"dont_push/ns20200625.dta\")\n#| eval: false\n#| echo: true\n\n# Stata 형식은 레이블을 분리하므로 다시 결합합니다.\nraw_nationscape_data &lt;-\n  to_factor(raw_nationscape_data)\n\n# 관련 변수만 유지합니다.\nnationscape_data &lt;-\n  raw_nationscape_data |&gt;\n  select(vote_2020, gender, education, state, age)\n#| eval: false\n#| echo: false\n\n# INTERNAL ONLY\n\narrow::write_parquet(\n  x = nationscape_data,\n  sink = \"outputs/data/nationscape_data_not_yet_nice.parquet\"\n)\n#| eval: true\n#| echo: false\n\nnationscape_data &lt;-\n  arrow::read_parquet(\n    file = \"outputs/data/nationscape_data_not_yet_nice.parquet\"\n)\nnationscape_data\n이 시점에서 몇 가지 문제를 정리하고 싶습니다. 예를 들어, 간단하게 하기 위해 트럼프나 바이든에게 투표하지 않은 사람은 제거합니다.\n#| eval: false\n#| echo: true\n\nnationscape_data &lt;-\n  nationscape_data |&gt;\n  filter(vote_2020 %in% c(\"Joe Biden\", \"Donald Trump\")) |&gt;\n  mutate(vote_biden = if_else(vote_2020 == \"Joe Biden\", 1, 0)) |&gt;\n  select(-vote_2020)\n그런 다음 관심 있는 몇 가지 변수를 만들고 싶습니다.\n#| eval: false\n#| echo: true\n\nnationscape_data &lt;-\n  nationscape_data |&gt;\n  mutate(\n    age_group = case_when(\n      age &lt;= 29 ~ \"18-29\",\n      age &lt;= 44 ~ \"30-44\",\n      age &lt;= 59 ~ \"45-59\",\n      age &gt;= 60 ~ \"60+\",\n      TRUE ~ \"Trouble\"\n    ),\n    gender = case_when(\n      gender == \"Female\" ~ \"female\",\n      gender == \"Male\" ~ \"male\",\n      TRUE ~ \"Trouble\"\n    ),\n    education_level = case_when(\n      education %in% c(\n        \"3rd Grade or less\",\n        \"Middle School - Grades 4 - 8\",\n        \"Completed some high school\",\n        \"High school graduate\"\n      ) ~ \"High school or less\",\n      education %in% c(\n        \"Other post high school vocational training\",\n        \"Completed some college, but no degree\"\n      ) ~ \"Some post sec\",\n      education %in% c(\n        \"Associate Degree\",\n        \"College Degree (such as B.A., B.S.)\",\n        \"Completed some graduate, but no degree\"\n      ) ~ \"Post sec +\",\n      education %in% c(\"Masters degree\",\n                       \"Doctorate degree\") ~ \"Grad degree\",\n      TRUE ~ \"Trouble\"\n    )\n  ) |&gt;\n  select(-education,-age)\n?sec-multilevel-regression-with-post-stratification에서 이 데이터셋을 사용할 것이므로 저장하겠습니다.\n#| eval: false\n#| include: true\n\nwrite_csv(x = nationscape_data,\n          file = \"nationscape_data.csv\")\n#| eval: false\n#| include: false\n\n# INTERNAL\n\narrow::write_parquet(x = nationscape_data,\n              sink = \"outputs/data/15-nationscape_data.parquet\")\n#| eval: true\n#| include: false\n\nnationscape_data &lt;-\n  arrow::read_parquet(file = \"outputs/data/15-nationscape_data.parquet\")\n일부 변수도 살펴볼 수 있습니다(?fig-nationscapesurveydataquickgraph).\n#| fig-cap: \"네이션스케이프 설문 조사 데이터셋의 일부 변수 검토\"\n#| label: fig-nationscapesurveydataquickgraph\n\nnationscape_data |&gt;\n  mutate(supports = if_else(vote_biden == 1, \"Biden\", \"Trump\")) |&gt; \n  mutate(supports = factor(supports, levels = c(\"Trump\", \"Biden\"))) |&gt; \n  ggplot(mapping = aes(x = age_group, fill = supports)) +\n  geom_bar(position = \"dodge2\") +\n  theme_minimal() +\n  labs(\n    x = \"Age-group of respondent\",\n    y = \"Number of respondents\",\n    fill = \"Voted for\"\n  ) +\n  facet_wrap(vars(gender)) +\n  guides(x = guide_axis(angle = 90)) +\n  theme(legend.position = \"bottom\") +\n  scale_fill_brewer(palette = \"Set1\")",
    "crumbs": [
      "획득",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>실험 및 설문조사</span>"
    ]
  },
  {
    "objectID": "08-hunt_ko.html#rct-예시",
    "href": "08-hunt_ko.html#rct-예시",
    "title": "8  실험 및 설문조사",
    "section": "8.4 RCT 예시",
    "text": "8.4 RCT 예시\n\n8.4.1 오리건 건강 보험 실험\n미국에서는 많은 선진국과 달리 저소득층을 포함한 모든 거주자에게 기본 건강 보험이 반드시 제공되는 것은 아닙니다. 오리건 건강 보험 실험은 2008년부터 2010년까지 미국 북서부의 한 주인 오리건의 저소득 성인을 대상으로 했습니다 (Finkelstein 기타 2012).\n\n\n\n\n\n\n거인의 어깨 위에 서서\n\n\n\n에이미 핀켈스타인 박사는 MIT의 존 앤 제니 S. 맥도널드 경제학 교수입니다. 2001년 MIT에서 경제학 박사 학위를 취득한 후, 그녀는 하버드 펠로우 협회의 주니어 펠로우였으며, 2005년 MIT로 돌아와 조교수로 재직하다가 2008년에 정교수로 승진했습니다. 그녀의 연구 분야 중 하나는 건강 경제학이며, 보험을 이해하기 위해 무작위 통제 시험을 사용합니다. 그녀는 오리건 건강 보험 실험을 조사한 (finkelstein2012oregon의?) 주요 연구원 중 한 명이었습니다. 그녀는 2012년에 존 베이츠 클라크 메달을, 2018년에 맥아더 재단 펠로우십을 수상했습니다.\n\n\n오리건은 저소득층에게 건강 보험을 제공하는 주 운영 메디케이드 프로그램에 10,000개의 자리를 지원했습니다. 이러한 자리를 할당하기 위해 복권이 사용되었으며, 이는 장소에 대한 수요가 공급을 초과할 것으로 예상되었기 때문에 공정하다고 판단되었습니다. 결국 89,824명이 등록했습니다.\n추첨은 6개월에 걸쳐 실시되었고 35,169명이 선정되었지만(추첨에 당첨된 사람의 가구에 기회가 주어졌습니다), 그 중 30%만이 자격이 있고 서류를 완료한 것으로 나타났습니다. 보험은 무기한 지속되었습니다. 이 무작위 보험 할당은 연구자들이 건강 보험의 효과를 이해할 수 있게 해주었습니다.\n이 무작위 할당이 중요한 이유는 일반적으로 건강 보험에 가입하는 사람들의 유형이 그렇지 않은 사람들과 다르기 때문에 보험이 있는 사람과 없는 사람을 비교할 수 없기 때문입니다. 그 결정은 다른 변수와 “혼동”되어 선택 편향을 초래합니다.\n건강 보험 신청 기회가 무작위로 할당되었기 때문에, 연구자들은 건강 보험을 받은 사람들의 건강과 소득을 평가하고 그렇지 않은 사람들과 비교할 수 있었습니다. 이를 위해 그들은 병원 퇴원 데이터, 일치된 신용 보고서, 그리고 드물게 사망 기록과 같은 행정 데이터를 사용했습니다. 이 데이터의 범위는 제한적이므로 설문 조사도 실시했습니다.\n이것의 구체적인 내용은 중요하지 않으며, ?sec-its-just-a-linear-model에서 더 자세히 설명하겠지만, 그들은 다음 모델을 추정합니다.\n\\[\ny_{ihj} = \\beta_0 + \\beta_1\\mbox{Lottery} + X_{ih}\\beta_2 + V_{ih}\\beta_3 + \\epsilon_{ihj}\n\\]\n?eq-oregon은 가구 \\(h\\)가 복권에 의해 선택되었는지 여부에 대한 지표 변수의 함수로서 가구 \\(h\\)의 개인 \\(i\\)에 대한 다양한 \\(j\\) 결과(예: 건강)를 설명합니다. 특히 관심 있는 것은 \\(\\beta_1\\) 계수입니다. 이것은 처리군과 대조군 간의 평균 차이의 추정치입니다. \\(X_{ih}\\)는 처리될 확률과 상관 관계가 있는 변수 집합입니다. 이것들은 그 영향을 어느 정도 조정합니다. 그 예로는 가구의 개인 수가 있습니다. 그리고 마지막으로, \\(V_{ih}\\)는 인구 통계 및 이전 병원 퇴원과 같이 복권과 상관 관계가 없는 변수 집합입니다.\n(randhealth와?) 같은 이전 연구와 마찬가지로, (finkelstein2012oregon은?) 치료군이 1차 및 예방 진료와 입원을 포함한 더 많은 의료 서비스를 사용했지만, 본인 부담 의료비는 더 낮다는 것을 발견했습니다. 더 일반적으로, 치료군은 더 나은 신체적, 정신적 건강을 보고했습니다.\n\n\n8.4.2 전 세계의 시민적 정직성\n신뢰는 우리가 정기적으로 생각하는 것이 아니지만, 경제적 및 개인적 상호 작용의 대부분에 기본적입니다. 예를 들어, 많은 사람들이 일을 한 후에 돈을 받습니다. 그들은 고용주가 약속을 지킬 것이라고 신뢰하고, 그 반대도 마찬가지입니다. 미리 돈을 받으면, 그들은 당신을 신뢰하고 있습니다. 엄밀히 순진하고, 거래 비용이 없는 일회성 세계에서는 이것이 의미가 없습니다. 미리 돈을 받으면, 그만두기 전 마지막 급여 기간에 돈을 가지고 도망가는 것이 인센티브이며, 역방향 귀납법을 통해 모든 것이 무너집니다. 우리는 그런 세상에 살고 있지 않습니다. 한 가지 이유는 거래 비용이 있고, 다른 하나는 일반적으로 반복적인 상호 작용이 있으며, 마지막으로 세상은 보통 상당히 작습니다.\n다른 국가의 정직성 정도를 이해하면 경제 발전 및 세금 준수와 같은 다른 관심 측면을 설명하는 데 도움이 될 수 있지만, 측정하기는 어렵습니다. 우리는 사람들에게 얼마나 정직한지 물을 수 없습니다. 거짓말쟁이는 거짓말을 할 것이고, 레몬 문제를 초래할 것입니다 (Akerlof 1970). 이것은 역선택의 상황이며, 거짓말쟁이는 자신이 거짓말쟁이라는 것을 알지만 다른 사람들은 모릅니다. 이를 해결하기 위해 (cohn2019civic은?) 40개국 355개 도시에서 비어 있거나 미화 13.45달러에 해당하는 현지 통화가 들어 있는 지갑을 “반납”하는 실험을 수행합니다. 그들은 “수령인”이 지갑을 반환하려고 시도했는지 여부에 관심이 있었습니다. 그들은 일반적으로 돈이 든 지갑이 없는 지갑보다 반환될 가능성이 더 높다는 것을 발견했습니다 (Cohn 기타 2019a, p. 1).\n총 (cohn2019civic은?) 은행, 박물관, 호텔 및 경찰서를 포함한 다양한 기관에 17,303개의 지갑을 “반납”합니다. 경제에 대한 그러한 기관의 중요성은 잘 알려져 있으며 (Acemoglu, Johnson, 와/과 Robinson 2001) 대부분의 국가에서 일반적입니다. 중요하게도, 실험을 위해, 그들은 보통 지갑을 반납할 수 있는 접수 구역이 있습니다 (Cohn 기타 2019a, p. 1).\n실험에서 연구 조교는 정해진 형식의 말을 사용하여 접수 구역의 직원에게 지갑을 건넸습니다. 연구 조교는 “수령인”의 성별, 연령대 및 바쁨과 같은 환경의 다양한 특징을 기록해야 했습니다. 지갑은 투명했으며 열쇠, 식료품 목록 및 이름과 이메일 주소가 있는 명함이 들어 있었습니다. 관심 있는 결과는 지갑의 명함에 있는 고유한 이메일 주소로 이메일이 전송되었는지 여부였습니다. 식료품 목록은 지갑 소유자가 현지인임을 알리기 위해 포함되었습니다. 열쇠는 현금과 달리 수령인이 아닌 지갑 소유자에게만 유용한 것으로 포함되어 이타주의를 조정했습니다. 언어와 통화는 현지 상황에 맞게 조정되었습니다.\n실험의 주요 처리는 지갑에 돈이 들어 있었는지 여부입니다. 주요 결과는 지갑이 반환되려고 시도되었는지 여부였습니다. 중앙값 응답 시간은 26분이었고, 이메일이 전송되면 보통 하루 안에 발생한다는 것이 밝혀졌습니다 (Cohn 기타 2019b, p. 10).\n논문에 대한 데이터를 사용하여 (Cohn 2019) 국가 간에 상당한 차이가 발견되었음을 알 수 있습니다(?fig-wallets). 거의 모든 국가에서 돈이 든 지갑이 없는 지갑보다 반환될 가능성이 더 높았습니다. 실험은 40개국에 걸쳐 수행되었으며, 인구가 100,000명 이상인 도시가 충분하고 연구 조교가 안전하게 방문하고 현금을 인출할 수 있는 능력을 기준으로 선택되었습니다. 해당 국가 내에서 도시는 가장 큰 도시부터 시작하여 선택되었으며, 일반적으로 각 국가에 400개의 관측치가 있었습니다 (Cohn 기타 2019b, p. 5). (cohn2019civic은?) 폴란드, 영국, 미국 세 국가에서 미화 94.15달러에 해당하는 금액으로 실험을 추가로 수행했으며, 보고율이 더욱 증가했음을 발견했습니다.\n#| fig-cap: \"돈이 들어 있었는지 여부에 따라 국가별로 반납된 지갑의 비율 비교\"\n#| echo: false\n#| label: fig-wallets\n#| message: false\n#| warning: false\n#| fig-height: 7\n\nwallet_data &lt;- read_csv(here::here(\"inputs/data/behavioral_data.csv\"))\n\nwallet_data |&gt;\n  filter(cond %in% c(0, 1)) |&gt;\n  count(Country, cond, response) |&gt;\n  mutate(freq = n / sum(n),\n         .by = c(Country, cond)) |&gt;\n  filter(response == 100) |&gt;\n  mutate(cond = factor(if_else(cond == 0, \"No\", \"Yes\"))) |&gt;\n  ggplot() +\n  geom_point(aes(x = reorder(Country, -freq, FUN = mean), y = freq, color = cond)) +\n  labs(\n    x = \"Country\",\n    y = \"Reporting rate (%)\",\n    color = \"Contained money?\"\n  ) +\n  scale_color_brewer(palette = \"Set1\") +\n  coord_flip() +\n  theme_classic() +\n  theme(legend.position = \"bottom\")\n실험 외에도 (cohn2019civic은?) 설문 조사를 실시하여 그들의 발견에 대한 몇 가지 이유를 이해할 수 있었습니다. 설문 조사 중에 참가자들은 시나리오 중 하나를 받고 질문에 답하도록 요청받았습니다. 설문 조사를 사용하면 응답자에 대해 구체적으로 알 수 있었습니다. 설문 조사에는 2,525명의 응답자가 참여했습니다(영국 829명, 폴란드 809명, 미국 887명) (Cohn 기타 2019b, p. 36). 참가자는 연령, 성별 및 거주지를 기반으로 한 주의력 확인 및 인구 통계 할당량을 사용하여 선택되었으며, 참여에 대해 미화 4.00달러를 받았습니다 (Cohn 기타 2019b, p. 36). 설문 조사에서는 더 많은 돈이 든 지갑을 반납하는 데 더 큰 보상이 기대된다는 것을 발견하지 못했습니다. 그러나 더 많은 돈이 든 지갑을 반납하지 않으면 응답자가 돈을 훔친 것처럼 더 많이 느끼게 된다는 것을 발견했습니다.",
    "crumbs": [
      "획득",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>실험 및 설문조사</span>"
    ]
  },
  {
    "objectID": "08-hunt_ko.html#ab-테스트",
    "href": "08-hunt_ko.html#ab-테스트",
    "title": "8  실험 및 설문조사",
    "section": "8.5 A/B 테스트",
    "text": "8.5 A/B 테스트\n지난 20년 동안 아마도 수십 배나 많은 실험이 실행되었을 것입니다. 이것은 기술 회사에서 A/B 테스트를 광범위하게 사용하기 때문입니다 (Kohavi 기타 2012). 오랫동안 어떤 글꼴을 사용할지와 같은 결정은 최고 연봉자의 의견(HIPPO)에 기반했습니다 (Christian 2012). 요즘에는 많은 대규모 기술 회사들이 실험을 위한 광범위한 인프라를 갖추고 있습니다. 그들은 두 그룹을 비교하기 때문에 A/B 테스트라고 부릅니다. 한 그룹은 처리 A를 받고 다른 그룹은 처리 B를 받거나 아무런 변화를 보지 않습니다 (Salganik 2018, p. 185). 우리는 추가로 두 개 이상의 옵션을 고려할 수 있으며, 이 시점에서 일반적으로 실험의 “팔”이라는 용어를 사용합니다.\n민간 부문에서 실험이 확산되면서 윤리적 우려가 많이 제기되었습니다. 일부 민간 기업에는 윤리 심의 위원회가 없으며, 학계와 비교하여 민간 부문에는 다른 윤리적 우려가 있습니다. 예를 들어, 많은 A/B 테스트는 소비자가 돈을 더 많이 쓰도록 명시적으로 설계되었습니다. 사회는 일반적으로 온라인 식료품 소매업체의 경우에 대해 우려하지 않을 수 있지만, 온라인 도박 웹사이트의 경우에는 문제가 있을 수 있습니다. 민간 부문에서 실험의 정도가 더 잘 알려짐에 따라 더 광범위한 법률과 민간 부문 윤리 모범 사례의 개발이 모두 가능성이 높습니다.\n온라인에 있을 때마다 아마도 수십, 수백 또는 수천 개의 다른 A/B 테스트를 받게 될 것입니다. 본질적으로는 데이터를 측정하기 위해 센서를 사용하는 실험일 뿐이지만, 그 자체로 흥미로운 많은 특별한 특징을 가지고 있습니다. 예를 들어, Kohavi, Tang, 와/과 Xu (2020, p. 3)는 마이크로소프트의 검색 엔진 빙의 예를 논의합니다. 그들은 광고를 표시하는 방법을 조사하기 위해 A/B 테스트를 사용했습니다. 이러한 테스트를 기반으로 그들은 광고의 제목을 길게 했습니다. 그들은 이것이 측정된 상당한 절충안 없이 연간 12% 또는 약 1억 달러의 수익을 증가시켰다는 것을 발견했습니다.\n이 책에서는 A/B 테스트라는 용어를 주로 기술 스택을 통해 실험을 구현하고, 웹사이트 변경이나 이와 유사한 인터넷과 관련된 것에 대해 주로 센서로 측정하는 상황을 지칭하는 데 사용합니다. 본질적으로는 실험일 뿐이지만, A/B 테스트에는 다양한 특정 우려 사항이 있습니다. (Bosch2022는?) 통계적 관점에서 이러한 일부를 자세히 설명합니다. 한 번의 실험을 몇 달에 걸쳐 수행하는 일반적인 RCT 설정과 비교하여, 항상 수만 개의 작은 실험을 수행하는 것에는 다른 점이 있습니다.\nRCT는 종종, 독점적이지는 않지만, 학계나 정부 기관에서 수행되지만, 많은 A/B 테스트는 업계에서 발생합니다. 이것은 업계에 있고 회사에 A/B 테스트를 도입하고 싶다면 문화 및 관계 구축과 같은 측면이 중요해질 수 있음을 의미합니다. 관리자에게 실험을 실행하도록 설득하는 것은 어려울 수 있습니다. 실제로, 때로는 처리군보다는 대조군을 만들기 위해 결정된 변경 사항을 전달하지 않거나 지연시켜 실험하는 것이 더 쉬울 수 있습니다 (Salganik 2018 p. 188). 때로는 A/B 테스트의 가장 어려운 측면은 분석이 아니라 정치입니다. 이것은 A/B 테스트에만 국한된 것이 아니며, 예를 들어 생물학의 역사를 보면, 세균 이론과 같은 측면조차도 실험이 아니라 이데올로기와 사회적 지위에 의해 해결되었다는 것을 알 수 있습니다 (Morange 2016, p. 124).\nKohavi, Tang, 와/과 Xu (2020, p. 153)에 따르면, A/B 테스트를 수행할 때, 모든 실험과 마찬가지로, 우리는 전달에 대해 우려해야 합니다. 실험의 경우, 그것이 어떻게 전달되는지는 보통 명확합니다. 예를 들어, 우리는 그 사람을 의사 진료소에 오게 한 다음 약물이나 위약을 주사할 수 있습니다. 그러나 A/B 테스트의 경우, 덜 명확합니다. 예를 들어, 웹사이트를 변경해야 할까요, 아니면 앱을 변경해야 할까요? 이 결정은 실험을 수행하고 그로부터 데이터를 수집하는 우리의 능력에 영향을 미칩니다. ((netflixabtesting은?) PlayStation 4에 앱이 설치되어 있다고 가정하고 Netflix에서 A/B 테스트에 대한 개요를 제공합니다.)\n웹사이트를 항상 업데이트하는 것은 비교적 쉽고 정상적입니다. 이것은 A/B 테스트가 그런 식으로 전달되면 작은 변경 사항을 쉽게 구현할 수 있음을 의미합니다. 그러나 앱의 경우 A/B 테스트를 수행하는 것이 더 큰 일이 됩니다. 예를 들어, 릴리스는 앱 스토어를 거쳐야 할 수 있으므로 정기적인 릴리스 주기의 일부여야 합니다. 선택 문제도 있습니다. 일부 사용자는 앱을 업데이트하지 않을 것이며, 정기적으로 앱을 업데이트하는 사용자와 다를 수 있습니다.\n전달 결정은 또한 A/B 테스트에서 데이터를 수집하는 우리의 능력에 영향을 미칩니다. 웹사이트 변경은 사용자가 상호 작용할 때마다 웹사이트에서 데이터를 얻기 때문에 큰 문제가 아닙니다. 그러나 앱의 경우 사용자가 오프라인으로 또는 제한된 데이터 업로드로 앱을 사용할 수 있으며, 이는 복잡성을 더할 수 있습니다.\n계획해야 합니다! 예를 들어, 앱 변경 다음 날에는 결과가 제공되지 않을 가능성이 높지만, 웹사이트 변경 다음 날에는 제공될 수 있습니다. 또한, 다른 장치 및 플랫폼의 맥락에서 결과를 고려해야 할 수 있으며, 잠재적으로 ?sec-its-just-a-linear-model에서 다룰 회귀를 사용할 수 있습니다.\n?sec-farm-data에서 소개된 두 번째 우려 사항은 계측입니다. 전통적인 실험을 수행할 때, 예를 들어, 응답자에게 설문 조사를 작성하도록 요청할 수 있습니다. 그러나 이것은 일반적으로 A/B 테스트에서는 수행되지 않습니다. 대신 우리는 일반적으로 다양한 센서를 사용합니다 (Kohavi, Tang, 와/과 Xu 2020, p. 162). 한 가지 접근 방식은 쿠키를 사용하는 것이지만, 다른 유형의 사용자는 다른 속도로 이것들을 지울 것입니다. 또 다른 접근 방식은 사용자가 어떤 작업을 완료했을 때 알 수 있도록 서버에서 작은 이미지를 다운로드하도록 강요하는 것입니다. 예를 들어, 이것은 사용자가 이메일을 열었는지 추적하는 데 일반적으로 사용됩니다. 그러나 다시 다른 유형의 사용자는 다른 속도로 이것들을 차단할 것입니다.\n세 번째 우려 사항은 우리가 무엇을 무작위화하고 있는가입니다 (Kohavi, Tang, 와/과 Xu 2020, p. 166)? 전통적인 실험의 경우, 이것은 종종 사람이거나 때로는 다양한 그룹의 사람들입니다. 그러나 A/B 테스트의 경우 덜 명확할 수 있습니다. 예를 들어, 페이지, 세션 또는 사용자를 무작위화하고 있습니까?\n이것을 생각하기 위해, 색상을 고려해 봅시다. 예를 들어, 홈페이지에서 로고를 빨간색에서 파란색으로 변경해야 하는지에 관심이 있다고 가정해 봅시다. 페이지 수준에서 무작위화하는 경우, 사용자가 웹사이트의 다른 페이지로 이동한 다음 홈페이지로 돌아오면 로고 색상이 변경될 수 있습니다. 세션 수준에서 무작위화하는 경우, 이번에 웹사이트를 사용하는 동안 파란색일 수 있고, 닫았다가 다시 오면 빨간색일 수 있습니다. 마지막으로, 사용자 수준에서 무작위화하는 경우, 한 사용자에게는 항상 빨간색일 수 있지만 다른 사용자에게는 항상 파란색일 수 있습니다.\n이것이 어느 정도 중요한지는 일관성과 중요성 사이의 절충안에 따라 다릅니다. 예를 들어, 제품 가격을 A/B 테스트하는 경우 일관성이 중요한 기능일 가능성이 높습니다. 그러나 배경색을 A/B 테스트하는 경우 일관성이 그다지 중요하지 않을 수 있습니다. 반면에, 로그인 버튼의 위치를 A/B 테스트하는 경우 한 사용자에 대해 너무 많이 움직이지 않는 것이 중요할 수 있지만, 사용자 간에는 덜 중요할 수 있습니다.\nA/B 테스트에서는 전통적인 실험과 마찬가지로, 처리군과 대조군이 처리를 제외하고는 동일하다는 점에 관심이 있습니다. 전통적인 실험의 경우, 실험이 수행된 후 우리가 가진 데이터를 기반으로 분석을 수행하여 이를 만족시킵니다. 두 그룹을 모두 처리하거나 통제하는 것이 이상하기 때문에 일반적으로 우리가 할 수 있는 전부입니다. 그러나 A/B 테스트의 경우, 실험 속도를 통해 처리군과 대조군을 무작위로 생성한 다음, 처리군을 처리에 노출시키기 전에 그룹이 동일한지 확인할 수 있습니다. 예를 들어, 각 그룹에 동일한 웹사이트를 보여주면, 두 그룹에 걸쳐 동일한 결과를 기대할 수 있습니다. 다른 결과를 발견하면 무작위화 문제가 있을 수 있다는 것을 알게 됩니다 (Taddy 2019, p. 129). 이것은 A/A 테스트라고 하며 ?sec-on-writing에서 언급되었습니다.\n우리는 일반적으로 특정 결과에 대해 절실히 신경 쓰기 때문에 A/B 테스트를 실행하는 것이 아니라, 그것이 우리가 신경 쓰는 다른 측정에 영향을 미치기 때문입니다. 예를 들어, 웹사이트가 아주 어두운 파란색인지 약간 어두운 파란색인지 신경 씁니까? 아마도 아닐 것입니다. 우리는 아마도 실제로 회사 주가에 대해 신경 쓸 것입니다. 그러나 최고의 파란색에 대한 A/B 테스트 결과가 주가에 비용을 초래한다면 어떨까요?\n이를 설명하기 위해, 우리가 음식 배달 앱에서 일하고 있고, 운전자 유지에 대해 우려하고 있다고 가정해 봅시다. A/B 테스트를 수행하고 운전자가 고객에게 음식을 더 빨리 배달할 수 있을 때 항상 유지될 가능성이 더 높다는 것을 발견했다고 가정해 봅시다. 우리의 가상적인 발견은 운전자 유지에 대해 더 빠른 것이 항상 더 좋다는 것입니다. 그러나 더 빠른 배달을 달성하는 한 가지 방법은 운전자가 음식의 온도를 유지하는 뜨거운 상자에 음식을 넣지 않는 것입니다. 이와 같은 것은 30초를 절약할 수 있으며, 이는 10분 배달에서 상당합니다. 불행히도, 운전자 유지를 최적화하기 위해 설계된 A/B 테스트를 기반으로 그렇게 결정하더라도, 그러한 결정은 고객 경험을 악화시킬 가능성이 높습니다. 고객이 뜨거워야 할 차가운 음식을 받으면 앱 사용을 중단할 수 있으며, 이는 비즈니스에 좋지 않을 것입니다. (fbdiscoverslongterm은?) 알림 측면에서 페이스북에서 비슷한 상황을 발견했다고 설명합니다. 알림 수를 줄이면 단기적으로는 사용자 참여가 감소했지만, 장기적으로는 사용자 만족도와 앱 사용량이 모두 증가했습니다.\n이 절충안은 가상 운전자 실험 중에 고객 불만을 살펴보면 알 수 있습니다. 소규모 팀에서는 A/B 테스트 분석가가 해당 티켓에 노출될 수 있지만, 대규모 팀에서는 그렇지 않을 수 있습니다. A/B 테스트가 잘못된 최적화를 초래하지 않도록 하는 것이 특히 중요합니다. 이것은 우리가 일반적으로 일반적인 실험에서 걱정해야 하는 것이 아닙니다. 이 예의 또 다른 예로 (duolingo는?) 일반 Duolingo 사용자가 오프라인일 때 Duolingo Plus 광고를 제공하는 Duolingo, 언어 학습 응용 프로그램의 기능을 테스트하는 것을 설명합니다. 이 기능은 Duolingo의 수익에 긍정적인 것으로 나타났지만, 고객 학습 습관에는 부정적이었습니다. 아마도 충분한 고객 부정성은 결국 수익에 부정적인 영향을 미쳤을 것입니다. 이와 관련하여, 우리는 우리가 기대하는 결과의 성격에 대해 신중하게 생각하고 싶습니다. 예를 들어, 파란색 음영 예에서는 상당한 놀라움을 발견할 가능성이 낮으므로, 작은 범위의 파란색을 시도하는 것으로 충분할 수 있습니다. 그러나 더 다양한 색상을 고려한다면 어떨까요?\n\n\n\n\n\n\n거인의 어깨 위에 서서\n\n\n\n수잔 애시 박사는 스탠포드 대학교의 기술 경제학 교수입니다. 1995년 스탠포드에서 경제학 박사 학위를 취득한 후, 그녀는 MIT에서 조교수로 재직했으며, 2001년 스탠포드로 돌아와 2004년에 정교수로 승진했습니다. 그녀의 연구 분야 중 하나는 응용 경제학이며, 특히 중요한 논문 중 하나는 표준 오차를 군집화해야 할 때를 고려하는 (Abadie2017입니다?). 다른 하나는 무작위 실험을 분석하는 방법을 고려하는 (Athey2017입니다?). 학술적 임명 외에도, 그녀는 마이크로소프트 및 기타 기술 회사에서 근무했으며, 이 맥락에서 실험을 실행하는 데 광범위하게 관여했습니다. 그녀는 2007년에 존 베이츠 클라크 메달을 수상했습니다.\n\n\n\n8.5.1 업워디\n많은 A/B 테스트의 문제는 민간 기업에서 수행되므로 일반적으로 데이터셋에 액세스할 수 없다는 것입니다. 그러나 (upworthy는?) A/B 테스트를 사용하여 콘텐츠를 최적화한 미디어 웹사이트인 업워디의 A/B 테스트 데이터셋에 대한 액세스를 제공합니다. (aboutupworthy는?) 업워디에 대한 더 많은 배경 정보를 제공합니다. 그리고 A/B 테스트의 데이터셋은 여기에서 사용할 수 있습니다.\n데이터셋이 어떻게 생겼는지 살펴보고 이름과 발췌문을 보고 감을 잡을 수 있습니다.\n#| include: true\n#| message: false\n#| warning: false\n#| eval: false\n\nupworthy &lt;- read_csv(\"https://osf.io/vy8mj/download\")\n#| eval: false\n#| include: false\n#| warning: false\n#| message: false\n\n# INTERNAL\n\narrow::write_parquet(x = upworthy, sink = \"inputs/data/upworthy.parquet\")\n#| eval: true\n#| include: false\n#| warning: false\n#| message: false\n\n# INTERNAL\n\nupworthy &lt;- arrow::read_parquet(file = \"inputs/data/upworthy.parquet\")\n#| message: false\n#| warning: false\n\nupworthy |&gt;\n  names()\n\nupworthy |&gt;\n  head()\n데이터셋에 대한 문서를 살펴보는 것도 유용합니다. 이것은 데이터셋의 구조를 설명하며, 테스트 내에 패키지가 있습니다. 패키지는 테스트의 일부로 웹사이트의 다른 방문자에게 무작위로 표시된 헤드라인 및 이미지 모음입니다. 테스트에는 많은 패키지가 포함될 수 있습니다. 데이터셋의 각 행은 패키지이며, 속한 테스트는 “clickability_test_id” 열로 지정됩니다.\n많은 변수가 있습니다. 우리는 다음에 집중할 것입니다.\n\n“created_at”;\n비교 그룹을 만들기 위한 “clickability_test_id”;\n“headline”;\n패키지를 본 사람 수인 “impressions”; 그리고\n해당 패키지의 클릭 수인 “clicks”.\n\n각 테스트 배치 내에서, 우리는 다양한 헤드라인이 노출 및 클릭 수에 미치는 영향에 관심이 있습니다.\nupworthy_restricted &lt;-\n  upworthy |&gt;\n  select(\n    created_at, clickability_test_id, headline, impressions, clicks\n    )\n#| message: false\n#| warning: false\n\nhead(upworthy_restricted)\n헤드라인에 포함된 텍스트에 초점을 맞추고, 질문을 하는 헤드라인이 그렇지 않은 헤드라인보다 더 많은 클릭을 얻었는지 살펴볼 것입니다. 다른 이미지의 효과를 제거하고 싶으므로 동일한 이미지를 가진 테스트에 집중할 것입니다. 헤드라인이 질문을 하는지 확인하기 위해 물음표를 검색합니다. 더 복잡한 구성을 사용할 수도 있지만, 이것으로 시작하기에 충분합니다.\nupworthy_restricted &lt;-\n  upworthy_restricted |&gt;\n  mutate(\n    asks_question =\n      str_detect(string = headline, pattern = \"\\\\?\")\n    )\n\nupworthy_restricted |&gt;\n  count(asks_question)\n모든 테스트와 모든 사진에 대해, 질문을 하는 것이 클릭 수에 영향을 미쳤는지 알고 싶습니다.\n#| message: false\n\nquestion_or_not &lt;-\n  upworthy_restricted |&gt;\n  summarise(\n    ave_clicks = mean(clicks),\n    .by = c(clickability_test_id, asks_question)\n  ) \n\nquestion_or_not |&gt;\n  pivot_wider(names_from = asks_question,\n              values_from = ave_clicks,\n              names_prefix = \"ave_clicks_\") |&gt;\n  drop_na(ave_clicks_FALSE, ave_clicks_TRUE) |&gt;\n  mutate(difference_in_clicks = ave_clicks_TRUE - ave_clicks_FALSE) |&gt; \n  summarise(average_differce = mean(difference_in_clicks))\n교차표도 고려할 수 있습니다(?tbl-datasummaryupworthy).\n#| label: tbl-datasummaryupworthy\n#| tbl-cap: \"평균 클릭 수의 차이\"\n\nquestion_or_not |&gt; \n  summarise(mean = mean(ave_clicks),\n            .by = asks_question) |&gt; \n  tt() |&gt; \n  style_tt(j = 1:2, align = \"lr\") |&gt; \n  format_tt(digits = 0, num_fmt = \"decimal\") |&gt; \n  setNames(c(\"Asks a question?\", \"Mean clicks\"))\n일반적으로 헤드라인에 질문이 있으면 헤드라인의 클릭 수가 약간 감소할 수 있지만, 효과가 있더라도 그다지 크지 않은 것 같습니다(?fig-upworthy).\n#| fig-cap: \"헤드라인에 물음표가 포함된 경우와 그렇지 않은 경우의 평균 클릭 수 비교\"\n#| echo: false\n#| label: fig-upworthy\n#| message: false\n#| warning: false\n\nquestion_or_not |&gt;\n  ggplot(aes(\n    x = log(ave_clicks),\n    fill = asks_question\n  )) +\n  geom_histogram() +\n  labs(\n    x = \"Log average number of clicks\",\n    y = \"Number of observations\",\n    fill = \"Has a question mark?\"\n  ) +\n  theme_classic() +\n  scale_fill_brewer(palette = \"Set1\") +\n  theme(legend.position = \"bottom\")",
    "crumbs": [
      "획득",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>실험 및 설문조사</span>"
    ]
  },
  {
    "objectID": "08-hunt_ko.html#연습-문제",
    "href": "08-hunt_ko.html#연습-문제",
    "title": "8  실험 및 설문조사",
    "section": "8.6 연습 문제",
    "text": "8.6 연습 문제\n\n연습\n\n(계획) 다음 시나리오를 고려하십시오: 정치 후보자는 선거 운동 과정에서 두 가지 여론 조사 값, 즉 지지율과 득표율이 어떻게 변하는지에 관심이 있습니다. 이 두 가지는 백분율로 측정되며, 어느 정도 상관 관계가 있습니다. 후보자 간의 토론이 있을 때 큰 변화가 있는 경향이 있습니다. 데이터셋이 어떻게 생겼을지 스케치한 다음, 모든 관측치를 보여주기 위해 만들 수 있는 그래프를 스케치하십시오.\n(시뮬레이션) 관계를 포함하여 상황을 시뮬레이션한 다음, 시뮬레이션된 데이터셋에 대한 테스트를 작성하십시오.\n(수집) 시나리오와 유사한 실제 데이터를 얻고, 이 실제 데이터에 대한 시뮬레이션된 테스트를 업데이트하는 스크립트를 추가하십시오.\n(탐색) 실제 데이터를 사용하여 그래프와 표를 만드십시오.\n(전달) Quarto를 사용하여 짧은 논문을 작성하고 고품질 GitHub 리포지토리 링크를 제출하십시오.\n\n\n\n퀴즈\n\n다음 중 인과 추론의 근본적인 문제를 가장 잘 설명하는 것은 무엇입니까(하나 선택)?\n\n무작위화는 실험의 모든 편향을 제거할 수 없습니다.\n설문 조사는 개인의 선호를 정확하게 측정할 수 없습니다.\n동일한 개인에 대해 처리 및 통제 결과를 동시에 관찰할 수 없습니다.\n어떤 실험에서도 외부 타당성을 확립하는 것은 불가능합니다.\n\n네이만-루빈 잠재적 결과 프레임워크에서 실험을 수행할 때 주요 목표는 무엇입니까(하나 선택)?\n\n처리군과 대조군을 비교하여 인과 효과를 추정합니다.\n내부 타당성보다 외부 타당성에 집중합니다.\n더 큰 통계적 검정력을 위해 표본 크기를 최대화합니다.\n모든 참가자가 어느 시점에서든 치료를 받도록 보장합니다.\n\n(gertler2016impact에서?) 기본 영향 평가 공식 \\(\\Delta = (Y_i|t=1) - (Y_i|t=0)\\)은 무엇을 나타냅니까(하나 선택)?\n\n처리군과 비교군 간의 결과 차이.\n참가자 급여의 평균 변화.\n결과에 대한 외부 시장 세력의 영향.\n프로그램의 총 비용.\n\n실험 설계에서 무작위화가 중요한 이유는 무엇입니까(하나 선택)?\n\n표본이 인구를 대표하도록 보장합니다.\n대조군이 필요하지 않습니다.\n외부 타당성을 보장합니다.\n처리를 제외하고 유사한 처리군과 대조군을 만드는 데 도움이 됩니다.\n\n(gertler2016impact에서?) 반사실을 측정하려고 할 때 일반적인 문제는 무엇입니까(하나 선택)?\n\n무작위 시험만이 반사실을 제공할 수 있습니다.\n대조군에 대한 데이터는 항상 부정확합니다.\n동일한 개인에 대해 처리 및 비처리 결과를 모두 관찰하는 것은 불가능합니다.\n프로그램에는 일반적으로 충분한 참가자가 없습니다.\n\n(gertler2016impact에서?) 선택 편향은 언제 발생합니까(하나 선택)?\n\n프로그램 평가에 재정적 지원이 부족합니다.\n프로그램이 국가적 규모로 시행됩니다.\n참가자가 무작위로 할당되지 않습니다.\n데이터 수집이 불완전합니다.\n\n외부 타당성이란 무엇입니까(하나 선택)?\n\n여러 번 반복된 실험의 결과.\n해당 환경에서 실험의 결과가 유지됩니다.\n코드와 데이터를 사용할 수 있는 실험의 결과.\n해당 환경 외부에서 실험의 결과가 유지됩니다.\n\n내부 타당성이란 무엇입니까(하나 선택)?\n\n코드와 데이터를 사용할 수 있는 실험의 결과.\n여러 번 반복된 실험의 결과.\n해당 환경에서 실험의 결과가 유지됩니다.\n해당 환경 외부에서 실험의 결과가 유지됩니다.\n\n(gertler2016impact에서?) 영향 평가에서 내부 타당성은 무엇을 의미합니까(하나 선택)?\n\n프로그램의 인과 효과 측정의 정확성.\n다른 인구에 대한 결과 일반화 능력.\n프로그램 관리의 효율성.\n프로그램의 장기적인 지속 가능성.\n\n(gertler2016impact에서?) 영향 평가에서 외부 타당성은 무엇을 의미합니까(하나 선택)?\n\n프로그램의 행정 비용.\n적격 인구에 대한 결과 일반화 능력.\n무작위 통제 시험의 효과.\n결과가 정책 변화를 반영하는 정도.\n\n다음 데이터셋에 대해 사람들을 두 그룹 중 하나에 무작위로 할당하는 코드를 작성하십시오.\n\nnetflix_data &lt;-\n  tibble(\n    person = c(\"Ian\", \"Ian\", \"Roger\", \"Roger\",\n      \"Roger\", \"Patricia\", \"Patricia\", \"Helen\"\n    ),\n    tv_show = c(\n      \"Broadchurch\", \"Duty-Shame\", \"Broadchurch\", \"Duty-Shame\",\n      \"Shetland\", \"Broadchurch\", \"Shetland\", \"Duty-Shame\"\n    ),\n    hours = c(6.8, 8.0, 0.8, 9.2, 3.2, 4.0, 0.2, 10.2)\n  )\n\n(gertler2016impact에서?) 유효한 비교 그룹은 다음 특성을 모두 가져야 하지만 예외는 무엇입니까(하나 선택)?\n\n처리군과 동일한 평균 특성.\n처리군과 동일한 방식으로 결과가 변경됩니다.\n프로그램에 의해 직간접적으로 영향을 받습니다.\n프로그램을 받으면 유사한 방식으로 프로그램에 반응합니다.\n\n(gertler2016impact에서?) 이전-이후 비교가 위조 추정치로 간주되는 이유는 무엇입니까(하나 선택)?\n\n무작위 할당을 포함합니다.\n중요하지 않은 지표에 집중합니다.\n대규모 데이터 표본이 필요합니다.\n시간이 지남에 따라 결과가 변하지 않는다고 가정합니다.\n\n(gertler2016impact에서?) 어떤 시나리오가 프로그램 할당 도구로서 무작위 할당을 윤리적으로 허용할 수 있습니까(하나 선택)?\n\n모든 참가자는 소득 수준에 따라 등록됩니다.\n모든 적격 참가자는 프로그램에 의해 수용될 수 있습니다.\n프로그램은 특정 그룹에만 서비스를 제공합니다.\n프로그램에 사용 가능한 공간보다 더 많은 적격 참가자가 있습니다.\n\n터스키기 매독 연구는 어떤 윤리 원칙을 위반한 예입니까(하나 선택)?\n\n참가자 데이터의 기밀 유지.\n실험 설계에서 통계적 검정력 보장.\n참가자로부터 정보에 입각한 동의 얻기.\n참가자에게 금전적 보상 제공.\n\n임상 시험의 맥락에서 평형 상태는 무엇을 의미합니까(하나 선택)?\n\n표본 크기가 같을 때 달성되는 통계적 평형.\n모든 참가자가 치료에 동등하게 접근할 수 있는 상태.\n치료 효능과 부작용 간의 균형.\n치료의 효과에 대한 진정한 불확실성의 윤리적 요구 사항.\n\nWare (1989, p. 299)는 무작위 동의를 언급하고 “이 환경에서 매력적이었는데, 왜냐하면 정보에 입각한 동의에 대한 표준 접근 방식은 사망 직전의 영아의 부모에게 침습적 수술 절차에 대한 정보에 입각한 동의를 얻도록 접근해야 하며, 그러면 어떤 경우에는 시행되지 않을 것이기 때문입니다. 신생아 집중 치료실에서 아이를 갖는 고통스러운 경험에 익숙한 사람들은 정보에 입각한 동의를 얻는 과정이 부모에게 무섭고 스트레스가 될 것이라는 것을 이해할 수 있습니다.”라고 계속합니다. 특히 Ware (1989, p. 305)가 언급했듯이 “기존의 의료 요법(CMT)을 받는 영아의 부모에게 연구에 대한 정보를 보류해야 할 필요성”을 감안할 때 이 입장에 어느 정도 동의하십니까?\n다음 중 설문 질문을 설계할 때 중요한 것은 무엇입니까(하나 선택)?\n\n시간을 절약하기 위해 한 번에 여러 질문을 합니다.\n더 신뢰성 있게 보이기 위해 전문 용어를 사용합니다.\n질문이 관련성이 있고 응답자가 쉽게 이해할 수 있도록 보장합니다.\n응답자를 원하는 답변으로 유도합니다.\n\n실험의 맥락에서 교란 변수란 무엇입니까(하나 선택)?\n\n실험 프로토콜을 따르지 않는 참가자.\n연구자가 의도적으로 조작하는 변수.\n통제되지 않고 결과에 영향을 미칠 수 있는 변수.\n유효하지 않은 결과를 초래하는 데이터 수집 오류.\n\n오리건 건강 보험 실험은 주로 무엇의 영향을 평가하는 것을 목표로 했습니까(하나 선택)?\n\n건강 결과를 연구하기 위해 저소득 성인에게 무작위로 메디케이드를 제공합니다.\n새로운 민간 건강 보험 계획 도입.\n건강 개입의 비용 효율성 평가.\n만성 질환에 대한 다른 의료 치료 비교.\n\n설문 조사 설계에서 파일럿 연구의 목적은 무엇입니까(하나 선택)?\n\n모든 응답자가 연구의 가설을 이해하도록 보장하기 위해.\n전체 배포 전에 설문 조사 도구를 테스트하고 개선하기 위해.\n더 나은 통계적 검정력을 위해 표본 크기를 늘리기 위해.\n출판을 위한 예비 데이터 수집.\n\nA/B 테스트의 맥락에서 A/A 테스트가 수행되는 이유는 무엇입니까(하나 선택)?\n\n대조 조건의 효과를 테스트하기 위해.\n무작위화가 비교 가능한 그룹을 적절하게 생성했는지 확인하기 위해.\n완전히 다른 두 가지 치료법을 비교하기 위해.\n새로운 치료법을 구현하지 않음으로써 자원을 절약하기 위해.\n\n산업 환경에서 A/B 테스트와 특히 관련된 윤리적 우려는 무엇입니까(하나 선택)?\n\n실험 수행의 높은 비용.\n장기적인 효과 측정의 어려움.\n실험 대상인 사용자로부터 정보에 입각한 동의 부족.\n대규모 데이터셋에서 통계적 유의성 보장.\n\n대규모 컨설팅 회사의 주니어 분석가로 일한다고 가정해 봅시다. 또한, 컨설팅 회사가 정부 국경 보안 부서를 위한 안면 인식 모델을 구성하는 계약을 체결했다고 가정해 봅시다. 이 문제에 대한 윤리에 관한 생각을 예시와 참고 문헌을 포함하여 최소 세 단락으로 작성하십시오.\n평균 처리 효과(ATE)는 무엇을 의미합니까(하나 선택)?\n\n단일 개인에 대한 처리 효과.\n대조군에서 관찰된 평균 결과.\n전체 표본에 걸쳐 처리군과 대조군 간의 결과 차이.\n관찰된 모든 처리 효과의 총합.\n\n실험의 맥락에서 “눈가림”은 무엇을 의미합니까(하나 선택)?\n\n데이터를 분석하기 위해 복잡한 통계적 방법을 사용합니다.\n참가자가 치료를 받는지 대조를 받는지 모르게 합니다.\n참가자에게 표본 크기를 숨깁니다.\n할당을 기록하지 않고 무작위로 치료를 할당합니다.\n\n실제 실험 데이터를 분석하기 전에 시뮬레이션을 하는 이유는 무엇입니까(하나 선택)?\n\n시뮬레이션은 실제 데이터 분석보다 더 정확합니다.\n시뮬레이션은 계산 능력이 덜 필요합니다.\n시뮬레이션은 실제 데이터 수집이 필요하지 않습니다.\n시뮬레이션은 예상 결과를 이해하고 분석의 잠재적 오류를 이해하는 데 도움이 됩니다.\n\n다음 중 선택 편향의 개념을 가장 잘 포착하는 진술은 무엇입니까(하나 선택)?\n\n표본이 목표 모집단을 정확하게 나타냅니다.\n처리 변수를 제외한 모든 변수가 통제됩니다.\n참가자가 무작위로 연구에서 탈락합니다.\n참가자를 선택하는 방법으로 인해 표본이 대표적이지 않게 됩니다.\n\n업워디 분석을 다시 수행하되, “?” 대신 “!”에 대해 수행하십시오. 클릭 수의 차이는 얼마입니까(하나 선택)?\n\n-8.3\n-7.2\n-5.6\n-4.5\n\n\n(pewletterman에서?), 편향을 도입하지 않고 소규모 종교 집단의 응답자를 포함할 가능성을 높이기 위해 어떤 표본 추출 방법론이 사용되었습니까(하나 선택)?\n\n눈덩이 표본 추출.\n할당 표본 추출.\n무작위 숫자 다이얼링.\n크기의 복합 측정.\n\n(pewletterman에서?), 연구자들은 설문 조사가 윤리적으로 수행되었는지 어떻게 확인했습니까(하나 선택)?\n\n인도 기관 연구 검토 위원회(IRB)의 승인을 받았습니다.\n자원한 개인만 설문 조사했습니다.\n인구 통계 정보를 수집하지 않음으로써 데이터를 익명화했습니다.\n참여에 대한 금전적 인센티브를 제공했습니다.\n\n(Stantcheva2023에서?), 설문 표본 추출에서 포괄 범위 오류란 무엇입니까(하나 선택)?\n\n응답자의 부주의로 인한 오류.\n목표 모집단과 표본 프레임 간의 차이.\n소수 민족의 과잉 표본 추출로 인한 편향.\n계획된 표본과 실제 응답자 간의 차이.\n\n(Stantcheva2023에서?), 온건 응답 편향이란 무엇입니까(하나 선택)?\n\n질문 내용에 관계없이 중간 옵션을 선택하는 경향.\n질문 순서로 인한 편향.\n척도에서 극단적인 값을 선택하는 경향.\n측량사의 예상 답변에 동의하는 경향.\n\n(Stantcheva2023에서?), 온라인 설문 조사에서 사회적 바람직성 편향을 최소화하는 방법은 무엇입니까(하나 선택)?\n\n높은 금전적 보상 제공.\n응답의 기밀성에 대한 확신 제공.\n응답자의 신원을 공개합니다.\n설문 질문을 길고 복잡하게 유지합니다.\n\n(Stantcheva2023에서?), 응답 순서 편향은 무엇을 의미합니까(하나 선택)?\n\n응답자가 민감한 질문을 건너뜁니다.\n응답자가 체계적으로 극단적인 값을 선택합니다.\n응답자가 질문을 이해하지 못합니다.\n응답자가 순서에 따라 답변을 선택합니다.\n\n(Stantcheva2023에서?), 설문 조사를 관리하는 동안 다음을 제외한 모든 것을 해야 합니다(하나 선택)?\n\n데이터를 확인합니다.\n설문 조사를 모니터링합니다.\n통계적 가설을 테스트합니다.\n설문 조사를 소프트 런칭합니다.\n\n질문 순서 효과를 최소화하는 일반적인 접근 방식은 질문 순서를 무작위화하는 것입니다. 이것이 어느 정도 효과적이라고 생각하십니까?\n(Stantcheva2023에서?), 온라인 설문 조사에서 응답자를 모집하는 좋은 방법은 무엇입니까(하나 선택)?\n\n가능한 가장 높은 금전적 인센티브 제공.\n처음에는 설문 조사의 목적에 대한 최소한의 정보 제공.\n초대 이메일에서 설문 조사의 주제 공개.\n참여도를 높이기 위해 설문 조사의 길이를 강조합니다.\n\n(Stantcheva2023에서?), 설문 조사의 이탈은 무엇을 의미합니까(하나 선택)?\n\n초대를 받은 총 인원 수.\n수집된 데이터의 정확성.\n응답자와 비응답자 간의 차이.\n응답자가 설문 조사를 완료하기 전에 중도 탈락하는 비율.\n\n\n\n\n수업 활동\n\n스타터 폴더를 사용하고 새 리포지토리를 만듭니다. 수업의 공유 Google 문서에 GitHub 리포지토리 링크를 추가합니다.\n피셔의 차 시음 실험을 고려하십시오. 먼저, 각각 별도로 수행된 소수의 차 시음 실험 결과가 있다고 가정해 봅시다. 데이터 표와 결과로 만들 수 있는 그래프를 스케치하십시오. 그런 다음 이것들을 시뮬레이션하십시오. 그런 다음 소그룹으로 실험을 수행하십시오(이것은 생각보다 더 어려울 것입니다). 그룹의 결과를 전체 수업의 결과에 추가한 다음 그래프를 만드십시오.\n?sec-reproducible-workflows를 따라 빠른 개인 웹사이트를 만들고 GitHub Pages를 사용하여 배포하십시오.\n다른 웹사이트를 만들되, 이번에는 Google Analytics를 추가하십시오. Netlify를 사용하여 배포하십시오. 웹사이트의 일부 측면을 변경하고, 다른 추적기를 추가하고, 새 브랜치에 푸시하십시오. 그런 다음 Netlify를 사용하여 A/B 테스트를 수행하십시오.\n논문 검토: (Hammond2022를?) 참조하여 실험 설계, 정보에 입각한 동의 및 평형 상태에 대해 논의하십시오. 최소 두 페이지를 작성하십시오.\n\n\n\n과제\n조사 통계 및 방법론 저널의 무응답률 및 무응답 조정에 관한 특별 가상 호를 고려하십시오. 사설의 한 측면에 초점을 맞추고, 관련 문헌을 참조하여 최소 두 페이지로 논의하십시오. Quarto를 사용하고, 적절한 제목, 저자, 날짜, GitHub 리포지토리 링크 및 인용을 포함하십시오. PDF를 제출하십시오.\n\n\n논문\n이 시점에서 ?sec-papers의 Howrah 논문이 적절할 것입니다.\n\n\n\n\nAcemoglu, Daron, Simon Johnson, 와/과 James Robinson. 2001. “The colonial origins of comparative development: An empirical investigation”. American Economic Review 91 (5): 1369–1401. https://doi.org/10.1257/aer.91.5.1369.\n\n\nAkerlof, George. 1970. “The market for ‘lemons’: Quality uncertainty and the market mechanism”. The Quarterly Journal of Economics 84 (3): 488–500. https://doi.org/10.2307/1879431.\n\n\nAlsan, Marcella, 와/과 Amy Finkelstein. 2021. “Beyond Causality: Additional Benefits of Randomized Controlled Trials for Improving Health Care Delivery”. The Milbank Quarterly 99 (4): 864–81. https://doi.org/10.1111/1468-0009.12521.\n\n\nAngrist, Joshua, 와/과 Jörn-Steffen Pischke. 2010. “The credibility revolution in empirical economics: How better research design is taking the con out of econometrics”. Journal of Economic Perspectives 24 (2): 3–30. https://doi.org/10.1257/jep.24.2.3.\n\n\nArel-Bundock, Vincent. 2024. tinytable: Simple and Configurable Tables in “HTML”, “LaTeX”, “Markdown”, “Word”, “PNG”, “PDF”, and “Typst” Formats. https://vincentarelbundock.github.io/tinytable/.\n\n\nBanerjee, Abhijit, 와/과 Esther Duflo. 2011. Poor Economics: A Radical Rethinking of the Way to Fight Global Poverty. New York: PublicAffairs.\n\n\nBlair, Ed, Seymour Sudman, Norman M Bradburn, 와/과 Carol Stocking. 1977. “How to ask questions about drinking and sex: Response effects in measuring consumer behavior”. Journal of Marketing Research 14 (3): 316–21. https://doi.org/10.2307/3150769.\n\n\nBrandt, Allan. 1978. “Racism and research: the case of the Tuskegee Syphilis Study”. Hastings center report, 21–29. https://doi.org/10.2307/3561468.\n\n\nChristian, Brian. 2012. “The A/B Test: Inside the Technology That’s Changing the Rules of Business”. Wired, 4월. https://www.wired.com/2012/04/ff-abtesting/.\n\n\nCohn, Alain. 2019. “Data and code for: Civic Honesty Around the Globe”. Harvard Dataverse. https://doi.org/10.7910/dvn/ykbodn.\n\n\nCohn, Alain, Michel André Maréchal, David Tannenbaum, 와/과 Christian Lukas Zünd. 2019a. “Civic honesty around the globe”. Science 365 (6448): 70–73. https://doi.org/10.1126/science.aau8712.\n\n\n———. 2019b. “Supplementary Materials for: Civic honesty around the globe”. Science 365 (6448): 70–73.\n\n\nCunningham, Scott. 2021. Causal Inference: The Mixtape. 1st ed. New Haven: Yale Press. https://mixtape.scunning.com.\n\n\nDeaton, Angus. 2010. “Instruments, Randomization, and Learning about Development”. Journal of Economic Literature 48 (2): 424–55. https://doi.org/10.1257/jel.48.2.424.\n\n\nDillman, Don, Jolene Smyth, 와/과 Leah Christian. (1978년) 2014. Internet, Phone, Mail, and Mixed-Mode Surveys: The Tailored Design Method. 4th ed. Wiley.\n\n\nDruckman, James, 와/과 Donald Green. 2021. “A New Era of Experimental Political Science”. In Advances in Experimental Political Science, 1–16. Cambridge: Cambridge University Press. https://doi.org/10.1017/9781108777919.002.\n\n\nDuflo, Esther. 2020. “Field Experiments and the Practice of Policy”. American Economic Review 110 (7): 1952–73. https://doi.org/10.1257/aer.110.7.1952.\n\n\nEdwards, Jonathan. 2017. “PACE team response shows a disregard for the principles of science”. Journal of Health Psychology 22 (9): 1155–58. https://doi.org/10.1177/1359105317700886.\n\n\nElliott, Michael, Brady West, Xinyu Zhang, 와/과 Stephanie Coffey. 2022. “The anchoring method: Estimation of interviewer effects in the absence of interpenetrated sample assignment”. Survey Methodology 48 (1): 25–48. http://www.statcan.gc.ca/pub/12-001-x/2022001/article/00005-eng.htm.\n\n\nElson, Malte. 2018. “Question wording and item formulation”. https://doi.org/10.31234/osf.io/e4ktc.\n\n\nEnns, Peter, 와/과 Jake Rothschild. 2022. “Do you know where your survey data come from?”, 5월. https://medium.com/3streams/surveys-3ec95995dde2.\n\n\nFinkelstein, Amy, Sarah Taubman, Bill Wright, Mira Bernstein, Jonathan Gruber, Joseph Newhouse, Heidi Allen, Katherine Baicker, 와/과 Oregon Health Study Group. 2012. “The Oregon Health Insurance Experiment: Evidence from the First Year”. The Quarterly Journal of Economics 127 (3): 1057–1106. https://doi.org/10.1093/qje/qjs020.\n\n\nFisher, Ronald. (1935년) 1949. The Design of Experiments. 5th ed. London: Oliver; Boyd.\n\n\nFry, Hannah. 2020. “Big tech is testing you”. The New Yorker, 2월, 61–65. https://www.newyorker.com/magazine/2020/03/02/big-tech-is-testing-you.\n\n\nGertler, Paul, Sebastian Martinez, Patrick Premand, Laura Rawlings, 와/과 Christel Vermeersch. 2016. Impact evaluation in practice. 2nd ed. The World Bank. https://doi.org/10.1596/978-1-4648-0779-4.\n\n\nGordon, Brett, Robert Moakler, 와/과 Florian Zettelmeyer. 2022. “Close Enough? A Large-Scale Exploration of Non-Experimental Approaches to Advertising Measurement”. Marketing Science, 11월. https://doi.org/10.1287/mksc.2022.1413.\n\n\nGordon, Brett, Florian Zettelmeyer, Neha Bhargava, 와/과 Dan Chapsky. 2019. “A Comparison of Approaches to Advertising Measurement: Evidence from Big Field Experiments at Facebook”. Marketing Science 38 (2): 193–225. https://doi.org/10.1287/mksc.2018.1135.\n\n\nGroves, Robert. 2011. “Three Eras of Survey Research”. Public Opinion Quarterly 75 (5): 861–71. https://doi.org/10.1093/poq/nfr057.\n\n\nHill, Austin Bradford. 1965. “The Environment and Disease: Association or Causation?” Proceedings of the Royal Society of Medicine 58 (5): 295–300.\n\n\nHolland, Paul. 1986. “Statistics and Causal Inference”. Journal of the American statistical Association 81 (396): 945–60. https://doi.org/10.2307/2289064.\n\n\nKohavi, Ron, Alex Deng, Brian Frasca, Roger Longbotham, Toby Walker, 와/과 Ya Xu. 2012. “Trustworthy online controlled experiments”. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD 12, 1st ed. ACM Press. https://doi.org/10.1145/2339530.2339653.\n\n\nKohavi, Ron, Diane Tang, 와/과 Ya Xu. 2020. Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing. Cambridge University Press.\n\n\nLarmarange, Joseph. 2023. labelled: Manipulating Labelled Data. https://CRAN.R-project.org/package=labelled.\n\n\nLetterman, Clark. 2021. “Q&A: How Pew Research Center surveyed nearly 30,000 people in India”, 7월. https://medium.com/pew-research-center-decoded/q-a-how-pew-research-center-surveyed-nearly-30-000-people-in-india-7c778f6d650e.\n\n\nLevay, Kevin, Jeremy Freese, 와/과 James Druckman. 2016. “The Demographic and Political Composition of Mechanical Turk Samples”. SAGE Open 6 (1): 1–17. https://doi.org/10.1177/2158244016636433.\n\n\nLight, Richard, Judith Singer, 와/과 John Willett. 1990. By Design: Planning Research on Higher Education. 1st ed. Cambridge: Harvard University Press.\n\n\nMorange, Michel. 2016. A History of Biology. New Jersey: Princeton University Press.\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nSalganik, Matthew. 2018. Bit by bit: Social Research in the Digital Age. New Jersey: Princeton University Press.\n\n\nSmith, Matthew. 2018. “Should milk go in a cup of tea first or last?”, 7월. https://yougov.co.uk/topics/consumer/articles-reports/2018/07/30/should-milk-go-cup-tea-first-or-last.\n\n\nStantcheva, Stefanie. 2023. “How to Run Surveys: A guide to creating your own identifying variation and revealing the invisible”. Annual Review of Economics 15 (1): 205–34. https://doi.org/10.1146/annurev-economics-091622-010157.\n\n\nTaddy, Matt. 2019. Business Data Science. 1st ed. McGraw Hill.\n\n\nTausanovitch, Chris, 와/과 Lynn Vavreck. 2021. “Democracy Fund + UCLA Nationscape Project”. https://www.voterstudygroup.org/data/nationscape.\n\n\nTourangeau, Roger, Lance Rips, 와/과 Kenneth Rasinski. 2000. The Psychology of Survey Response. 1st ed. Cambridge University Press. https://doi.org/10.1017/CBO9780511819322.003.\n\n\nVavreck, Lynn, 와/과 Chris Tausanovitch. 2021. “Democracy Fund + UCLA Nationscape Project User Guide”. https://www.voterstudygroup.org/data/nationscape.\n\n\nWare, James. 1989. “Investigating therapies of potentially great benefit: ECMO”. Statistical Science 4 (4): 298–306. https://doi.org/10.1214/ss/1177012384.\n\n\nWickham, Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, 기타. 2019. “Welcome to the Tidyverse”. Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Evan Miller, 와/과 Danny Smith. 2023. haven: Import and Export “SPSS” “Stata” and “SAS” Files. https://CRAN.R-project.org/package=haven.\n\n\nXu, Ya. 2020. “Causal Inference Challenges in Industry: A perspective from experiences at LinkedIn”. YouTube, 7월. https://youtu.be/OoKsLAvyIYA.\n\n\nYoshioka, Alan. 1998. “Use of randomisation in the Medical Research Council’s clinical trial of streptomycin in pulmonary tuberculosis in the 1940s”. BMJ 317 (7167): 1220–23. https://doi.org/10.1136/bmj.317.7167.1220.",
    "crumbs": [
      "획득",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>실험 및 설문조사</span>"
    ]
  },
  {
    "objectID": "11-eda_ko.html",
    "href": "11-eda_ko.html",
    "title": "11  탐색적 데이터 분석",
    "section": "",
    "text": "11.1 소개\n선수 지식\n주요 개념 및 기술\n소프트웨어 및 패키지\n탐색적 데이터 분석은 결코 끝나지 않습니다. 그것은 우리의 데이터를 탐색하고 익숙해지는 능동적인 과정입니다. 농부가 흙에 손을 담그는 것처럼, 우리는 데이터의 모든 윤곽과 측면을 알아야 합니다. 우리는 그것이 어떻게 변하는지, 무엇을 보여주고, 숨기는지, 그리고 그 한계가 무엇인지 알아야 합니다. 탐색적 데이터 분석(EDA)은 이것을 하는 비정형적인 과정입니다.\nEDA는 목적을 위한 수단입니다. 전체 논문, 특히 데이터 섹션에 정보를 제공하지만, 일반적으로 최종 논문에 포함되는 것은 아닙니다. 진행하는 방법은 별도의 Quarto 문서를 만드는 것입니다. 이동 중에 코드와 간략한 메모를 추가하십시오. 이전 코드를 삭제하지 말고 그냥 추가하십시오. 끝날 때쯤이면 데이터셋 탐색을 캡처하는 유용한 노트북을 만들게 될 것입니다. 이것은 후속 분석 및 모델링을 안내할 문서입니다.\nEDA는 다양한 기술을 활용하며 EDA를 수행할 때 많은 옵션이 있습니다 (Staniak 와/과 Biecek 2019). 모든 도구를 고려해야 합니다. 데이터를 보고 스크롤하십시오. 표, 플롯, 요약 통계, 심지어 일부 모델을 만드십시오. 핵심은 반복하고, 완벽하게 하기보다는 빠르게 움직이고, 데이터에 대한 철저한 이해에 도달하는 것입니다. 흥미롭게도, 우리가 가진 데이터를 철저히 이해하게 되면 종종 우리가 가지지 않은 것을 이해하는 데 도움이 됩니다.\n우리는 다음 과정에 관심이 있습니다.\nEDA를 수행하고 완료하는 데 필요한 올바른 프로세스나 단계는 하나도 없습니다. 대신, 관련 단계와 도구는 데이터와 관심 있는 질문에 따라 다릅니다. 따라서 이 장에서는 미국 주 인구, 토론토 지하철 지연, 런던의 에어비앤비 목록을 포함한 다양한 EDA 예를 통해 EDA에 대한 접근 방식을 설명합니다. 또한 ?sec-farm-data를 기반으로 하고 결측 데이터로 돌아갑니다.",
    "crumbs": [
      "준비",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>탐색적 데이터 분석</span>"
    ]
  },
  {
    "objectID": "11-eda_ko.html#소개",
    "href": "11-eda_ko.html#소개",
    "title": "11  탐색적 데이터 분석",
    "section": "",
    "text": "데이터 분석의 미래는 위대한 진보, 실제 어려움의 극복, 그리고 모든 과학 및 기술 분야에 대한 훌륭한 서비스 제공을 포함할 수 있습니다. 그렇게 될까요? 그것은 우리에게, 비현실적인 가정, 임의적인 기준, 그리고 실제적인 애착이 없는 추상적인 결과의 순탄한 길보다 실제 문제의 험난한 길을 택하려는 우리의 의지에 달려 있습니다. 누가 도전에 응하겠습니까?\nTukey (1962, p. 64).\n\n\n\n\n\n\n개별 변수의 분포 및 속성 이해.\n변수 간의 관계 이해.\n존재하지 않는 것 이해.",
    "crumbs": [
      "준비",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>탐색적 데이터 분석</span>"
    ]
  },
  {
    "objectID": "11-eda_ko.html#년-미국-인구-및-소득-데이터",
    "href": "11-eda_ko.html#년-미국-인구-및-소득-데이터",
    "title": "11  탐색적 데이터 분석",
    "section": "11.2 1975년 미국 인구 및 소득 데이터",
    "text": "11.2 1975년 미국 인구 및 소득 데이터\n첫 번째 예로, 1975년 기준 미국 주 인구를 고려합니다. 이 데이터셋은 state.x77을 사용하여 R에 내장되어 있습니다. 데이터셋은 다음과 같습니다.\n\nus_populations &lt;-\n  state.x77 |&gt;\n  as_tibble() |&gt;\n  clean_names() |&gt;\n  mutate(state = rownames(state.x77)) |&gt;\n  select(state, population, income)\n\nus_populations\n\n# A tibble: 50 × 3\n   state       population income\n   &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;\n 1 Alabama           3615   3624\n 2 Alaska             365   6315\n 3 Arizona           2212   4530\n 4 Arkansas          2110   3378\n 5 California       21198   5114\n 6 Colorado          2541   4884\n 7 Connecticut       3100   5348\n 8 Delaware           579   4809\n 9 Florida           8277   4815\n10 Georgia           4931   4091\n# ℹ 40 more rows\n\n\n우리는 데이터에 대한 빠른 감을 얻고 싶습니다. 첫 번째 단계는 head()와 tail()로 위아래를 살펴본 다음, 무작위 선택을 하고, 마지막으로 glimpse()로 변수와 클래스에 집중하는 것입니다. 무작위 선택은 중요한 측면이며, head()를 사용할 때 무작위 선택도 빠르게 고려해야 합니다.\n\nus_populations |&gt;\n  head()\n\n# A tibble: 6 × 3\n  state      population income\n  &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n1 Alabama          3615   3624\n2 Alaska            365   6315\n3 Arizona          2212   4530\n4 Arkansas         2110   3378\n5 California      21198   5114\n6 Colorado         2541   4884\n\nus_populations |&gt;\n  tail()\n\n# A tibble: 6 × 3\n  state         population income\n  &lt;chr&gt;              &lt;dbl&gt;  &lt;dbl&gt;\n1 Vermont              472   3907\n2 Virginia            4981   4701\n3 Washington          3559   4864\n4 West Virginia       1799   3617\n5 Wisconsin           4589   4468\n6 Wyoming              376   4566\n\nus_populations |&gt;\n  slice_sample(n = 6)\n\n# A tibble: 6 × 3\n  state         population income\n  &lt;chr&gt;              &lt;dbl&gt;  &lt;dbl&gt;\n1 North Dakota         637   5087\n2 Georgia             4931   4091\n3 Virginia            4981   4701\n4 Vermont              472   3907\n5 New Hampshire        812   4281\n6 Louisiana           3806   3545\n\nus_populations |&gt;\n  glimpse()\n\nRows: 50\nColumns: 3\n$ state      &lt;chr&gt; \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"…\n$ population &lt;dbl&gt; 3615, 365, 2212, 2110, 21198, 2541, 3100, 579, 8277, 4931, …\n$ income     &lt;dbl&gt; 3624, 6315, 4530, 3378, 5114, 4884, 5348, 4809, 4815, 4091,…\n\n\n그런 다음 기본 R의 summary()와 관측치 수를 사용하여 숫자 변수에 대한 최소값, 중앙값 및 최대값과 같은 주요 요약 통계를 이해하는 데 관심이 있습니다.\n\nus_populations |&gt;\n  summary()\n\n    state             population        income    \n Length:50          Min.   :  365   Min.   :3098  \n Class :character   1st Qu.: 1080   1st Qu.:3993  \n Mode  :character   Median : 2838   Median :4519  \n                    Mean   : 4246   Mean   :4436  \n                    3rd Qu.: 4968   3rd Qu.:4814  \n                    Max.   :21198   Max.   :6315  \n\n\n마지막으로, 한계에서 이러한 주요 요약 통계의 동작을 이해하는 것이 특히 중요합니다. 특히, 한 가지 접근 방식은 일부 관측치를 무작위로 제거하고 그것들에게 무슨 일이 일어나는지 비교하는 것입니다. 예를 들어, 어떤 관측치가 제거되었는지에 따라 다른 5개의 데이터셋을 무작위로 만들 수 있습니다. 그런 다음 요약 통계를 비교할 수 있습니다. 그 중 어느 것이라도 특히 다르다면, 제거된 관측치에 영향력이 큰 관측치가 포함될 수 있으므로 살펴보고 싶을 것입니다.\n\nsample_means &lt;- tibble(seed = c(), mean = c(), states_ignored = c())\n\nfor (i in c(1:5)) {\n  set.seed(i)\n  dont_get &lt;- c(sample(x = state.name, size = 5))\n  sample_means &lt;-\n    sample_means |&gt;\n    rbind(tibble(\n      seed = i,\n      mean =\n        us_populations |&gt;\n          filter(!state %in% dont_get) |&gt;\n          summarise(mean = mean(population)) |&gt;\n          pull(),\n      states_ignored = str_c(dont_get, collapse = \", \")\n    ))\n}\n\nsample_means |&gt;\n  tt() |&gt; \n  style_tt(j = 1:3, align = \"lrr\") |&gt; \n  format_tt(digits = 0, num_mark_big = \",\", num_fmt = \"decimal\") |&gt; \n  setNames(c(\"Seed\", \"Mean\", \"Ignored states\"))\n\n\n\n표 11.1: 다른 주가 무작위로 제거되었을 때의 평균 인구 비교\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Seed\n                Mean\n                Ignored states\n              \n        \n        \n        \n                \n                  1\n                  4,469\n                  Arkansas, Rhode Island, Alabama, North Dakota, Minnesota\n                \n                \n                  2\n                  4,027\n                  Massachusetts, Iowa, Colorado, West Virginia, New York\n                \n                \n                  3\n                  4,086\n                  California, Idaho, Rhode Island, Oklahoma, South Carolina\n                \n                \n                  4\n                  4,391\n                  Hawaii, Arizona, Connecticut, Utah, New Jersey\n                \n                \n                  5\n                  4,340\n                  Alaska, Texas, Iowa, Hawaii, South Dakota\n                \n        \n      \n    \n\n\n\n\n\n\n미국 주 인구의 경우, 캘리포니아와 뉴욕과 같은 더 큰 주가 평균 추정치에 큰 영향을 미칠 것이라는 것을 알고 있습니다. ?tbl-summarystatesrandom은 시드 2와 3을 사용할 때 평균이 더 낮다는 것을 보여줌으로써 그것을 뒷받침합니다.",
    "crumbs": [
      "준비",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>탐색적 데이터 분석</span>"
    ]
  },
  {
    "objectID": "11-eda_ko.html#결측-데이터",
    "href": "11-eda_ko.html#결측-데이터",
    "title": "11  탐색적 데이터 분석",
    "section": "11.3 결측 데이터",
    "text": "11.3 결측 데이터\n우리는 이 책 전반에 걸쳐, 특히 ?sec-farm-data에서 결측 데이터에 대해 많이 논의했습니다. 여기서 우리는 결측 데이터를 이해하는 것이 EDA의 상당한 초점인 경향이 있기 때문에 다시 돌아갑니다. 결측 데이터를 발견했을 때—그리고 어떤 종류든 항상 결측 데이터가 있습니다—우리는 어떤 유형의 결측을 다루고 있는지 확인하고 싶습니다. 알려진 결측 관측치, 즉 데이터셋에서 누락된 것으로 볼 수 있는 관측치에 초점을 맞춰, Gelman, Hill, 와/과 Vehtari (2020, p. 323)을 기반으로 세 가지 주요 결측 데이터 범주를 고려합니다.\n\n완전히 무작위로 누락;\n무작위로 누락; 그리고\n무작위가 아닌 누락.\n\n데이터가 완전히 무작위로 누락(MCAR)될 때, 관측치는 데이터셋에 있든 없든 다른 변수와 독립적으로 데이터셋에서 누락됩니다. ?sec-farm-data에서 논의했듯이, 데이터가 MCAR일 때 요약 통계 및 추론에 대한 우려가 적지만, 데이터는 거의 MCAR이 아닙니다. 그렇다 하더라도 이것을 확신하기는 어려울 것입니다. 그럼에도 불구하고 우리는 예를 시뮬레이션할 수 있습니다. 예를 들어, 무작위로 선택된 세 개 주의 인구 데이터를 제거할 수 있습니다.\n\nset.seed(853)\n\nremove_random_states &lt;-\n  sample(x = state.name, size = 3, replace = FALSE)\n\nus_states_MCAR &lt;-\n  us_populations |&gt;\n  mutate(\n    population =\n      if_else(state %in% remove_random_states, NA_real_, population)\n  )\n\nsummary(us_states_MCAR)\n\n    state             population        income    \n Length:50          Min.   :  365   Min.   :3098  \n Class :character   1st Qu.: 1174   1st Qu.:3993  \n Mode  :character   Median : 2861   Median :4519  \n                    Mean   : 4308   Mean   :4436  \n                    3rd Qu.: 4956   3rd Qu.:4814  \n                    Max.   :21198   Max.   :6315  \n                    NA's   :3                     \n\n\n관측치가 무작위로 누락(MAR)될 때, 그것들은 데이터셋의 다른 변수와 관련된 방식으로 데이터셋에서 누락됩니다. 예를 들어, 소득과 성별이 정치 참여에 미치는 영향을 이해하는 데 관심이 있고, 그래서 이 세 변수에 대한 정보를 수집할 수 있습니다. 그러나 어떤 이유로 남성이 소득에 대한 질문에 응답할 가능성이 적을 수 있습니다.\n미국 주 데이터셋의 경우, 인구가 가장 많은 세 미국 주가 소득에 대한 관측치를 갖지 않도록 하여 MAR 데이터셋을 시뮬레이션할 수 있습니다.\n\nhighest_income_states &lt;-\n  us_populations |&gt;\n  slice_max(income, n = 3) |&gt;\n  pull(state)\n\nus_states_MAR &lt;-\n  us_populations |&gt;\n  mutate(population =\n           if_else(state %in% highest_income_states, NA_real_, population)\n         )\n\nsummary(us_states_MAR)\n\n    state             population        income    \n Length:50          Min.   :  376   Min.   :3098  \n Class :character   1st Qu.: 1101   1st Qu.:3993  \n Mode  :character   Median : 2816   Median :4519  \n                    Mean   : 4356   Mean   :4436  \n                    3rd Qu.: 5147   3rd Qu.:4814  \n                    Max.   :21198   Max.   :6315  \n                    NA's   :3                     \n\n\n마지막으로 관측치가 무작위가 아닌 누락(MNAR)될 때, 그것들은 관찰되지 않은 변수나 누락된 변수 자체와 관련된 방식으로 데이터셋에서 누락됩니다. 예를 들어, 소득이 높은 응답자나 교육 수준이 높은 응답자(우리가 수집하지 않은 변수)가 소득을 기입할 가능성이 적을 수 있습니다.\n미국 주 데이터셋의 경우, 인구가 가장 많은 세 미국 주가 인구에 대한 관측치를 갖지 않도록 하여 MNAR 데이터셋을 시뮬레이션할 수 있습니다.\n\nhighest_population_states &lt;-\n  us_populations |&gt;\n  slice_max(population, n = 3) |&gt;\n  pull(state)\n\nus_states_MNAR &lt;-\n  us_populations |&gt;\n  mutate(population =\n           if_else(state %in% highest_population_states,\n                   NA_real_,\n                   population))\n\nus_states_MNAR\n\n# A tibble: 50 × 3\n   state       population income\n   &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;\n 1 Alabama           3615   3624\n 2 Alaska             365   6315\n 3 Arizona           2212   4530\n 4 Arkansas          2110   3378\n 5 California          NA   5114\n 6 Colorado          2541   4884\n 7 Connecticut       3100   5348\n 8 Delaware           579   4809\n 9 Florida           8277   4815\n10 Georgia           4931   4091\n# ℹ 40 more rows\n\n\n최상의 접근 방식은 상황에 따라 맞춤화되겠지만, 일반적으로 우리는 선택의 의미를 더 잘 이해하기 위해 시뮬레이션을 사용하고 싶습니다. 데이터 측면에서 우리는 누락된 관측치를 제거하거나 값을 입력하도록 선택할 수 있습니다. (모델 측면에도 옵션이 있지만, 이 책의 범위를 벗어납니다.) 이러한 접근 방식은 제자리를 찾았지만, 겸손하게 사용하고 잘 전달해야 합니다. 시뮬레이션의 사용은 중요합니다.\n미국 주 데이터셋으로 돌아가서, 일부 결측 데이터를 생성하고, 결측 데이터를 처리하는 몇 가지 일반적인 접근 방식을 고려하고, 각 주에 대한 암시된 값과 전체 미국 평균 인구를 비교할 수 있습니다. 우리는 다음 옵션을 고려합니다.\n\n결측 데이터가 있는 관측치 삭제.\n결측 데이터가 없는 관측치의 평균 대체.\n다중 대체 사용.\n\n결측 데이터가 있는 관측치를 삭제하려면 mean()을 사용할 수 있습니다. 기본적으로 계산에서 결측값이 있는 관측치를 제외합니다. 평균을 대체하려면 결측 데이터가 제거된 두 번째 데이터셋을 구성합니다. 그런 다음 인구 열의 평균을 계산하고 원래 데이터셋의 결측값에 그것을 대체합니다. 다중 대체는 많은 잠재적인 데이터셋을 만들고, 추론을 수행한 다음, 잠재적으로 평균을 통해 그것들을 하나로 모으는 것을 포함합니다 (Gelman 와/과 Hill 2007, p. 542). mice의 mice()를 사용하여 다중 대체를 구현할 수 있습니다.\n\nmultiple_imputation &lt;-\n  mice(\n    us_states_MCAR,\n    print = FALSE\n  )\n\nmice_estimates &lt;-\n  complete(multiple_imputation) |&gt;\n  as_tibble()\n\n\n\n\n\n표 11.2: 세 미국 주에 대한 인구의 대체된 값과 전체 평균 인구 비교\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Observation\n                Drop missing\n                Input mean\n                Multiple imputation\n                Actual\n              \n        \n        \n        \n                \n                  Florida\n                  NA\n                  4,308\n                  11,197\n                  8,277\n                \n                \n                  Montana\n                  NA\n                  4,308\n                  4,589\n                  746\n                \n                \n                  New Hampshire\n                  NA\n                  4,308\n                  813\n                  812\n                \n                \n                  Overall\n                  4,308\n                  4,308\n                  4,382\n                  4,246\n                \n        \n      \n    \n\n\n\n\n\n\n?tbl-imputationoptions는 이러한 접근 방식 중 어느 것도 순진하게 부과해서는 안 된다는 것을 분명히 합니다. 예를 들어, 플로리다의 인구는 8,277이어야 합니다. 모든 주에 걸쳐 평균을 대체하면 4,308의 추정치가 나오고, 다중 대체는 11,197의 추정치를 초래하며, 전자는 너무 낮고 후자는 너무 높습니다. 대체가 답이라면, 다른 질문을 찾는 것이 더 나을 수 있습니다. 개인 정보의 공개 공개를 제한하는 특정 상황을 위해 개발되었다는 점을 지적할 가치가 있습니다 (Horton 와/과 Lipsitz 2001).\n결측 데이터를 보충할 수 있는 것은 아무것도 없습니다 (Manski 2022). 평균 또는 다중 대체를 기반으로 한 예측을 대체하는 것이 합리적인 조건은 흔하지 않으며, 그것들을 확인할 수 있는 능력은 훨씬 더 드뭅니다. 무엇을 해야 하는지는 상황과 분석 목적에 따라 다릅니다. 우리가 가진 데이터의 일부를 제거하고 다양한 옵션을 구현하는 시뮬레이션은 우리가 직면한 절충안을 더 잘 이해하는 데 도움이 될 수 있습니다. 어떤 선택을 하든—그리고 명확한 해결책은 거의 없습니다—수행된 작업을 문서화하고 전달하고, 후속 추정치에 대한 다른 선택의 효과를 탐색하십시오. 우리는 우리가 가진 데이터의 일부를 제거하는 다른 시나리오를 시뮬레이션하고, 접근 방식이 어떻게 다른지 평가하여 진행할 것을 권장합니다.\n마지막으로, 더 산문적이지만 똑같이 중요한 것은, 때때로 결측 데이터가 특정 값으로 변수에 인코딩된다는 것입니다. 예를 들어, R에는 “NA” 옵션이 있지만, 때때로 숫자 데이터는 누락된 경우 “-99” 또는 “9999999”와 같은 매우 큰 정수로 입력됩니다. ?sec-hunt-data에서 소개된 네이션스케이프 설문 조사 데이터셋의 경우, 세 가지 유형의 알려진 결측 데이터가 있습니다.\n\n“888”: “이 웨이브에서 질문했지만, 이 응답자에게는 질문하지 않음”\n“999”: “확실하지 않음, 모름”\n“.”: 응답자가 건너뜀\n\n속하지 않는 것처럼 보이는 값을 명시적으로 찾고 조사하는 것은 항상 가치가 있습니다. 그래프와 표는 이 목적에 특히 유용합니다.",
    "crumbs": [
      "준비",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>탐색적 데이터 분석</span>"
    ]
  },
  {
    "objectID": "11-eda_ko.html#ttc-지하철-지연",
    "href": "11-eda_ko.html#ttc-지하철-지연",
    "title": "11  탐색적 데이터 분석",
    "section": "11.4 TTC 지하철 지연",
    "text": "11.4 TTC 지하철 지연\n두 번째이자 더 복잡한 EDA 예로, ?sec-fire-hose에서 소개된 opendatatoronto와 tidyverse를 사용하여 토론토 지하철 시스템에 대한 데이터를 얻고 탐색합니다. 우리는 발생한 지연에 대한 감을 얻고 싶습니다.\n시작하려면, 2021년 토론토 교통 위원회(TTC) 지하철 지연에 대한 데이터를 다운로드합니다. 데이터는 각 월에 대해 별도의 시트가 있는 엑셀 파일로 제공됩니다. 우리는 2021년에 관심이 있으므로 해당 연도만 필터링한 다음 opendatatoronto의 get_resource()를 사용하여 다운로드하고 bind_rows()로 월을 하나로 합칩니다.\n\nall_2021_ttc_data &lt;-\n  list_package_resources(\"996cfe8d-fb35-40ce-b569-698d51fc683b\") |&gt;\n  filter(name == \"ttc-subway-delay-data-2021\") |&gt;\n  get_resource() |&gt;\n  bind_rows() |&gt;\n  clean_names()\n\nwrite_csv(all_2021_ttc_data, \"all_2021_ttc_data.csv\")\n\nall_2021_ttc_data\n\n\n\n# A tibble: 16,370 × 10\n   date                time   day    station code  min_delay min_gap bound line \n   &lt;dttm&gt;              &lt;time&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n 1 2021-01-01 00:00:00 00:33  Friday BLOOR … MUPAA         0       0 N     YU   \n 2 2021-01-01 00:00:00 00:39  Friday SHERBO… EUCO          5       9 E     BD   \n 3 2021-01-01 00:00:00 01:07  Friday KENNED… EUCD          5       9 E     BD   \n 4 2021-01-01 00:00:00 01:41  Friday ST CLA… MUIS          0       0 &lt;NA&gt;  YU   \n 5 2021-01-01 00:00:00 02:04  Friday SHEPPA… MUIS          0       0 &lt;NA&gt;  YU   \n 6 2021-01-01 00:00:00 02:35  Friday KENNED… MUIS          0       0 &lt;NA&gt;  BD   \n 7 2021-01-01 00:00:00 02:39  Friday VAUGHA… MUIS          0       0 &lt;NA&gt;  YU   \n 8 2021-01-01 00:00:00 06:00  Friday TORONT… MUO           0       0 &lt;NA&gt;  YU   \n 9 2021-01-01 00:00:00 06:00  Friday TORONT… MUO           0       0 &lt;NA&gt;  SHP  \n10 2021-01-01 00:00:00 06:00  Friday TORONT… MRO           0       0 &lt;NA&gt;  SRT  \n# ℹ 16,360 more rows\n# ℹ 1 more variable: vehicle &lt;dbl&gt;\n\n\n데이터셋에는 다양한 열이 있으며, 코드북을 다운로드하여 각 열에 대해 더 자세히 알아볼 수 있습니다. 각 지연의 이유는 코딩되어 있으므로 설명도 다운로드할 수 있습니다. 관심 있는 변수 중 하나는 분 단위의 지연 정도를 나타내는 “min_delay”입니다.\n\n# 데이터 코드북\ndelay_codebook &lt;-\n  list_package_resources(\n    \"996cfe8d-fb35-40ce-b569-698d51fc683b\"\n  ) |&gt;\n  filter(name == \"ttc-subway-delay-data-readme\") |&gt;\n  get_resource() |&gt;\n  clean_names()\n\nwrite_csv(delay_codebook, \"delay_codebook.csv\")\n\n# 지연 코드 설명\ndelay_codes &lt;-\n  list_package_resources(\n    \"996cfe8d-fb35-40ce-b569-698d51fc683b\"\n  ) |&gt;\n  filter(name == \"ttc-subway-delay-codes\") |&gt;\n  get_resource() |&gt;\n  clean_names()\n\nwrite_csv(delay_codes, \"delay_codes.csv\")\n\nEDA를 수행하는 동안 데이터셋을 탐색하는 올바른 방법은 하나도 없지만, 우리는 일반적으로 다음에 특히 관심이 있습니다.\n\n변수는 어떻게 보여야 합니까? 예를 들어, 클래스는 무엇이며, 값은 무엇이며, 이들의 분포는 어떻게 보입니까?\n이상치와 같이 우리가 예상하지 못한 데이터가 있는 측면과, 결측 데이터와 같이 우리가 예상했지만 없는 데이터 측면에서 놀라운 측면은 무엇입습니까?\n분석 목표 개발. 예를 들어, 이 경우, 역 및 시간대와 같은 지연과 관련된 요인을 이해하는 것일 수 있습니다. 여기서 이러한 질문에 공식적으로 답하지는 않겠지만, 답이 어떻게 보일 수 있는지 탐색할 수 있습니다.\n\n진행하면서 모든 측면을 문서화하고 놀라운 점을 기록하는 것이 중요합니다. 우리는 진행하면서 내린 단계와 가정의 기록을 만들려고 합니다. 왜냐하면 이것들은 모델링할 때 중요하기 때문입니다. 자연 과학에서 이러한 유형의 연구 노트는 법적 문서가 될 수도 있습니다 (Ryan 2015).\n\n11.4.1 개별 변수의 분포 및 속성\n변수가 말하는 대로인지 확인해야 합니다. 그렇지 않은 경우, 무엇을 해야 할지 알아내야 합니다. 예를 들어, 변경해야 할까요, 아니면 제거해야 할까요? 변수의 클래스가 예상대로인지 확인하는 것도 중요합니다. 예를 들어, 요인이어야 하는 변수는 요인이고, 문자여야 하는 변수는 문자입니다. 그리고 예를 들어, 요인을 숫자로, 또는 그 반대로 실수로 갖지 않도록 합니다. 한 가지 방법은 unique()를 사용하는 것이고, 다른 하나는 table()을 사용하는 것입니다. 어떤 변수가 특정 클래스여야 하는지에 대한 보편적인 답은 없습니다. 왜냐하면 답은 맥락에 따라 다르기 때문입니다.\n\nunique(all_2021_ttc_data$day)\n\n[1] \"Friday\"    \"Saturday\"  \"Sunday\"    \"Monday\"    \"Tuesday\"   \"Wednesday\"\n[7] \"Thursday\" \n\nunique(all_2021_ttc_data$line)\n\n [1] \"YU\"                     \"BD\"                     \"SHP\"                   \n [4] \"SRT\"                    \"YU/BD\"                  NA                      \n [7] \"YONGE/UNIVERSITY/BLOOR\" \"YU / BD\"                \"YUS\"                   \n[10] \"999\"                    \"SHEP\"                   \"36 FINCH WEST\"         \n[13] \"YUS & BD\"               \"YU & BD LINES\"          \"35 JANE\"               \n[16] \"52\"                     \"41 KEELE\"               \"YUS/BD\"                \n\ntable(all_2021_ttc_data$day)\n\n\n   Friday    Monday  Saturday    Sunday  Thursday   Tuesday Wednesday \n     2600      2434      2073      1942      2425      2481      2415 \n\ntable(all_2021_ttc_data$line)\n\n\n               35 JANE          36 FINCH WEST               41 KEELE \n                     1                      1                      1 \n                    52                    999                     BD \n                     1                      1                   5734 \n                  SHEP                    SHP                    SRT \n                     1                    657                    656 \nYONGE/UNIVERSITY/BLOOR                     YU                YU / BD \n                     1                   8880                     17 \n         YU & BD LINES                  YU/BD                    YUS \n                     1                    346                     18 \n              YUS & BD                 YUS/BD \n                     1                      1 \n\n\n지하철 노선 측면에서 문제가 있을 가능성이 높습니다. 일부는 명확한 해결책이 있지만 전부는 아닙니다. 한 가지 옵션은 그것들을 삭제하는 것이지만, 이러한 오류가 관심 있는 것과 상관 관계가 있을 수 있는지 여부를 생각해야 합니다. 만약 그렇다면, 우리는 중요한 정보를 삭제하고 있을 수 있습니다. 일반적으로 올바른 답은 하나도 없습니다. 왜냐하면 그것은 일반적으로 우리가 데이터를 무엇에 사용하는지에 따라 달라지기 때문입니다. 우리는 EDA를 계속하면서 문제를 기록하고 나중에 무엇을 할지 결정할 것입니다. 지금은 코드북을 기반으로 올바른 것으로 알고 있는 노선이 아닌 모든 노선을 제거할 것입니다.\n\ndelay_codebook |&gt;\n  filter(field_name == \"Line\")\n\n# A tibble: 1 × 3\n  field_name description                               example\n  &lt;chr&gt;      &lt;chr&gt;                                     &lt;chr&gt;  \n1 Line       TTC subway line i.e. YU, BD, SHP, and SRT YU     \n\nall_2021_ttc_data_filtered_lines &lt;-\n  all_2021_ttc_data |&gt;\n  filter(line %in% c(\"YU\", \"BD\", \"SHP\", \"SRT\"))\n\n전체 경력은 결측 데이터를 이해하는 데 소비되며, 결측값의 존재 또는 부재는 분석을 괴롭힐 수 있습니다. 시작하려면, 각 변수에 대한 NA인 알려진-알 수 없는 것을 볼 수 있습니다. 예를 들어, 변수별로 개수를 만들 수 있습니다.\n이 경우 “bound”에 많은 결측값이 있고 “line”에 두 개가 있습니다. 이러한 알려진-알 수 없는 것에 대해, ?sec-farm-data에서 논의했듯이, 우리는 그것들이 무작위로 누락되었는지 여부에 관심이 있습니다. 이상적으로는 데이터가 그냥 빠져나갔다는 것을 보여주고 싶습니다. 그러나 이것은 가능성이 낮으므로, 우리는 일반적으로 데이터가 누락된 방식에 대해 체계적인 것이 무엇인지 살펴보려고 합니다.\n때로는 데이터가 중복될 수 있습니다. 이것을 알아차리지 못하면, 분석이 우리가 일관되게 예상할 수 없는 방식으로 잘못될 것입니다. 중복된 행을 찾는 방법에는 여러 가지가 있지만, janitor의 get_dupes()가 특히 유용합니다.\n\nget_dupes(all_2021_ttc_data_filtered_lines)\n\n# A tibble: 36 × 11\n   date                time   day    station code  min_delay min_gap bound line \n   &lt;dttm&gt;              &lt;time&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n 1 2021-09-13 00:00:00 06:00  Monday TORONT… MRO           0       0 &lt;NA&gt;  SRT  \n 2 2021-09-13 00:00:00 06:00  Monday TORONT… MRO           0       0 &lt;NA&gt;  SRT  \n 3 2021-09-13 00:00:00 06:00  Monday TORONT… MRO           0       0 &lt;NA&gt;  SRT  \n 4 2021-09-13 00:00:00 06:00  Monday TORONT… MUO           0       0 &lt;NA&gt;  SHP  \n 5 2021-09-13 00:00:00 06:00  Monday TORONT… MUO           0       0 &lt;NA&gt;  SHP  \n 6 2021-09-13 00:00:00 06:00  Monday TORONT… MUO           0       0 &lt;NA&gt;  SHP  \n 7 2021-03-31 00:00:00 05:45  Wedne… DUNDAS… MUNCA         0       0 &lt;NA&gt;  BD   \n 8 2021-03-31 00:00:00 05:45  Wedne… DUNDAS… MUNCA         0       0 &lt;NA&gt;  BD   \n 9 2021-06-08 00:00:00 14:40  Tuesd… VAUGHA… MUNOA         3       6 S     YU   \n10 2021-06-08 00:00:00 14:40  Tuesd… VAUGHA… MUNOA         3       6 S     YU   \n# ℹ 26 more rows\n# ℹ 2 more variables: vehicle &lt;dbl&gt;, dupe_count &lt;int&gt;\n\n\n이 데이터셋에는 많은 중복이 있습니다. 우리는 체계적인 일이 일어나고 있는지 여부에 관심이 있습니다. EDA 동안 우리는 데이터셋에 대한 감을 빠르게 얻으려고 노력하고 있다는 것을 기억하면서, 한 가지 방법은 이것을 나중에 돌아와서 탐색할 문제로 표시하고, 지금은 distinct()를 사용하여 중복을 제거하는 것입니다.\n\nall_2021_ttc_data_no_dupes &lt;-\n  all_2021_ttc_data_filtered_lines |&gt;\n  distinct()\n\n역 이름에는 많은 오류가 있습니다.\n\nall_2021_ttc_data_no_dupes |&gt;\n  count(station) |&gt;\n  filter(str_detect(station, \"WEST\"))\n\n# A tibble: 17 × 2\n   station                    n\n   &lt;chr&gt;                  &lt;int&gt;\n 1 DUNDAS WEST STATION      198\n 2 EGLINTON WEST STATION    142\n 3 FINCH WEST STATION       126\n 4 FINCH WEST TO LAWRENCE     3\n 5 FINCH WEST TO WILSON       1\n 6 LAWRENCE WEST CENTRE       1\n 7 LAWRENCE WEST STATION    127\n 8 LAWRENCE WEST TO EGLIN     1\n 9 SHEPPARD WEST - WILSON     1\n10 SHEPPARD WEST STATION    210\n11 SHEPPARD WEST TO LAWRE     3\n12 SHEPPARD WEST TO ST CL     2\n13 SHEPPARD WEST TO WILSO     7\n14 ST CLAIR WEST STATION    205\n15 ST CLAIR WEST TO ST AN     1\n16 ST. CLAIR WEST TO KING     1\n17 ST.CLAIR WEST TO ST.A      1\n\n\n우리는 혼돈에 약간의 질서를 부여하기 위해 첫 단어 또는 처음 몇 단어만 취하여 빠르게 시도할 수 있으며, 이름이 “ST”로 시작하는지 확인하여 “ST. CLAIR” 및 “ST. PATRICK”과 같은 이름을 고려하고, 이름에 “WEST”가 포함되어 있는지 확인하여 “DUNDAS” 및 “DUNDAS WEST”와 같은 역을 구별합니다. 다시 말하지만, 우리는 데이터를 이해하려고 노력하고 있으며, 반드시 여기서 구속력 있는 결정을 내리는 것은 아닙니다. stringr의 word()를 사용하여 역 이름에서 특정 단어를 추출합니다.\n\nall_2021_ttc_data_no_dupes &lt;-\n  all_2021_ttc_data_no_dupes |&gt;\n  mutate(\n    station_clean =\n      case_when(\n        str_starts(station, \"ST\") &\n          str_detect(station, \"WEST\") ~ word(station, 1, 3),\n        str_starts(station, \"ST\") ~ word(station, 1, 2),\n        str_detect(station, \"WEST\") ~ word(station, 1, 2),\n        TRUE ~ word(station, 1)\n      )\n  )\n\nall_2021_ttc_data_no_dupes\n\n# A tibble: 15,908 × 11\n   date                time   day    station code  min_delay min_gap bound line \n   &lt;dttm&gt;              &lt;time&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n 1 2021-01-01 00:00:00 00:33  Friday BLOOR … MUPAA         0       0 N     YU   \n 2 2021-01-01 00:00:00 00:39  Friday SHERBO… EUCO          5       9 E     BD   \n 3 2021-01-01 00:00:00 01:07  Friday KENNED… EUCD          5       9 E     BD   \n 4 2021-01-01 00:00:00 01:41  Friday ST CLA… MUIS          0       0 &lt;NA&gt;  YU   \n 5 2021-01-01 00:00:00 02:04  Friday SHEPPA… MUIS          0       0 &lt;NA&gt;  YU   \n 6 2021-01-01 00:00:00 02:35  Friday KENNED… MUIS          0       0 &lt;NA&gt;  BD   \n 7 2021-01-01 00:00:00 02:39  Friday VAUGHA… MUIS          0       0 &lt;NA&gt;  YU   \n 8 2021-01-01 00:00:00 06:00  Friday TORONT… MUO           0       0 &lt;NA&gt;  YU   \n 9 2021-01-01 00:00:00 06:00  Friday TORONT… MUO           0       0 &lt;NA&gt;  SHP  \n10 2021-01-01 00:00:00 06:00  Friday TORONT… MRO           0       0 &lt;NA&gt;  SRT  \n# ℹ 15,898 more rows\n# ℹ 2 more variables: vehicle &lt;dbl&gt;, station_clean &lt;chr&gt;\n\n\n우리는 데이터를 이해하기 위해 원래 상태로 데이터를 봐야 하며, 종종 막대 차트, 산점도, 선 그래프 및 히스토그램을 사용합니다. EDA 동안 우리는 그래프가 멋지게 보이는지에 대해 그다지 신경 쓰지 않고, 대신 가능한 한 빨리 데이터에 대한 감을 얻으려고 노력합니다. 관심 있는 결과 중 하나인 “min_delay”의 분포를 살펴보는 것으로 시작할 수 있습니다.\nall_2021_ttc_data_no_dupes |&gt;\n  ggplot(aes(x = min_delay)) +\n  geom_histogram(bins = 30)\n\nall_2021_ttc_data_no_dupes |&gt;\n  ggplot(aes(x = min_delay)) +\n  geom_histogram(bins = 30) +\n  scale_x_log10()\n\n\n\n\n\n\n\n\n\n\n\n(a) 지연 분포\n\n\n\n\n\n\n\n\n\n\n\n(b) 로그 스케일 사용\n\n\n\n\n\n\n\n그림 11.1: 분 단위 지연 분포\n\n\n\n?fig-delayhist-1의 대체로 비어 있는 그래프는 이상치의 존재를 시사합니다. 무슨 일이 일어나고 있는지 이해하려고 시도하는 다양한 방법이 있지만, 한 가지 빠른 방법은 로그를 사용하는 것입니다. 0 값은 사라질 것으로 예상한다는 것을 기억하십시오(그림 11.1 (b)).\n이 초기 탐색은 우리가 더 자세히 탐색하고 싶은 소수의 큰 지연이 있음을 시사합니다. 우리는 무슨 일이 일어나고 있는지 이해하기 위해 이 데이터셋을 “delay_codes”와 결합할 것입니다.\n\nfix_organization_of_codes &lt;-\n  rbind(\n    delay_codes |&gt;\n      select(sub_rmenu_code, code_description_3) |&gt;\n      mutate(type = \"sub\") |&gt;\n      rename(\n        code = sub_rmenu_code,\n        code_desc = code_description_3\n      ),\n    delay_codes |&gt;\n      select(srt_rmenu_code, code_description_7) |&gt;\n      mutate(type = \"srt\") |&gt;\n      rename(\n        code = srt_rmenu_code,\n        code_desc = code_description_7\n      )\n  )\n\nall_2021_ttc_data_no_dupes_with_explanation &lt;-\n  all_2021_ttc_data_no_dupes |&gt;\n  mutate(type = if_else(line == \"SRT\", \"srt\", \"sub\")) |&gt;\n  left_join(\n    fix_organization_of_codes,\n    by = c(\"type\", \"code\")\n  )\n\nall_2021_ttc_data_no_dupes_with_explanation |&gt;\n  select(station_clean, code, min_delay, code_desc) |&gt;\n  arrange(-min_delay)\n\n# A tibble: 15,908 × 4\n   station_clean code  min_delay code_desc                                  \n   &lt;chr&gt;         &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;                                      \n 1 MUSEUM        PUTTP       348 Traction Power Rail Related                \n 2 EGLINTON      PUSTC       343 Signals - Track Circuit Problems           \n 3 WOODBINE      MUO         312 Miscellaneous Other                        \n 4 MCCOWAN       PRSL        275 Loop Related Failures                      \n 5 SHEPPARD WEST PUTWZ       255 Work Zone Problems - Track                 \n 6 ISLINGTON     MUPR1       207 Priority One - Train in Contact With Person\n 7 SHEPPARD WEST MUPR1       191 Priority One - Train in Contact With Person\n 8 ROYAL         SUAP        182 Assault / Patron Involved                  \n 9 ROYAL         MUPR1       180 Priority One - Train in Contact With Person\n10 SHEPPARD      MUPR1       171 Priority One - Train in Contact With Person\n# ℹ 15,898 more rows\n\n\n이것으로부터 우리는 348분의 지연이 “견인 전력 레일 관련” 때문이었고, 343분의 지연이 “신호 - 궤도 회로 문제” 때문이었으며, 기타 등등이라는 것을 알 수 있습니다.\n우리가 찾고 있는 또 다른 것은 데이터의 다양한 그룹화이며, 특히 하위 그룹이 결국 소수의 관측치만 갖게 될 수 있는 경우입니다. 이것은 우리의 분석이 그것들에 의해 특히 영향을 받을 수 있기 때문입니다. 이것을 하는 한 가지 빠른 방법은 관심 있는 변수, 예를 들어 “line”을 기준으로 데이터를 그룹화하고 색상을 사용하는 것입니다.\nall_2021_ttc_data_no_dupes_with_explanation |&gt;\n  ggplot() +\n  geom_histogram(\n    aes(\n      x = min_delay,\n      y = ..density..,\n      fill = line\n    ),\n    position = \"dodge\",\n    bins = 10\n  ) +\n  scale_x_log10()\n\nall_2021_ttc_data_no_dupes_with_explanation |&gt;\n  ggplot() +\n  geom_histogram(\n    aes(x = min_delay, fill = line),\n    position = \"dodge\",\n    bins = 10\n  ) +\n  scale_x_log10()\n\n\n\n\n\n\n\n\n\n\n\n(a) 밀도\n\n\n\n\n\n\n\n\n\n\n\n(b) 빈도\n\n\n\n\n\n\n\n그림 11.2: 분 단위 지연 분포\n\n\n\n?fig-delaydensity-1은 분포를 더 비교 가능하게 볼 수 있도록 밀도를 사용하지만, 빈도의 차이도 인식해야 합니다(그림 11.2 (b)). 이 경우, “SHP”와 “SRT”의 개수가 훨씬 적다는 것을 알 수 있습니다.\n다른 변수로 그룹화하려면 패싯을 추가할 수 있습니다(그림 11.3).\n\nall_2021_ttc_data_no_dupes_with_explanation |&gt;\n  ggplot() +\n  geom_histogram(\n    aes(x = min_delay, fill = line),\n    position = \"dodge\",\n    bins = 10\n  ) +\n  scale_x_log10() +\n  facet_wrap(vars(day)) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n그림 11.3: 요일별 분 단위 지연 분포의 빈도\n\n\n\n\n\n우리는 또한 평균 지연 시간별 상위 5개 역을 노선별로 패싯하여 플롯할 수 있습니다(그림 11.4). 이것은 우리가 추적해야 할 사항, 즉 “YU”의 “ZONE”이 무엇인지에 대한 질문을 제기합니다.\n\nall_2021_ttc_data_no_dupes_with_explanation |&gt;\n  summarise(mean_delay = mean(min_delay), n_obs = n(),\n            .by = c(line, station_clean)) |&gt;\n  filter(n_obs &gt; 1) |&gt;\n  arrange(line, -mean_delay) |&gt;\n  slice(1:5, .by = line) |&gt;\n  ggplot(aes(station_clean, mean_delay)) +\n  geom_col() +\n  coord_flip() +\n  facet_wrap(vars(line), scales = \"free_y\")\n\n\n\n\n\n\n\n그림 11.4: 평균 지연 및 노선별 상위 5개 역\n\n\n\n\n\n?sec-clean-and-prepare에서 논의했듯이, 날짜는 문제가 발생하기 쉽기 때문에 작업하기 어려운 경우가 많습니다. 이러한 이유로 EDA 중에 날짜를 고려하는 것이 특히 중요합니다. 1년 동안 계절성이 있는지 확인하기 위해 주별로 그래프를 만들어 보겠습니다. 날짜를 사용할 때 lubridate는 특히 유용합니다. 예를 들어, week()를 사용하여 주를 구성하여 주별로 지연된 것의 평균 지연을 볼 수 있습니다(그림 11.5).\n\nall_2021_ttc_data_no_dupes_with_explanation |&gt;\n  filter(min_delay &gt; 0) |&gt;\n  mutate(week = week(date)) |&gt;\n  summarise(mean_delay = mean(min_delay),\n            .by = c(week, line)) |&gt;\n  ggplot(aes(week, mean_delay, color = line)) +\n  geom_point() +\n  geom_smooth() +\n  facet_wrap(vars(line), scales = \"free_y\")\n\n\n\n\n\n\n\n그림 11.5: 토론토 지하철의 주별 평균 지연 시간(분)\n\n\n\n\n\n이제 10분 이상 지연된 비율을 살펴보겠습니다(그림 11.6).\n\nall_2021_ttc_data_no_dupes_with_explanation |&gt;\n  mutate(week = week(date)) |&gt;\n  summarise(prop_delay = sum(min_delay &gt; 10) / n(),\n            .by = c(week, line)) |&gt;\n  ggplot(aes(week, prop_delay, color = line)) +\n  geom_point() +\n  geom_smooth() +\n  facet_wrap(vars(line), scales = \"free_y\")\n\n\n\n\n\n\n\n그림 11.6: 토론토 지하철의 주별 10분 이상 지연\n\n\n\n\n\n이러한 그림, 표 및 분석은 최종 논문에 포함되지 않을 수 있습니다. 대신, 우리가 데이터에 익숙해지도록 해줍니다. 우리는 각각에 대해 눈에 띄는 측면과 경고 및 반환할 의미나 측면을 기록합니다.\n\n\n11.4.2 변수 간의 관계\n우리는 또한 두 변수 간의 관계를 살펴보는 데 관심이 있습니다. 이를 위해 그래프를 많이 활용할 것입니다. 다른 상황에 적합한 유형은 ?sec-static-communication에서 논의되었습니다. 산점도는 연속 변수에 특히 유용하며, 모델링의 좋은 전조입니다. 예를 들어, 열차 간의 분 수인 지연과 간격 간의 관계에 관심이 있을 수 있습니다(그림 11.7).\n\nall_2021_ttc_data_no_dupes_with_explanation |&gt;\n  ggplot(aes(x = min_delay, y = min_gap, alpha = 0.1)) +\n  geom_point() +\n  scale_x_log10() +\n  scale_y_log10()\n\n\n\n\n\n\n\n그림 11.7: 2021년 토론토 지하철의 지연과 간격 간의 관계\n\n\n\n\n\n범주형 변수 간의 관계는 더 많은 작업이 필요하지만, 예를 들어 역별 지연의 상위 5가지 이유를 살펴볼 수도 있습니다. 우리는 그것들이 다른지, 그리고 어떤 차이가 어떻게 모델링될 수 있는지에 관심이 있을 수 있습니다(그림 11.8).\n\nall_2021_ttc_data_no_dupes_with_explanation |&gt;\n  summarise(mean_delay = mean(min_delay),\n            .by = c(line, code_desc)) |&gt;\n  arrange(-mean_delay) |&gt;\n  slice(1:5) |&gt;\n  ggplot(aes(x = code_desc, y = mean_delay)) +\n  geom_col() +\n  facet_wrap(vars(line), scales = \"free_y\", nrow = 4) +\n  coord_flip()\n\n\n\n\n\n\n\n그림 11.8: 2021년 토론토 지하철의 범주형 변수 간의 관계",
    "crumbs": [
      "준비",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>탐색적 데이터 분석</span>"
    ]
  },
  {
    "objectID": "11-eda_ko.html#런던-영국의-에어비앤비-목록",
    "href": "11-eda_ko.html#런던-영국의-에어비앤비-목록",
    "title": "11  탐색적 데이터 분석",
    "section": "11.5 런던, 영국의 에어비앤비 목록",
    "text": "11.5 런던, 영국의 에어비앤비 목록\n이 사례 연구에서는 2023년 3월 14일 기준 영국 런던의 에어비앤비 목록을 살펴봅니다. 데이터셋은 인사이드 에어비앤비 (Cox 2021)에서 가져왔으며, 웹사이트에서 읽어들인 다음 로컬 사본을 저장할 것입니다. 데이터셋이 있는 곳에 대한 링크를 read_csv()에 제공하면 다운로드됩니다. 이것은 출처가 명확하기 때문에 재현성에 도움이 됩니다. 그러나 해당 링크는 언제든지 변경될 수 있으므로, 장기적인 재현성과 인사이드 에어비앤비 서버에 미치는 영향을 최소화하려는 욕구는 데이터의 로컬 사본도 저장한 다음 그것을 사용해야 함을 시사합니다.\n필요한 데이터셋을 얻으려면 인사이드 에어비앤비 \\(\\rightarrow\\) “데이터” \\(\\rightarrow\\) “데이터 가져오기”로 이동한 다음 런던으로 스크롤합니다. 우리는 “목록 데이터셋”에 관심이 있으며, 마우스 오른쪽 버튼을 클릭하여 필요한 URL을 얻습니다(그림 11.9). 인사이드 에어비앤비는 제공하는 데이터를 업데이트하므로, 사용 가능한 특정 데이터셋은 시간이 지남에 따라 변경됩니다.\n\n\n\n\n\n\n그림 11.9: 인사이드 에어비앤비에서 에어비앤비 데이터 얻기\n\n\n\n원본 데이터셋은 우리 것이 아니므로, 먼저 서면 허가를 받지 않고는 공개해서는 안 됩니다. 예를 들어, 입력 폴더에 추가하고 싶을 수 있지만, ?sec-reproducible-workflows에서 다룬 “.gitignore” 항목을 사용하여 GitHub에 푸시하지 않도록 합니다. read_csv()의 “guess_max” 옵션은 열 유형을 지정할 필요가 없도록 도와줍니다. 일반적으로 read_csv()는 처음 몇 행을 기반으로 열 유형을 가장 잘 추측합니다. 그러나 때로는 처음 몇 행이 오해의 소지가 있으므로 “guess_max”는 무슨 일이 일어나고 있는지 알아내기 위해 더 많은 행을 보도록 강제합니다. 인사이드 에어비앤비에서 복사한 URL을 URL 부분에 붙여넣습니다. 그리고 다운로드되면 로컬 사본을 저장합니다.\n\nurl &lt;-\n  paste0(\n    \"http://data.insideairbnb.com/united-kingdom/england/\",\n    \"london/2023-03-14/data/listings.csv.gz\"\n  )\n\nairbnb_data &lt;-\n  read_csv(\n    file = url,\n    guess_max = 20000\n  )\n\nwrite_csv(airbnb_data, \"airbnb_data.csv\")\n\nairbnb_data\n\n데이터를 탐색하기 위해 스크립트를 실행할 때마다 인사이드 에어비앤비 서버에 데이터를 요청하는 대신 이 로컬 데이터 사본을 참조해야 합니다. 실수로 서비스를 스트레스 주지 않도록 서버에 대한 이 호출을 주석 처리하는 것이 좋습니다.\n다시, 이 파일 이름—“airbnb_data.csv”—을 “.gitignore” 파일에 추가하여 GitHub에 푸시되지 않도록 합니다. 데이터셋의 크기는 우리가 피하고 싶은 복잡성을 야기할 것입니다.\n이 CSV는 원본, 편집되지 않은 데이터이므로 보관해야 하지만, 100MB가 넘으면 다루기 약간 번거롭습니다. 탐색 목적으로 선택한 변수가 있는 파케이 파일을 만들 것입니다(이것은 names(airbnb_data)를 사용하여 변수 이름을 알아내는 반복적인 방식으로 수행합니다).\n\nairbnb_data_selected &lt;-\n  airbnb_data |&gt;\n  select(\n    host_id,\n    host_response_time,\n    host_is_superhost,\n    host_total_listings_count,\n    neighbourhood_cleansed,\n    bathrooms,\n    bedrooms,\n    price,\n    number_of_reviews,\n    review_scores_rating,\n    review_scores_accuracy,\n    review_scores_value\n  )\n\nwrite_parquet(\n  x = airbnb_data_selected, \n  sink = \n    \"2023-03-14-london-airbnblistings-select_variables.parquet\"\n  )\n\nrm(airbnb_data)\n\n\n11.5.1 개별 변수의 분포 및 속성\n먼저 가격에 관심이 있을 수 있습니다. 현재는 문자이므로 숫자로 변환해야 합니다. 이것은 일반적인 문제이며, 모두 NA로 변환되지 않도록 약간 주의해야 합니다. 가격 변수를 강제로 숫자로 만들면 “$”와 같이 숫자 등가물이 불분명한 문자가 많기 때문에 NA가 됩니다. 먼저 해당 문자를 제거해야 합니다.\n\nairbnb_data_selected$price |&gt;\n  head()\n\n[1] \"$100.00\" \"$65.00\"  \"$132.00\" \"$100.00\" \"$120.00\" \"$43.00\" \n\nairbnb_data_selected$price |&gt;\n  str_split(\"\") |&gt;\n  unlist() |&gt;\n  unique()\n\n [1] \"$\" \"1\" \"0\" \".\" \"6\" \"5\" \"3\" \"2\" \"4\" \"9\" \"8\" \"7\" \",\"\n\nairbnb_data_selected |&gt;\n  select(price) |&gt;\n  filter(str_detect(price, \",\"))\n\n# A tibble: 1,629 × 1\n   price    \n   &lt;chr&gt;    \n 1 $3,070.00\n 2 $1,570.00\n 3 $1,480.00\n 4 $1,000.00\n 5 $1,100.00\n 6 $1,433.00\n 7 $1,800.00\n 8 $1,000.00\n 9 $1,000.00\n10 $1,000.00\n# ℹ 1,619 more rows\n\nairbnb_data_selected &lt;-\n  airbnb_data_selected |&gt;\n  mutate(\n    price = str_remove_all(price, \"[\\\\$,]\"),\n    price = as.integer(price)\n  )\n\n이제 가격 분포를 볼 수 있습니다(그림 11.10 (a)). 이상치가 있으므로 다시 로그 스케일로 고려하고 싶을 수 있습니다(그림 11.10 (b)).\nairbnb_data_selected |&gt;\n  ggplot(aes(x = price)) +\n  geom_histogram(binwidth = 10) +\n  theme_classic() +\n  labs(\n    x = \"Price per night\",\n    y = \"Number of properties\"\n  )\n\nairbnb_data_selected |&gt;\n  filter(price &gt; 1000) |&gt;\n  ggplot(aes(x = price)) +\n  geom_histogram(binwidth = 10) +\n  theme_classic() +\n  labs(\n    x = \"Price per night\",\n    y = \"Number of properties\"\n  ) +\n  scale_y_log10()\n\n\n\n\n\n\n\n\n\n\n\n(a) 가격 분포\n\n\n\n\n\n\n\n\n\n\n\n(b) 1,000달러 이상 가격에 대한 로그 스케일 사용\n\n\n\n\n\n\n\n그림 11.10: 2023년 3월 런던 에어비앤비 임대료 가격 분포\n\n\n\n?fig-airbnbpricesbunch로 넘어가서, 1,000달러 미만의 가격에 집중하면, 대부분의 숙소의 1박 가격이 250달러 미만임을 알 수 있습니다(그림 11.11 (a)). ?sec-clean-and-prepare에서 연령에 일부 집중 현상이 있었던 것과 마찬가지로, 여기서도 가격에 일부 집중 현상이 있는 것 같습니다. 이것이 0이나 9로 끝나는 숫자 주변에서 일어나고 있을 수 있습니다. 관심사에서 90달러에서 210달러 사이의 가격만 확대해 보되, 빈을 더 작게 변경해 보겠습니다(그림 11.11 (b)).\nairbnb_data_selected |&gt;\n  filter(price &lt; 1000) |&gt;\n  ggplot(aes(x = price)) +\n  geom_histogram(binwidth = 10) +\n  theme_classic() +\n  labs(\n    x = \"Price per night\",\n    y = \"Number of properties\"\n  )\n\nairbnb_data_selected |&gt;\n  filter(price &gt; 90) |&gt;\n  filter(price &lt; 210) |&gt;\n  ggplot(aes(x = price)) +\n  geom_histogram(binwidth = 1) +\n  theme_classic() +\n  labs(\n    x = \"Price per night\",\n    y = \"Number of properties\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n(a) 1,000달러 미만의 가격은 일부 집중 현상을 시사합니다\n\n\n\n\n\n\n\n\n\n\n\n(b) 90달러에서 210달러 사이의 가격은 집중 현상을 더 명확하게 보여줍니다\n\n\n\n\n\n\n\n그림 11.11: 2023년 3월 런던 에어비앤비 목록의 가격 분포\n\n\n\n지금은 999달러 이상의 모든 가격을 제거하겠습니다.\n\nairbnb_data_less_1000 &lt;-\n  airbnb_data_selected |&gt;\n  filter(price &lt; 1000)\n\n슈퍼호스트는 특히 경험이 풍부한 에어비앤비 호스트이며, 그들에 대해 더 자세히 알고 싶을 수 있습니다. 예를 들어, 호스트는 슈퍼호스트이거나 아니거나이므로 NA를 예상하지 않습니다. 그러나 NA가 있음을 알 수 있습니다. 호스트가 목록을 제거했거나 유사한 것일 수 있지만, 이것은 더 자세히 살펴봐야 할 사항입니다.\n\nairbnb_data_less_1000 |&gt;\n  filter(is.na(host_is_superhost))\n\n# A tibble: 13 × 12\n     host_id host_response_time host_is_superhost host_total_listings_count\n       &lt;dbl&gt; &lt;chr&gt;              &lt;lgl&gt;                                 &lt;dbl&gt;\n 1 317054510 within an hour     NA                                        5\n 2 316090383 within an hour     NA                                        6\n 3 315016947 within an hour     NA                                        2\n 4 374424554 within an hour     NA                                        2\n 5  97896300 N/A                NA                                       10\n 6 316083765 within an hour     NA                                        7\n 7 310628674 N/A                NA                                        5\n 8 179762278 N/A                NA                                       10\n 9 315037299 N/A                NA                                        1\n10 316090018 within an hour     NA                                        6\n11 375515965 within an hour     NA                                        2\n12 341372520 N/A                NA                                        7\n13 180634347 within an hour     NA                                        5\n# ℹ 8 more variables: neighbourhood_cleansed &lt;chr&gt;, bathrooms &lt;lgl&gt;,\n#   bedrooms &lt;dbl&gt;, price &lt;int&gt;, number_of_reviews &lt;dbl&gt;,\n#   review_scores_rating &lt;dbl&gt;, review_scores_accuracy &lt;dbl&gt;,\n#   review_scores_value &lt;dbl&gt;\n\n\n우리는 또한 이것으로부터 이진 변수를 만들고 싶을 것입니다. 현재는 참/거짓이며, 모델링에는 괜찮지만, 0/1이 있으면 더 쉬운 몇 가지 상황이 있습니다. 그리고 지금은 슈퍼호스트인지 여부에 대한 NA가 있는 사람은 모두 제거할 것입니다.\n\nairbnb_data_no_superhost_nas &lt;-\n  airbnb_data_less_1000 |&gt;\n  filter(!is.na(host_is_superhost)) |&gt;\n  mutate(\n    host_is_superhost_binary =\n      as.numeric(host_is_superhost)\n  )\n\n에어비앤비에서 게스트는 청결도, 정확성, 가치 등 다양한 측면에 대해 1~5점의 평점을 줄 수 있습니다. 그러나 우리 데이터셋의 리뷰를 보면, 사실상 이진이며, 거의 모든 경우에 평점이 5점이거나 그렇지 않다는 것이 분명합니다(그림 11.12).\n\nairbnb_data_no_superhost_nas |&gt;\n  ggplot(aes(x = review_scores_rating)) +\n  geom_bar() +\n  theme_classic() +\n  labs(\n    x = \"Review scores rating\",\n    y = \"Number of properties\"\n  )\n\n\n\n\n\n\n\n그림 11.12: 2023년 3월 런던 에어비앤비 임대료에 대한 리뷰 점수 평점 분포\n\n\n\n\n\n“review_scores_rating”의 NA를 처리하고 싶지만, 이것은 많은 수가 있기 때문에 더 복잡합니다. 이것은 단지 리뷰가 없기 때문일 수 있습니다.\n\nairbnb_data_no_superhost_nas |&gt;\n  filter(is.na(review_scores_rating)) |&gt;\n  nrow()\n\n[1] 17681\n\nairbnb_data_no_superhost_nas |&gt;\n  filter(is.na(review_scores_rating)) |&gt;\n  select(number_of_reviews) |&gt;\n  table()\n\nnumber_of_reviews\n    0 \n17681 \n\n\n이러한 숙소는 아직 리뷰가 충분하지 않아 리뷰 평점이 없습니다. 전체의 거의 5분의 1에 달하는 큰 비율이므로 개수를 사용하여 더 자세히 살펴보고 싶을 수 있습니다.  우리는 이러한 숙소에 체계적인 일이 일어나고 있는지 여부에 관심이 있습니다. 예를 들어, NA가 최소 리뷰 수 요구 사항과 같은 것에 의해 주도된다면, 우리는 그것들이 모두 누락될 것으로 예상할 것입니다.   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n한 가지 접근 방식은 누락되지 않은 것과 주요 리뷰 점수에만 집중하는 것입니다(그림 11.13).\n\nairbnb_data_no_superhost_nas |&gt;\n  filter(!is.na(review_scores_rating)) |&gt;\n  ggplot(aes(x = review_scores_rating)) +\n  geom_histogram(binwidth = 1) +\n  theme_classic() +\n  labs(\n    x = \"Average review score\",\n    y = \"Number of properties\"\n  )\n\n\n\n\n\n\n\n그림 11.13: 2023년 3월 런던 에어비앤비 임대료에 대한 리뷰 점수 분포\n\n\n\n\n\n지금은 주요 리뷰 점수에 NA가 있는 사람은 모두 제거할 것이지만, 이것은 대략 20%의 관측치를 제거할 것입니다. 이 데이터셋을 실제 분석에 사용하게 된다면, 부록이나 이와 유사한 곳에서 이 결정을 정당화하고 싶을 것입니다.\n\nairbnb_data_has_reviews &lt;-\n  airbnb_data_no_superhost_nas |&gt;\n  filter(!is.na(review_scores_rating))\n\n또 다른 중요한 요소는 호스트가 문의에 얼마나 빨리 응답하는지입니다. 에어비앤비는 호스트에게 24시간 이내에 응답하도록 허용하지만, 1시간 이내에 응답하도록 권장합니다.\n\nairbnb_data_has_reviews |&gt;\n  count(host_response_time)\n\n# A tibble: 5 × 2\n  host_response_time     n\n  &lt;chr&gt;              &lt;int&gt;\n1 N/A                19479\n2 a few days or more   712\n3 within a day        4512\n4 within a few hours  6894\n5 within an hour     24321\n\n\n호스트가 NA의 응답 시간을 가질 수 있는 방법은 불분명합니다. 이것이 다른 변수와 관련이 있을 수 있습니다. 흥미롭게도 “host_response_time” 변수의 “NA”처럼 보이는 것이 적절한 NA로 코딩되지 않고 대신 다른 범주로 처리되는 것 같습니다. 우리는 그것들을 실제 NA로 다시 코딩하고 변수를 요인으로 변경할 것입니다.\n\nairbnb_data_has_reviews &lt;-\n  airbnb_data_has_reviews |&gt;\n  mutate(\n    host_response_time = if_else(\n      host_response_time == \"N/A\",\n      NA_character_,\n      host_response_time\n    ),\n    host_response_time = factor(host_response_time)\n  )\n\nNA가 많기 때문에 문제가 있습니다. 예를 들어, 리뷰 점수와 관계가 있는지 확인하는 데 관심이 있을 수 있습니다(그림 11.14). 전체 리뷰가 100인 것이 많습니다.\n\nairbnb_data_has_reviews |&gt;\n  filter(is.na(host_response_time)) |&gt;\n  ggplot(aes(x = review_scores_rating)) +\n  geom_histogram(binwidth = 1) +\n  theme_classic() +\n  labs(\n    x = \"Average review score\",\n    y = \"Number of properties\"\n  )\n\n\n\n\n\n\n\n그림 11.14: 2023년 3월 런던 에어비앤비 임대료에 대한 NA 응답 시간이 있는 숙소의 리뷰 점수 분포\n\n\n\n\n\n일반적으로 결측값은 ggplot2에 의해 삭제됩니다. naniar의 geom_miss_point()를 사용하여 그래프에 포함할 수 있습니다(그림 11.15).\n\nairbnb_data_has_reviews |&gt;\n  ggplot(aes(\n    x = host_response_time,\n    y = review_scores_accuracy\n  )) +\n  geom_miss_point() +\n  labs(\n    x = \"Host response time\",\n    y = \"Review score accuracy\",\n    color = \"Is missing?\"\n  ) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\n\n\n\n그림 11.15: 호스트 응답 시간별 런던 에어비앤비 데이터의 결측값\n\n\n\n\n\n지금은 응답 시간에 NA가 있는 사람은 모두 제거할 것입니다. 이것은 다시 대략 20%의 관측치를 제거할 것입니다.\n\nairbnb_data_selected &lt;-\n  airbnb_data_has_reviews |&gt;\n  filter(!is.na(host_response_time))\n\n호스트가 에어비앤비에 얼마나 많은 숙소를 가지고 있는지에 관심이 있을 수 있습니다(그림 11.16).\n\nairbnb_data_selected |&gt;\n  ggplot(aes(x = host_total_listings_count)) +\n  geom_histogram() +\n  scale_x_log10() +\n  labs(\n    x = \"Total number of listings, by host\",\n    y = \"Number of hosts\"\n  )\n\n\n\n\n\n\n\n그림 11.16: 2023년 3월 런던 에어비앤비 임대료에 대한 호스트가 에어비앤비에 가지고 있는 숙소 수 분포\n\n\n\n\n\n?fig-airbnbhostlisting을 기반으로, 2-500개 범위의 숙소를 가진 사람이 많고, 일반적인 긴 꼬리가 있음을 알 수 있습니다. 그렇게 많은 목록을 가진 수는 예상치 못한 것이며 추적할 가치가 있습니다. 그리고 처리해야 할 NA가 많이 있습니다.\n\nairbnb_data_selected |&gt;\n  filter(host_total_listings_count &gt;= 500) |&gt;\n  head()\n\n# A tibble: 6 × 13\n    host_id host_response_time host_is_superhost host_total_listings_count\n      &lt;dbl&gt; &lt;fct&gt;              &lt;lgl&gt;                                 &lt;dbl&gt;\n1 439074505 within an hour     FALSE                                  3627\n2 156158778 within an hour     FALSE                                   558\n3 156158778 within an hour     FALSE                                   558\n4 156158778 within an hour     FALSE                                   558\n5 156158778 within an hour     FALSE                                   558\n6 156158778 within an hour     FALSE                                   558\n# ℹ 9 more variables: neighbourhood_cleansed &lt;chr&gt;, bathrooms &lt;lgl&gt;,\n#   bedrooms &lt;dbl&gt;, price &lt;int&gt;, number_of_reviews &lt;dbl&gt;,\n#   review_scores_rating &lt;dbl&gt;, review_scores_accuracy &lt;dbl&gt;,\n#   review_scores_value &lt;dbl&gt;, host_is_superhost_binary &lt;dbl&gt;\n\n\n10개 이상의 목록을 가진 사람들에 대해 즉시 이상하게 눈에 띄는 것은 없지만, 동시에 여전히 명확하지 않습니다. 지금은 단순화를 위해 하나의 숙소만 있는 사람들에게만 집중하고 넘어갈 것입니다.\n\nairbnb_data_selected &lt;-\n  airbnb_data_selected |&gt;\n  add_count(host_id) |&gt;\n  filter(n == 1) |&gt;\n  select(-n)\n\n\n\n11.5.2 변수 간의 관계\n변수 간에 명확해지는 관계가 있는지 확인하기 위해 몇 가지 그래프를 만들고 싶을 수 있습니다. 떠오르는 몇 가지 측면은 가격을 보고 리뷰, 슈퍼호스트, 숙소 수 및 이웃과 비교하는 것입니다.\n하나 이상의 리뷰가 있는 숙소에 대해 가격과 리뷰, 그리고 슈퍼호스트인지 여부의 관계를 볼 수 있습니다(그림 11.17).\n\nairbnb_data_selected |&gt;\n  filter(number_of_reviews &gt; 1) |&gt;\n  ggplot(aes(x = price, y = review_scores_rating, \n             color = host_is_superhost)) +\n  geom_point(size = 1, alpha = 0.1) +\n  theme_classic() +\n  labs(\n    x = \"Price per night\",\n    y = \"Average review score\",\n    color = \"Superhost\"\n  ) +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n그림 11.17: 2023년 3월 런던 에어비앤비 임대료에 대한 가격과 리뷰, 그리고 호스트가 슈퍼호스트인지 여부의 관계\n\n\n\n\n\n누군가를 슈퍼호스트로 만드는 측면 중 하나는 문의에 얼마나 빨리 응답하는지일 수 있습니다. 슈퍼호스트가 되는 것은 문의에 빨리 예 또는 아니오라고 말하는 것을 포함한다고 상상할 수 있습니다. 데이터를 살펴보겠습니다. 먼저, 응답 시간별로 슈퍼호스트의 가능한 값을 살펴보고 싶습니다.\n\nairbnb_data_selected |&gt;\n  count(host_is_superhost) |&gt;\n  mutate(\n    proportion = n / sum(n),\n    proportion = round(proportion, digits = 2)\n  )\n\n# A tibble: 2 × 3\n  host_is_superhost     n proportion\n  &lt;lgl&gt;             &lt;int&gt;      &lt;dbl&gt;\n1 FALSE             10480       0.74\n2 TRUE               3672       0.26\n\n\n다행히도, 리뷰 행을 제거했을 때 슈퍼호스트인지 여부에서 NA를 제거한 것 같습니다. 그러나 다시 돌아가서 살펴보면 다시 확인해야 할 수도 있습니다. janitor의 tabyl()을 사용하여 슈퍼호스트인지 여부에 따라 호스트의 응답 시간을 살펴보는 표를 만들 수 있습니다. 호스트가 한 시간 이내에 응답하지 않으면 슈퍼호스트가 될 가능성이 낮다는 것이 분명합니다.\n\nairbnb_data_selected |&gt;\n  tabyl(host_response_time, host_is_superhost) |&gt;\n  adorn_percentages(\"col\") |&gt;\n  adorn_pct_formatting(digits = 0) |&gt;\n  adorn_ns() |&gt;\n  adorn_title()\n\n                    host_is_superhost            \n host_response_time             FALSE        TRUE\n a few days or more        5%   (489)  0%     (8)\n       within a day       22% (2,322) 11%   (399)\n within a few hours       23% (2,440) 25%   (928)\n     within an hour       50% (5,229) 64% (2,337)\n\n\n마지막으로, 이웃을 볼 수 있습니다. 데이터 제공자가 우리를 위해 이웃 변수를 정리하려고 시도했으므로 지금은 해당 변수를 사용할 것입니다. 실제 분석에 이 변수를 사용하게 된다면 어떻게 구성되었는지 검토하고 싶을 것입니다.\n\nairbnb_data_selected |&gt;\n  tabyl(neighbourhood_cleansed) |&gt;\n  adorn_pct_formatting() |&gt;\n  arrange(-n) |&gt;\n  filter(n &gt; 100) |&gt;\n  adorn_totals(\"row\") |&gt;\n  head()\n\n neighbourhood_cleansed    n percent\n                Hackney 1172    8.3%\n            Westminster  965    6.8%\n          Tower Hamlets  956    6.8%\n              Southwark  939    6.6%\n                Lambeth  914    6.5%\n             Wandsworth  824    5.8%\n\n\n데이터셋에 대한 모델을 빠르게 실행할 것입니다. ?sec-its-just-a-linear-model에서 모델링에 대해 더 자세히 다룰 것이지만, EDA 중에 모델을 사용하여 데이터셋의 여러 변수 간에 존재할 수 있는 관계에 대한 더 나은 감을 얻는 데 도움이 될 수 있습니다. 예를 들어, 누군가가 슈퍼호스트인지 여부를 예측할 수 있는지, 그리고 그것을 설명하는 데 들어가는 요인을 보고 싶을 수 있습니다. 결과가 이진이므로 로지스틱 회귀를 사용하기에 좋은 기회입니다. 우리는 슈퍼호스트 상태가 더 빠른 응답과 더 나은 리뷰와 관련이 있을 것으로 예상합니다. 구체적으로, 우리가 추정하는 모델은 다음과 같습니다.\n\\[\\mbox{Prob(슈퍼호스트} = 1) = \\mbox{logit}^{-1}\\left( \\beta_0 + \\beta_1 \\mbox{응답 시간} + \\beta_2 \\mbox{리뷰} + \\epsilon\\right)\\]\nglm을 사용하여 모델을 추정합니다.\n\nlogistic_reg_superhost_response_review &lt;-\n  glm(\n    host_is_superhost ~\n      host_response_time +\n      review_scores_rating,\n    data = airbnb_data_selected,\n    family = binomial\n  )\n\nmodelsummary를 설치하고 로드한 후 modelsummary()를 사용하여 결과를 빠르게 살펴볼 수 있습니다(표 11.3).\n\nmodelsummary(logistic_reg_superhost_response_review)\n\n\n\n표 11.3: 응답 시간을 기준으로 호스트가 슈퍼호스트인지 여부 설명\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  -16.369\n                \n                \n                  \n                  (0.673)\n                \n                \n                  host_response_timewithin a day\n                  2.230\n                \n                \n                  \n                  (0.361)\n                \n                \n                  host_response_timewithin a few hours\n                  3.035\n                \n                \n                  \n                  (0.359)\n                \n                \n                  host_response_timewithin an hour\n                  3.279\n                \n                \n                  \n                  (0.358)\n                \n                \n                  review_scores_rating\n                  2.545\n                \n                \n                  \n                  (0.116)\n                \n                \n                  Num.Obs.\n                  14152\n                \n                \n                  AIC\n                  14948.4\n                \n                \n                  BIC\n                  14986.2\n                \n                \n                  Log.Lik.\n                  -7469.197\n                \n                \n                  F\n                  197.407\n                \n                \n                  RMSE\n                  0.42\n                \n        \n      \n    \n\n\n\n\n\n\n각 수준이 슈퍼호스트일 확률과 양의 상관 관계가 있음을 알 수 있습니다. 그러나 한 시간 이내에 응답하는 호스트를 갖는 것은 우리 데이터셋에서 슈퍼호스트인 개인과 관련이 있습니다.\n이 분석 데이터셋을 저장하겠습니다.\n\nwrite_parquet(\n  x = airbnb_data_selected, \n  sink = \"2023-05-05-london-airbnblistings-analysis_dataset.parquet\"\n  )",
    "crumbs": [
      "준비",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>탐색적 데이터 분석</span>"
    ]
  },
  {
    "objectID": "11-eda_ko.html#결론",
    "href": "11-eda_ko.html#결론",
    "title": "11  탐색적 데이터 분석",
    "section": "11.6 결론",
    "text": "11.6 결론\n이 장에서는 데이터셋에 익숙해지는 능동적인 과정인 탐색적 데이터 분석(EDA)을 고려했습니다. 우리는 결측 데이터, 변수의 분포 및 변수 간의 관계에 중점을 두었습니다. 그리고 이것을 하기 위해 그래프와 표를 광범위하게 사용했습니다.\nEDA에 대한 접근 방식은 맥락과 데이터셋에서 마주치는 문제 및 특징에 따라 달라집니다. 또한 기술에 따라 달라집니다. 예를 들어, 회귀 모델과 차원 축소 접근 방식을 고려하는 것이 일반적입니다.",
    "crumbs": [
      "준비",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>탐색적 데이터 분석</span>"
    ]
  },
  {
    "objectID": "11-eda_ko.html#연습-문제",
    "href": "11-eda_ko.html#연습-문제",
    "title": "11  탐색적 데이터 분석",
    "section": "11.7 연습 문제",
    "text": "11.7 연습 문제\n\n연습\n\n(계획) 다음 시나리오를 고려하십시오: 우리는 미국 인구의 약 80%가 플랫폼에 있는 소셜 미디어 회사의 연령에 대한 일부 데이터를 가지고 있습니다. 해당 데이터셋이 어떻게 생겼을지 스케치한 다음 모든 관측치를 보여주기 위해 만들 수 있는 그래프를 스케치하십시오.\n(시뮬레이션) 설명된 시나리오를 더 고려하고 상황을 시뮬레이션하십시오. 크기 때문에 파케이를 사용하십시오. 시뮬레이션된 데이터를 기반으로 10개의 테스트를 포함하십시오. 코드가 포함된 GitHub Gist 링크를 제출하십시오.\n(수집) 이러한 데이터셋의 가능한 출처 하나를 설명하십시오.\n(탐색) 스케치한 그래프를 ggplot2를 사용하여 만드십시오. 코드가 포함된 GitHub Gist 링크를 제출하십시오.\n(전달) 당신이 한 일에 대해 한 페이지를 작성하고, 표본을 기반으로 한 추정치에 대한 몇 가지 위협에 대해 신중하게 논의하십시오.\n\n\n\n퀴즈\n\n(tukey1962future를?) 몇 단락으로 요약한 다음 데이터 과학과 관련시키십시오.\n자신의 말로 탐색적 데이터 분석이란 무엇입니까(최소 세 단락을 작성하고, 인용 및 예를 포함하십시오)?\n“my_data”라는 데이터셋이 있고, “first_col”과 “second_col”이라는 두 개의 열이 있다고 가정합니다. 그래프를 생성하는 R 코드를 작성하십시오(그래프 유형은 중요하지 않습니다). 코드가 포함된 GitHub Gist 링크를 제출하십시오.\n500개의 관측치와 3개의 변수가 있는 데이터셋을 고려하십시오. 따라서 1,500개의 셀이 있습니다. 100개의 행에 하나 이상의 열에 대한 셀이 누락된 경우, 다음 중 무엇을 하시겠습니까? a) 데이터셋에서 전체 행을 제거하시겠습니까? b) 있는 그대로의 데이터에 대한 분석을 시도하시겠습니까? 또는 c) 다른 절차를 사용하시겠습니까? 데이터셋에 10,000개의 행이 있지만 누락된 행의 수가 동일하다면 어떻게 하시겠습니까? 최소 세 단락으로 예시와 인용을 들어 논의하십시오.\n특이한 값을 식별하는 세 가지 방법을 논의하고, 각각에 대해 최소 한 단락씩 작성하십시오.\n범주형 변수와 연속형 변수의 차이점은 무엇입까?\n요인 변수와 정수 변수의 차이점은 무엇입니까?\n데이터셋에서 체계적으로 제외된 사람에 대해 어떻게 생각할 수 있습니까?\nopendatatoronto를 사용하여 2014년 시장 선거 캠페인 기부금에 대한 데이터를 다운로드하십시오. (참고: get_resource()에서 얻을 2014년 파일에는 많은 시트가 포함되어 있으므로 시장 선거와 관련된 시트만 유지하십시오).\n\n데이터 형식을 정리하십시오(파싱 문제를 수정하고 janitor를 사용하여 열 이름을 표준화하십시오).\n데이터셋의 변수를 요약하십시오. 결측값이 있습니까? 그렇다면 걱정해야 합니까? 모든 변수가 있어야 할 형식입니까? 그렇지 않은 경우, 올바른 형식의 새 변수를 만드십시오.\n기부금 값의 분포를 시각적으로 탐색하십시오. 어떤 기부금이 주목할 만한 이상치입니까? 유사한 특성을 공유합니까? 대부분의 데이터를 더 잘 이해하기 위해 이러한 이상치 없이 기부금 분포를 플로팅하는 것이 유용할 수 있습니다.\n다음 각 범주에서 상위 5명의 후보를 나열하십시오: 1) 총 기부금; 2) 평균 기부금; 3) 기부 횟수.\n후보자 자신의 기부금 없이 해당 프로세스를 반복하십시오.\n한 명 이상의 후보에게 돈을 기부한 기부자는 몇 명입니까?\n\nggplot()에서 막대가 있는 그래프를 생성하는 세 가지 geom을 나열하십시오.\n10,000개의 관측치와 27개의 변수가 있는 데이터셋을 고려하십시오. 각 관측치에 대해 하나 이상의 결측 변수가 있습니다. 무슨 일이 일어나고 있는지 이해하기 위해 취할 단계를 한두 단락으로 논의하십시오.\n알려진 결측 데이터는 데이터셋에 구멍을 남기는 데이터입니다. 그러나 수집되지 않은 데이터는 어떻습니까? McClelland (2019) 및 (luscombe2020policing을?) 살펴보십시오. 그들이 데이터셋을 어떻게 수집했는지, 그리고 이것을 구성하는 데 무엇이 필요했는지 조사하십시오. 데이터셋에 무엇이 있고 왜 있습니까? 무엇이 누락되었고 왜 그렇습니까? 이것이 결과에 어떤 영향을 미칠 수 있습니까? 사용했거나 읽은 다른 데이터셋에 유사한 편향이 어떻게 들어갈 수 있습니까?\n\n\n\n수업 활동\n\n다음 파일 이름을 수정하십시오.\n\nexample_project/\n├── .gitignore\n├── Example project.Rproj\n├── scripts\n│   ├── simulate data.R\n│   ├── DownloadData.R\n│   ├── data-cleaning.R\n│   ├── test(new)data.R\n\n?sec-static-communication에서 소개된 앤스콤의 콰르텟을 고려하십시오. 특정 관측치를 무작위로 제거할 것입니다. 결측 데이터가 있는 데이터셋이 주어졌다고 가정하십시오. 장 6 및 ?sec-exploratory-data-analysis에서 결측 데이터를 처리하는 접근 방식 중 하나를 선택한 다음, 선택한 것을 구현하는 코드를 작성하십시오. 비교:\n\n실제 관측치와의 결과;\n실제 요약 통계와의 요약 통계; 그리고\n결측 데이터와 실제 데이터를 하나의 그래프에 보여주는 그래프를 만드십시오.\n\n\n\nset.seed(853)\n\ntidy_anscombe &lt;-\n  anscombe |&gt;\n  pivot_longer(everything(),\n               names_to = c(\".value\", \"set\"),\n               names_pattern = \"(.)(.)\")\n\ntidy_anscombe_MCAR &lt;-\n  tidy_anscombe |&gt;\n  mutate(row_number = row_number()) |&gt;\n  mutate(\n    x = if_else(row_number %in% sample(\n      x = 1:nrow(tidy_anscombe), size = 10\n    ), NA_real_, x),\n    y = if_else(row_number %in% sample(\n      x = 1:nrow(tidy_anscombe), size = 10\n    ), NA_real_, y)\n  ) |&gt;\n  select(-row_number)\n\ntidy_anscombe_MCAR\n\n# A tibble: 44 × 3\n   set       x     y\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 1        10  8.04\n 2 2        10  9.14\n 3 3        NA NA   \n 4 4         8 NA   \n 5 1         8  6.95\n 6 2         8  8.14\n 7 3         8  6.77\n 8 4         8  5.76\n 9 1        NA  7.58\n10 2        13  8.74\n# ℹ 34 more rows\n\n# 여기에 코드 추가\n\n\n다음 데이터셋으로 연습을 다시 하십시오. 이 경우 주요 차이점은 무엇입니까?\n\n\ntidy_anscombe_MNAR &lt;-\n  tidy_anscombe |&gt;\n  arrange(desc(x)) |&gt;\n  mutate(\n    ordered_x_rows = 1:nrow(tidy_anscombe),\n    x = if_else(ordered_x_rows %in% 1:10, NA_real_, x)\n  ) |&gt;\n  select(-ordered_x_rows) |&gt;\n  arrange(desc(y)) |&gt;\n  mutate(\n    ordered_y_rows = 1:nrow(tidy_anscombe),\n    y = if_else(ordered_y_rows %in% 1:10, NA_real_, y)\n  ) |&gt;\n  arrange(set) |&gt;\n  select(-ordered_y_rows)\n\ntidy_anscombe_MNAR\n\n# A tibble: 44 × 3\n   set       x     y\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 1        NA NA   \n 2 1        NA NA   \n 3 1         9 NA   \n 4 1        11  8.33\n 5 1        10  8.04\n 6 1        NA  7.58\n 7 1         6  7.24\n 8 1         8  6.95\n 9 1         5  5.68\n10 1         7  4.82\n# ℹ 34 more rows\n\n# 여기에 코드 추가\n\n\n페어 프로그래밍을 사용하여(5분마다 전환해야 함), 새 R 프로젝트를 만들고, (Bombieri2023에서?) 다음 데이터셋을 읽어들인 다음, Quarto 문서에 코드와 메모를 추가하여 탐색하십시오.\n\n\ndownload.file(url = \"https://doi.org/10.1371/journal.pbio.3001946.s005\",\n              destfile = \"data.xlsx\")\n\ndata &lt;-\n  read_xlsx(path = \"data.xlsx\",\n            col_types = \"text\") |&gt;\n  clean_names() |&gt;\n  mutate(date = convert_to_date(date))\n\n\n다른 학생과 짝을 이루어 데이터 과학자 역할을 수행하여 주제 전문가와 파트너 관계를 맺습니다. 파트너는 주제와 질문을 선택해야 하며, 그들이 잘 알고 있지만 당신은 모르는 것이어야 합니다(아마도 그들의 나라에 대한 것, 만약 그들이 유학생이라면). 그들과 협력하여 분석 계획을 개발하고, 일부 데이터를 시뮬레이션하고, 그들이 사용할 수 있는 그래프를 만들어야 합니다.\n\n\n\n과제\n다음 옵션 중 하나를 선택하십시오. Quarto를 사용하고, 적절한 제목, 저자, 날짜, GitHub 리포지토리 링크 및 인용을 포함하십시오. PDF를 제출하십시오.\n옵션 1:\npalmerpenguins에서 사용할 수 있는 penguins() 데이터셋의 “bill_length_mm” 변수에 대해 미국 주 및 인구에 대해 수행된 결측 데이터 연습을 반복하십시오. 대체된 값과 실제 값을 비교하십시오.\n당신이 한 일과 발견한 것에 대해 최소 두 페이지를 작성하십시오.\n그런 다음 다른 학생과 짝을 이루어 작성한 작업을 교환하십시오. 그들의 피드백을 기반으로 업데이트하고, 논문에서 그들의 이름을 언급하여 감사를 표하십시오.\n옵션 2:\n파리에 대한 에어비앤비 EDA를 수행하십시오.\n옵션 3:\n“결측 데이터란 무엇이며 어떻게 해야 합니까?”라는 주제에 대해 최소 두 페이지를 작성하십시오.\n그런 다음 다른 학생과 짝을 이루어 작성한 작업을 교환하십시오. 그들의 피드백을 기반으로 업데이트하고, 논문에서 그들의 이름을 언급하여 감사를 표하십시오.\n\n\n\n\nArel-Bundock, Vincent. 2022. “modelsummary: Data and Model Summaries in R”. Journal of Statistical Software 103 (1): 1–23. https://doi.org/10.18637/jss.v103.i01.\n\n\n———. 2024. tinytable: Simple and Configurable Tables in “HTML”, “LaTeX”, “Markdown”, “Word”, “PNG”, “PDF”, and “Typst” Formats. https://vincentarelbundock.github.io/tinytable/.\n\n\nCox, Murray. 2021. “Inside Airbnb—Toronto Data”. http://insideairbnb.com/get-the-data.html.\n\n\nFirke, Sam. 2023. janitor: Simple Tools for Examining and Cleaning Dirty Data. https://CRAN.R-project.org/package=janitor.\n\n\nGelfand, Sharla. 2022. opendatatoronto: Access the City of Toronto Open Data Portal. https://CRAN.R-project.org/package=opendatatoronto.\n\n\nGelman, Andrew, 와/과 Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Models. 1st ed. Cambridge University Press.\n\n\nGelman, Andrew, Jennifer Hill, 와/과 Aki Vehtari. 2020. Regression and Other Stories. Cambridge University Press. https://avehtari.github.io/ROS-Examples/.\n\n\nGrolemund, Garrett, 와/과 Hadley Wickham. 2011. “Dates and Times Made Easy with lubridate”. Journal of Statistical Software 40 (3): 1–25. https://doi.org/10.18637/jss.v040.i03.\n\n\nHorton, Nicholas, 와/과 Stuart Lipsitz. 2001. “Multiple Imputation in Practice”. The American Statistician 55 (3): 244–54. https://doi.org/10.1198/000313001317098266.\n\n\nManski, Charles. 2022. “Inference with Imputed Data: The Allure of Making Stuff Up”. arXiv. https://doi.org/10.48550/arXiv.2205.07388.\n\n\nMcClelland, Alexander. 2019. “‘Lock this whore Up’: legal violence and flows of information precipitating personal violence against people criminalised for HIV-related crimes in Canada”. European Journal of Risk Regulation 10 (1): 132–47. https://doi.org/10.1017/err.2019.20.\n\n\nOsborne, Jason. 2012. Best Practices in Data Cleaning: A Complete Guide to Everything You Need to Do Before and After Collecting Your Data. SAGE Publications.\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nRichardson, Neal, Ian Cook, Nic Crane, Dewey Dunnington, Romain François, Jonathan Keane, Dragoș Moldovan-Grünfeld, Jeroen Ooms, 와/과 Apache Arrow. 2023. arrow: Integration to Apache Arrow. https://CRAN.R-project.org/package=arrow.\n\n\nRyan, Philip. 2015. “Keeping a Lab Notebook”. YouTube, 5월. https://youtu.be/-MAIuaOL64I.\n\n\nStaniak, Mateusz, 와/과 Przemysław Biecek. 2019. “The Landscape of R Packages for Automated Exploratory Data Analysis”. The R Journal 11 (2): 347–69. https://doi.org/10.32614/RJ-2019-033.\n\n\nTierney, Nicholas, Di Cook, Miles McBain, 와/과 Colin Fay. 2021. naniar: Data Structures, Summaries, and Visualisations for Missing Data. https://CRAN.R-project.org/package=naniar.\n\n\nTukey, John. 1962. “The future of data analysis”. The annals of mathematical statistics 33 (1): 1–67. https://doi.org/10.1214/aoms/1177704711.\n\n\nvan Buuren, Stef, 와/과 Karin Groothuis-Oudshoorn. 2011. “mice: Multivariate Imputation by Chained Equations in R”. Journal of Statistical Software 45 (3): 1–67. https://doi.org/10.18637/jss.v045.i03.\n\n\nWickham, Hadley. 2018. “Whole game”. YouTube, 1월. https://youtu.be/go5Au01Jrvs.\n\n\nWickham, Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, 기타. 2019. “Welcome to the Tidyverse”. Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, 와/과 Garrett Grolemund. (2016년) 2023. R for Data Science. 2nd ed. O’Reilly Media. https://r4ds.hadley.nz.",
    "crumbs": [
      "준비",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>탐색적 데이터 분석</span>"
    ]
  },
  {
    "objectID": "13-prediction_ko.html",
    "href": "13-prediction_ko.html",
    "title": "14  예측",
    "section": "",
    "text": "14.1 소개\n선수 지식\n주요 개념 및 기술\n소프트웨어 및 패키지\n?sec-its-just-a-linear-model에서 논의했듯이, 모델은 추론 또는 예측에 초점을 맞추는 경향이 있습니다. 일반적으로, 초점에 따라 다른 문화가 있습니다. 이것의 한 가지 이유는 ?sec-causality-from-observational-data에서 소개될 인과 관계에 대한 다른 강조 때문입니다. 여기서 매우 일반적으로 말하고 있지만, 종종 추론에서는 인과 관계에 대해 매우 우려하고, 예측에서는 덜 우려합니다. 즉, 우리 모델이 예상했던 것과 조건이 상당히 다를 때 예측의 질이 저하될 것이지만, 조건이 우리가 걱정할 만큼 충분히 다른지 어떻게 알 수 있습니까?\n이 문화적 차이의 또 다른 방법은 특히 데이터 과학과 기계 학습의 부상이 컴퓨터 과학 또는 공학 배경을 가진 사람들에 의해 파이썬으로 모델이 개발됨에 따라 크게 주도되었기 때문입니다. 이것은 추론의 많은 부분이 통계에서 나왔기 때문에 추가적인 어휘 차이가 있음을 의미합니다. 다시 말하면, 이것은 모두 매우 광범위하게 말하는 것입니다.\n이 장에서는 tidymodels의 R 접근 방식을 사용하여 예측에 초점을 맞추는 것으로 시작합니다. 그런 다음 회색 영역 중 하나인 라쏘 회귀를 소개합니다. 이것은 통계학자에 의해 개발되었지만, 주로 예측에 사용됩니다. 마지막으로, 이 모든 것을 파이썬으로 소개합니다.",
    "crumbs": [
      "모델링",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>예측</span>"
    ]
  },
  {
    "objectID": "13-prediction_ko.html#tidymodels를-사용한-예측",
    "href": "13-prediction_ko.html#tidymodels를-사용한-예측",
    "title": "14  예측",
    "section": "14.2 tidymodels를 사용한 예측",
    "text": "14.2 tidymodels를 사용한 예측\n\n14.2.1 선형 모델\n예측에 초점을 맞출 때, 우리는 종종 많은 모델을 적합시키고 싶을 것입니다. 이것을 하는 한 가지 방법은 코드를 여러 번 복사하여 붙여넣는 것입니다. 이것은 괜찮고, 대부분의 사람들이 시작하는 방식이지만, 찾기 어려운 오류를 범하기 쉽습니다. 더 나은 접근 방식은 다음과 같습니다.\n\n더 쉽게 확장됩니다.\n과적합에 대해 신중하게 생각할 수 있게 해줍니다. 그리고\n모델 평가를 추가합니다.\n\ntidymodels (Kuhn 와/과 Wickham 2020)의 사용은 다양한 모델을 쉽게 적합시킬 수 있는 일관된 문법을 제공함으로써 이러한 기준을 충족합니다. tidyverse와 마찬가지로, 패키지의 패키지입니다.\n설명을 위해, 시뮬레이션된 실행 데이터에 대해 다음 모델을 추정하고 싶습니다.\n\\[\n\\begin{aligned}\ny_i | \\mu_i &\\sim \\mbox{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\beta_0 +\\beta_1x_i\n\\end{aligned}\n\\]\n여기서 \\(y_i\\)는 일부 개인 \\(i\\)의 마라톤 시간을 나타내고 \\(x_i\\)는 그들의 5킬로미터 시간을 나타냅니다. 여기서 우리는 일부 개인 \\(i\\)의 마라톤 시간이 평균 \\(\\mu\\)와 표준 편차 \\(\\sigma\\)를 갖는 정규 분포를 따른다고 말하며, 여기서 평균은 두 개의 매개변수 \\(\\beta_0\\)와 \\(\\beta_1\\) 및 그들의 5킬로미터 시간에 따라 달라집니다. 여기서 “~”는 “~로 분포됨”을 의미합니다. 우리는 분포가 사용되고 있음을 더 명시적으로 하기 위해 이전과 약간 다른 표기법을 사용하지만, 이 모델은 \\(\\epsilon\\)이 정규 분포를 따르는 \\(y_i=\\beta_0+\\beta_1 x_i + \\epsilon_i\\)와 동일합니다.\n예측에 초점을 맞추고 있으므로, 데이터에 과적합하는 것에 대해 걱정하고 있으며, 이는 다른 데이터셋에 대한 주장을 하는 능력을 제한할 것입니다. 이것을 부분적으로 해결하는 한 가지 방법은 initial_split()을 사용하여 데이터셋을 두 개로 나누는 것입니다.\n\nsim_run_data &lt;-\n  read_parquet(file = here::here(\"outputs/data/running_data.parquet\"))\n\nset.seed(853)\n\nsim_run_data_split &lt;-\n  initial_split(\n    data = sim_run_data,\n    prop = 0.80\n  )\n\nsim_run_data_split\n\n&lt;Training/Testing/Total&gt;\n&lt;160/40/200&gt;\n\n\n데이터를 분할한 후, training() 및 testing()을 사용하여 훈련 및 테스트 데이터셋을 만듭니다.\n\nsim_run_data_train &lt;- training(sim_run_data_split)\n\nsim_run_data_test &lt;- testing(sim_run_data_split)\n\n데이터셋의 80%를 훈련 데이터셋에 배치했습니다. 모델의 매개변수를 추정하는 데 사용할 것입니다. 나머지 20%는 보류했으며, 모델을 평가하는 데 사용할 것입니다. 왜 이렇게 할까요? 우리의 우려는 모델링의 모든 측면을 괴롭히는 편향-분산 절충안입니다. 우리는 결과가 우리가 가진 데이터셋에 너무 특수하여 다른 데이터셋에 적용할 수 없을까 걱정합니다. 극단적인 예를 들어, 10개의 관측치가 있는 데이터셋을 생각해 보십시오. 우리는 해당 관측치를 완벽하게 맞추는 모델을 만들 수 있습니다. 그러나 그 모델을 다른 데이터셋, 심지어 동일한 기본 프로세스에 의해 생성된 데이터셋에 적용하면 정확하지 않을 것입니다.\n이 우려를 해결하는 한 가지 방법은 이런 식으로 데이터를 분할하는 것입니다. 우리는 훈련 데이터를 사용하여 계수 추정치를 알리고, 그런 다음 테스트 데이터를 사용하여 모델을 평가합니다. 훈련 데이터의 데이터와 너무 밀접하게 일치하는 모델은 훈련 데이터에 너무 특수하기 때문에 테스트 데이터에서 잘 작동하지 않을 것입니다. 이 테스트-훈련 분할을 사용하면 적절한 모델을 구축할 기회를 얻을 수 있습니다.\n이 분리를 적절하게 하는 것은 처음에 생각하는 것보다 더 어렵습니다. 우리는 테스트 데이터셋의 측면이 훈련 데이터셋에 존재하는 상황을 피하고 싶습니다. 왜냐하면 이것은 앞으로 일어날 일을 부적절하게 예고하기 때문입니다. 이것을 데이터 누출이라고 합니다. 그러나 전체 데이터셋을 포함할 가능성이 있는 데이터 정리 및 준비를 고려하면, 각각의 일부 특징이 서로 영향을 미칠 수 있습니다. (kapoornarayanan2022는?) 많은 연구를 무효화할 수 있는 기계 학습 응용 프로그램에서 광범위한 데이터 누출을 발견했습니다.\ntidymodels를 사용하려면 먼저 linear_reg()로 선형 회귀에 관심이 있음을 지정합니다. 그런 다음 set_engine()으로 선형 회귀 유형, 이 경우 다중 선형 회귀를 지정합니다. 마지막으로 fit()으로 모델을 지정합니다. 이것은 위에서 자세히 설명한 기본 R 접근 방식보다 상당히 많은 인프라가 필요하지만, 이 접근 방식의 장점은 많은 모델을 적합시키는 데 사용할 수 있다는 것입니다. 말하자면 모델 공장을 만든 것입니다.\n\nsim_run_data_first_model_tidymodels &lt;-\n  linear_reg() |&gt;\n  set_engine(engine = \"lm\") |&gt;\n  fit(\n    marathon_time ~ five_km_time + was_raining,\n    data = sim_run_data_train\n  )\n\n추정된 계수는 ?tbl-modelsummarybayesbetter의 첫 번째 열에 요약되어 있습니다. 예를 들어, 우리 데이터셋에서 평균적으로 5킬로미터 달리기 시간이 1분 더 긴 것은 마라톤 시간이 약 8분 더 긴 것과 관련이 있음을 발견합니다.\n\n\n14.2.2 로지스틱 회귀\n로지스틱 회귀 문제에도 tidymodels를 사용할 수 있습니다. 이를 위해 먼저 종속 변수의 클래스를 요인으로 변경해야 합니다. 왜냐하면 이것은 분류 모델에 필요하기 때문입니다.\n\nweek_or_weekday &lt;-\n  read_parquet(file = \"outputs/data/week_or_weekday.parquet\")\n\nset.seed(853)\n\nweek_or_weekday &lt;-\n  week_or_weekday |&gt;\n  mutate(is_weekday = as_factor(is_weekday))\n\nweek_or_weekday_split &lt;- initial_split(week_or_weekday, prop = 0.80)\nweek_or_weekday_train &lt;- training(week_or_weekday_split)\nweek_or_weekday_test &lt;- testing(week_or_weekday_split)\n\nweek_or_weekday_tidymodels &lt;-\n  logistic_reg(mode = \"classification\") |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(\n    is_weekday ~ number_of_cars,\n    data = week_or_weekday_train\n  )\n\n이전과 마찬가지로, 실제 결과와 추정치를 비교하는 그래프를 만들 수 있습니다. 그러나 이것의 한 가지 좋은 점은 테스트 데이터셋을 사용하여 모델의 예측 능력을 더 철저하게 평가할 수 있다는 것입니다. 예를 들어, 혼동 행렬을 통해 예측이 진실이었던 것에 대한 각 예측의 개수를 지정합니다. 모델이 보류된 데이터셋에서 잘 작동한다는 것을 알 수 있습니다. 모델이 평일이라고 예측하고 실제로 평일이었던 관측치가 90개 있었고, 모델이 주말이라고 예측하고 실제로 주말이었던 관측치가 95개 있었습니다. 15개의 관측치에 대해 틀렸으며, 이들은 평일이라고 예측했지만 주말이었던 7개와 그 반대인 8개로 나뉘었습니다.\n\nweek_or_weekday_tidymodels |&gt;\n  predict(new_data = week_or_weekday_test) |&gt;\n  cbind(week_or_weekday_test) |&gt;\n  conf_mat(truth = is_weekday, estimate = .pred_class)\n\n          Truth\nPrediction  0  1\n         0 90  8\n         1  7 95\n\n\n\n14.2.2.1 미국 정치적 지지\n한 가지 접근 방식은 tidymodels를 사용하여 이전과 동일한 방식으로 예측 중심의 로지스틱 회귀 모델을 구축하는 것입니다. 즉, 검증 세트 접근 방식입니다 (James 기타 [2013년] 2021, p. 176). 이 경우, 확률은 바이든에게 투표할 확률이 될 것입니다.\n\nces2020 &lt;-\n  read_parquet(file = \"outputs/data/ces2020.parquet\")\n\nset.seed(853)\n\nces2020_split &lt;- initial_split(ces2020, prop = 0.80)\nces2020_train &lt;- training(ces2020_split)\nces2020_test &lt;- testing(ces2020_split)\n\nces_tidymodels &lt;-\n  logistic_reg(mode = \"classification\") |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(\n    voted_for ~ gender + education,\n    data = ces2020_train\n  )\n\nces_tidymodels\n\nparsnip model object\n\n\nCall:  stats::glm(formula = voted_for ~ gender + education, family = stats::binomial, \n    data = data)\n\nCoefficients:\n                  (Intercept)                     genderMale  \n                       0.2157                        -0.4697  \neducationHigh school graduate          educationSome college  \n                      -0.1857                         0.3502  \n              education2-year                education4-year  \n                       0.2311                         0.6700  \n           educationPost-grad  \n                       0.9898  \n\nDegrees of Freedom: 34842 Total (i.e. Null);  34836 Residual\nNull Deviance:      47000 \nResidual Deviance: 45430    AIC: 45440\n\n\n그런 다음 테스트 세트에서 평가합니다. 모델이 트럼프 지지자를 식별하는 데 어려움을 겪고 있는 것 같습니다.\n\nces_tidymodels |&gt;\n  predict(new_data = ces2020_test) |&gt;\n  cbind(ces2020_test) |&gt;\n  conf_mat(truth = voted_for, estimate = .pred_class)\n\n          Truth\nPrediction Trump Biden\n     Trump   656   519\n     Biden  2834  4702\n\n\ntidymodels를 소개할 때, 훈련 및 테스트 세트를 무작위로 구성하는 것의 중요성에 대해 논의했습니다. 우리는 훈련 데이터셋을 사용하여 매개변수를 추정하고, 그런 다음 테스트 세트에서 모델을 평가합니다. 왜 우리가 무작위성의 변덕에 따라야 하는지, 그리고 우리가 데이터를 최대한 활용하고 있는지 묻는 것은 당연합니다. 예를 들어, 테스트 세트에 일부 무작위 포함으로 인해 좋은 모델이 제대로 평가되지 않으면 어떻게 될까요? 또한, 큰 테스트 세트가 없으면 어떻게 될까요?\n이것을 어느 정도 해결하는 일반적으로 사용되는 리샘플링 방법 중 하나는 \\(k\\)-겹 교차 검증입니다. 이 접근 방식에서는 데이터셋에서 비복원 추출로 \\(k\\)개의 다른 표본 또는 “폴드”를 만듭니다. 그런 다음 첫 번째 \\(k-1\\) 폴드에 모델을 적합시키고 마지막 폴드에서 평가합니다. 이것을 모든 폴드에 대해 한 번씩, 총 \\(k\\)번 수행하여 모든 관측치가 훈련에 \\(k-1\\)번, 테스트에 한 번 사용되도록 합니다. 그런 다음 \\(k\\)-겹 교차 검증 추정치는 평균 제곱 오차의 평균입니다 (James 기타 [2013년] 2021, p. 181). 예를 들어, tidymodels의 vfold_cv()를 사용하여 10개의 폴드를 만들 수 있습니다.\n\nset.seed(853)\n\nces2020_10_folds &lt;- vfold_cv(ces2020, v = 10)\n\n그런 다음 fit_resamples()를 사용하여 다른 폴드 조합에 걸쳐 모델을 적합시킬 수 있습니다. 이 경우, 모델은 10번 적합됩니다.\n\nces2020_cross_validation &lt;-\n  fit_resamples(\n    object = logistic_reg(mode = \"classification\") |&gt; set_engine(\"glm\"),\n    preprocessor = recipe(voted_for ~ gender + education,\n                          data = ces2020),\n    resamples = ces2020_10_folds,\n    metrics = metric_set(accuracy, sens),\n    control = control_resamples(save_pred = TRUE)\n  )\n\n모델의 성능을 이해하는 데 관심이 있을 수 있으며, collect_metrics()를 사용하여 폴드 전체에서 집계할 수 있습니다(표 14.1 (a)). 이러한 유형의 세부 정보는 일반적으로 논문의 주요 내용에 지나가듯이 언급되지만, 부록에 매우 자세하게 포함됩니다. 폴드 전체에서 모델의 평균 정확도는 0.61이고, 평균 민감도는 0.19, 평균 특이도는 0.90입니다.\ncollect_metrics(ces2020_cross_validation) |&gt;\n  select(.metric, mean) |&gt;\n  tt() |&gt;\n  style_tt(j = 1:2, align = \"lr\") |&gt;\n  format_tt(digits = 2, num_mark_big = \",\", num_fmt = \"decimal\") |&gt;\n  setNames(c(\"Metric\", \"Mean\"))\nconf_mat_resampled(ces2020_cross_validation) |&gt;\n  mutate(Proportion = Freq / sum(Freq)) |&gt;\n  tt() |&gt;\n  style_tt(j = 1:4, align = \"llrr\") |&gt;\n  format_tt(digits = 2, num_mark_big = \",\", num_fmt = \"decimal\")\n\n\n\n표 14.1: 유권자 선호도를 예측하기 위한 로지스틱 회귀의 10개 폴드에 걸친 평균 메트릭\n\n\n\n\n\n\n\n(a) 주요 성능 메트릭\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Metric\n                Mean\n              \n        \n        \n        \n                \n                  accuracy\n                  0.61\n                \n                \n                  sens\n                  0.19\n                \n        \n      \n    \n\n\n\n\n\n\n\n\n\n\n(b) 혼동 행렬\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Prediction\n                Truth\n                Freq\n                Proportion\n              \n        \n        \n        \n                \n                  Trump\n                  Trump\n                  327.5\n                  0.08\n                \n                \n                  Trump\n                  Biden\n                  267.7\n                  0.06\n                \n                \n                  Biden\n                  Trump\n                  1,428.3\n                  0.33\n                \n                \n                  Biden\n                  Biden\n                  2,331.9\n                  0.54\n                \n        \n      \n    \n\n\n\n\n\n\n\n\n\n\n이것은 무엇을 의미합니까? 정확도는 올바르게 분류된 관측치의 비율입니다. 0.61의 결과는 모델이 동전 던지기보다 더 잘하고 있지만, 그 이상은 아니라는 것을 시사합니다. 민감도는 참으로 식별된 참 관측치의 비율입니다 (James 기타 [2013년] 2021, p. 145). 이 경우, 모델이 응답자가 트럼프에게 투표했다고 예측하고 실제로 그렇게 했다는 것을 의미합니다. 특이도는 거짓으로 식별된 거짓 관측치의 비율입니다 (James 기타 [2013년] 2021, p. 145). 이 경우, 바이든에게 투표한 유권자 중 바이든에게 투표할 것으로 예측된 비율입니다. 이것은 모델이 트럼프 지지자를 식별하는 데 어려움을 겪고 있다는 우리의 초기 생각을 확인시켜 줍니다.\n혼동 행렬을 보면 이것을 더 자세히 알 수 있습니다(표 14.1 (b)). 교차 검증과 같은 리샘플링 접근 방식과 함께 사용될 때, 혼동 행렬은 각 폴드에 대해 계산된 다음 평균됩니다. 모델은 2020년 선거가 얼마나 접전이었는지에 대한 우리의 지식에서 예상할 수 있는 것보다 훨씬 더 많이 바이든을 예측하고 있습니다. 이것은 우리 모델이 더 나은 작업을 수행하기 위해 추가 변수가 필요할 수 있음을 시사합니다.\n마지막으로, 개별 수준 결과에 관심이 있을 수 있으며, collect_predictions()를 사용하여 데이터셋에 추가할 수 있습니다.\n\nces2020_with_predictions &lt;-\n  cbind(\n    ces2020,\n    collect_predictions(ces2020_cross_validation) |&gt;\n      arrange(.row) |&gt;\n      select(.pred_class)\n  ) |&gt;\n  as_tibble()\n\n예를 들어, 모델이 고등학교를 졸업하지 않았거나, 고등학교를 졸업했거나, 2년제 대학을 졸업한 남성을 제외하고 모든 개인에 대해 기본적으로 바이든에 대한 지지를 예측하고 있음을 알 수 있습니다(표 14.2).\n\nces2020_with_predictions |&gt;\n  group_by(gender, education, voted_for) |&gt;\n  count(.pred_class) |&gt;\n  tt() |&gt;\n  style_tt(j = 1:5, align = \"llllr\") |&gt;\n  format_tt(digits = 0, num_mark_big = \",\", num_fmt = \"decimal\") |&gt;\n  setNames(c(\n      \"Gender\",\n      \"Education\",\n      \"Voted for\",\n      \"Predicted vote\",\n      \"Number\"\n    ))\n\n\n\n표 14.2: 모델은 교육에 관계없이 모든 여성과 많은 남성에 대해 바이든에 대한 지지를 예측하고 있습니다\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Gender\n                Education\n                Voted for\n                Predicted vote\n                Number\n              \n        \n        \n        \n                \n                  Female\n                  No HS\n                  Trump\n                  Biden\n                  206\n                \n                \n                  Female\n                  No HS\n                  Biden\n                  Biden\n                  228\n                \n                \n                  Female\n                  High school graduate\n                  Trump\n                  Biden\n                  3,204\n                \n                \n                  Female\n                  High school graduate\n                  Biden\n                  Biden\n                  3,028\n                \n                \n                  Female\n                  Some college\n                  Trump\n                  Biden\n                  1,842\n                \n                \n                  Female\n                  Some college\n                  Biden\n                  Biden\n                  3,325\n                \n                \n                  Female\n                  2-year\n                  Trump\n                  Biden\n                  1,117\n                \n                \n                  Female\n                  2-year\n                  Biden\n                  Biden\n                  1,739\n                \n                \n                  Female\n                  4-year\n                  Trump\n                  Biden\n                  1,721\n                \n                \n                  Female\n                  4-year\n                  Biden\n                  Biden\n                  4,295\n                \n                \n                  Female\n                  Post-grad\n                  Trump\n                  Biden\n                  745\n                \n                \n                  Female\n                  Post-grad\n                  Biden\n                  Biden\n                  2,853\n                \n                \n                  Male\n                  No HS\n                  Trump\n                  Trump\n                  132\n                \n                \n                  Male\n                  No HS\n                  Biden\n                  Trump\n                  123\n                \n                \n                  Male\n                  High school graduate\n                  Trump\n                  Trump\n                  2,054\n                \n                \n                  Male\n                  High school graduate\n                  Biden\n                  Trump\n                  1,528\n                \n                \n                  Male\n                  Some college\n                  Trump\n                  Biden\n                  1,992\n                \n                \n                  Male\n                  Some college\n                  Biden\n                  Biden\n                  2,131\n                \n                \n                  Male\n                  2-year\n                  Trump\n                  Trump\n                  1,089\n                \n                \n                  Male\n                  2-year\n                  Biden\n                  Trump\n                  1,026\n                \n                \n                  Male\n                  4-year\n                  Trump\n                  Biden\n                  2,208\n                \n                \n                  Male\n                  4-year\n                  Biden\n                  Biden\n                  3,294\n                \n                \n                  Male\n                  Post-grad\n                  Trump\n                  Biden\n                  1,248\n                \n                \n                  Male\n                  Post-grad\n                  Biden\n                  Biden\n                  2,426\n                \n        \n      \n    \n\n\n\n\n\n\n\n\n\n14.2.3 포아송 회귀\npoissonreg (Kuhn 와/과 Frick 2022)를 사용하여 포아송 모델을 추정하기 위해 tidymodels를 사용할 수 있습니다(?tbl-modelsummarypoisson).\n\ncount_of_A &lt;-\n  read_parquet(file = \"outputs/data/count_of_A.parquet\")\n\nset.seed(853)\n\ncount_of_A_split &lt;-\n  initial_split(count_of_A, prop = 0.80)\ncount_of_A_train &lt;- training(count_of_A_split)\ncount_of_A_test &lt;- testing(count_of_A_split)\n\ngrades_tidymodels &lt;-\n  poisson_reg(mode = \"regression\") |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(\n    number_of_As ~ department,\n    data = count_of_A_train\n  )\n\n이 추정 결과는 ?tbl-modelsummarypoisson의 두 번째 열에 있습니다. glm()의 추정치와 유사하지만, 분할로 인해 관측치 수가 적습니다.",
    "crumbs": [
      "모델링",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>예측</span>"
    ]
  },
  {
    "objectID": "13-prediction_ko.html#라쏘-회귀",
    "href": "13-prediction_ko.html#라쏘-회귀",
    "title": "14  예측",
    "section": "14.3 라쏘 회귀",
    "text": "14.3 라쏘 회귀\n\n\n\n\n\n\n\n거인의 어깨 위에 서서\n\n\n\n로버트 팁시라니 박사는 스탠포드 대학교 통계 및 생의학 데이터 과학과의 교수입니다. 1981년 스탠포드 대학교에서 통계학 박사 학위를 취득한 후, 그는 토론토 대학교의 조교수로 부임했습니다. 그는 1994년에 정교수로 승진했고 1998년에 스탠포드로 옮겼습니다. 그는 위에서 언급한 GAM과 자동화된 변수 선택 방법인 라쏘 회귀를 포함하여 근본적인 기여를 했습니다. 그는 (islr의?) 저자입니다. 그는 1996년에 COPSS 회장상을 수상했고 2019년에 왕립 학회 펠로우로 임명되었습니다.",
    "crumbs": [
      "모델링",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>예측</span>"
    ]
  },
  {
    "objectID": "13-prediction_ko.html#파이썬으로-예측하기",
    "href": "13-prediction_ko.html#파이썬으로-예측하기",
    "title": "14  예측",
    "section": "14.4 파이썬으로 예측하기",
    "text": "14.4 파이썬으로 예측하기\n\n14.4.1 설정\nVSCode 내에서 파이썬을 사용할 것입니다. VSCode는 여기에서 다운로드할 수 있는 마이크로소프트의 무료 IDE입니다. 그런 다음 Quarto 및 파이썬 확장을 설치합니다.\n\n\n14.4.2 데이터\n파케이를 사용하여 데이터 읽기.\n판다스를 사용하여 조작\n\n\n14.4.3 모델\n\n14.4.3.1 scikit-learn\n\n\n14.4.3.2 텐서플로우",
    "crumbs": [
      "모델링",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>예측</span>"
    ]
  },
  {
    "objectID": "13-prediction_ko.html#연습-문제",
    "href": "13-prediction_ko.html#연습-문제",
    "title": "14  예측",
    "section": "14.5 연습 문제",
    "text": "14.5 연습 문제\n\n연습\n\n(계획) 다음 시나리오를 고려하십시오: 매일 1년 동안 삼촌과 당신은 다트를 합니다. 각 라운드는 각각 세 개의 다트를 던지는 것으로 구성됩니다. 각 라운드가 끝나면 세 개의 다트가 맞은 점수를 더합니다. 그래서 3, 5, 10을 맞히면 해당 라운드의 총 점수는 18입니다. 삼촌은 다소 자비로워서, 5보다 작은 숫자를 맞히면 그것을 보지 못한 척하고, 해당 다트를 다시 던질 기회를 줍니다. 매일 15라운드를 한다고 가정하십시오. 해당 데이터셋이 어떻게 생겼을지 스케치한 다음 모든 관측치를 보여주기 위해 만들 수 있는 그래프를 스케치하십시오.\n(시뮬레이션) 설명된 시나리오를 더 고려하고 상황을 시뮬레이션하십시오. 다시 던질 기회가 없었다면 삼촌의 총점과 당신의 총점을 비교하고, 실제로 얻게 되는 총점을 비교하십시오. 시뮬레이션된 데이터를 기반으로 최소 10개의 테스트를 포함하십시오.\n(수집) 이러한 데이터셋(또는 관심 있는 동등한 스포츠 또는 상황)의 가능한 출처 하나를 설명하십시오.\n(탐색) 스케치한 그래프를 ggplot2를 사용하여 만드십시오. 그런 다음 tidymodels를 사용하여 누가 이기는지에 대한 예측 모델을 만드십시오.\n(전달) 당신이 한 일에 대해 두 단락을 작성하십시오.\n\n\n\n퀴즈\n\n\n수업 활동\n\n\n과제\nnflverse를 사용하여 정규 시즌 동안 NFL 쿼터백에 대한 일부 통계를 로드하십시오. 데이터 사전은 데이터를 이해하는 데 유용할 것입니다.\n\nqb_regular_season_stats &lt;-\n  load_player_stats(seasons = TRUE) |&gt;\n  filter(season_type == \"REG\" & position == \"QB\")\n\nNFL 분석가라고 가정하고, 2023년 정규 시즌의 절반, 즉 9주차가 막 끝났다고 가정하십시오. 저는 시즌의 나머지 기간, 즉 10-18주차에 각 팀에 대해 생성할 수 있는 passing_epa의 최상의 예측 모델에 관심이 있습니다.\nQuarto를 사용하고, 적절한 제목, 저자, 날짜, GitHub 리포지토리 링크, 섹션 및 인용을 포함하고, 경영진을 위한 2-3페이지 보고서를 작성하십시오.\n\n\n\n\nArel-Bundock, Vincent. 2024. tinytable: Simple and Configurable Tables in “HTML”, “LaTeX”, “Markdown”, “Word”, “PNG”, “PDF”, and “Typst” Formats. https://vincentarelbundock.github.io/tinytable/.\n\n\nCarl, Sebastian, Ben Baldwin, Lee Sharpe, Tan Ho, 와/과 John Edwards. 2023. nflverse: Easily Install and Load the ’nflverse’. https://CRAN.R-project.org/package=nflverse.\n\n\nCongelio, Bradley. 2024. Introduction to NFL Analytics with R. 1st ed. Chapman; Hall/CRC. https://bradcongelio.com/nfl-analytics-with-r-book/.\n\n\nFrick, Hannah, Fanny Chow, Max Kuhn, Michael Mahoney, Julia Silge, 와/과 Hadley Wickham. 2022. rsample: General Resampling Infrastructure. https://CRAN.R-project.org/package=rsample.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, 와/과 Robert Tibshirani. (2013년) 2021. An Introduction to Statistical Learning with Applications in R. 2nd ed. Springer. https://www.statlearning.com.\n\n\nKuhn, Max. 2022. tune: Tidy Tuning Tools. https://CRAN.R-project.org/package=tune.\n\n\nKuhn, Max, 와/과 Hannah Frick. 2022. poissonreg: Model Wrappers for Poisson Regression. https://CRAN.R-project.org/package=poissonreg.\n\n\nKuhn, Max, 와/과 Davis Vaughan. 2022. parsnip: A Common API to Modeling and Analysis Functions. https://CRAN.R-project.org/package=parsnip.\n\n\nKuhn, Max, Davis Vaughan, 와/과 Emil Hvitfeldt. 2022. yardstick: Tidy Characterizations of Model Performance. https://CRAN.R-project.org/package=yardstick.\n\n\nKuhn, Max, 와/과 Hadley Wickham. 2020. tidymodels: a collection of packages for modeling and machine learning using tidyverse principles. https://www.tidymodels.org.\n\n\n———. 2022. recipes: Preprocessing and Feature Engineering Steps for Modeling. https://CRAN.R-project.org/package=recipes.\n\n\nMcKinney, Wes. (2011년) 2022. Python for Data Analysis. 3rd ed. https://wesmckinney.com/book/.\n\n\nRichardson, Neal, Ian Cook, Nic Crane, Dewey Dunnington, Romain François, Jonathan Keane, Dragoș Moldovan-Grünfeld, Jeroen Ooms, 와/과 Apache Arrow. 2023. arrow: Integration to Apache Arrow. https://CRAN.R-project.org/package=arrow.\n\n\nWickham, Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, 기타. 2019. “Welcome to the Tidyverse”. Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.",
    "crumbs": [
      "모델링",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>예측</span>"
    ]
  },
  {
    "objectID": "14-causality_from_obs_ko.html",
    "href": "14-causality_from_obs_ko.html",
    "title": "15  관찰 데이터로부터의 인과성",
    "section": "",
    "text": "15.1 소개\n선수 지식\n주요 개념 및 기술\n소프트웨어 및 패키지\n인과 관계에 대해 말할 수 있도록 실험을 수행할 수 있을 때 삶은 멋집니다. 그러나 실험을 실행할 수 없는 상황이 있지만, 그럼에도 불구하고 인과적 주장을 하고 싶을 때가 있습니다. 그리고 실험 외부의 데이터는 실험이 갖지 못한 가치를 가지고 있습니다. 이 장에서는 관찰 데이터를 사용하여 인과 관계에 대해 말할 수 있는 상황과 방법을 논의합니다. 우리는 통계뿐만 아니라 경제학 및 정치학을 포함한 다양한 사회 과학 및 역학에서 가져온 비교적 간단한 방법을 정교한 방식으로 사용합니다.\n예를 들어, (dagan2021bnt162b2는?) 관찰 데이터를 사용하여 화이자-바이오엔테크 백신의 효과를 확인합니다. 그들은 이런 식으로 관찰 데이터를 사용하는 것에 대한 한 가지 우려가 교란 변수라는 것을 논의하는데, 여기서 우리는 예측 변수와 결과 변수 모두에 영향을 미치고 가짜 관계를 초래할 수 있는 일부 변수가 있다는 것을 우려합니다. (dagan2021bnt162b2는?) 연령, 성별, 지리적 위치 및 의료 사용과 같은 잠재적인 교란 변수 목록을 먼저 만든 다음, 예방 접종을 받은 사람과 그렇지 않은 사람 사이에서 일대일로 매칭하여 각각을 조정함으로써 이것을 조정합니다. 실험 데이터는 관찰 데이터의 사용을 안내했으며, 후자의 더 큰 크기는 특정 연령 그룹과 질병의 정도에 초점을 맞출 수 있게 했습니다.\n이 장은 관찰 데이터를 정교한 방식으로 사용하는 것에 관한 것입니다. A/B 테스트나 RCT를 실행할 수 없을 때에도 어떻게 인과적 진술을 하는 데 편안함을 느낄 수 있습니까? 실제로, 어떤 상황에서 우리는 그것들을 실행하지 않거나 그것들에 추가하여 관찰 기반 접근 방식을 실행하는 것을 선호할 수 있습니까? 우리는 세 가지 주요 방법인 차이-안의-차이, 회귀 불연속성 및 도구 변수를 다룹니다.",
    "crumbs": [
      "적용",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>관찰 데이터로부터의 인과성</span>"
    ]
  },
  {
    "objectID": "14-causality_from_obs_ko.html#두-가지-일반적인-역설",
    "href": "14-causality_from_obs_ko.html#두-가지-일반적인-역설",
    "title": "15  관찰 데이터로부터의 인과성",
    "section": "15.2 두 가지 일반적인 역설",
    "text": "15.2 두 가지 일반적인 역설\n데이터가 우리를 속일 수 있는 두 가지 상황이 너무 흔해서 명시적으로 살펴볼 것입니다. 이것들은 다음과 같습니다.\n\n심슨의 역설, 그리고\n버크슨의 역설\n\n\n15.2.1 심슨의 역설\n심슨의 역설은 데이터의 하위 집합에 대해 어떤 관계를 추정하지만, 전체 데이터셋을 고려할 때 다른 관계를 추정할 때 발생합니다 (Simpson 1951). 이것은 그룹을 기반으로 개인에 대한 주장을 하려고 할 때 발생하는 생태학적 오류의 특정 사례입니다. 예를 들어, 각 학과를 개별적으로 고려할 때 두 학과에서 학부 성적과 대학원 성적 사이에 긍정적인 관계가 있을 수 있습니다. 그러나 한 학과에서 학부 성적이 다른 학과보다 높은 경향이 있고 대학원 성적은 반대 경향이 있다면, 학부 성적과 대학원 성적 사이에 부정적인 관계를 발견할 수 있습니다. 이것을 더 명확하게 보여주기 위해 일부 데이터를 시뮬레이션할 수 있습니다(그림 15.1).\n\nset.seed(853)\n\nnumber_in_each &lt;- 1000\n\ndepartment_one &lt;-\n  tibble(\n    undergrad = runif(n = number_in_each, min = 0.7, max = 0.9),\n    noise = rnorm(n = number_in_each, 0, sd = 0.1),\n    grad = undergrad + noise,\n    type = \"Department 1\"\n  )\n\ndepartment_two &lt;-\n  tibble(\n    undergrad = runif(n = number_in_each, min = 0.6, max = 0.8),\n    noise = rnorm(n = number_in_each, 0, sd = 0.1),\n    grad = undergrad + noise + 0.3,\n    type = \"Department 2\"\n  )\n\nboth_departments &lt;- rbind(department_one, department_two)\n\nboth_departments\n\n# A tibble: 2,000 × 4\n   undergrad   noise  grad type        \n       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       \n 1     0.772 -0.0566 0.715 Department 1\n 2     0.724 -0.0312 0.693 Department 1\n 3     0.797  0.0770 0.874 Department 1\n 4     0.763 -0.0664 0.697 Department 1\n 5     0.707  0.0717 0.779 Department 1\n 6     0.781 -0.0165 0.764 Department 1\n 7     0.726 -0.104  0.623 Department 1\n 8     0.749  0.0527 0.801 Department 1\n 9     0.732 -0.0471 0.684 Department 1\n10     0.738  0.0552 0.793 Department 1\n# ℹ 1,990 more rows\n\n\n\nboth_departments |&gt;\n  ggplot(aes(x = undergrad, y = grad)) +\n  geom_point(aes(color = type), alpha = 0.1) +\n  geom_smooth(aes(color = type), method = \"lm\", formula = \"y ~ x\") +\n  geom_smooth(method = \"lm\", formula = \"y ~ x\", color = \"black\") +\n  labs(\n    x = \"Undergraduate results\",\n    y = \"Graduate results\",\n    color = \"Department\"\n  ) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n그림 15.1: 심슨의 역설을 보여주는 시뮬레이션된 데이터의 그림\n\n\n\n\n\n심슨의 역설은 종종 캘리포니아 대학교 버클리의 대학원 입학에 대한 실제 데이터를 사용하여 설명됩니다 (Bickel, Hammel, 와/과 O’Connell 1975). 이 논문은 ?sec-on-writing에서 지금까지 출판된 가장 위대한 부제목 중 하나를 가진 것으로 언급되었습니다. (Hernn2011은?) 관계와 역설의 원인을 더욱 명확하게 하는 DAG를 만듭니다.\n더 최근에는, 문서에서 언급했듯이, palmerpenguins의 “penguins” 데이터셋은 다른 종의 펭귄에서 체질량과 부리 깊이 간의 관계에 대한 실제 데이터를 사용하여 심슨의 역설의 예를 제공합니다(그림 15.2). 전반적인 부정적인 추세는 젠투 펭귄이 아델리 펭귄과 턱끈 펭귄에 비해 더 무겁지만 부리가 더 짧은 경향이 있기 때문에 발생합니다.\n\npenguins |&gt;\n  ggplot(aes(x = body_mass_g, y = bill_depth_mm)) +\n  geom_point(aes(color = species), alpha = 0.1) +\n  geom_smooth(aes(color = species), method = \"lm\", formula = \"y ~ x\") +\n  geom_smooth(\n    method = \"lm\",\n    formula = \"y ~ x\",\n    color = \"black\"\n  ) +\n  labs(\n    x = \"Body mass (grams)\",\n    y = \"Bill depth (millimeters)\",\n    color = \"Species\"\n  ) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n그림 15.2: 펭귄 부리 깊이와 체질량 비교 데이터셋에서 심슨의 역설 그림\n\n\n\n\n\n\n\n15.2.2 버크슨의 역설\n버크슨의 역설은 우리가 가진 데이터셋을 기반으로 어떤 관계를 추정하지만, 데이터셋이 너무 선택적이어서 더 일반적인 데이터셋에서는 관계가 다를 때 발생합니다 (Berkson 1946). 예를 들어, 프로 사이클리스트의 데이터셋이 있다면, VO2 max와 자전거 경주에서 이길 확률 사이에 관계가 없다는 것을 발견할 수 있습니다 (Coyle 기타 1988; Podlogar, Leo, 와/과 Spragg 2022). 그러나 일반 인구의 데이터셋이 있다면, 이 두 변수 사이에 관계를 발견할 수 있습니다. 프로 데이터셋은 관계가 사라질 정도로 선택되었습니다. 충분히 좋은 VO2 max가 없으면 프로 사이클리스트가 될 수 없지만, 프로 사이클리스트 중에는 모두 충분히 좋은 VO2 max를 가지고 있습니다. 다시 말하지만, 이것을 더 명확하게 보여주기 위해 일부 데이터를 시뮬레이션할 수 있습니다(그림 15.3).\n\nset.seed(853)\n\nnum_pros &lt;- 100\nnum_public &lt;- 1000\n\nprofessionals &lt;- tibble(\n  VO2 = runif(num_pros, 0.7, 0.9),\n  chance_of_winning = runif(num_pros, 0.7, 0.9),\n  type = \"Professionals\"\n)\n\ngeneral_public &lt;- tibble(\n  VO2 = runif(num_public, 0.6, 0.8),\n  chance_of_winning = VO2 + rnorm(num_public, 0, 0.03) + 0.1,\n  type = \"Public\"\n)\n\nprofessionals_and_public &lt;- bind_rows(professionals, general_public)\n\n\nprofessionals_and_public |&gt;\n  ggplot(aes(x = VO2, y = chance_of_winning)) +\n  geom_point(aes(color = type), alpha = 0.1) +\n  geom_smooth(aes(color = type), method = \"lm\", formula = \"y ~ x\") +\n  geom_smooth(method = \"lm\", formula = \"y ~ x\", color = \"black\") +\n  labs(\n    x = \"VO2 max\",\n    y = \"Chance of winning a bike race\",\n    color = \"Type\"\n  ) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n그림 15.3: 버크슨의 역설을 보여주는 시뮬레이션된 데이터의 그림",
    "crumbs": [
      "적용",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>관찰 데이터로부터의 인과성</span>"
    ]
  },
  {
    "objectID": "14-causality_from_obs_ko.html#차이-안의-차이",
    "href": "14-causality_from_obs_ko.html#차이-안의-차이",
    "title": "15  관찰 데이터로부터의 인과성",
    "section": "15.3 차이-안의-차이",
    "text": "15.3 차이-안의-차이\n실험을 수행할 수 있는 이상적인 상황은 거의 없습니다. 넷플릭스가 가격을 변경하도록 허용할 것이라고 합리적으로 기대할 수 있습니까? 그리고 한 번 허용하더라도, 다시, 그리고 다시, 그리고 다시 허용할까요? 또한, 처리군과 대조군을 명시적으로 만들 수 있는 경우는 거의 없습니다. 마지막으로, 실험은 비싸거나 비윤리적일 수 있습니다. 대신, 우리는 우리가 가진 것으로 만족해야 합니다. 무작위화를 통해 반사실이 우리에게 오고, 따라서 처리를 제외하고는 둘이 동일하다는 것을 아는 대신, 우리는 처리를 제외하고는 유사한 그룹을 식별하려고 노력하고, 따라서 모든 차이는 처리에 기인할 수 있습니다.\n관찰 데이터를 사용하면, 처리 전에 두 그룹 간에 차이가 있는 경우가 있습니다. 이러한 처리 전 차이가 본질적으로 차이가 일관되고, 처리가 없는 경우에도 일관성이 계속될 것으로 예상되는 가정을 충족하는 한— “병행 추세” 가정—그러면 우리는 차이의 차이를 처리의 효과로 볼 수 있습니다. 차이-안의-차이 분석의 측면 중 하나는 예를 들어 (Tang2015와?) 같이 비교적 간단한 방법을 사용하여 수행할 수 있다는 것입니다. 이진 변수를 사용한 선형 회귀는 시작하고 설득력 있는 작업을 수행하기에 충분합니다.\n새로운 테니스 라켓이 서브 속도에 미치는 영향을 알고 싶다고 가정해 봅시다. 이것을 테스트하는 한 가지 방법은, 예를 들어, 로저 페더러의 테니스 라켓 없는 서브 속도와 열정적인 아마추어, 예를 들어 빌의 테니스 라켓 있는 서브 속도 간의 차이를 측정하는 것입니다. 예, 우리는 차이를 발견할 것이지만, 테니스 라켓에 얼마나 기인하는지 알 수 있을까요? 또 다른 방법은 빌의 새로운 테니스 라켓 없는 서브 속도와 새로운 테니스 라켓 있는 서브 속도 간의 차이를 고려하는 것입니다. 그러나 시간이 지남에 따라 서브가 자연스럽게 더 빨라진다면 어떨까요? 대신, 우리는 두 가지 접근 방식을 결합하여 차이의 차이를 살펴봅니다.\n우리는 페더러의 서브 속도를 측정하고 그것을 빌의 서브 속도와 비교하는 것으로 시작합니다. 둘 다 새로운 라켓이 없습니다. 그런 다음 페더러의 서브 속도를 다시 측정하고, 빌의 서브 속도를 새로운 라켓으로 측정합니다. 그 차이의 차이가 새로운 라켓의 효과에 대한 추정치가 될 것입니다. 이 분석이 적절한지 확인하기 위해 물어봐야 할 몇 가지 주요 질문이 있습니다.\n\n빌의 서브 속도에 영향을 미칠 수 있는 빌에게만 영향을 미치고 페더러에게는 영향을 미치지 않는 다른 것이 있습니까?\n페더러와 빌이 동일한 서브 속도 향상 궤적을 가질 가능성이 있습니까? 이것은 “병행 추세” 가정이며, 차이-안의-차이 분석에 대한 많은 논의를 지배합니다.\n마지막으로, 페더러와 빌의 서브 속도의 분산이 동일할 가능성이 있습니까?\n\n이러한 요구 사항에도 불구하고, 차이-안의-차이는 처리 전에 처리군과 대조군이 동일할 필요가 없기 때문에 강력한 접근 방식입니다. 우리는 단지 그들이 어떻게 다른지에 대한 좋은 아이디어를 가지고 있으면 됩니다.\n\n15.3.1 시뮬레이션 예시: 테니스 서브 속도\n상황에 대해 더 구체적으로 설명하기 위해, 데이터를 시뮬레이션합니다. 우리는 처음에 다른 사람들의 서브 속도 사이에 1의 차이가 있고, 새로운 테니스 라켓 이후에는 6의 차이가 있는 상황을 시뮬레이션할 것입니다. 그래프를 사용하여 상황을 설명할 수 있습니다(그림 15.4).\n\nset.seed(853)\n\nsimulated_diff_in_diff &lt;-\n  tibble(\n    person = rep(c(1:1000), times = 2),\n    time = c(rep(0, times = 1000), rep(1, times = 1000)),\n    treat_group = rep(sample(x = 0:1, size = 1000, replace = TRUE ), times = 2)\n    ) |&gt;\n  mutate(\n    treat_group = as.factor(treat_group),\n    time = as.factor(time)\n  )\n\nsimulated_diff_in_diff &lt;-\n  simulated_diff_in_diff |&gt;\n  rowwise() |&gt;\n  mutate(\n    serve_speed = case_when(\n      time == 0 & treat_group == 0 ~ rnorm(n = 1, mean = 5, sd = 1),\n      time == 1 & treat_group == 0 ~ rnorm(n = 1, mean = 6, sd = 1),\n      time == 0 & treat_group == 1 ~ rnorm(n = 1, mean = 8, sd = 1),\n      time == 1 & treat_group == 1 ~ rnorm(n = 1, mean = 14, sd = 1)\n    )\n  )\n\nsimulated_diff_in_diff\n\n# A tibble: 2,000 × 4\n# Rowwise: \n   person time  treat_group serve_speed\n    &lt;int&gt; &lt;fct&gt; &lt;fct&gt;             &lt;dbl&gt;\n 1      1 0     0                  4.43\n 2      2 0     1                  6.96\n 3      3 0     1                  7.77\n 4      4 0     0                  5.31\n 5      5 0     0                  4.09\n 6      6 0     0                  4.85\n 7      7 0     0                  6.43\n 8      8 0     0                  5.77\n 9      9 0     1                  6.13\n10     10 0     1                  7.32\n# ℹ 1,990 more rows\n\n\n\nsimulated_diff_in_diff |&gt;\n  ggplot(aes(x = time, y = serve_speed, color = treat_group)) +\n  geom_point(alpha = 0.2) +\n  geom_line(aes(group = person), alpha = 0.1) +\n  theme_minimal() +\n  labs(x = \"Time period\", y = \"Serve speed\", color = \"Person got a new racket\") +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n그림 15.4: 새로운 테니스 라켓을 얻기 전후의 차이를 보여주는 시뮬레이션된 데이터의 그림\n\n\n\n\n\n우리는 차이의 평균 차이를 살펴봄으로써 수동으로 추정치를 얻을 수 있습니다. 그렇게 할 때, 우리는 새로운 테니스 라켓의 효과를 5.06으로 추정하며, 이는 우리가 시뮬레이션한 것과 유사합니다.\n\nave_diff &lt;-\n  simulated_diff_in_diff |&gt;\n  pivot_wider(\n    names_from = time,\n    values_from = serve_speed,\n    names_prefix = \"time_\"\n  ) |&gt;\n  mutate(difference = time_1 - time_0) |&gt;\n  # 그룹 내에서 오래된 라켓과 새 라켓 서브 속도 간의 평균 차이\n  summarise(average_difference = mean(difference),\n            .by = treat_group)\n\n# 각 그룹의 평균 차이 간의 차이\nave_diff$average_difference[2] - ave_diff$average_difference[1]\n\n[1] 5.058414\n\n\n그리고 선형 회귀를 사용하여 동일한 결과를 얻을 수 있습니다. 관심 있는 모델은 다음과 같습니다.\n\\[Y_{i,t} = \\beta_0 + \\beta_1\\times\\mbox{Treatment}_i + \\beta_2\\times\\mbox{Time}_t + \\beta_3\\times(\\mbox{Treatment} \\times\\mbox{Time})_{i,t} + \\epsilon_{i,t}\\]\n별도의 측면도 포함해야 하지만, 우리가 관심 있는 것은 상호 작용의 추정치입니다. 이 경우 \\(\\beta_3\\)입니다. 그리고 우리는 추정된 효과가 5.06이라는 것을 발견합니다(표 15.1).\n\ndiff_in_diff_example_regression &lt;-\n  stan_glm(\n    formula = serve_speed ~ treat_group * time,\n    data = simulated_diff_in_diff,\n    family = gaussian(),\n    prior = normal(location = 0, scale = 2.5, autoscale = TRUE),\n    prior_intercept = normal(0, 2.5, autoscale = TRUE),\n    prior_aux = exponential(rate = 1, autoscale = TRUE),\n    seed = 853\n  )\n\nsaveRDS(\n  diff_in_diff_example_regression,\n  file = \"diff_in_diff_example_regression.rds\"\n)\n\n\ndiff_in_diff_example_regression &lt;-\n  readRDS(file = \"diff_in_diff_example_regression.rds\")\n\n\nmodelsummary(\n  diff_in_diff_example_regression\n)\n\n\n\n표 15.1: 새로운 테니스 라켓을 얻기 전후의 차이를 보여주는 시뮬레이션된 데이터의 그림\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  4.971\n                \n                \n                  treatment_group1\n                  3.035\n                \n                \n                  time1\n                  1.006\n                \n                \n                  treatment_group1 × time1\n                  5.057\n                \n                \n                  Num.Obs.\n                  2000\n                \n                \n                  R2\n                  0.927\n                \n                \n                  R2 Adj.\n                  0.927\n                \n                \n                  Log.Lik.\n                  -2802.166\n                \n                \n                  ELPD\n                  -2806.3\n                \n                \n                  ELPD s.e.\n                  32.1\n                \n                \n                  LOOIC\n                  5612.5\n                \n                \n                  LOOIC s.e.\n                  64.2\n                \n                \n                  WAIC\n                  5612.5\n                \n                \n                  RMSE\n                  0.98\n                \n        \n      \n    \n\n\n\n\n\n\n\n\n15.3.2 가정\n차이-안의-차이를 사용하려면 가정을 충족해야 합니다. 이전에 언급된 세 가지가 있었지만, 여기서는 “병행 추세” 가정에 초점을 맞출 것입니다. 병행 추세 가정은 차이-안의-차이 분석과 관련된 모든 것을 괴롭힙니다. 왜냐하면 우리는 그것을 결코 증명할 수 없기 때문입니다. 우리는 단지 그것에 대해 확신하고 다른 사람들을 설득하려고 노력할 수 있습니다.\n왜 우리가 그것을 결코 증명할 수 없는지 보기 위해, 프로 스포츠 팀의 승패에 대한 새로운 경기장의 효과를 알고 싶은 예를 생각해 보십시오. 이를 위해 우리는 두 프로 농구 팀인 골든스테이트 워리어스와 토론토 랩터스를 고려합니다. 워리어스는 2019-20 시즌 초에 경기장을 변경했고, 랩터스는 그렇지 않았으므로, 우리는 2016-17 시즌, 2017-18 시즌, 2018-19 시즌, 그리고 마지막으로 그들이 이사한 후의 성과를 비교할 2019-20 시즌의 네 가지 기간을 고려할 것입니다. 여기서 랩터스는 우리의 반사실 역할을 합니다. 이것은 우리가 새로운 경기장이 없는 경우 워리어스와 랩터스 간의 관계가 일관된 방식으로 계속 변할 것이라고 가정한다는 것을 의미합니다. 그러나 인과 추론의 근본적인 문제는 우리가 그것을 확실히 알 수 없다는 것입니다. 우리는 독자가 가질 수 있는 모든 우려를 완화하기 위해 충분한 증거를 제시해야 합니다.\n차이-안의-차이를 사용할 때 타당성에 대한 네 가지 주요 위협이 있으며, 우리는 그것들을 모두 해결해야 합니다 (Cunningham 2021, p. 272–277):\n\n비병행 추세. 처리군과 대조군은 차이를 기반으로 할 수 있습니다. 따라서 병행 추세를 설득력 있게 주장하기 어려울 수 있습니다. 이 경우, 모델에서 일부를 조정할 수 있는 다른 요인을 고려해 보십시오. 이것은 삼중 차이 접근 방식이 필요할 수 있습니다. 예를 들어, 이전 예에서, 우리는 워리어스와 동일한 넓은 지리적 영역에 있는 축구팀인 샌프란시스코 49ers를 추가할 수 있습니다. 또는 분석을 다시 생각하여 다른 대조군을 만들 수 있는지 확인할 수 있습니다. 더 이른 기간을 추가하면 도움이 될 수 있지만, 세 번째 요점에서 언급했듯이 더 많은 문제를 야기할 수 있습니다.\n구성적 차이. 이것은 반복적인 횡단면으로 작업할 때 우려되는 사항입니다. 해당 횡단면의 구성이 변경되면 어떻게 될까요? 예를 들어, 빠르게 성장하는 앱에서 작업하고 있고, 일부 변경의 효과를 보고 싶다면 말입니다. 초기 횡단면에서는 대부분 젊은 사람들이 있을 수 있지만, 후속 횡단면에서는 앱 사용의 인구 통계가 변경됨에 따라 더 많은 노인이 있을 수 있습니다. 따라서 우리의 결과는 우리가 관심 있는 변경의 효과가 아니라 연령 효과일 수 있습니다.\n장기적인 효과와 신뢰성 비교. ?sec-hunt-data에서 논의했듯이, 우리가 실행하는 분석의 길이 사이에는 절충안이 있습니다. 분석을 더 오래 실행할수록 다른 요인이 결과에 영향을 미칠 기회가 더 많아집니다. 치료받지 않은 사람이 치료받을 가능성도 증가합니다. 그러나 다른 한편으로는 단기적인 결과가 장기적으로 계속될 것이라고 설득력 있게 주장하기 어려울 수 있습니다.\n함수 형태 의존성. 이것은 결과가 유사할 때 덜 문제가 되지만, 다른 경우 함수 형태가 결과의 일부 측면에 책임이 있을 수 있습니다.\n\n\n\n15.3.3 1960년에서 1974년 사이의 프랑스 신문 가격\n이 사례 연구에서는 (angelucci2019newspapers를?) 소개합니다. 그들은 텔레비전 도입이 프랑스 신문에 미치는 영향을 이해하는 데 관심이 있습니다. 우리는 주요 결과 중 하나를 복제할 것입니다.\n신문의 비즈니스 모델은 인터넷에 의해 도전을 받았고 많은 지역 신문이 문을 닫았습니다. 이 문제는 새로운 것이 아닙니다. 텔레비전이 도입되었을 때도 비슷한 우려가 있었습니다. (angelucci2019newspapers는?) 1967년에 발표된 프랑스의 텔레비전 광고 도입을 사용하여 광고 수익 감소가 신문에 미치는 영향을 조사합니다. 그들은 1960년부터 1974년까지의 프랑스 신문 데이터셋을 만들고, 광고 수익 감소가 신문의 내용과 가격에 미치는 영향을 조사하기 위해 차이-안의-차이를 사용합니다. 그들이 초점을 맞추는 변화는 텔레비전 광고의 도입이며, 이것이 지역 신문보다 전국 신문에 더 많은 영향을 미쳤다고 주장합니다. 그들은 이 변화가 신문의 저널리즘 내용 감소와 신문 가격 하락을 모두 초래한다는 것을 발견합니다. 이 변화에 초점을 맞추고 차이-안의-차이를 사용하여 분석하는 것은 몇 가지 경쟁적인 효과를 분리할 수 있게 해주기 때문에 중요합니다. 예를 들어, 신문이 광고에 대해 높은 가격을 더 이상 청구할 수 없었기 때문에 중복되었습니까, 아니면 소비자가 텔레비전에서 뉴스를 얻는 것을 선호했기 때문입니까?\n등록 후 (angelucci2019newspapers를?) 뒷받침하는 데이터에 무료로 액세스할 수 있습니다. 데이터셋은 Stata 데이터 형식인 “.dta”이며, haven의 read_dta()로 읽을 수 있습니다. 관심 있는 파일은 “dta” 폴더에 있는 “Angelucci_Cage_AEJMicro_dataset.dta”입니다.\n\nnewspapers &lt;- read_dta(\"Angelucci_Cage_AEJMicro_dataset.dta\")\n\n데이터셋에는 1,196개의 관측치와 52개의 변수가 있습니다. (angelucci2019newspapers는?) 약 100개의 신문이 있는 1960-1974년 기간에 관심이 있습니다. 기간 초에는 14개의 전국 신문이 있었고, 끝에는 12개가 있었습니다. 핵심 기간은 1967년으로, 프랑스 정부가 텔레비전 광고를 허용하겠다고 발표한 때입니다. (angelucci2019newspapers는?) 이 변화가 전국 신문에 영향을 미쳤지만 지역 신문에는 영향을 미치지 않았다고 주장합니다. 전국 신문은 처리군이고 지역 신문은 대조군입니다.\n우리는 헤드라인 차이-안의-차이 결과에만 초점을 맞추고 요약 통계를 구성합니다.\n\nnewspapers &lt;-\n  newspapers |&gt;\n  select(\n    year, id_news, after_national, local, national, ra_cst, ps_cst, qtotal\n    ) |&gt; \n  mutate(ra_cst_div_qtotal = ra_cst / qtotal, \n         across(c(id_news, after_national, local, national), as.factor),\n         year = as.integer(year))\n\nnewspapers\n\n# A tibble: 1,196 × 9\n    year id_news after_national local national    ra_cst ps_cst  qtotal\n   &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;          &lt;fct&gt; &lt;fct&gt;        &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1  1960 1       0              1     0         52890272   2.29  94478.\n 2  1961 1       0              1     0         56601060   2.20  96289.\n 3  1962 1       0              1     0         64840752   2.13  97313.\n 4  1963 1       0              1     0         70582944   2.43 101068.\n 5  1964 1       0              1     0         74977888   2.35 102103.\n 6  1965 1       0              1     0         74438248   2.29 105169.\n 7  1966 1       0              1     0         81383000   2.31 126235.\n 8  1967 1       0              1     0         80263152   2.88 128667.\n 9  1968 1       0              1     0         87165704   3.45 131824.\n10  1969 1       0              1     0        102596384   3.28 132417.\n# ℹ 1,186 more rows\n# ℹ 1 more variable: ra_cst_div_qtotal &lt;dbl&gt;\n\n\n우리는 1967년 이후에 무슨 일이 일어났는지, 특히 광고 수익 측면에서, 그리고 그것이 지역 신문과 비교하여 전국 신문에 대해 다른지 여부에 관심이 있습니다(그림 15.5). y축을 조정하기 위해 scales를 사용합니다.\n\nnewspapers |&gt;\n  mutate(type = if_else(local == 1, \"Local\", \"National\")) |&gt;\n  ggplot(aes(x = year, y = ra_cst)) +\n  geom_point(alpha = 0.5) +\n  scale_y_continuous(\n    labels = dollar_format(\n      prefix = \"$\",\n      suffix = \"M\",\n      scale = 0.000001)) +\n  labs(x = \"Year\", y = \"Advertising revenue\") +\n  facet_wrap(vars(type), nrow = 2) +\n  theme_minimal() +\n  geom_vline(xintercept = 1966.5, linetype = \"dashed\")\n\n\n\n\n\n\n\n그림 15.5: 프랑스 신문의 수익(1960-1974), 지역 또는 전국 여부에 따라\n\n\n\n\n\n관심 있는 모델은 다음과 같습니다.\n\\[\\mbox{ln}(y_{n,t}) = \\beta_0 + \\beta_1\\times(\\mbox{National binary}\\times\\mbox{1967 onward binary}) + \\lambda_n + \\gamma_t + \\epsilon\\]\n특히 관심 있는 것은 \\(\\beta_1\\) 계수입니다. stan_glm()을 사용하여 모델을 추정합니다.\n\nad_revenue &lt;-\n  stan_glm(\n    formula = log(ra_cst) ~ after_national + id_news + year,\n    data = newspapers,\n    family = gaussian(),\n    prior = normal(location = 0, scale = 2.5, autoscale = TRUE),\n    prior_intercept = normal(0, 2.5, autoscale = TRUE),\n    prior_aux = exponential(rate = 1, autoscale = TRUE),\n    seed = 853\n  )\n\nsaveRDS(\n  ad_revenue,\n  file = \"ad_revenue.rds\"\n)\n\nad_revenue_div_circulation &lt;-\n  stan_glm(\n    formula = log(ra_cst_div_qtotal) ~ after_national + id_news + year,\n    data = newspapers,\n    family = gaussian(),\n    prior = normal(location = 0, scale = 2.5, autoscale = TRUE),\n    prior_intercept = normal(0, 2.5, autoscale = TRUE),\n    prior_aux = exponential(rate = 1, autoscale = TRUE),\n    seed = 853\n  )\n\nsaveRDS(\n  ad_revenue_div_circulation,\n  file = \"ad_revenue_div_circulation.rds\"\n)\n\n# 소비자 측\nsubscription_price &lt;-\n  stan_glm(\n    formula = log(ps_cst) ~ after_national + id_news + year,\n    data = newspapers,\n    family = gaussian(),\n    prior = normal(location = 0, scale = 2.5, autoscale = TRUE),\n    prior_intercept = normal(0, 2.5, autoscale = TRUE),\n    prior_aux = exponential(rate = 1, autoscale = TRUE),\n    seed = 853\n  )\n\nsaveRDS(\n  subscription_price,\n  file = \"subscription_price.rds\"\n)\n\n\nad_revenue &lt;-\n  readRDS(file = \"ad_revenue.rds\")\n\nad_revenue_div_circulation &lt;-\n  readRDS(file = \"ad_revenue_div_circulation\")\n\nsubscription_price &lt;-\n  readRDS(file = \"subscription_price.rds\")\n\n?tbl-frenchnewspapersadvertising에서 수익 및 가격과 같은 광고 측 변수를 보면 일관되게 음의 계수를 발견합니다.\n\nselected_variables &lt;- c(\"year\" = \"Year\", \"after_national1\" = \"After change\")\n\nmodelsummary(\n  models = list(\n    \"Ad revenue\" = ad_revenue,\n    \"Ad revenue over circulation\" = ad_revenue_div_circulation,\n    \"Subscription price\" = subscription_price\n  ),\n  fmt = 2,\n  coef_map = selected_variables\n)\n\n\n\n표 15.2: 변경된 텔레비전 광고법이 프랑스 신문 수익에 미치는 영향(1960-1974)\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                Ad revenue\n                Ad revenue over circulation\n                Subscription price\n              \n        \n        \n        \n                \n                  Year\n                  0.05\n                  0.04\n                  0.05\n                \n                \n                  After change\n                  -0.23\n                  -0.15\n                  -0.04\n                \n                \n                  Num.Obs.\n                  1052\n                  1048\n                  1044\n                \n                \n                  R2\n                  0.984\n                  0.896\n                  0.868\n                \n                \n                  R2 Adj.\n                  0.983\n                  0.886\n                  0.852\n                \n                \n                  Log.Lik.\n                  336.539\n                  441.471\n                  875.559\n                \n                \n                  ELPD\n                  257.4\n                  362.3\n                  793.5\n                \n                \n                  ELPD s.e.\n                  34.4\n                  45.6\n                  24.3\n                \n                \n                  LOOIC\n                  -514.8\n                  -724.6\n                  -1586.9\n                \n                \n                  LOOIC s.e.\n                  68.9\n                  91.2\n                  48.6\n                \n                \n                  WAIC\n                  -515.9\n                  -725.5\n                  -1588.9\n                \n                \n                  RMSE\n                  0.17\n                  0.16\n                  0.10\n                \n        \n      \n    \n\n\n\n\n\n\n(angelucci2019newspapers의?) 주요 결과를 복제할 수 있으며, 많은 경우 1967년 이후에 차이가 있는 것으로 보입니다. Angelucci 와/과 Cagé (2019, pp. 353-358)는 또한 차이-안의-차이 모델에 필요한 해석, 외부 타당성 및 견고성에 대한 논의의 훌륭한 예를 포함합니다.",
    "crumbs": [
      "적용",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>관찰 데이터로부터의 인과성</span>"
    ]
  },
  {
    "objectID": "14-causality_from_obs_ko.html#성향-점수-매칭",
    "href": "14-causality_from_obs_ko.html#성향-점수-매칭",
    "title": "15  관찰 데이터로부터의 인과성",
    "section": "15.4 성향 점수 매칭",
    "text": "15.4 성향 점수 매칭\n차이-안의-차이는 강력한 분석 프레임워크입니다. 그러나 적절한 처리군과 대조군을 식별하기 어려울 수 있습니다. (alexander2018age는?) 한 형제는 대부분의 교육을 다른 나라에서 받았고, 다른 형제는 대부분의 교육을 미국에서 받은 이민자 형제를 비교합니다. 사용 가능한 데이터를 감안할 때, 이 매치는 합리적인 처리군과 대조군을 제공합니다. 그러나 친구나 사촌과 같은 다른 매치는 다른 결과를 초래했을 수 있습니다.\n관찰 가능한 변수를 기반으로만 매칭할 수 있습니다. 예를 들어, 연령 그룹 또는 교육입니다. 두 다른 시간에 우리는 한 도시의 18세 흡연율을 다른 도시의 18세 흡연율과 비교합니다. 이것은 우리가 일반적으로 관찰하는 변수, 예를 들어 성별 및 교육 측면에서도 18세 사이에 많은 차이가 있다는 것을 알기 때문에 대략적인 매치가 될 것입니다. 이것을 처리하는 한 가지 방법은 하위 그룹을 만드는 것입니다. 18세 남성, 고등학교 교육 등입니다. 그러나 그러면 표본 크기가 빠르게 작아집니다. 또한 연속 변수를 처리하는 문제도 있습니다. 그리고 18세가 정말로 19세와 그렇게 다른가요? 그들과도 비교하지 않는 이유는 무엇입니까?\n진행하는 한 가지 방법은 최근접 이웃 접근 방식을 고려하는 것이지만, 이 접근 방식에는 불확실성에 대한 우려가 제한적일 수 있습니다. 또한 많은 변수가 있을 때 문제가 있을 수 있습니다. 왜냐하면 우리는 고차원 그래프로 끝나기 때문입니다. 이것은 성향 점수 매칭으로 이어집니다. 여기서 우리는 성향 점수 매칭의 과정과 그것에 대해 일반적으로 제기되는 몇 가지 우려 사항을 설명합니다.\n성향 점수 매칭은 각 관측치에 어떤 확률, 즉 “성향 점수”를 할당하는 것을 포함합니다. 우리는 처리가 없는 예측 변수에 대한 관측치의 값을 기반으로 해당 확률을 구성합니다. 해당 확률은 실제로 처리되었는지 여부에 관계없이 관측치가 처리될 확률에 대한 최선의 추측입니다. 예를 들어, 18세 남성이 처리되었지만 19세 남성은 처리되지 않은 경우, 일반적으로 18세와 19세 남성 사이에 큰 차이가 없으므로 할당된 확률은 유사할 것입니다. 그런 다음 유사한 성향 점수를 가진 관측치의 결과를 비교합니다.\n\n15.4.1 시뮬레이션 예시: 무료 배송\n성향 점수 매칭의 한 가지 장점은 한 번에 많은 예측 변수를 쉽게 고려할 수 있다는 것이며, 로지스틱 회귀를 사용하여 구성할 수 있다는 것입니다. 더 구체적으로 말하면, 일부 데이터를 시뮬레이션할 수 있습니다. 우리는 대규모 온라인 소매업체에서 일한다고 가정할 것입니다. 우리는 일부 개인에게 무료 배송을 처리하여 평균 구매에 어떤 일이 일어나는지 볼 것입니다.\n\nset.seed(853)\n\nsample_size &lt;- 10000\n\npurchase_data &lt;-\n  tibble(\n    unique_person_id = 1:sample_size,\n    age = sample(x = 18:100, size = sample_size, replace = TRUE),\n    gender = sample(\n      x = c(\"Female\", \"Male\", \"Other/decline\"),\n      size = sample_size,\n      replace = TRUE,\n      prob = c(0.49, 0.47, 0.02)\n    ),\n    income = rnorm(n = sample_size, mean = 60000, sd = 15000) |&gt; round(0)\n  ) \n\npurchase_data\n\n# A tibble: 10,000 × 4\n   unique_person_id   age gender income\n              &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt;\n 1                1    26 Male    68637\n 2                2    81 Female  71486\n 3                3    34 Male    75652\n 4                4    46 Male    68068\n 5                5   100 Female  73206\n 6                6    20 Male    41872\n 7                7    50 Female  75957\n 8                8    36 Female  56566\n 9                9    72 Male    54621\n10               10    52 Female  40722\n# ℹ 9,990 more rows\n\n\n그런 다음 무료 배송으로 처리될 확률을 추가해야 합니다. 우리는 그것이 우리의 예측 변수에 따라 달라지며, 젊고, 소득이 높고, 남성인 개인이 이 처리를 더 가능성 있게 만든다고 말할 것입니다. 우리는 시뮬레이션된 상황이기 때문에만 그것을 알고 있습니다. 실제 데이터를 사용하고 있다면 알지 못할 것입니다.\n\npurchase_data &lt;- \n  purchase_data |&gt;\n  mutate(\n    # 특성을 제한된 숫자로 변경\n    age_num = rank(1 / age, ties.method = \"random\") %/% 3000,\n    # 0과 3 사이로 강제\n    gender_num = case_when(\n      gender == \"Male\" ~ 3,\n      gender == \"Female\" ~ 2,\n      gender == \"Other/decline\" ~ 1\n    ),\n    income_num = rank(income, ties.method = \"random\") %/% 3000\n  ) |&gt;\n  mutate(\n    sum_num = age_num + gender_num + income_num,\n    softmax_prob = exp(sum_num) / exp(max(sum_num) + 0.5),\n    free_shipping = rbinom(n = sample_size, size = 1, prob = softmax_prob)) |&gt;\n  select(-(age_num:softmax_prob))\n\n마지막으로, 개인의 평균 지출에 대한 어떤 측정이 필요합니다. 우리는 이것이 소득에 따라 증가한다고 가정할 것입니다. 우리는 무료 배송을 받은 사람들이 그렇지 않은 사람들보다 약간 더 높기를 원합니다.\n\npurchase_data &lt;-\n  purchase_data |&gt;\n  mutate(\n    noise = rnorm(n = nrow(purchase_data), mean = 5, sd = 2),\n    spend = income / 1000 + noise,\n    spend = if_else(free_shipping == 1, spend + 10, spend),\n    spend = as.integer(spend)\n    ) |&gt;\n  select(-noise) |&gt;\n  mutate(across(c(gender, free_shipping), as.factor))\n\npurchase_data\n\n# A tibble: 10,000 × 6\n   unique_person_id   age gender income free_shipping spend\n              &lt;int&gt; &lt;int&gt; &lt;fct&gt;   &lt;dbl&gt; &lt;fct&gt;         &lt;int&gt;\n 1                1    26 Male    68637 0                72\n 2                2    81 Female  71486 0                73\n 3                3    34 Male    75652 0                80\n 4                4    46 Male    68068 0                75\n 5                5   100 Female  73206 0                78\n 6                6    20 Male    41872 0                45\n 7                7    50 Female  75957 0                78\n 8                8    36 Female  56566 0                62\n 9                9    72 Male    54621 0                55\n10               10    52 Female  40722 0                47\n# ℹ 9,990 more rows\n\n\n순진하게 우리는 무료 배송을 받은 사람과 그렇지 않은 사람 사이에 평균 지출에 차이가 있음을 알 수 있습니다(표 15.3). 그러나 근본적인 우려는 무료 배송을 받지 않았다면 무료 배송을 받은 사람들의 지출이 어땠을까 하는 것입니다. ?tbl-heyheybigspender는 평균 비교를 보여주지만 모든 사람이 무료 배송을 받을 기회가 동일하지는 않았습니다. 그래서 우리는 평균 비교의 사용의 타당성에 의문을 제기합니다. 대신 우리는 성향 점수 매칭을 사용하여 실제로 무료 배송을 받은 각 관측치를 관찰 가능한 변수를 기반으로 가능한 한 유사하다고 간주되는 무료 배송을 받지 않은 관측치와 “연결”합니다.\n\npurchase_data |&gt;\n  summarise(average_spend = round(mean(spend), 2), .by = free_shipping) |&gt;\n  mutate(free_shipping = if_else(free_shipping == 0, \"No\", \"Yes\")) |&gt;\n  tt() |&gt; \n  style_tt(j = 1:2, align = \"lr\") |&gt; \n  setNames(c(\"Received free shipping?\", \"Average spend\"))\n\n\n\n표 15.3: 무료 배송 여부에 따른 평균 지출 차이\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Received free shipping?\n                Average spend\n              \n        \n        \n        \n                \n                  No\n                  64.44\n                \n                \n                  Yes\n                  86.71\n                \n        \n      \n    \n\n\n\n\n\n\nMatchIt의 matchit()을 사용하여 로지스틱 회귀를 구현하고 일치된 그룹을 만듭니다. 그런 다음 match.data()를 사용하여 실제로 무료 배송으로 처리된 254명 모두와 성향 점수를 기반으로 가능한 한 유사하다고 간주되는 처리되지 않은 사람을 모두 포함하는 일치 데이터를 얻습니다. 결과는 508개의 관측치가 있는 데이터셋입니다.\n\nmatched_groups &lt;- \n  matchit(\n  free_shipping ~ age + gender + income,\n  data = purchase_data,\n  method = \"nearest\",\n  distance = \"glm\"\n)\n\nmatched_groups\n\nA `matchit` object\n - method: 1:1 nearest neighbor matching without replacement\n - distance: Propensity score\n             - estimated with logistic regression\n - number of obs.: 10000 (original), 508 (matched)\n - target estimand: ATT\n - covariates: age, gender, income\n\nmatched_dataset &lt;- match.data(matched_groups)\n\nmatched_dataset\n\n# A tibble: 508 × 9\n   unique_person_id   age gender     income free_shipping spend distance weights\n              &lt;int&gt; &lt;int&gt; &lt;fct&gt;       &lt;dbl&gt; &lt;fct&gt;         &lt;int&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1               23    28 Female      65685 1                79  0.0334        1\n 2               24    67 Male        71150 0                76  0.0220        1\n 3               32    22 Female      86071 0                92  0.131         1\n 4               48    66 Female     100105 0               108  0.0473        1\n 5               59    25 Male        55548 1                68  0.0541        1\n 6               82    66 Male        70721 0                75  0.0224        1\n 7               83    58 Male        83443 0                88  0.0651        1\n 8               87    46 Male        59073 1                73  0.0271        1\n 9              119    89 Other/dec…  72284 0                74  0.00301       1\n10              125    51 Female      81164 1                96  0.0303        1\n# ℹ 498 more rows\n# ℹ 1 more variable: subclass &lt;fct&gt;\n\n\n마지막으로, 선형 회귀를 사용하여 평균 지출에 대한 처리 효과를 추정할 수 있습니다(표 15.4). 우리는 특히 처리 변수, 이 경우 무료 배송과 관련된 계수에 관심이 있습니다.\n\npropensity_score_regression &lt;- lm(\n  spend ~ age + gender + income + free_shipping,\n  data = matched_dataset\n)\n\nmodelsummary(propensity_score_regression)\n\n\n\n표 15.4: 시뮬레이션된 데이터를 사용한 처리 효과\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  3.862\n                \n                \n                  \n                  (0.506)\n                \n                \n                  age\n                  0.007\n                \n                \n                  \n                  (0.005)\n                \n                \n                  genderMale\n                  0.013\n                \n                \n                  \n                  (0.202)\n                \n                \n                  genderOther/decline\n                  -0.509\n                \n                \n                  \n                  (0.847)\n                \n                \n                  income\n                  0.001\n                \n                \n                  \n                  (0.000)\n                \n                \n                  free_shipping1\n                  10.073\n                \n                \n                  \n                  (0.180)\n                \n                \n                  Num.Obs.\n                  508\n                \n                \n                  R2\n                  0.983\n                \n                \n                  R2 Adj.\n                  0.983\n                \n                \n                  AIC\n                  2167.6\n                \n                \n                  BIC\n                  2197.2\n                \n                \n                  Log.Lik.\n                  -1076.811\n                \n                \n                  F\n                  5911.747\n                \n                \n                  RMSE\n                  2.02\n                \n        \n      \n    \n\n\n\n\n\n\n?tbl-treatedexample에서는 일치된 표본만을 기반으로 하여, 우리가 시뮬레이션한 효과를 발견합니다. 즉, 무료 배송을 받은 사람과 그렇지 않은 사람의 평균 지출 사이에 10의 차이가 있습니다. 이는 전체 표본을 기반으로 한 ?tbl-heyheybigspender와 대조됩니다.\n성향 점수 매칭은 널리 사용되기 때문에 다룹니다. 그러나 절충안이 있습니다. 사용될 때 투명성이 필요합니다 (Greifer 2021). 이러한 우려 사항은 다음과 같습니다 (King 와/과 Nielsen 2019):\n\n관찰 불가능한 것. 성향 점수 매칭은 관찰되지 않은 변수에 대해 매칭할 수 없습니다. 이것은 교실 환경에서는 괜찮을 수 있지만, 더 현실적인 환경에서는 문제를 일으킬 가능성이 높습니다. 차이를 일으키는 관찰되지 않은 것이 없다면, 그렇게 유사해 보이는 개인이 왜 다른 치료를 받았는지 이해하기 어렵습니다. 성향 점수 매칭은 이것들을 설명할 수 없으므로, 어떤 특징이 실제로 함께 모이는지 알기 어렵습니다.\n모델링. 성향 점수 매칭의 결과는 사용되는 모델에 따라 달라지는 경향이 있습니다. 어떤 모델을 사용할지에 대한 상당한 유연성이 있으므로, 연구자들은 자신에게 맞는 것을 찾기 위해 매치를 선택할 수 있습니다. 또한, 두 회귀 단계(매칭 및 분석)가 별도로 수행되기 때문에 불확실성이 전파되지 않습니다.\n\n관찰 불가능한 것의 근본적인 문제는 그것이 관찰되지 않은 데이터를 요구하기 때문에 결코 중요하지 않다고 보여줄 수 없습니다. 성향 점수 매칭 및 기타 매칭 방법을 사용하려는 사람들은 그것이 적절하다고 설득력 있게 주장할 수 있어야 합니다. (mckenzieforthedefence는?) 예를 들어, 용량 제한이 있을 때 이것이 가능한 몇 가지 사례를 제시합니다. 이 책의 공통된 주제와 마찬가지로, 그러한 사례는 데이터에 집중하고 그것을 생산한 상황에 대한 깊은 이해를 요구할 것입니다.",
    "crumbs": [
      "적용",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>관찰 데이터로부터의 인과성</span>"
    ]
  },
  {
    "objectID": "14-causality_from_obs_ko.html#회귀-불연속성-설계",
    "href": "14-causality_from_obs_ko.html#회귀-불연속성-설계",
    "title": "15  관찰 데이터로부터의 인과성",
    "section": "15.5 회귀 불연속성 설계",
    "text": "15.5 회귀 불연속성 설계\n회귀 불연속성 설계(RDD)는 (thistlethwaite1960regression에?) 의해 확립되었으며, 처리를 결정하는 컷오프가 있는 연속 변수가 있을 때 인과 관계를 얻는 인기 있는 방법입니다. 79%를 받은 학생과 80%를 받은 학생 사이에 차이가 있습니까? 아마도 많지는 않겠지만, 한 명은 A-를 받고 다른 한 명은 B+를 받을 수 있습니다. 성적표에서 그것을 보는 것은 누가 직업을 얻고 소득에 영향을 미칠 수 있는지에 영향을 미칠 수 있습니다. 이 경우, 백분율은 “강제 변수” 또는 “강제 함수”이고 A-에 대한 컷오프는 “임계값”입니다. 처리가 강제 변수에 의해 결정되므로 해당 변수를 제어해야 합니다. 이러한 임의적으로 보이는 컷오프는 항상 볼 수 있습니다. 따라서 RDD를 사용하는 많은 작업이 있었습니다.\nRDD와 관련하여 때때로 약간 다른 용어가 사용됩니다. 예를 들어, (Cunningham2021은?) 강제 함수를 실행 변수라고 합니다. 사용되는 정확한 용어는 일관되게 사용하는 한 중요하지 않습니다.\n\n15.5.1 시뮬레이션 예시: 소득 및 성적\n상황에 대해 더 구체적으로 설명하기 위해, 데이터를 시뮬레이션합니다. 우리는 소득과 성적 간의 관계를 고려하고, 학생이 80점 이상을 받으면 변화가 있다고 시뮬레이션할 것입니다(그림 15.6).\n\nset.seed(853)\n\nnum_observations &lt;- 1000\n\nrdd_example_data &lt;- tibble(\n  person = c(1:num_observations),\n  mark = runif(num_observations, min = 78, max = 82),\n  income = rnorm(num_observations, 10, 1)\n)\n\n## 점수가 80점 이상이면 소득이 더 높을 가능성이 있도록 만듭니다.\nrdd_example_data &lt;-\n  rdd_example_data |&gt;\n  mutate(\n    noise = rnorm(n = num_observations, mean = 2, sd = 1),\n    income = if_else(mark &gt;= 80, income + noise, income)\n  )\n\nrdd_example_data\n\n# A tibble: 1,000 × 4\n   person  mark income noise\n    &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1      1  79.4   9.43 1.87 \n 2      2  78.5   9.69 2.26 \n 3      3  79.9  10.8  1.14 \n 4      4  79.3   9.34 2.50 \n 5      5  78.1  10.7  2.21 \n 6      6  79.6   9.83 2.47 \n 7      7  78.5   8.96 4.22 \n 8      8  79.0  10.5  3.11 \n 9      9  78.6   9.53 0.671\n10     10  78.8  10.6  2.46 \n# ℹ 990 more rows\n\n\n\nrdd_example_data |&gt;\n  ggplot(aes(\n    x = mark,\n    y = income\n  )) +\n  geom_point(alpha = 0.2) +\n  geom_smooth(\n    data = rdd_example_data |&gt; filter(mark &lt; 80),\n    method = \"lm\",\n    color = \"black\",\n    formula = \"y ~ x\"\n  ) +\n  geom_smooth(\n    data = rdd_example_data |&gt; filter(mark &gt;= 80),\n    method = \"lm\",\n    color = \"black\",\n    formula = \"y ~ x\"\n  ) +\n  theme_minimal() +\n  labs(\n    x = \"Mark\",\n    y = \"Income ($)\"\n  )\n\n\n\n\n\n\n\n그림 15.6: 79점과 비교하여 80점을 받은 것이 소득에 미치는 영향을 보여주는 시뮬레이션된 데이터의 그림\n\n\n\n\n\n이진 변수와 선형 회귀를 사용하여 80점 이상의 점수를 받은 것이 소득에 미치는 영향을 추정할 수 있습니다. 우리는 계수가 약 2일 것으로 예상하며, 이것이 우리가 시뮬레이션한 것이고, 우리가 발견한 것입니다(표 15.5).\n\nrdd_example_data &lt;-\n  rdd_example_data |&gt;\n  mutate(mark_80_and_over = if_else(mark &lt; 80, 0, 1))\n\nrdd_example &lt;-\n  stan_glm(\n    formula = income ~ mark + mark_80_and_over,\n    data = rdd_example_data,\n    family = gaussian(),\n    prior = normal(location = 0, scale = 2.5, autoscale = TRUE),\n    prior_intercept = normal(0, 2.5, autoscale = TRUE),\n    prior_aux = exponential(rate = 1, autoscale = TRUE),\n    seed = 853\n  )\n\nsaveRDS(\n  rdd_example,\n  file = \"rdd_example.rds\"\n)\n\n\nrdd_example &lt;-\n  readRDS(file = \"rdd_example.rds\")\n\n\nmodelsummary(\n  models = rdd_example,\n  fmt = 2\n)\n\n\n\n표 15.5: 시뮬레이션된 데이터를 사용한 회귀 불연속성 예시\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  5.22\n                \n                \n                  mark\n                  0.06\n                \n                \n                  mark_80_and_over\n                  1.89\n                \n                \n                  Num.Obs.\n                  1000\n                \n                \n                  R2\n                  0.417\n                \n                \n                  R2 Adj.\n                  0.415\n                \n                \n                  Log.Lik.\n                  -1591.847\n                \n                \n                  ELPD\n                  -1595.1\n                \n                \n                  ELPD s.e.\n                  25.4\n                \n                \n                  LOOIC\n                  3190.3\n                \n                \n                  LOOIC s.e.\n                  50.9\n                \n                \n                  WAIC\n                  3190.3\n                \n                \n                  RMSE\n                  1.19\n                \n        \n      \n    \n\n\n\n\n\n\n이 추정치에는 논의할 다양한 주의 사항이 있지만, RDD의 필수 사항은 여기에 있습니다. 적절한 설정과 모델이 주어지면, RDD는 무작위 시험과 유리하게 비교될 수 있습니다 (Bloom, Bell, 와/과 Reiman 2020).\nrdrobust를 사용하여 RDD를 구현할 수도 있습니다. 이 접근 방식의 장점은 많은 일반적인 확장을 쉽게 사용할 수 있다는 것입니다.\n\nrdrobust(\n  y = rdd_example_data$income,\n  x = rdd_example_data$mark,\n  c = 80,\n  h = 2,\n  all = TRUE\n) |&gt;\n  summary()\n\nSharp RD estimates using local polynomial regression.\n\nNumber of Obs.                 1000\nBW type                      Manual\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                  497          503\nEff. Number of Obs.             497          503\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   2.000        2.000\nBW bias (b)                   2.000        2.000\nrho (h/b)                     1.000        1.000\nUnique Obs.                     497          503\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional     1.913     0.161    11.876     0.000     [1.597 , 2.229]     \nBias-Corrected     1.966     0.161    12.207     0.000     [1.650 , 2.282]     \n        Robust     1.966     0.232     8.461     0.000     [1.511 , 2.422]     \n=============================================================================\n\n\n\n\n15.5.2 가정\nRDD의 주요 가정은 (Cunningham 2021, p. 163)입니다.\n\n컷오프는 구체적이고, 고정되어 있으며, 모두에게 알려져 있습니다.\n강제 함수는 연속적입니다.\n\n첫 번째 가정은 대체로 컷오프를 조작할 수 없다는 것에 관한 것이며, 컷오프가 의미를 갖도록 보장합니다. 두 번째 가정은 임계값의 양쪽에 있는 사람들이 임계값의 양쪽에 우연히 떨어지는 것을 제외하고는 본질적으로 유사하다고 확신할 수 있게 해줍니다.\n?sec-hunt-data에서 무작위 통제 시험과 A/B 테스트를 논의했을 때, 치료의 무작위 할당은 대조군과 치료군이 치료를 제외하고는 동일하다는 것을 의미했습니다. 그런 다음 차이-안의-차이로 이동했고, 치료군과 대조군 사이에 공통된 추세가 있다고 가정했습니다. 우리는 그룹이 다를 수 있지만, 그들의 차이를 “차이로 빼낼” 수 있다고 허용했습니다. 마지막으로, 우리는 매칭을 고려했고, 대조군과 치료군이 다르게 보이더라도, 치료받지 않았다는 사실을 제외하고는 모든 면에서 그들과 같은 그룹과 어느 정도 일치시킬 수 있다고 말했습니다.\n회귀 불연속성에서는 약간 다른 설정을 고려합니다. 두 그룹은 강제 변수 측면에서 완전히 다릅니다. 그들은 임계값의 양쪽에 있습니다. 전혀 겹치지 않습니다. 그러나 우리는 임계값을 알고 있으며 양쪽에 있는 사람들이 본질적으로 일치한다고 믿습니다. 2019년 NBA 동부 컨퍼런스 준결승—토론토와 필라델피아를 생각해 봅시다.\n\n1차전: 랩터스 승리 108-95;\n2차전: 76ers 승리 94-89;\n3차전: 76ers 승리 116-95;\n4차전: 랩터스 승리 101-96;\n5차전: 랩터스 승리 125-89;\n6차전: 76ers 승리 112-101; 그리고 마지막으로,\n7차전: 랩터스 승리 92-90, 공이 림에 네 번 튕긴 후 들어갔기 때문입니다.\n\n팀들 사이에 정말 그렇게 큰 차이가 있었을까요?\n연속성 가정은 중요하지만, 반사실을 기반으로 하므로 테스트할 수 없습니다. 대신, 우리는 사람들을 설득해야 합니다. 이를 위한 방법은 다음과 같습니다.\n\n테스트/훈련 세트 설정 사용.\n다른 사양 시도. 선형 또는 2차 함수만으로 결과가 대체로 지속되지 않으면 특히 우려됩니다.\n데이터의 다른 하위 집합 고려.\n다른 창 고려, 이것은 우리가 컷오프의 양쪽을 얼마나 멀리 검사하는지에 대한 용어입니다.\n특히 그래프에서 불확실성 구간을 명확히 합니다.\n생략된 변수의 가능성에 대한 우려를 논의하고 완화합니다.\n\n임계값도 중요합니다. 예를 들어, 실제 이동이 있습니까, 아니면 비선형 관계가 있습니까?\nRDD에는 다음과 같은 다양한 약점이 있습니다.\n\n외부 타당성이 어려울 수 있습니다. 예를 들어, A-/B+ 예를 생각할 때, 그것들이 B-/C+ 학생들에게도 일반화될 것이라고 보기 어렵습니다.\n중요한 응답은 컷오프에 가까운 응답입니다. 이것은 A와 B 학생이 많더라도 별로 도움이 되지 않는다는 것을 의미합니다. 따라서 많은 데이터가 필요하거나 주장을 뒷받침할 능력에 대한 우려가 있을 수 있습니다 (Green 기타 2009).\n연구자로서, 우리는 다른 옵션을 구현할 많은 자유가 있습니다. 이것은 개방형 과학 모범 사례가 필수적이라는 것을 의미합니다.\n\n이 시점까지 우리는 “날카로운” RDD를 고려했습니다. 즉, 임계값이 엄격합니다. 그러나 실제로는 종종 경계가 약간 덜 엄격합니다. 날카로운 RDD 설정에서, 강제 함수의 값을 알면 결과를 알 수 있습니다. 예를 들어, 학생이 80점을 받으면 A-를 받았다는 것을 알지만, 79점을 받으면 B+를 받았다는 것을 압니다. 그러나 퍼지 RDD에서는 어떤 확률로만 알려져 있습니다.\n우리는 가능한 한 “날카로운” 효과를 원하지만, 임계값이 알려져 있으면 게임이 될 것입니다. 예를 들어, 사람들이 특정 마라톤 시간을 위해 달린다는 많은 증거가 있으며, 사람들이 특정 성적을 목표로 한다는 것을 알고 있습니다. 마찬가지로, 다른 쪽에서는 강사가 B를 정당화해야 하는 것보다 A를 그냥 주는 것이 훨씬 쉽습니다. 이것을 보는 한 가지 방법은 표본이 임계값의 양쪽에서 얼마나 “균형”을 이루고 있는지 고려하는 것입니다. 적절한 빈으로 히스토그램을 사용하여 이것을 할 수 있습니다. 예를 들어, ?sec-clean-and-prepare에서 정리된 케냐 인구 조사 데이터에서 발견한 연령 집중을 생각해 보십시오.\nRDD의 또 다른 핵심 요소는 모델 선택 결정의 가능한 효과입니다. 예를 들어, ?fig-rddissuperconcerning은 선형(그림 15.7 (a))과 다항식(그림 15.7 (b))의 차이를 보여줍니다.\nsome_data &lt;-\n  tibble(\n    outcome = rnorm(n = 100, mean = 1, sd = 1),\n    running_variable = c(1:100),\n    location = \"before\"\n  )\n\nsome_more_data &lt;-\n  tibble(\n    outcome = rnorm(n = 100, mean = 2, sd = 1),\n    running_variable = c(101:200),\n    location = \"after\"\n  )\n\nboth &lt;-\n  rbind(some_data, some_more_data)\n\nboth |&gt;\n  ggplot(aes(x = running_variable, y = outcome, color = location)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(formula = y ~ x, method = \"lm\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\nboth |&gt;\n  ggplot(aes(x = running_variable, y = outcome, color = location)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(formula = y ~ poly(x, 3), method = \"lm\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n(a) 선형\n\n\n\n\n\n\n\n\n\n\n\n(b) 다항식\n\n\n\n\n\n\n\n그림 15.7: 다른 함수로 동일한 상황을 고려한 결과 비교\n\n\n\n결과는 결과의 추정치가 모델의 선택에 따라 달라진다는 것입니다. 우리는 이 문제가 RDD에서 자주 발생한다는 것을 알 수 있으며 (Gelman 2019), 특히 고차 다항식을 사용하지 않고 대신 모델의 선택이 선형, 2차 또는 다른 부드러운 함수가 되도록 권장됩니다 (Gelman 와/과 Imbens 2019).\nRDD는 인기 있는 접근 방식이지만, 메타 분석에 따르면 표준 오차가 종종 부적절하게 작고, 이것이 가짜 결과를 초래할 수 있다고 합니다 (Stommes, Aronow, 와/과 Sävje 2023). RDD를 사용하는 경우, 소프트웨어 패키지에서 보고된 것보다 훨씬 더 넓은 표준 오차의 가능성과 이것이 결론에 미칠 영향에 대해 논의하는 것이 중요합니다.\n\n\n15.5.3 캘리포니아의 알코올 및 범죄\n회귀 불연속성 설계를 사용할 기회는 많습니다. 예를 들어, 한 후보가 간신히 이기는 선거에서 종종 볼 수 있습니다. (caugheysekhon2011은?) 1942년에서 2008년 사이의 미국 하원 선거를 조사하고, 간신히 이긴 사람과 간신히 진 사람 사이에 상당한 차이가 있음을 보여주었습니다. 그들은 회귀 불연속성의 장점 중 하나가 가정을 테스트할 수 있다는 사실을 강조합니다. 또 다른 일반적인 용도는 다소 임의적인 컷오프가 있을 때입니다. 예를 들어, 미국 대부분 지역에서 법적 음주 연령은 21세입니다. (carpenterdobkin은?) 캘리포니아에서 21세 전후인 사람들의 체포 및 기타 기록을 비교하여 알코올이 범죄에 미치는 가능한 영향을 고려합니다. 그들은 21세가 약간 넘은 사람들이 21세가 약간 안 된 사람들보다 체포될 가능성이 약간 더 높다는 것을 발견합니다. 우리는 캘리포니아의 범죄 맥락에서 (carpenterdobkin을?) 다시 방문할 것입니다.\n여기에서 복제 데이터 (Carpenter 와/과 Dobkin 2014)를 얻을 수 있습니다. (carpenterdobkin은?) 많은 변수를 고려하고 비율을 구성하며, 이 비율을 2주에 걸쳐 평균하지만, 단순화를 위해 폭행, 가중 폭행, 음주 운전 및 교통 위반과 같은 몇 가지 변수에 대한 숫자만 고려할 것입니다(그림 15.8).\n\ncarpenter_dobkin &lt;-\n  read_dta(\n    \"P01 Age Profile of Arrest Rates 1979-2006.dta\"\n  )\n\n\ncarpenter_dobkin_prepared &lt;-\n  carpenter_dobkin |&gt;\n  mutate(age = 21 + days_to_21 / 365) |&gt;\n  select(age, assault, aggravated_assault, dui, traffic_violations) |&gt;\n  pivot_longer(\n    cols = c(assault, aggravated_assault, dui, traffic_violations),\n    names_to = \"arrested_for\",\n    values_to = \"number\"\n  )\n\ncarpenter_dobkin_prepared |&gt;\n  mutate(\n    arrested_for =\n      case_when(\n        arrested_for == \"assault\" ~ \"Assault\",\n        arrested_for == \"aggravated_assault\" ~ \"Aggravated assault\",\n        arrested_for == \"dui\" ~ \"DUI\",\n        arrested_for == \"traffic_violations\" ~ \"Traffic violations\"\n      )\n  ) |&gt;\n  ggplot(aes(x = age, y = number)) +\n  geom_point(alpha = 0.05) +\n  facet_wrap(facets = vars(arrested_for), scales = \"free_y\") +\n  theme_minimal()\n\n\n\n\n\n\n\n그림 15.8: 선택된 이유에 대해 21세 전후의 체포 건수 비교\n\n\n\n\n\n\ncarpenter_dobkin_aggravated_assault_only &lt;-\n  carpenter_dobkin_prepared |&gt;\n  filter(\n    arrested_for == \"aggravated_assault\",\n    abs(age - 21) &lt; 2\n  ) |&gt;\n  mutate(is_21_or_more = if_else(age &lt; 21, 0, 1))\n\n\nrdd_carpenter_dobkin &lt;-\n  stan_glm(\n    formula = number ~ age + is_21_or_more,\n    data = carpenter_dobkin_aggravated_assault_only,\n    family = gaussian(),\n    prior = normal(location = 0, scale = 2.5, autoscale = TRUE),\n    prior_intercept = normal(0, 2.5, autoscale = TRUE),\n    prior_aux = exponential(rate = 1, autoscale = TRUE),\n    seed = 853\n  )\n\nsaveRDS(\n  rdd_example,\n  file = \"rdd_example.rds\"\n)\n\n\nrdd_carpenter_dobkin &lt;-\n  readRDS(file = \"rdd_carpenter_dobkin.rds\")\n\n\nmodelsummary(\n  models = rdd_carpenter_dobkin,\n  fmt = 2\n)\n\n\n\n표 15.6: 캘리포니아의 알코올과 범죄의 영향 검토\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  145.54\n                \n                \n                  age\n                  3.87\n                \n                \n                  is_21_or_more\n                  13.24\n                \n                \n                  Num.Obs.\n                  1459\n                \n                \n                  R2\n                  0.299\n                \n                \n                  R2 Adj.\n                  0.297\n                \n                \n                  Log.Lik.\n                  -6153.757\n                \n                \n                  ELPD\n                  -6157.3\n                \n                \n                  ELPD s.e.\n                  32.9\n                \n                \n                  LOOIC\n                  12314.6\n                \n                \n                  LOOIC s.e.\n                  65.7\n                \n                \n                  WAIC\n                  12314.6\n                \n                \n                  RMSE\n                  16.42\n                \n        \n      \n    \n\n\n\n\n\n\n그리고 rdrobust를 사용하면 결과가 비슷합니다.\n\nrdrobust(\n  y = carpenter_dobkin_aggravated_assault_only$number,\n  x = carpenter_dobkin_aggravated_assault_only$age,\n  c = 21,\n  h = 2,\n  all = TRUE\n) |&gt;\n  summary()\n\nSharp RD estimates using local polynomial regression.\n\nNumber of Obs.                 1459\nBW type                      Manual\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                  729          730\nEff. Number of Obs.             729          730\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   2.000        2.000\nBW bias (b)                   2.000        2.000\nrho (h/b)                     1.000        1.000\nUnique Obs.                     729          730\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional    14.126     1.918     7.364     0.000    [10.366 , 17.886]    \nBias-Corrected    16.708     1.918     8.709     0.000    [12.948 , 20.468]    \n        Robust    16.708     2.879     5.804     0.000    [11.066 , 22.350]    \n=============================================================================",
    "crumbs": [
      "적용",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>관찰 데이터로부터의 인과성</span>"
    ]
  },
  {
    "objectID": "14-causality_from_obs_ko.html#도구-변수",
    "href": "14-causality_from_obs_ko.html#도구-변수",
    "title": "15  관찰 데이터로부터의 인과성",
    "section": "15.6 도구 변수",
    "text": "15.6 도구 변수\n도구 변수(IV)는 어떤 유형의 처리 및 통제가 진행 중이지만, 다른 변수와 많은 상관 관계가 있고, 관심 있는 것을 실제로 측정하는 변수가 없을 수 있을 때 유용한 접근 방식입니다. 관찰 가능한 것을 조정하는 것만으로는 좋은 추정치를 만드는 데 충분하지 않을 것입니다. 대신, 우리는 다음과 같은 일부 변수, 즉 동명의 도구 변수를 찾습니다.\n\n처리 변수와 상관 관계가 있지만,\n결과와 상관 관계가 없습니다.\n\n이것은 도구 변수가 처리 변수를 통해서만 효과를 가질 수 있기 때문에 문제를 해결하며, 따라서 처리 변수의 효과에 대한 우리의 이해를 적절하게 조정할 수 있습니다. 절충안은 도구 변수가 여러 다른 가정을 충족해야 한다는 것이며, 솔직히 말해서, 사전에 식별하기 어렵다는 것입니다. 그럼에도 불구하고, 우리가 그것들을 사용할 수 있을 때, 그것들은 인과 관계에 대해 말하는 강력한 도구입니다.\n정규적인 도구 변수 예는 흡연입니다. 요즘 우리는 흡연이 암을 유발한다는 것을 알고 있습니다. 그러나 흡연은 교육과 같은 다른 많은 변수와 상관 관계가 있기 때문에, 실제로 암을 유발하는 것은 교육일 수 있습니다. RCT가 가능할 수 있지만, 속도와 윤리 측면에서 문제가 있을 가능성이 높으므로, 대신 우리는 흡연과 상관 관계가 있지만, 그 자체로는 폐암과 상관 관계가 없는 다른 변수를 찾습니다. 이 경우, 우리는 담배에 대한 세율 및 기타 정책 대응을 살펴봅니다. 담배에 대한 세율은 흡연되는 담배 수와 상관 관계가 있지만, 담배 흡연에 미치는 영향을 제외하고는 폐암과 상관 관계가 없으므로, 그것들을 통해 우리는 흡연된 담배가 폐암에 미치는 영향을 평가할 수 있습니다.\n도구 변수를 구현하기 위해, 우리는 먼저 담배 흡연에 대한 세율을 회귀하여 도구 변수에 대한 일부 계수를 얻은 다음, (별도의 회귀에서) 폐암에 대한 세율을 회귀하여 다시 도구 변수에 대한 일부 계수를 얻습니다. 그러면 우리의 추정치는 이러한 계수의 비율이며, 이는 “왈드 추정치”라고 설명됩니다 (Gelman 와/과 Hill 2007, p. 219).\n때로는 ?sec-hunt-data에서 소개된 오리건 건강 보험 실험과 같이 치료의 무작위 할당 맥락에서 도구 변수가 사용됩니다. 문제는 복권이 건강 보험에 신청하도록 할당된 개인을 선택하는 데 사용되었지만, 그들이 이것을 하도록 강요하는 것은 아무것도 없었다는 것입니다. 그러면 우리의 접근 방식은 선택되는 것과 건강 보험에 가입하는 것 사이의 관계를 고려한 다음, 다양한 건강 결과와 보험 가입 사이의 관계를 고려하는 것입니다. 비율이 될 우리의 도구 변수 추정치는 선택되었기 때문에 건강 보험에 가입한 사람들만 추정할 것입니다.\nGelman 와/과 Hill (2007, p. 216)의 언어에 따르면, 도구 변수를 사용할 때 우리는 다음을 포함한 다양한 가정을 합니다.\n\n도구의 무시 가능성.\n도구 변수와 처리 변수 간의 상관 관계.\n단조성.\n제외 제한.\n\n여담으로, 도구 변수의 역사는 흥미롭고, (stock2003retrospectives는?) (Cunningham2021을?) 통해 간략한 개요를 제공합니다. 이 방법은 (wright1928tariff에서?) 처음 발표되었습니다. 이것은 동물 및 식물성 기름에 대한 관세의 효과에 관한 책입니다. 동물 및 식물성 기름에 대한 관세에 관한 책에서 도구 변수가 왜 중요할까요? 근본적인 문제는 관세의 효과가 공급과 수요 모두에 달려 있다는 것입니다. 그러나 우리는 가격과 수량만 알고 있으므로, 무엇이 효과를 주도하는지 알 수 없습니다. 우리는 인과 관계를 파악하기 위해 도구 변수를 사용할 수 있습니다. 흥미로운 측면은 도구 변수 논의가 해당 책의 “부록 B”에만 있다는 것입니다. 주요 통계적 돌파구를 부록에 relegated하는 것은 이상하게 보일 것입니다. 또한, 책의 저자인 필립 G. 라이트에게는 통계 및 “부록 B”에서 사용된 특정 방법에 상당한 전문 지식을 가진 아들 시월 라이트가 있었습니다. 따라서 “부록 B”의 미스터리: 필립 또는 시월이 그것을 썼습니까? Cunningham (2021), Stock 와/과 Trebbi (2003) 및 (angristkrueger는?) 더 자세히 설명하지만, 대체로 필립이 작업을 저술했을 가능성이 높다고 생각합니다.\n\n15.6.1 시뮬레이션 예시: 건강 상태, 흡연 및 세율\n데이터를 생성해 봅시다. 우리는 건강 상태, 흡연 및 세율의 정규적인 예와 관련된 시뮬레이션을 탐색할 것입니다. 우리는 흡연량에 따라 누군가의 건강 상태를 설명하려고 하며, 흡연에 대한 세율을 통해 설명하려고 합니다. 우리는 주별로 다른 세율을 생성할 것입니다. 담배에 대한 세율은 현재 캐나다 주 전역에서 유사하지만, 이것은 상당히 최근의 일입니다. 앨버타는 낮은 세율을, 노바스코샤는 높은 세율을 가졌다고 가정해 봅시다.\n우리는 설명 목적으로 데이터를 시뮬레이션하고 있으므로, 원하는 답을 부과해야 합니다. 실제로 도구 변수를 사용할 때는 프로세스를 역으로 수행하게 됩니다.\n\nset.seed(853)\n\nnum_observations &lt;- 10000\n\niv_example_data &lt;- tibble(\n  person = c(1:num_observations),\n  smoker = \n    sample(x = c(0:1), size = num_observations, replace = TRUE)\n  )\n\n이제 누군가가 피운 담배 수와 건강을 관련시켜야 합니다. 우리는 건강 상태를 정규 분포에서 추출한 것으로 모델링할 것이며, 그 사람이 흡연하는지 여부에 따라 평균이 높거나 낮을 것입니다.\n\niv_example_data &lt;-\n  iv_example_data |&gt;\n  mutate(health = if_else(\n    smoker == 0,\n    rnorm(n = n(), mean = 1, sd = 1),\n    rnorm(n = n(), mean = 0, sd = 1)\n  ))\n\n이제 담배와 주 간의 관계가 필요합니다(이 그림에서는 주마다 세율이 다르기 때문입니다).\n\niv_example_data &lt;- iv_example_data |&gt;\n  mutate(\n    province = case_when(\n      smoker == 0 ~ sample(\n        c(\"Nova Scotia\", \"Alberta\"),\n        size = n(),\n        replace = TRUE,\n        prob = c(1/2, 1/2)\n      ),\n      smoker == 1 ~ sample(\n        c(\"Nova Scotia\", \"Alberta\"),\n        size = n(),\n        replace = TRUE,\n        prob = c(1/4, 3/4)\n      )\n    ),\n    tax = case_when(province == \"Alberta\" ~ 0.3, \n                    province == \"Nova Scotia\" ~ 0.5,\n                    TRUE ~ 9999999\n                    )\n    )\n\niv_example_data\n\n# A tibble: 10,000 × 5\n   person smoker  health province      tax\n    &lt;int&gt;  &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;\n 1      1      0  1.11   Alberta       0.3\n 2      2      1 -0.0831 Alberta       0.3\n 3      3      1 -0.0363 Alberta       0.3\n 4      4      0  2.48   Alberta       0.3\n 5      5      0  0.617  Nova Scotia   0.5\n 6      6      0  0.748  Alberta       0.3\n 7      7      0  0.499  Alberta       0.3\n 8      8      0  1.05   Nova Scotia   0.5\n 9      9      1  0.113  Alberta       0.3\n10     10      1 -0.0105 Alberta       0.3\n# ℹ 9,990 more rows\n\n\n이제 데이터를 볼 수 있습니다.\n\niv_example_data |&gt;\n  mutate(smoker = as_factor(smoker)) |&gt;\n  ggplot(aes(x = health, fill = smoker)) +\n  geom_histogram(position = \"dodge\", binwidth = 0.2) +\n  theme_minimal() +\n  labs(\n    x = \"Health rating\",\n    y = \"Number of people\",\n    fill = \"Smoker\"\n  ) +\n  scale_fill_brewer(palette = \"Set1\") +\n  facet_wrap(vars(province))\n\n\n\n\n\n\n\n\n마지막으로, 세율을 도구 변수로 사용하여 흡연이 건강에 미치는 영향을 추정할 수 있습니다.\n\nhealth_on_tax &lt;- lm(health ~ tax, data = iv_example_data)\nsmoker_on_tax &lt;- lm(smoker ~ tax, data = iv_example_data)\n\ntibble(\n  coefficient = c(\"health ~ tax\", \"smoker ~ tax\", \"ratio\"),\n  value = c(\n    coef(health_on_tax)[\"tax\"],\n    coef(smoker_on_tax)[\"tax\"],\n    coef(health_on_tax)[\"tax\"] / coef(smoker_on_tax)[\"tax\"]\n  )\n)\n\n# A tibble: 3 × 2\n  coefficient   value\n  &lt;chr&gt;         &lt;dbl&gt;\n1 health ~ tax  1.24 \n2 smoker ~ tax -1.27 \n3 ratio        -0.980\n\n\n흡연과 건강 모두에 대한 세율의 영향을 이해함으로써, 흡연하면 흡연하지 않는 것보다 건강이 나빠질 가능성이 높다는 것을 알 수 있습니다.\nestimatr의 iv_robust()를 사용하여 IV를 추정할 수 있습니다(표 15.7). 이것을 하는 한 가지 좋은 이유는 모든 것을 정리하고 표준 오차를 조정하는 데 도움이 될 수 있다는 것입니다.\n\niv_robust(health ~ smoker | tax, data = iv_example_data) |&gt;\n  modelsummary()\n\n\n\n표 15.7: 시뮬레이션된 데이터를 사용한 도구 변수 예시\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  0.977\n                \n                \n                  \n                  (0.041)\n                \n                \n                  smoker\n                  -0.980\n                \n                \n                  \n                  (0.081)\n                \n                \n                  Num.Obs.\n                  10000\n                \n                \n                  R2\n                  0.201\n                \n                \n                  R2 Adj.\n                  0.201\n                \n                \n                  AIC\n                  28342.1\n                \n                \n                  BIC\n                  28363.7\n                \n                \n                  RMSE\n                  1.00\n                \n        \n      \n    \n\n\n\n\n\n\n\n\n15.6.2 가정\n도구 변수의 설정은 ?fig-dot-taxrebateasiv-quarto에 설명되어 있으며, 이는 소득과 행복 간의 관계에 대한 교란 변수로서의 교육을 보여줍니다. 세금 환급은 소득에만 영향을 미치고 교육에는 영향을 미치지 않을 가능성이 높으며, 도구 변수로 사용될 수 있습니다.\n\ndigraph D {\n  \n  node [shape=plaintext, fontname = \"helvetica\"];\n  a [label = \"Income\"]\n  b [label = \"Happiness\"]\n  c [label = \"Education\"]\n  d [label = \"Tax rebate\"]\n  { rank=same a b};\n  \n  a-&gt;b\n  c-&gt;a\n  c-&gt;b\n  d-&gt;a\n}\n\n\n\n\n\n\n\nD\n\n\n\na\nIncome\n\n\n\nb\nHappiness\n\n\n\na-&gt;b\n\n\n\n\n\nc\nEducation\n\n\n\nc-&gt;a\n\n\n\n\n\nc-&gt;b\n\n\n\n\n\nd\nTax rebate\n\n\n\nd-&gt;a\n\n\n\n\n\n\n\n\n그림 15.9: 소득과 행복 간의 관계에 대한 교란 변수로서의 교육과 도구 변수로서의 세금 환급\n\n\n\n\n\n앞서 논의했듯이, 도구 변수를 사용할 때 다양한 가정이 있습니다. 가장 중요한 두 가지는 다음과 같습니다.\n\n제외 제한. 이 가정은 도구 변수가 관심 있는 예측 변수를 통해서만 결과 변수에 영향을 미친다는 것입니다.\n관련성. 도구 변수와 예측 변수 사이에 실제로 관계가 있어야 합니다.\n\n이 둘 사이에는 일반적으로 절충안이 있습니다. 하나를 만족시키는 변수는 많지만, 다른 하나를 만족시키지 않기 때문에 그렇습니다. Cunningham (2021, p. 211)은 좋은 도구의 한 가지 테스트는 사람들이 처음에 혼란스러워하다가 설명해주면 나중에야 당연하게 생각하는 것이라고 설명합니다.\n관련성은 회귀 및 기타 상관 관계 테스트를 사용하여 테스트할 수 있습니다. 제외 제한은 테스트할 수 없습니다. 우리는 증거와 설득력 있는 주장을 제시해야 합니다. 어려운 측면은 제외 제한의 의미이기 때문에 도구가 관련 없어 보여야 한다는 것입니다 (Cunningham 2021 p. 225).\n도구 변수는 명시적인 무작위화 없이도 인과적 추정치를 얻을 수 있기 때문에 유용한 접근 방식입니다. 도구 변수를 찾는 것은 특히 학계에서 흰 고래를 찾는 것과 같았습니다. 그러나 A/B 테스트의 하류에서 IV 접근 방식의 사용이 증가했습니다 (Taddy 2019, p. 162).\n오랫동안 정규적인 도구 변수는 강우량, 또는 더 일반적으로는 날씨였습니다. 그러나 문제는 도구 변수가 다른, 잠재적으로 관찰되지 않은 변수와 상관 관계가 있다면, 관심 있는 변수와 상관 관계가 있을 수 있다는 것입니다. 이것은 위의 성향 점수 매칭에 대한 비판과 유사합니다. (mellontakingourtoys는?) 도구 변수 논문에서 많은 변수가 날씨와 연결되어 있음을 발견했습니다. 일부에서 잘못 추정된 효과의 가능성이 상당히 높을 것으로 보입니다.\n도구 변수 접근 방식을 고려할 때, 이 두 가지 가정 모두에 상당한 시간을 할애해야 합니다. (mellontakingourtoys는?) 이 특정 세금 환급이 소득에만 영향을 미치고, 그 자체로 관심 있는 변수와 연결될 수 있는 다른 변수에는 영향을 미치지 않는다는 점에 특히 우려하고 있음을 보여줍니다. 도구 변수를 기반으로 한 접근 방식은 연구자에게 광범위한 자유를 제공하며, (brodeurcookheyes는?) RCT 및 RDD와 비교하여 p-해킹 및 선택 보고와 더 관련이 있음을 발견합니다. 다중 대체 및 성향 점수 매칭과 마찬가지로, IV를 사용할 때 주의를 기울이고, 결코 순진하게 사용해서는 안 된다고 권장합니다. 실제로, (betzcookhollenbach2018은?) 공간적 도구가 거의 유효하지 않다고까지 말합니다.",
    "crumbs": [
      "적용",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>관찰 데이터로부터의 인과성</span>"
    ]
  },
  {
    "objectID": "14-causality_from_obs_ko.html#연습-문제",
    "href": "14-causality_from_obs_ko.html#연습-문제",
    "title": "15  관찰 데이터로부터의 인과성",
    "section": "15.7 연습 문제",
    "text": "15.7 연습 문제\n\n연습\n\n(계획) 다음 시나리오를 고려하십시오: 두 아이는 구급차가 지나갈 때 모두 쳐다보지만, 나이가 많은 아이만 전차가 지나갈 때 쳐다보고, 어린 아이만 자전거가 지나갈 때 쳐다봅니다. 해당 데이터셋이 어떻게 생겼을지 스케치한 다음 모든 관측치를 보여주기 위해 만들 수 있는 그래프를 스케치하십시오.\n(시뮬레이션) 설명된 시나리오를 더 고려하고 상황을 시뮬레이션하십시오. 시뮬레이션된 데이터를 기반으로 최소 10개의 테스트를 포함하십시오.\n(수집) 이러한 데이터셋의 가능한 출처 하나를 설명하십시오.\n(탐색) 스케치한 그래프를 ggplot2를 사용하여 만드십시오. 그런 다음 rstanarm을 사용하여 모델을 만드십시오.\n(전달) 당신이 한 일에 대해 두 단락을 작성하십시오.\n\n\n\n퀴즈\n\n차이-안의-차이를 사용할 때 핵심 가정은 무엇입니까?\n(markupinsurance를?) 읽고 다음을 논의하십시오: i) 두 가지 통계적 측면; ii) 두 가지 윤리적 측면.\n(markupinsurance와?) 관련된 GitHub 페이지로 이동하십시오. 좋은 점 두 가지와 개선할 수 있는 다른 두 가지 점을 나열하십시오.\n회귀 불연속성 설계의 근본적인 특징은 무엇이며, 회귀 불연속성 설계를 사용할 수 있기 위해 필요한 조건은 무엇입니까?\n회귀 불연속성 설계 추정치의 타당성에 대한 몇 가지 위협은 무엇입니까?\n(Meng2021What에?) 따르면 “데이터 과학은 \\(\\dots\\)를 통해 설득할 수 있습니다”(해당하는 모든 항목 선택).\n\n공정한 고품질 데이터 수집에서 증거의 신중한 확립\n처리 및 분석\n결과의 정직한 해석 및 전달\n대규모 표본 크기\n\n(riedererdesignpatterns에?) 따르면, “날카로운 컷오프로 분할된 분리된 처리 및 비처리 그룹”이 있는 경우, 그룹 간의 접합점에서 국소 처리 효과를 측정하기 위해 어떤 방법을 사용해야 합니까(하나 선택)?\n\n회귀 불연속성\n매칭\n차이-안의-차이\n이벤트 연구 방법\n\n(riedererdesignpatterns에?) 따르면 인과 추론에 필요한 것은 무엇입니까(해당하는 모든 항목 선택)?\n\n데이터 관리\n도메인 지식\n확률적 추론\n\n토론토에 거주하는 두 자녀와 박사 학위를 가진 30-39세 호주 남성을 고려하십시오. 다음 중 어느 것과 가장 밀접하게 일치한다고 생각하며 그 이유는 무엇입니까(한두 단락으로 설명하십시오)?\n\n토론토에 거주하는 한 자녀와 학사 학위를 가진 30-39세 호주 남성\n토론토에 거주하는 한 자녀와 박사 학위를 가진 30-39세 캐나다 남성\n오타와에 거주하는 한 자녀와 박사 학위를 가진 30-39세 호주 남성\n토론토에 거주하는 한 자녀와 박사 학위를 가진 18-29세 캐나다 남성\n\n성향 점수 매칭이란 무엇입니까? 사람들을 매칭한다면, 어떤 특징을 매칭하고 싶습니까? 그러한 정보를 수집하고 저장하는 것은 어떤 윤리적 질문을 제기합니까? (각 질문에 대해 최소 한 단락씩 작성하십시오.)\n(bronnerftw에서?) 설명한 충돌 편향을 설명하는 DAG를 그리십시오.\n(NoiseKahneman은?) “\\(\\dots\\)상관 관계가 인과 관계를 의미하지는 않지만, 인과 관계는 상관 관계를 의미합니다. 인과 관계가 있는 곳에서는 상관 관계를 찾아야 합니다.”라고 말합니다. Cunningham (2021, 1장)을 참조하여, 그들이 맞습니까, 틀립니까? 그리고 그 이유는 무엇입니까?\n\n\n\n수업 활동\n\n“아빠, 나 면도해요.” 세 살짜리가 아버지 옆에 서 있습니다. 아버지는 면도를 하고, 수염이 없습니다. 세 살짜리는 “면도”를 하고, 수염이 없습니다. DAG를 사용하여 상황과 누락되었을 수 있는 것을 설명하십시오.\n다음 상황을 고려하십시오: “병원에 두 명의 외과 의사가 있습니다. 한 명은 훌륭하다고 여겨져서, 분류는 평균적으로 어려운 사례를 그에게 보내는 경향이 있으므로, 그의 환자 중 많은 수가 사망합니다. 다른 외과 의사는 좋다고 여겨져서, 분류는 평균적으로 중간 정도의 사례를 그에게 보내는 경향이 있으므로, 그의 환자 중 평균적인 수가 사망합니다.” 환자 이름, 외과 의사 이름 및 환자 결과만 포함하여 상황을 시뮬레이션하십시오. 회귀를 실행하십시오. 어떤 결론을 내립니까? 그런 다음 해당 회귀에서 누락된 데이터를 지정하는 DAG를 그리십시오. 해당 변수를 시뮬레이션에 추가하고 회귀를 다시 실행하십시오. 결론이 어떻게 다릅니까?\n심슨의 역설 또는 버크슨의 역설 중 하나를 선택한 다음, 예를 생각하고 그것을 조정하기 위한 분석적 접근 방식을 논의하십시오.\n\n\n\n과제\n당신은 사람들의 친구 그룹의 특징과 그러한 특징이 개인 수준의 결과, 특히 경제적 측정과 어떻게 관련되는지에 관심이 있습니다.\n소셜 미디어 웹사이트의 개인 수준 데이터에 액세스할 수 있으며, 여기에는 웹사이트에서의 사회적 상호 작용(게시물에 대한 댓글, 태그 등)에 대한 정보와 다양한 개인 수준의 특징이 포함되어 있습니다.\n\n소셜 미디어 웹사이트는 매우 인기가 있지만, 관심 있는 인구의 모든 사람이 계정을 가지고 있는 것은 아니며, 계정을 가진 모든 사람이 웹사이트에서 활동적인 것은 아닙니다. 경제적 측정에 관심이 있다는 점을 감안할 때, 이러한 데이터를 사용하여 더 넓은 인구에 대한 추론을 하는 데 어떤 가능한 문제가 있습니까?\n데이터에는 개인 수준의 소득에 대한 정보가 포함되어 있지 않습니다. 그러나 표본의 약 20%에 대해 개인의 “인구 조사 블록”에 대한 정보가 있습니다. 배경 정보로, 인구 조사 블록에는 3,000명 이하의 개인이 포함됩니다. 각 인구 조사 블록의 중앙값 소득은 알려져 있습니다. 따라서 다음과 같이 개인 수준의 소득을 추정하기로 결정합니다.\n\n각 인구 조사 블록의 중앙값 소득을 일련의 개인 수준 특징(예: 연령, 교육, 결혼 상태, 성별, \\(\\dots\\))에 대해 회귀합니다.\n이러한 추정치를 사용하여 위치 정보가 없는 개인의 소득을 예측합니다. 이 접근 방식의 장단점, 특히 친구 그룹의 소득 특징 연구에 어떤 영향을 미칠 수 있는지 간략하게 논의하십시오. 생태학적 오류를 반드시 다루십시오.\n\n당연히, 소셜 미디어 웹사이트는 개인 수준 데이터의 무제한 배포를 허용하지 않을 것입니다. 그럼에도 불구하고 작업의 재현성을 향상시킬 수 있는 몇 가지 방법은 무엇입니까?\n\n이것은 최소 두 페이지가 걸립니다.\n\n\n\n\nAngelucci, Charles, 와/과 Julia Cagé. 2019. “Newspapers in times of low advertising revenues”. American Economic Journal: Microeconomics 11 (3): 319–64. https://doi.org/10.1257/mic.20170306.\n\n\nArel-Bundock, Vincent. 2022. “modelsummary: Data and Model Summaries in R”. Journal of Statistical Software 103 (1): 1–23. https://doi.org/10.18637/jss.v103.i01.\n\n\n———. 2024. tinytable: Simple and Configurable Tables in “HTML”, “LaTeX”, “Markdown”, “Word”, “PNG”, “PDF”, and “Typst” Formats. https://vincentarelbundock.github.io/tinytable/.\n\n\nBerkson, Joseph. 1946. “Limitations of the Application of Fourfold Table Analysis to Hospital Data”. Biometrics Bulletin 2 (3): 47–53. https://doi.org/10.2307/3002000.\n\n\nBickel, Peter, Eugene Hammel, 와/과 William O’Connell. 1975. “Sex Bias in Graduate Admissions: Data from Berkeley: Measuring bias is harder than is usually assumed, and the evidence is sometimes contrary to expectation”. Science 187 (4175): 398–404. https://doi.org/10.1126/science.187.4175.398.\n\n\nBlair, Graeme, Jasper Cooper, Alexander Coppock, Macartan Humphreys, 와/과 Luke Sonnet. 2021. estimatr: Fast Estimators for Design-Based Inference. https://CRAN.R-project.org/package=estimatr.\n\n\nBloom, Howard, Andrew Bell, 와/과 Kayla Reiman. 2020. “Using Data from Randomized Trials to Assess the Likely Generalizability of Educational Treatment-Effect Estimates from Regression Discontinuity Designs”. Journal of Research on Educational Effectiveness 13 (3): 488–517. https://doi.org/10.1080/19345747.2019.1634169.\n\n\nBolker, Ben, 와/과 David Robinson. 2022. broom.mixed: Tidying Methods for Mixed Models. https://CRAN.R-project.org/package=broom.mixed.\n\n\nCalonico, Sebastian, Matias Cattaneo, Max Farrell, 와/과 Rocio Titiunik. 2021. rdrobust: Robust Data-Driven Statistical Inference in Regression-Discontinuity Designs. https://CRAN.R-project.org/package=rdrobust.\n\n\nCarpenter, Christopher, 와/과 Carlos Dobkin. 2014. “Replication data for: The Minimum Legal Drinking Age and Crime”. https://doi.org/10.7910/DVN/27070.\n\n\nCoyle, Edward, Andrew Coggan, Mari Hopper, 와/과 Thomas Walters. 1988. “Determinants of Endurance in Well-Trained Cyclists”. Journal of Applied Physiology 64 (6): 2622–30. https://doi.org/10.1152/jappl.1988.64.6.2622.\n\n\nCunningham, Scott. 2021. Causal Inference: The Mixtape. 1st ed. New Haven: Yale Press. https://mixtape.scunning.com.\n\n\nDagan, Noa, Noam Barda, Eldad Kepten, Oren Miron, Shay Perchik, Mark Katz, Miguel Hernán, Marc Lipsitch, Ben Reis, 와/과 Ran Balicer. 2021. “BNT162b2 mRNA Covid-19 Vaccine in a Nationwide Mass Vaccination Setting”. New England Journal of Medicine 384 (15): 1412–23. https://doi.org/10.1056/NEJMoa2101765.\n\n\nGelman, Andrew. 2019. “Another Regression Discontinuity Disaster and what can we learn from it”, 6월. https://statmodeling.stat.columbia.edu/2019/06/25/another-regression-discontinuity-disaster-and-what-can-we-learn-from-it/.\n\n\nGelman, Andrew, 와/과 Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Models. 1st ed. Cambridge University Press.\n\n\nGelman, Andrew, 와/과 Guido Imbens. 2019. “Why High-Order Polynomials Should Not Be Used in Regression Discontinuity Designs”. Journal of Business & Economic Statistics 37 (3): 447–56. https://doi.org/10.1080/07350015.2017.1366909.\n\n\nGoodrich, Ben, Jonah Gabry, Imad Ali, 와/과 Sam Brilleman. 2023. “rstanarm: Bayesian applied regression modeling via Stan”. https://mc-stan.org/rstanarm.\n\n\nGreen, Donald, Terence Leong, Holger Kern, Alan Gerber, 와/과 Christopher Larimer. 2009. “Testing the accuracy of regression discontinuity analysis using experimental benchmarks”. Political Analysis 17 (4): 400–417. https://doi.org/10.1093/pan/mpp018.\n\n\nGreifer, Noah. 2021. “Why do we do matching for causal inference vs regressing on confounders?” Cross Validated, 9월. https://stats.stackexchange.com/q/544958.\n\n\nHo, Daniel, Kosuke Imai, Gary King, 와/과 Elizabeth Stuart. 2011. “MatchIt: Nonparametric Preprocessing for Parametric Causal Inference”. Journal of Statistical Software 42 (8): 1–28. https://doi.org/10.18637/jss.v042.i08.\n\n\nHorst, Allison Marie, Alison Presmanes Hill, 와/과 Kristen Gorman. 2020. palmerpenguins: Palmer Archipelago (Antarctica) penguin data. https://doi.org/10.5281/zenodo.3960218.\n\n\nHuntington-Klein, Nick. 2021. The Effect: An Introduction to Research Design and Causality. 1st ed. Chapman & Hall. https://theeffectbook.net.\n\n\nKing, Gary, 와/과 Richard Nielsen. 2019. “Why Propensity Scores Should Not Be Used for Matching”. Political Analysis 27 (4): 435–54. https://doi.org/10.1017/pan.2019.11.\n\n\nPodlogar, Tim, Peter Leo, 와/과 James Spragg. 2022. “Using VO2max as a marker of training status in athletes—Can we do better?” Journal of Applied Physiology 133 (6): 144–47. https://doi.org/10.1152/japplphysiol.00723.2021.\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nRiederer, Emily. 2021. “Causal design patterns for data analysts”, 1월. https://emilyriederer.netlify.app/post/causal-design-patterns/.\n\n\nRobinson, David, Alex Hayes, 와/과 Simon Couch. 2022. broom: Convert Statistical Objects into Tidy Tibbles. https://CRAN.R-project.org/package=broom.\n\n\nSekhon, Jasjeet, 와/과 Rocío Titiunik. 2017. “Understanding regression discontinuity designs as observational studies”. Observational Studies 3 (2): 174–82. https://doi.org/10.1353/obs.2017.0005.\n\n\nSimpson, Edward. 1951. “The interpretation of interaction in contingency tables”. Journal of the Royal Statistical Society: Series B (Methodological) 13 (2): 238–41. https://doi.org/10.1111/j.2517-6161.1951.tb00088.x.\n\n\nStock, James, 와/과 Francesco Trebbi. 2003. “Retrospectives: Who invented instrumental variable regression?” Journal of Economic Perspectives 17 (3): 177–94. https://doi.org/10.1257/089533003769204416.\n\n\nStommes, Drew, P. M. Aronow, 와/과 Fredrik Sävje. 2023. “On the reliability of published findings using the regression discontinuity design in political science”. Research & Politics 10 (2). https://doi.org/https://doi.org/10.1177/2053168023116645.\n\n\nTaddy, Matt. 2019. Business Data Science. 1st ed. McGraw Hill.\n\n\nWickham, Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, 기타. 2019. “Welcome to the Tidyverse”. Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Evan Miller, 와/과 Danny Smith. 2023. haven: Import and Export “SPSS” “Stata” and “SAS” Files. https://CRAN.R-project.org/package=haven.\n\n\nWickham, Hadley, 와/과 Dana Seidel. 2022. scales: Scale Functions for Visualization. https://CRAN.R-project.org/package=scales.",
    "crumbs": [
      "적용",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>관찰 데이터로부터의 인과성</span>"
    ]
  },
  {
    "objectID": "15-mrp_ko.html",
    "href": "15-mrp_ko.html",
    "title": "16  다단계 회귀와 사후 층화",
    "section": "",
    "text": "17 INTERNAL\n선수 지식\n주요 개념 및 기술\n소프트웨어 및 패키지\n```{ r} #| message: false #| warning: false\nlibrary(arrow) library(broom.mixed) library(gutenbergr) library(haven) library(labelled) library(modelsummary) library(rstanarm) library(tidybayes) library(tidyverse) library(tinytable)\n그룹별 개수는 상당히 유사하다는 것을 알 수 있습니다(?tbl-teapreferencecounts).\n```{ r} #| message: false #| warning: false #| label: tbl-teapreferencecounts #| tbl-cap: “연령 및 국적별 차 선호도”\nsim_population |&gt; count(age, nationality, prefers_tea) |&gt; tt() |&gt; style_tt(j = 1:4, align = “lllr”) |&gt; format_tt(digits = 0, num_mark_big = “,”, num_fmt = “decimal”) |&gt; setNames( c(“Age”, “Nationality”, “Prefers tea”, “Number”))\n```{ r} #| message: false #| warning: false #| label: tbl-teapreferencesamplecounts #| tbl-cap: “차를 좋아하는 사람들을 과도하게 표본 추출한, 연령 및 국적별 차 선호도에 대한 편향된 표본”\ntea_sample |&gt; count(age, nationality, prefers_tea) |&gt; tt() |&gt; style_tt(j = 1:4, align = “lllr”) |&gt; format_tt(digits = 0, num_mark_big = “,”, num_fmt = “decimal”) |&gt; setNames(c(“Age”, “Nationality”, “Prefers tea”, “Number”))\n```{ r} #| echo: false #| eval: false #| message: false #| warning: false\nsaveRDS( tea_preference_model, file = “outputs/model/tea_preference_model.rds” )\n```{ r} #| eval: true #| echo: false #| warning: false #| message: false\ntea_preference_model &lt;- readRDS(file = “outputs/model/tea_preference_model.rds”)\n?fig-teamodelresultsplots는 각 다른 그룹에 대한 추출 분포를 보여줍니다.\n```{ r} #| echo: true #| eval: true #| message: false #| warning: false #| label: fig-teamodelresultsplots #| fig-cap: “각 그룹에 대한 추출 분포 검토”\ntea_preference_model |&gt; spread_draws((Intercept), b[, group]) |&gt; mutate(condition_mean = (Intercept) + b) |&gt; ggplot(aes(y = group, x = condition_mean)) + stat_halfeye() + theme_minimal()\n이것은 사후 층화 데이터셋에 개인 수준의 데이터가 있다고 가정하는 이상적인 예입니다. 그 세계에서는 각 개인에게 모델을 적용할 수 있습니다.\n```{ r} predicted_tea_preference &lt;- tea_preference_model |&gt; add_epred_draws(newdata = tea_poststrat_dataset, value = “preference”) |&gt; ungroup() |&gt; summarise( average_preference = mean(preference), lower = quantile(preference, 0.025), upper = quantile(preference, 0.975), .by = c(age, nationality, .row) )\npredicted_tea_preference |&gt; count(age, nationality, average_preference)\n이 경우, MRP 접근 방식은 편향된 표본을 취하여 진실을 반영하는 차 선호도 추정치를 산출하는 데 좋은 역할을 했습니다.",
    "crumbs": [
      "적용",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>다단계 회귀와 사후 층화</span>"
    ]
  },
  {
    "objectID": "15-mrp_ko.html#년-미국-선거-예측",
    "href": "15-mrp_ko.html#년-미국-선거-예측",
    "title": "16  다단계 회귀와 사후 층화",
    "section": "17.1 2020년 미국 선거 예측",
    "text": "17.1 2020년 미국 선거 예측\n미국의 대통령 선거는 미국에 고유한 많은 특징을 가지고 있지만, 여기서 우리가 만들 모델은 다양한 환경에 일반화될 수 있을 것입니다. 우리는 ?sec-hunt-data에서 소개된 민주주의 기금 유권자 연구 그룹의 설문 조사 데이터를 사용할 것입니다. 그들은 미국 선거를 앞두고 여론 조사를 실시했으며, 등록 후 이를 공개합니다. 우리는 ?sec-farm-data에서 소개된 IPUMS를 사용하여 2019년 미국 지역사회 조사(ACS)를 사후 층화 데이터셋으로 사용할 것입니다. 우리는 주, 연령 그룹, 성별 및 교육을 설명 변수로 사용할 것입니다.\n\n17.1.1 설문 조사 데이터\n민주주의 기금 유권자 연구 그룹 네이션스케이프 설문 조사 데이터셋을 사용할 것입니다. MRP의 한 가지 까다로운 측면은 설문 조사 데이터셋과 사후 층화 데이터셋 간의 일관성을 보장하는 것입니다. 이 경우, ?sec-hunt-data에서 정리한 데이터셋을 읽어들인 후, 변수를 일관성 있게 만들기 위해 약간의 작업을 해야 합니다.\n```{ r} #| eval: false #| include: true\nnationscape_data &lt;- read_csv(file = “nationscape_data.csv”)\n\n\n```{\nr}\n#| eval: true\n#| include: false\n\nnationscape_data &lt;- \n  read_parquet(file = \"outputs/data/15-nationscape_data_cleaned.parquet\")\n{ r} nationscape_data\n```{ r} # IPUMS와 일치하도록 주 이름 서식 지정 states_names_and_abbrevs &lt;- tibble(stateicp = state.name, state = state.abb)\nnationscape_data &lt;- nationscape_data |&gt; left_join(states_names_and_abbrevs, by = “state”)\nrm(states_names_and_abbrevs)",
    "crumbs": [
      "적용",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>다단계 회귀와 사후 층화</span>"
    ]
  },
  {
    "objectID": "15-mrp_ko.html#연습-문제",
    "href": "15-mrp_ko.html#연습-문제",
    "title": "16  다단계 회귀와 사후 층화",
    "section": "21.1 연습 문제",
    "text": "21.1 연습 문제\n\n연습\n\n(계획) 다음 시나리오를 고려하십시오: 정당에 대한 지지는 이진(예/아니오)이며, 연령 그룹, 성별, 소득 그룹 및 최고 학력과 관련이 있습니다. 해당 데이터셋이 어떻게 생겼을지 스케치한 다음 모든 관측치를 보여주기 위해 만들 수 있는 그래프를 스케치하십시오.\n(시뮬레이션) 설명된 시나리오를 더 고려하고 상황을 시뮬레이션하십시오. 시뮬레이션된 데이터를 기반으로 최소 10개의 테스트를 포함하십시오.\n(수집) 이러한 데이터셋의 가능한 출처 하나를 설명하십시오.\n(탐색) 스케치한 그래프를 ggplot2를 사용하여 만드십시오. rstanarm을 사용하여 모델을 추정하십시오.\n(전달) 당신이 한 일에 대해 두 단락을 작성하십시오.\n\n\n\n퀴즈\n\nMRP가 무엇인지, 그리고 장단점을 설명하고, 사용하는 기술 용어를 설명하십시오(최소 세 단락 작성).\n(wang2015forecasting에?) 대해 좋아하는 점 세 가지는 무엇입니까? 좋아하지 않는 점 세 가지는 무엇입니까? 논문을 어느 정도 재현할 수 있습니까?\n(wang2015forecasting과?) 관련하여, MRP의 약점은 무엇입니까(하나 선택)?\n\n상세한 데이터 요구 사항.\n편향된 데이터 사용 허용.\n수행 비용이 비쌉니다.\n\n(wang2015forecasting과?) 관련하여, Xbox 표본에 대해 우려되는 점은 무엇입니까(하나 선택)?\n\n비대표적.\n작은 표본 크기.\n동일한 응답자의 여러 응답.\n\n다가오는 미국 대통령 선거에서 투표 의향이 개인의 소득에 따라 어떻게 다른지 연구하는 데 관심이 있습니다. 우리는 이 관계를 연구하기 위해 로지스틱 회귀 모델을 설정합니다. 이 연구에서 가능한 예측 변수는 무엇입니까(해당하는 모든 항목 선택)?\n\n응답자가 투표 등록을 했는지 여부(예/아니오).\n응답자가 민주당 후보에게 투표할 것인지 여부(예/아니오).\n응답자의 인종(백인/비백인).\n응답자의 결혼 상태(기혼/미혼).\n\n(cohn2016에?) 대해 생각해 보십시오. 왜 이런 종류의 연습이 더 많이 수행되지 않습니까? 동일한 배경과 정량적 정교함 수준을 가진 다른 그룹이 동일한 데이터를 사용할 때에도 왜 그렇게 다른 추정치를 가질 수 있다고 생각하십니까?\n설문 조사를 기반으로 모델을 훈련시킨 다음, ACS 데이터셋을 사용하여 사후 층화합니다. 이 작업을 수행할 때 우리가 직면할 수 있는 몇 가지 실제적인 고려 사항은 무엇입니까?\n다음 연령 그룹이 있는 설문 조사 데이터셋이 있다고 가정합니다: 18-29; 30-44; 45-60; 60+. 그리고 다음 연령 그룹이 있는 사후 층화 데이터셋이 있습니다: 18-34; 35-49; 50-64; 65+. 이것들을 하나로 모으기 위해 취할 접근 방식에 대해 작성하십시오.\n\n\n\n수업 활동\n\n논문 검토: (wang2015forecasting을?) 읽고 검토하십시오.\n\n\n\n과제\n(ghitza2020voter와?) 유사한 방식으로, 민간 기업의 미국 유권자 파일 기록에 액세스할 수 있다고 가정하십시오. 2020년 미국 협력 선거 연구에 대한 모델을 훈련시키고, 해당 유권자 파일을 기반으로 개인별로 사후 층화합니다.\n\n(gebru2021datasheets에?) 따라 유권자 파일 데이터셋에 대한 데이터시트를 작성하십시오. 다시 말하지만, 데이터시트는 데이터셋과 함께 제공되며 “동기, 구성, 수집 과정, 권장 사용” 등을 문서화합니다.\n(Mitchell_2019에?) 따라 모델에 대한 모델 카드를 만드십시오. 다시 말하지만, 모델 카드는 “모델 세부 정보, 의도된 사용, 메트릭, 훈련 데이터, 윤리적 고려 사항, 그리고 주의 사항 및 권장 사항”과 같은 측면을 보고하는 의도적으로 간단한 한두 페이지 문서입니다 (Mitchell 기타 2019).\n모델에서 사용하는 특징과 관련된 세 가지 윤리적 측면을 논의하십시오. [각 요점에 대해 한두 단락을 작성하십시오.]\n데이터셋, 모델 및 예측 측면에서 마련할 테스트를 자세히 설명하십시오.\n\n\n\n논문\n이 시점에서 ?sec-papers의 Spofforth 논문이 적절할 것입니다.\n\n\n\n\nAlexander, Monica. 2019. “Analyzing name changes after marriage using a non-representative survey”, 8월. https://www.monicaalexander.com/posts/2019-08-07-mrp/.\n\n\nArel-Bundock, Vincent. 2022. “modelsummary: Data and Model Summaries in R”. Journal of Statistical Software 103 (1): 1–23. https://doi.org/10.18637/jss.v103.i01.\n\n\n———. 2024. tinytable: Simple and Configurable Tables in “HTML”, “LaTeX”, “Markdown”, “Word”, “PNG”, “PDF”, and “Typst” Formats. https://vincentarelbundock.github.io/tinytable/.\n\n\nBolker, Ben, 와/과 David Robinson. 2022. broom.mixed: Tidying Methods for Mixed Models. https://CRAN.R-project.org/package=broom.mixed.\n\n\nGalef, Julia. 2020. “Episode 248: Are Democrats being irrational? (David Shor)”. Rationally Speaking, 12월. http://rationallyspeakingpodcast.org/248-are-democrats-being-irrational-david-shor/.\n\n\nGelman, Andrew. 2020. “Statistical Models of Election Outcomes”. YouTube, 8월. https://youtu.be/7gjDnrbLQ4k.\n\n\nGoodrich, Ben, Jonah Gabry, Imad Ali, 와/과 Sam Brilleman. 2023. “rstanarm: Bayesian applied regression modeling via Stan”. https://mc-stan.org/rstanarm.\n\n\nGreen, Eric. 2020. “Nivi Research: Mister P helps us understand vaccine hesitancy”, 12월. https://research.nivi.io/posts/2020-12-08-mister-p-helps-us-understand-vaccine-hesitancy/.\n\n\nJohnston, Myfanwy, 와/과 David Robinson. 2022. gutenbergr: Download and Process Public Domain Works from Project Gutenberg. https://CRAN.R-project.org/package=gutenbergr.\n\n\nKay, Matthew. 2022. tidybayes: Tidy Data and Geoms for Bayesian Models. https://doi.org/10.5281/zenodo.1308151.\n\n\nLarmarange, Joseph. 2023. labelled: Manipulating Labelled Data. https://CRAN.R-project.org/package=labelled.\n\n\nMitchell, Margaret, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, 와/과 Timnit Gebru. 2019. “Model Cards for Model Reporting”. Proceedings of the Conference on Fairness, Accountability, and Transparency, 1월. https://doi.org/10.1145/3287560.3287596.\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nRichardson, Neal, Ian Cook, Nic Crane, Dewey Dunnington, Romain François, Jonathan Keane, Dragoș Moldovan-Grünfeld, Jeroen Ooms, 와/과 Apache Arrow. 2023. arrow: Integration to Apache Arrow. https://CRAN.R-project.org/package=arrow.\n\n\nWang, Wei, David Rothschild, Sharad Goel, 와/과 Andrew Gelman. 2015. “Forecasting elections with non-representative polls”. International Journal of Forecasting 31 (3): 980–91. https://doi.org/10.1016/j.ijforecast.2014.06.001.\n\n\nWickham, Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, 기타. 2019. “Welcome to the Tidyverse”. Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Evan Miller, 와/과 Danny Smith. 2023. haven: Import and Export “SPSS” “Stata” and “SAS” Files. https://CRAN.R-project.org/package=haven.",
    "crumbs": [
      "적용",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>다단계 회귀와 사후 층화</span>"
    ]
  },
  {
    "objectID": "16-text_ko.html",
    "href": "16-text_ko.html",
    "title": "17  데이터로서의 텍스트",
    "section": "",
    "text": "17.1 소개\n선수 지식\n주요 개념 및 기술\n소프트웨어 및 패키지\n텍스트는 우리 주변에 있습니다. 많은 경우, 텍스트는 우리가 접하는 가장 초기 유형의 데이터입니다. 계산 능력의 증가, 새로운 방법의 개발, 그리고 엄청난 양의 텍스트 가용성은 텍스트를 데이터로 사용하는 데 많은 관심을 불러일으켰습니다. 텍스트를 데이터로 사용하면 독특한 분석 기회를 제공합니다. 예를 들어:\n텍스트 분석에 대한 이전 접근 방식은 단어를 맥락과 분리하여 숫자로 변환하는 경향이 있습니다. 그런 다음 로지스틱 회귀의 변형과 같은 전통적인 접근 방식을 사용하여 분석할 수 있습니다. 더 최근의 방법은 텍스트에 내재된 구조를 활용하려고 시도하며, 이는 추가적인 의미를 가져올 수 있습니다. 차이점은 아마도 비슷한 색상을 그룹화할 수 있는 아이와, 어떤 물체인지 아는 아이의 차이와 같습니다. 악어와 나무는 모두 녹색이고, 그 지식으로 무언가를 할 수 있지만, 악어는 당신을 먹을 수 있지만 나무는 아마도 그렇지 않을 것이라는 것을 아는 것이 유용합니다.\n텍스트는 이 책 전반에 걸쳐 사용한 데이터셋의 다루기 힘들지만 유사한 버전으로 간주될 수 있습니다. 주요 차이점은 일반적으로 각 변수가 단어 또는 더 일반적으로 토큰인 넓은 데이터로 시작한다는 것입니다. 종종 각 항목은 개수입니다. 그런 다음 일반적으로 이것을 단어의 한 변수와 개수의 다른 변수가 있는 다소 긴 데이터로 변환합니다. 텍스트를 데이터로 간주하는 것은 자연스럽게 맥락에서 일부 추상화를 요구합니다. 그러나 이것이 역사적 불평등을 영속시킬 수 있으므로 완전히 분리되어서는 안 됩니다. 예를 들어, (koenecke2020은?) 자동 음성 인식 시스템이 백인 화자에 비해 흑인 화자에 대해 훨씬 더 나쁜 성능을 보인다는 것을 발견했으며, (davidson2019racial은?) 구체적으로 정의된 기술 용어인 흑인 미국 영어를 사용하는 트윗이 표준 미국 영어의 유사한 트윗보다 더 높은 비율로 증오 발언으로 분류된다는 것을 발견했습니다.\n텍스트 데이터의 한 가지 흥미로운 측면은 일반적으로 우리 분석의 목적을 위해 생성되지 않는다는 것입니다. 절충안은 일반적으로 우리가 작업할 수 있는 형태로 만들기 위해 훨씬 더 많은 작업을 해야 한다는 것입니다. 데이터 정리 및 준비 단계에서 내려야 할 많은 결정이 있습니다.\n텍스트 데이터셋의 크기가 크다는 것은 분석에 있어서 시뮬레이션하고 작게 시작하는 것이 특히 중요하다는 것을 의미합니다. 텍스트를 데이터로 사용하는 것은 우리에게 사용 가능한 텍스트의 양과 다양성 때문에 흥미롭습니다. 그러나 일반적으로 텍스트 데이터셋을 다루는 것은 지저분합니다. 일반적으로 필요한 정리 및 준비가 많이 있습니다. 종종 텍스트 데이터셋은 큽니다. 따라서 재현 가능한 워크플로를 마련하고 결과를 명확하게 전달하는 것이 중요해집니다. 그럼에도 불구하고, 흥미로운 분야입니다.\n이 장에서는 먼저 텍스트 데이터셋 준비를 고려합니다. 그런 다음 용어 빈도-역 문서 빈도(TF-IDF) 및 주제 모델을 고려합니다.",
    "crumbs": [
      "적용",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>데이터로서의 텍스트</span>"
    ]
  },
  {
    "objectID": "16-text_ko.html#소개",
    "href": "16-text_ko.html#소개",
    "title": "17  데이터로서의 텍스트",
    "section": "",
    "text": "아프리카 국가의 국영 신문 텍스트 분석은 정부에 의한 조작을 식별할 수 있습니다 (Hassan 2022);\n영국 일간 신문의 텍스트는 GDP 및 인플레이션에 대한 더 나은 예측을 생성하는 데 사용될 수 있으며 (Kalamara 기타 2022), 마찬가지로 뉴욕 타임스는 미국 경제 활동과 상관 관계가 있는 불확실성 지수를 만드는 데 사용될 수 있습니다 (Alexopoulos 와/과 Cohen 2015);\n전자 건강 기록(EHR)의 메모 분석은 질병 예측의 효율성을 향상시킬 수 있습니다 (Gronsbell 기타 2019); 그리고\n미국 의회 기록 분석은 여성 의원이 남성에 의해 얼마나 자주 방해받는지 보여줍니다 (Miller 와/과 Sutherland 2022).\n\n\n\n\n\n\n\n\n\n\n\n거인의 어깨 위에 서서\n\n\n\n케네스 베누아 교수는 런던 정치 경제 대학교(LSE)의 계산 사회 과학 교수이자 데이터 과학 연구소 소장입니다. 1998년 하버드 대학교에서 게리 킹과 케네스 솁슬의 지도를 받아 정부학 박사 학위를 취득한 후, 그는 트리니티 칼리지 더블린에서 직책을 맡았고, 2007년에 교수로 승진했습니다. 그는 2020년에 LSE로 옮겼습니다. 그는 특히 정치 텍스트와 소셜 미디어와 같은 텍스트 데이터를 분석하기 위해 정량적 방법을 사용하는 전문가입니다. 그의 중요한 논문 중 일부는 정치 텍스트에서 정책 입장을 추출하고 정치학에서 “데이터로서의 텍스트” 하위 분야를 시작하는 데 도움이 된 (laver2003을?) 포함합니다. 그는 또한 수십 개국의 독창적인 전문가 설문 조사 입장을 제공한 (benoitbook과?) 전문가 설문 조사를 정당 정책 입장의 수작업 분석과 비교한 (benoit2007과?) 같이 정책 입장을 추정하기 위한 다른 방법에서도 광범위하게 작업했습니다. 핵심 기여는 텍스트 데이터를 쉽게 분석할 수 있게 해주는 “텍스트 데이터의 정량적 분석”을 위한 R 패키지 제품군인 quanteda (Benoit 기타 2018)입니다.",
    "crumbs": [
      "적용",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>데이터로서의 텍스트</span>"
    ]
  },
  {
    "objectID": "16-text_ko.html#텍스트-정리-및-준비",
    "href": "16-text_ko.html#텍스트-정리-및-준비",
    "title": "17  데이터로서의 텍스트",
    "section": "17.2 텍스트 정리 및 준비",
    "text": "17.2 텍스트 정리 및 준비\n텍스트 모델링은 흥미로운 연구 분야입니다. 그러나, 그리고 이것은 더 일반적으로 사실이지만, 정리 및 준비 측면은 종종 모델링만큼 어렵습니다. 우리는 몇 가지 필수 사항을 다루고 구축할 수 있는 기초를 제공할 것입니다.\n첫 번째 단계는 일부 데이터를 얻는 것입니다. ?sec-gather-data에서 데이터 수집에 대해 논의했으며, 다음을 포함한 많은 출처를 지나가듯이 언급했습니다.\n\n리뷰에서 텍스트를 제공하는 인사이드 에어비앤비 사용.\n저작권이 만료된 책의 텍스트를 제공하는 구텐베르크 프로젝트.\n위키피디아 또는 다른 웹사이트 스크래핑.\n\n텍스트 정리 및 준비에 필요한 주력 패키지는 tidyverse의 일부인 stringr와 quanteda입니다.\n설명을 위해, 토니 모리슨의 Beloved, 헬렌 드윗의 The Last Samurai, 샬럿 브론테의 Jane Eyre 세 권의 책에서 첫 문장 또는 두 문장의 코퍼스를 구성합니다.\n#| message: false\n#| warning: false\n\nlast_samurai &lt;-\"아버님의 아버지는 감리교 목사였습니다.\"\n\nbeloved &lt;- \"124는 악의적이었습니다. 아기의 독으로 가득 차 있었습니다.\"\n\njane_eyre &lt;- \"그날은 산책을 할 가능성이 없었습니다.\"\n\nbookshelf &lt;-\n  tibble(\n    book = c(\"Last Samurai\", \"Beloved\", \"Jane Eyre\"),\n    first_sentence = c(last_samurai, beloved, jane_eyre)\n  )\n\nbookshelf\n우리는 일반적으로 각 관측치에 문서, 각 열에 단어, 그리고 각 조합에 대한 개수와 관련 메타데이터가 있는 문서-특징 행렬을 구성하고 싶습니다. 예를 들어, 우리 코퍼스가 에어비앤비 리뷰의 텍스트라면, 각 문서는 리뷰일 수 있고, 일반적인 특징에는 “The”, “Airbnb”, “was”, “great”이 포함될 수 있습니다. 여기서 문장이 다른 단어로 분리되었다는 점에 유의하십시오. 우리는 일반적으로 관심 있는 측면의 다양성 때문에 단어에서 일반화하기 위해 “토큰”에 대해 이야기하지만, 단어는 일반적으로 사용됩니다.\nbooks_corpus &lt;-\n  corpus(bookshelf, \n         docid_field = \"book\", \n         text_field = \"first_sentence\")\n\nbooks_corpus\nquanteda (Benoit 기타 2018)의 dfm()을 사용하여 코퍼스의 토큰을 사용하여 문서-특징 행렬(DFM)을 구성합니다.\n#| message: false\n#| warning: false\n\nbooks_dfm &lt;-\n  books_corpus |&gt;\n  tokens() |&gt;\n  dfm()\n\nbooks_dfm\n이제 이 과정의 일부로 내려야 할 많은 결정 중 일부를 고려합니다. 명확한 정답이나 오답은 없습니다. 대신, 우리는 데이터셋을 무엇에 사용할지를 기반으로 결정을 내립니다.\n\n17.2.1 불용어\n불용어는 “the”, “and”, “a”와 같은 단어입니다. 오랫동안 불용어는 많은 의미를 전달하지 않는다고 생각되었고, 메모리가 제한된 계산에 대한 우려가 있었습니다. 텍스트 데이터셋을 준비하는 일반적인 단계는 불용어를 제거하는 것이었습니다. 이제 우리는 불용어가 많은 의미를 가질 수 있다는 것을 알고 있습니다 (Schofield, Magnusson, 와/과 Mimno 2017). 그것들을 제거하기로 한 결정은 상황에 따라 달라지는 미묘한 결정입니다.\nquanteda의 stopwords()를 사용하여 불용어 목록을 얻을 수 있습니다.\nstopwords(source = \"snowball\")[1:10]\n그런 다음 해당 목록의 모든 단어 인스턴스를 찾고 str_replace_all()로 대략적으로 제거할 수 있습니다.\nstop_word_list &lt;-\n  paste(stopwords(source = \"snowball\"), collapse = \" | \")\n\nbookshelf |&gt;\n  mutate(no_stops = str_replace_all(\n    string = first_sentence,\n    pattern = stop_word_list,\n    replacement = \" \")\n  )\n  select(no_stops, first_sentence)\n다른 사람들이 구성한 다양한 불용어 목록이 있습니다. 예를 들어, stopwords()는 “snowball”, “stopwords-iso”, “smart”, “marimo”, “ancient”, “nltk”를 포함한 목록을 사용할 수 있습니다. 더 일반적으로, 불용어를 사용하기로 결정하면, 종종 프로젝트별 단어로 그러한 목록을 보강해야 합니다. 코퍼스의 개별 단어 수를 만든 다음, 가장 일반적인 것을 기준으로 정렬하고 적절하게 불용어 목록에 추가하여 이것을 할 수 있습니다.\nstop_word_list_updated &lt;-\n  paste(\n    \"Methodist |\",\n    \"spiteful |\",\n    \"possibility |\",\n    stop_word_list,\n    collapse = \" | \"\n  )\n\nbookshelf |&gt;\n  mutate(no_stops = str_replace_all(\n    string = first_sentence,\n    pattern = stop_word_list_updated,\n    replacement = \" \")\n  )\n  select(no_stops)\nquanteda의 dfm_remove()를 사용하여 DFM 구성에 불용어 제거를 통합할 수 있습니다.\n#| message: false\n#| warning: false\n\nbooks_dfm |&gt;\n  dfm_remove(stopwords(source = \"snowball\"))\n불용어를 제거하면 데이터셋을 인위적으로 조정합니다. 때로는 그렇게 할 만한 충분한 이유가 있을 수 있습니다. 그러나 생각 없이 해서는 안 됩니다. 예를 들어, 장 6 및 ?sec-store-and-share에서 논의했듯이, 때로는 응답자의 개인 정보를 보호하기 위해 데이터셋을 검열, 절단 또는 다른 유사한 방식으로 조작해야 할 수 있습니다. 자연어 처리에서 불용어 제거를 기본 단계로 통합한 것은 이러한 방법이 개발되었을 때 더 제한적이었을 수 있는 계산 능력 때문일 수 있습니다. 어쨌든, Jurafsky 와/과 Martin ([2000년] 2023, p. 62)은 불용어를 제거하는 것이 텍스트 분류 성능을 향상시키지 않는다고 결론을 내립니다. 관련하여, (schofield2017은?) 가장 빈번한 단어 이외의 것을 제거하는 것으로는 주제 모델의 추론이 개선되지 않는다는 것을 발견합니다. 불용어를 제거해야 하는 경우, 주제가 구성된 후에 이 작업을 수행할 것을 권장합니다.\n\n\n17.2.2 대소문자, 숫자 및 구두점\n때로는 단어만 중요하고 대소문자나 구두점은 중요하지 않은 경우가 있습니다. 예를 들어, 텍스트 코퍼스가 특히 지저분하거나 특정 단어의 존재가 유익한 경우입니다. 우리는 정보를 잃는 대신 일을 단순화하는 이점을 얻습니다. str_to_lower()를 사용하여 소문자로 변환하고, str_replace_all()을 사용하여 “[:punct:]”로 구두점을, “[:digit:]”로 숫자를 제거할 수 있습니다.\nbookshelf |&gt;\n  mutate(lower_sentence = str_to_lower(string = first_sentence)) |&gt;\n  select(lower_sentence)\nbookshelf |&gt;\n  mutate(no_punctuation_numbers = str_replace_all(\n    string = first_sentence,\n    pattern = \"[:punct:]|[:digit:]\",\n    replacement = \" \"\n  )) |&gt;\n  select(no_punctuation_numbers)\n여담으로, str_replace_all()에서 “[:graph:]”를 사용하여 문자, 숫자 및 구두점을 제거할 수 있습니다. 교과서 예제에서는 거의 필요하지 않지만, 실제 데이터셋에서는 일반적으로 식별하고 제거해야 하는 소수의 예기치 않은 기호가 있기 때문에 특히 유용합니다. 우리는 익숙한 모든 것을 제거하는 데 사용하고, 익숙하지 않은 것만 남깁니다.\n더 일반적으로, quanteda()의 tokens()에 있는 인수를 사용하여 이것을 할 수 있습니다.\nbooks_corpus |&gt;\n  tokens(remove_numbers = TRUE, remove_punct = TRUE)\n\n\n17.2.3 오타 및 드문 단어\n그런 다음 오타 및 기타 사소한 문제에 대해 무엇을 할지 결정해야 합니다. 모든 실제 텍스트에는 오타가 있습니다. 때로는 이것들을 명확하게 수정해야 합니다. 그러나 체계적인 방식으로 만들어진 경우, 예를 들어, 특정 작가가 항상 같은 실수를 하는 경우, 작가별로 그룹화하는 데 관심이 있다면 가치가 있을 수 있습니다. ?sec-gather-data에서 본 것처럼 OCR을 사용하면 일반적인 문제가 발생할 수도 있습니다. 예를 들어, “the”는 종종 “thc”로 잘못 인식됩니다.\n불용어를 수정했던 것과 같은 방식으로 오타를 수정할 수 있습니다. 즉, 수정 목록을 사용합니다. 드문 단어의 경우, dfm_trim()을 사용하여 문서-특징 행렬 생성에 이것을 포함할 수 있습니다. 예를 들어, “min_termfreq = 2”를 사용하여 최소 두 번 이상 나타나지 않는 단어를 제거하거나, “min_docfreq = 0.05”를 사용하여 최소 5%의 문서에 없는 단어를 제거하거나, “max_docfreq = 0.90”을 사용하여 최소 90%의 문서에 있는 단어를 제거할 수 있습니다.\n#| message: false\n#| warning: false\n\nbooks_corpus |&gt;\n  tokens(remove_numbers = TRUE, remove_punct = TRUE) |&gt;\n  dfm(tolower = TRUE) |&gt;\n  dfm_trim(min_termfreq = 2)\n\n\n17.2.4 튜플\n튜플은 요소의 순서 있는 목록입니다. 텍스트의 맥락에서, 그것은 일련의 단어입니다. 튜플이 두 단어로 구성되면, 우리는 이것을 “바이그램”이라고 부르고, 세 단어는 “트라이그램” 등입니다. 이것들은 텍스트 정리 및 준비에 있어서 문제입니다. 왜냐하면 우리는 종종 공백을 기준으로 용어를 분리하기 때문입니다. 이것은 부적절한 분리를 초래할 것입니다.\n이것은 지명에 있어서 명백한 문제입니다. 예를 들어, “브리티시 컬럼비아”, “뉴햄프셔”, “영국”, “포트 헤들랜드”를 생각해 보십시오. 한 가지 방법은 그러한 장소 목록을 만들고 str_replace_all()을 사용하여 밑줄을 추가하는 것입니다. 예를 들어, “British_Columbia”, “New_Hampshire”, “United_Kingdom”, “Port_Hedland”입니다. 다른 옵션은 quanteda의 tokens_compound()를 사용하는 것입니다.\nsome_places &lt;- c(\"British Columbia\", \n                 \"New Hampshire\", \n                 \"United Kingdom\", \n                 \"Port Hedland\")\na_sentence &lt;-\nc(\"Vancouver is in British Columbia and New Hampshire is not\")\n\ntokens(a_sentence) |&gt;\n  tokens_compound(pattern = phrase(some_places))\n그 경우, 우리는 튜플이 무엇인지 알고 있었습니다. 그러나 코퍼스에서 일반적인 튜플이 무엇인지 확실하지 않을 수 있습니다. tokens_ngrams()를 사용하여 그것들을 식별할 수 있습니다. 예를 들어, 제인 에어의 발췌문에서 모든 바이그램을 요청할 수 있습니다. ?sec-its-just-a-generalized-linear-model에서 이 책의 텍스트를 구텐베르크 프로젝트에서 다운로드하는 방법을 보여주었으므로, 여기서는 이전에 저장한 로컬 버전을 로드합니다.\n#| eval: false\n#| echo: true\n\njane_eyre &lt;- read_csv(\n  \"jane_eyre.csv\",\n  col_types = cols(\n    gutenberg_id = col_integer(),\n    text = col_character()\n  )\n)\n\njane_eyre\n#| eval: true\n#| echo: false\n\n# INTERNAL\n\njane_eyre &lt;- read_csv(\n  \"inputs/jane_eyre.csv\",\n  col_types = cols(\n    gutenberg_id = col_integer(),\n    text = col_character()\n  )\n)\n\njane_eyre\n빈 줄이 많으므로 제거하겠습니다.\njane_eyre &lt;-\n  jane_eyre |&gt;\n  filter(!is.na(text))\njane_eyre_text &lt;- tibble(\n  book = \"Jane Eyre\",\n  text = paste(jane_eyre$text, collapse = \" \") |&gt;\n    str_replace_all(pattern = \"[:punct:]\",\n                    replacement = \" \") |&gt;\n    str_replace_all(pattern = stop_word_list,\n                    replacement = \" \")\n)\n\njane_eyre_corpus &lt;-\n  corpus(jane_eyre_text, docid_field = \"book\", text_field = \"text\")\nngrams &lt;- tokens_ngrams(tokens(jane_eyre_corpus), n = 2)\nngram_counts &lt;-\n  tibble(ngrams = unlist(ngrams)) |&gt;\n  count(ngrams, sort = TRUE)\n\nhead(ngram_counts)\n일부 일반적인 바이그램을 식별한 후, 변경할 목록에 추가할 수 있습니다. 이 예에는 “Mr Rochester” 및 “St John”과 같은 이름이 포함되어 있으며, 분석을 위해 함께 유지해야 합니다.\n\n\n17.2.5 어간 추출 및 표제어 추출\n단어의 어간 추출 및 표제어 추출은 텍스트 데이터셋의 차원을 줄이는 또 다른 일반적인 접근 방식입니다. 어간 추출은 단어의 마지막 부분을 제거하는 것을 의미하며, 이것이 더 일반적인 단어를 초래할 것이라는 기대가 있습니다. 예를 들어, “Canadians”, “Canadian”, “Canada”는 모두 “Canad”로 어간 추출됩니다. 표제어 추출은 유사하지만, 더 복잡합니다. 그것은 철자뿐만 아니라 정규형을 기반으로 단어를 변경하는 것을 의미합니다 (Grimmer, Roberts, 와/과 Stewart 2022, p. 54). 예를 들어, “Canadians”, “Canadian”, “Canucks”, “Canuck”은 모두 “Canada”로 변경될 수 있습니다.\ndfm_wordstem()으로 이것을 할 수 있습니다. 예를 들어, “minister”가 “minist”로 변경되었음을 알 수 있습니다.\nchar_wordstem(c(\"Canadians\", \"Canadian\", \"Canada\"))\n\nbooks_corpus |&gt;\n  tokens(remove_numbers = TRUE, remove_punct = TRUE) |&gt;\n  dfm(tolower = TRUE) |&gt;\n  dfm_wordstem()\n이것은 텍스트를 데이터로 사용하는 일반적인 단계이지만, (schofield2017understanding은?) 나중에 다룰 주제 모델링의 맥락에서 어간 추출이 거의 효과가 없으며 그것을 할 필요가 거의 없다는 것을 발견합니다.\n\n\n17.2.6 중복\n중복은 크기 때문에 텍스트 데이터셋에서 주요 관심사입니다. 예를 들어, (bandy2021addressing은?) BookCorpus 데이터셋에서 데이터의 약 30%가 부적절하게 중복되었음을 보여주었고, (schofield2017quantifying은?) 이것이 주요 관심사이며 결과에 상당한 영향을 미칠 수 있음을 보여줍니다. 그러나 이것은 미묘하고 진단하기 어려운 문제일 수 있습니다. 예를 들어, ?sec-its-just-a-generalized-linear-model에서 포아송 회귀의 맥락에서 다양한 저자의 페이지 수 개수를 고려했을 때, 각 희곡에 대한 항목뿐만 아니라 그것들을 모두 포함하는 많은 선집도 있기 때문에 각 셰익스피어 항목을 실수로 두 번 포함했을 수 있습니다. 데이터셋을 신중하게 고려하면 문제를 식별했지만, 대규모로는 어려울 것입니다.",
    "crumbs": [
      "적용",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>데이터로서의 텍스트</span>"
    ]
  },
  {
    "objectID": "16-text_ko.html#용어-빈도-역-문서-빈도tf-idf",
    "href": "16-text_ko.html#용어-빈도-역-문서-빈도tf-idf",
    "title": "17  데이터로서의 텍스트",
    "section": "17.3 용어 빈도-역 문서 빈도(TF-IDF)",
    "text": "17.3 용어 빈도-역 문서 빈도(TF-IDF)\n\n17.3.1 별자리 구별하기\n실제 데이터셋을 탐색하기 위해 별자리 데이터셋인 astrologer를 설치하고 로드합니다.\n그런 다음 “horoscopes” 데이터셋에 액세스할 수 있습니다.\nhoroscopes\n네 가지 변수가 있습니다: “startdate”, “zodiacsign”, “horoscope”, “url” (웹사이트가 업데이트되었기 때문에 URL이 오래되었다는 점에 유의하십시오. 예를 들어, 첫 번째는 여기를 참조합니다). 우리는 각 별자리의 별자리를 구별하는 데 사용되는 단어에 관심이 있습니다.\nhoroscopes |&gt;\n  count(zodiacsign)\n각 별자리에 대해 106개의 별자리가 있습니다. 이 예에서는 먼저 단어로 토큰화한 다음, 날짜가 아닌 별자리만 기준으로 개수를 만듭니다. (hvitfeldt2021supervised에서?) 광범위하게 사용되기 때문에 tidytext를 사용합니다.\nhoroscopes_by_word &lt;-\n  horoscopes |&gt;\n  select(-startdate,-url) |&gt;\n  unnest_tokens(output = word,\n                input = horoscope,\n                token = \"words\")\n\nhoroscopes_counts_by_word &lt;-\n  horoscopes_by_word |&gt;\n  count(zodiacsign, word, sort = TRUE)\n\nhoroscopes_counts_by_word\n가장 인기 있는 단어는 다른 별자리에 대해 유사하게 나타나는 것을 볼 수 있습니다. 이 시점에서, 우리는 다양한 방법으로 데이터를 사용할 수 있습니다.\n각 그룹을 특징짓는 단어, 즉 각 그룹에서만 일반적으로 사용되는 단어에 관심이 있을 수 있습니다. 우리는 먼저 단어의 용어 빈도(TF), 즉 각 별자리의 별자리에서 단어가 사용된 횟수를 살펴봄으로써 그렇게 할 수 있습니다. 문제는 맥락에 관계없이 일반적으로 사용되는 단어가 많다는 것입니다. 따라서 우리는 또한 많은 별자리의 별자리에서 나타나는 단어를 “벌점”을 주는 역 문서 빈도(IDF)를 살펴보고 싶을 수 있습니다. 많은 별자리의 별자리에서 나타나는 단어는 하나의 별자리의 별자리에서만 나타나는 단어보다 IDF가 낮을 것입니다. 그런 다음 용어 빈도-역 문서 빈도(tf-idf)는 이들의 곱입니다.\ntidytext의 bind_tf_idf()를 사용하여 이 값을 만들 수 있습니다. 각 측정에 대한 새 변수를 만듭니다.\nhoroscopes_counts_by_word_tf_idf &lt;-\n  horoscopes_counts_by_word |&gt;\n  bind_tf_idf(\n    term = word,\n    document = zodiacsign,\n    n = n\n  ) |&gt;\n  arrange(-tf_idf)\n\nhoroscopes_counts_by_word_tf_idf\n?tbl-zodiac에서는 각 별자리의 별자리를 구별하는 단어를 살펴봅니다. 가장 먼저 눈에 띄는 것은 일부는 자신의 별자리를 가지고 있다는 것입니다. 한편으로는 이것을 제거해야 한다는 주장이 있지만, 다른 한편으로는 모든 별자리에 대해 이것이 일어나지 않는다는 사실이 각 별자리의 별자리의 성격에 대해 유익할 수 있습니다.\n#| label: tbl-zodiac\n#| tbl-cap: \"특정 별자리에 고유한 별자리에서 가장 흔한 단어\"\n\nhoroscopes_counts_by_word_tf_idf |&gt;\n  slice(1:5,\n        .by = zodiacsign) |&gt;\n  select(zodiacsign, word) |&gt;\n  summarise(all = paste0(word, collapse = \"; \"),\n            .by = zodiacsign) |&gt;\n  tt() |&gt;\n  style_tt(j = 1:2, align = \"lr\") |&gt;\n  setNames(c(\"Zodiac sign\", \"Most common words unique to that sign\"))",
    "crumbs": [
      "적용",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>데이터로서의 텍스트</span>"
    ]
  },
  {
    "objectID": "16-text_ko.html#주제-모델",
    "href": "16-text_ko.html#주제-모델",
    "title": "17  데이터로서의 텍스트",
    "section": "17.4 주제 모델",
    "text": "17.4 주제 모델\n주제 모델은 많은 진술이 있고 유사한 단어를 사용하는 문장을 기반으로 그룹을 만들고 싶을 때 유용합니다. 우리는 유사한 단어 그룹을 주제를 정의하는 것으로 간주합니다. 각 진술의 주제에 대한 일관된 추정치를 얻는 한 가지 방법은 주제 모델을 사용하는 것입니다. 많은 변형이 있지만, 한 가지 방법은 (Blei2003latent의?) 잠재 디리클레 할당(LDA) 방법을 stm으로 구현하는 것입니다. 명확성을 위해, 이 장의 맥락에서 LDA는 잠재 디리클레 할당을 의미하며, 이것이 약어 LDA와 관련된 또 다른 일반적인 주제이지만 선형 판별 분석을 의미하지는 않습니다.\nLDA 방법의 핵심 가정은 각 진술, 즉 문서가 해당 문서에서 이야기하고 싶은 주제를 결정한 다음, 해당 주제에 적합한 단어, 즉 용어를 선택하는 사람에 의해 만들어진다는 것입니다. 주제는 용어 모음으로 생각할 수 있고, 문서는 주제 모음으로 생각할 수 있습니다. 주제는 사전에 지정되지 않습니다. 그것들은 방법의 결과입니다. 용어는 반드시 특정 주제에 고유하지 않으며, 문서는 하나 이상의 주제에 관한 것일 수 있습니다. 이것은 엄격한 단어 수 계산 방법과 같은 다른 접근 방식보다 더 많은 유연성을 제공합니다. 목표는 문서에서 발견된 단어가 스스로 그룹화하여 주제를 정의하도록 하는 것입니다.\nLDA는 각 문서를 주제에 대한 일부 확률 분포에 의해 생성된 것으로 간주합니다. 예를 들어, 5개의 주제와 2개의 문서가 있다면, 첫 번째 문서는 대부분 처음 몇 개의 주제로 구성될 수 있습니다. 다른 문서는 대부분 마지막 몇 개의 주제에 관한 것일 수 있습니다(?fig-topicsoverdocuments).\n#| echo: false\n#| fig-cap: \"주제에 대한 확률 분포\"\n#| label: fig-topicsoverdocuments\n#| layout-ncol: 2\n#| fig-subcap: [\"문서 1에 대한 분포\", \"문서 2에 대한 분포\"]\n\ntopics &lt;- c(\"topic 1\", \"topic 2\", \"topic 3\", \"topic 4\", \"topic 5\")\n\ndocument_1 &lt;- tibble(\n  Topics = topics,\n  Probability = c(0.40, 0.40, 0.1, 0.05, 0.05)\n)\n\ndocument_2 &lt;- tibble(\n  Topics = topics,\n  Probability = c(0.01, 0.04, 0.35, 0.20, 0.4)\n)\n\nggplot(document_1, aes(Topics, Probability)) +\n  geom_point() +\n  theme_classic() +\n  coord_cartesian(ylim = c(0, 0.4))\n\nggplot(document_2, aes(Topics, Probability)) +\n  geom_point() +\n  theme_classic() +\n  coord_cartesian(ylim = c(0, 0.4))\n마찬가지로, 각 주제는 용어에 대한 확률 분포로 간주될 수 있습니다. 각 문서에서 사용되는 용어를 선택하기 위해, 화자는 적절한 비율로 각 주제에서 용어를 선택합니다. 예를 들어, 10개의 용어가 있다면, 한 주제는 이민과 관련된 용어에 더 많은 가중치를 부여하여 정의될 수 있습니다. 그리고 다른 어떤 주제는 경제와 관련된 용어에 더 많은 가중치를 부여할 수 있습니다(?fig-topicsoverterms).\n#| echo: false\n#| fig-cap: \"용어에 대한 확률 분포\"\n#| label: fig-topicsoverterms\n#| layout-ncol: 2\n#| fig-subcap: [\"주제 1에 대한 분포\", \"주제 2에 대한 분포\"]\n\nsome_terms &lt;- c(\n  \"immigration\", \"race\", \"influx\", \"loans\", \"wealth\", \n  \"saving\", \"chinese\", \"france\", \"british\", \"english\")\n\ntopic_1 &lt;- tibble(\n  Terms = some_terms,\n  Probability = c(0.0083, 0.0083, 0.0083, 0.0083, 0.0083, 0.0083, 0.2, 0.15, 0.4, 0.2)\n)\n\ntopic_2 &lt;- tibble(\n  Terms = some_terms,\n  Probability = c(0.0142, 0.0142, 0.0142, 0.25, 0.35, 0.30, 0.0142, 0.0142, 0.0142, 0.0142)\n)\n\nggplot(topic_1, aes(Terms, Probability)) +\n  geom_point() +\n  theme_classic() +\n  coord_cartesian(ylim = c(0, 0.4))\nggplot(topic_2, aes(Terms, Probability)) +\n  geom_point() +\n  theme_classic() +\n  coord_cartesian(ylim = c(0, 0.4))\n\n\n\n\n\n\n배경 정보로, 디리클레 분포는 범주형 및 다항 변수에 대한 사전 확률로 일반적으로 사용되는 베타 분포의 변형입니다. 범주가 두 개뿐인 경우, 디리클레 분포와 베타 분포는 동일합니다. 대칭 디리클레 분포의 특별한 경우, \\(\\eta=1\\)이면 균등 분포와 동일합니다. \\(\\eta&lt;1\\)이면, 분포는 희소하고 더 적은 수의 값에 집중되며, 이 수는 \\(\\eta\\)가 감소함에 따라 감소합니다. 하이퍼파라미터는 이 사용법에서 사전 분포의 매개변수입니다.\n\n\n\n문서가 생성된 후, 그것들은 우리가 분석할 수 있는 전부입니다. 각 문서의 용어 사용은 관찰되지만, 주제는 숨겨져 있거나 “잠재적”입니다. 우리는 각 문서의 주제나 용어가 주제를 어떻게 정의했는지 알지 못합니다. 즉, ?fig-topicsoverdocuments 또는 ?fig-topicsoverterms의 확률 분포를 알지 못합니다. 어떤 의미에서 우리는 문서 생성 과정을 역으로 하려고 합니다. 우리는 용어를 가지고 있고, 주제를 발견하고 싶습니다.\n각 문서의 용어를 관찰하면, 주제의 추정치를 얻을 수 있습니다 (Steyvers 와/과 Griffiths 2006). LDA 과정의 결과는 확률 분포입니다. 이러한 분포가 주제를 정의합니다. 각 용어는 특정 주제의 구성원일 확률이 주어지고, 각 문서는 특정 주제에 관한 것일 확률이 주어집니다.\n\n\n문서 코퍼스가 주어졌을 때 LDA를 구현하는 초기 실제 단계는 일반적으로 불용어를 제거하는 것입니다. 이전에 언급했듯이, 이것이 반드시 필요한 것은 아니며, 그룹이 생성된 후에 수행하는 것이 더 나을 수 있습니다. 우리는 종종 구두점과 대소문자도 제거합니다. 그런 다음 quanteda의 dfm()을 사용하여 문서-특징 행렬을 구성합니다.\n데이터셋이 준비된 후, stm을 사용하여 LDA를 구현하고 사후 확률을 근사할 수 있습니다.  이 과정은 다른 모든 문서의 다른 모든 용어의 주제가 주어졌을 때 특정 문서의 특정 용어에 대한 주제를 찾으려고 시도합니다. 대체로, 이것은 먼저 모든 문서의 모든 용어를 디리클레 사전 확률에 의해 지정된 무작위 주제에 할당하는 것으로 수행됩니다.  그런 다음 특정 문서의 특정 용어를 선택하고 다른 모든 문서의 다른 모든 용어에 대한 주제가 주어진 조건부 분포를 기반으로 새 주제에 할당합니다 (Grün 와/과 Hornik 2011.6).   이것이 추정되면, 주제에 대한 단어 분포와 문서에 대한 주제 분포에 대한 추정치를 되돌릴 수 있습니다.\n조건부 분포는 용어가 이전에 해당 주제에 얼마나 자주 할당되었는지, 그리고 해당 문서에서 주제가 얼마나 일반적인지에 따라 주제를 할당합니다 (Steyvers 와/과 Griffiths 2006). 주제의 초기 무작위 할당은 문서 코퍼스를 통한 초기 통과의 결과가 좋지 않다는 것을 의미하지만, 충분한 시간이 주어지면 알고리즘은 적절한 추정치로 수렴합니다.\n주제 수 \\(k\\)의 선택은 결과에 영향을 미치며, 사전에 지정해야 합니다. 특정 수에 대한 강력한 이유가 있는 경우, 이것을 사용할 수 있습니다. 그렇지 않으면, 적절한 수를 선택할 한 가지 방법은 테스트 및 훈련 세트 프로세스를 사용하는 것입니다. 본질적으로, 이것은 k에 대한 다양한 가능한 값에 대해 프로세스를 실행한 다음, 잘 수행되는 적절한 값을 선택하는 것을 의미합니다.\nLDA 방법의 한 가지 약점은 단어의 순서가 중요하지 않은 “단어 가방”을 고려한다는 것입니다 (Blei 2012). 단어 순서에 조건성을 추가하고 단어 가방 가정의 영향을 줄이기 위해 모델을 확장할 수 있습니다. 또한, 디리클레 분포의 대안을 사용하여 상관 관계를 허용하도록 모델을 확장할 수 있습니다.\n\n17.4.1 캐나다 의회에서 논의되는 내용\n영국의 예를 따라, 캐나다 의회에서 말한 내용의 서면 기록은 “한사드”라고 불립니다. 완전히 축어적인 것은 아니지만, 매우 가깝습니다. (BeelenEtc2017에?) 의해 구성된 LiPaD에서 CSV 형식으로 사용할 수 있습니다.\n우리는 2018년 캐나다 의회에서 논의된 내용에 관심이 있습니다. 시작하려면, 여기에서 전체 코퍼스를 다운로드한 다음, 2018년을 제외한 모든 연도를 버릴 수 있습니다. 데이터셋이 “2018”이라는 폴더에 있는 경우, read_csv()를 사용하여 모든 CSV를 읽고 결합할 수 있습니다.\n#| echo: true\n#| eval: false\n\nfiles_of_interest &lt;-\n  dir_ls(path = \"2018/\", glob = \"*.csv\", recurse = 2)\n\nhansard_canada_2018 &lt;-\n  read_csv(\n    files_of_interest,\n    col_types = cols(\n      basepk = col_integer(),\n      speechdate = col_date(),\n      speechtext = col_character(),\n      speakerparty = col_character(),\n      speakerriding = col_character(),\n      speakername = col_character()\n    ),\n    col_select =\n      c(basepk, speechdate, speechtext, speakername, speakerparty, \n        speakerriding))\n  filter(!is.na(speakername))\n\nhansard_canada_2018\n#| echo: false\n#| eval: true\n\nfiles_of_interest &lt;-\n  dir_ls(\n    path = \"inputs/data/2018/\",\n    glob = \"*.csv\",\n    recurse = 2\n  )\n\nhansard_canada_2018 &lt;-\n  read_csv(\n    files_of_interest,\n    col_types = cols(\n      basepk = col_integer(),\n      speechdate = col_date(),\n      speechtext = col_character(),\n      speakerparty = col_character(),\n      speakerriding = col_character(),\n      speakername = col_character()\n    ),\n    col_select = c(\n      basepk,\n      speechdate,\n      speechtext,\n      speakername,\n      speakerparty,\n      speakerriding\n    )\n  ) |&gt;\n  filter(!is.na(speakername))\n\nhansard_canada_2018\n끝에 filter()를 사용하는 것은 때때로 “지시”와 같은 측면과 유사한 비연설 측면이 한사드에 포함되기 때문에 필요합니다. 예를 들어, 해당 filter()를 포함하지 않으면 첫 번째 줄은 “하원은 2017년 11월 9일부터 동의안 심의를 재개했습니다.”입니다. 그런 다음 코퍼스를 구성할 수 있습니다.\nhansard_canada_2018_corpus &lt;-\n  corpus(hansard_canada_2018, \n         docid_field = \"basepk\", \n         text_field = \"speechtext\")\n\nhansard_canada_2018_corpus\n코퍼스의 토큰을 사용하여 문서-특징 행렬을 구성합니다. 계산적으로 우리의 삶을 조금 더 쉽게 만들기 위해, 최소 두 번 이상 나타나지 않는 단어와 최소 두 문서에 나타나지 않는 단어를 제거합니다.\n#| message: false\n#| warning: false\n\nhansard_dfm &lt;-\n  hansard_canada_2018_corpus |&gt;\n  tokens(\n    remove_punct = TRUE,\n    remove_symbols = TRUE\n  ) |&gt;\n  dfm() |&gt;\n  dfm_trim(min_termfreq = 2, min_docfreq = 2) |&gt;\n  dfm_remove(stopwords(source = \"snowball\"))\n\nhansard_dfm\n이 시점에서 stm의 stm()을 사용하여 LDA 모델을 구현할 수 있습니다. 문서-특징 행렬과 주제 수를 지정해야 합니다. 주제 모델은 본질적으로 요약일 뿐입니다. 문서가 단어 모음이 되는 대신, 각 주제와 관련된 일부 확률을 가진 주제 모음이 됩니다. 그러나 그것은 실제 기본 의미보다는 비슷한 시기에 사용되는 경향이 있는 단어 모음을 제공할 뿐이므로, 관심 있는 주제 수를 지정해야 합니다. 이 결정은 큰 영향을 미칠 것이며, 몇 가지 다른 숫자를 고려해야 합니다.\n#| echo: true\n#| eval: false\n\nhansard_topics &lt;- stm(documents = hansard_dfm, K = 10)\n\nbeepr::beep()\n\nwrite_rds(\n  hansard_topics,\n  file = \"hansard_topics.rda\"\n)\n#| echo: false\n#| eval: false\n\n# INTERNAL\n\nhansard_topics &lt;- stm(documents = hansard_dfm, K = 10)\n\nbeep()\n\nwrite_rds(\n  hansard_topics,\n  file = \"outputs/hansard_topics.rda\"\n)\n이것은 시간이 좀 걸릴 것이며, 아마도 15-30분 정도 걸릴 것이므로, 모델이 완료되면 write_rds()를 사용하여 저장하고, 완료되면 알림을 받기 위해 beep을 사용하는 것이 유용합니다. 그런 다음 read_rds()를 사용하여 결과를 다시 읽어들일 수 있습니다.\n#| echo: true\n#| eval: false\n\nhansard_topics &lt;- read_rds(\n  file = \"hansard_topics.rda\"\n)\n#| echo: false\n#| eval: true\n\nhansard_topics &lt;- read_rds(\n  file = \"outputs/hansard_topics.rda\"\n)\nlabelTopics()를 사용하여 각 주제의 단어를 볼 수 있습니다.\nlabelTopics(hansard_topics)",
    "crumbs": [
      "적용",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>데이터로서의 텍스트</span>"
    ]
  },
  {
    "objectID": "16-text_ko.html#연습-문제",
    "href": "16-text_ko.html#연습-문제",
    "title": "17  데이터로서의 텍스트",
    "section": "17.5 연습 문제",
    "text": "17.5 연습 문제\n\n연습\n\n(계획) 다음 시나리오를 고려하십시오: 당신은 뉴스 웹사이트를 운영하고 있으며 익명 댓글을 허용할지 여부를 이해하려고 합니다. A/B 테스트를 하기로 결정했는데, 여기서 모든 것을 동일하게 유지하지만, 사이트의 한 버전에만 익명 댓글을 허용합니다. 결정해야 할 것은 테스트에서 얻는 텍스트 데이터뿐입니다. 해당 데이터셋이 어떻게 생겼을지 스케치한 다음 모든 관측치를 보여주기 위해 만들 수 있는 그래프를 스케치하십시오.\n(시뮬레이션) 설명된 시나리오를 더 고려하고 상황을 시뮬레이션하십시오. 시뮬레이션된 데이터를 기반으로 최소 10개의 테스트를 포함하십시오.\n(수집) 이러한 데이터셋의 가능한 출처 하나를 설명하십시오.\n(탐색) 스케치한 그래프를 ggplot2를 사용하여 만드십시오. rstanarm을 사용하여 모델을 만드십시오.\n(전달) 당신이 한 일에 대해 두 단락을 작성하십시오.\n\n\n\n퀴즈\n\nstr_replace_all()에 어떤 인수가 구두점을 제거합니까?\n\n“[:punct:]”\n“[:digit:]”\n“[:alpha:]”\n“[:lower:]”\n\nstopwords(source = \"snowball\")[1:10]을 변경하여 “nltk” 목록의 아홉 번째 불용어를 찾으십시오.\n\n“her”\n“my”\n“you”\n“i”\n\nquanteda()의 어떤 함수가 코퍼스를 토큰화합니까?\n\ntokenizer()\ntoken()\ntokenize()\ntokens()\n\n최소 두 번 이상 나타나는 용어만 포함하고 싶을 때 dfm_trim()에 어떤 인수를 사용해야 합니까? = 2)\n\n“min_wordfreq”\n“min_termfreq”\n“min_term_occur”\n“min_ occurrence”\n\n가장 좋아하는 트라이그램 예는 무엇입?\n게자리의 별자리에서 두 번째로 가장 흔하게 사용되는 단어는 무엇입니까?\n\nto\nyour\nthe\nyou\n\n물고기자리의 별자리에서 여섯 번째로 가장 흔하게 사용되는 단어 중 해당 별자리에 고유한 단어는 무엇입니까?\n\nshoes\nprayer\nfishes\npisces\n\n캐나다 주제 모델을 다시 실행하되, 5개의 주제만 포함하십시오. 각 주제의 단어를 보고, 각각이 무엇에 관한 것인지 어떻게 설명하시겠습니까?\n\n\n\n수업 활동\n\n아이들은 “개”, “고양이”, “새” 중 어느 것을 먼저 배울까요? 워드뱅크 데이터베이스를 사용하십시오.\n\n\n\n과제\nR에서의 텍스트 분석을 위한 지도 기계 학습의 5.2장 “직접 찾아봄으로써 단어 임베딩 이해하기”에서 (hvitfeldt2021supervised의?) 코드를 따르십시오. 여기에서 무료로 사용할 수 있으며, LiPaD에서 1년치 데이터에 대한 자신만의 단어 임베딩을 구현하십시오.\n\n\n\n\nAlexopoulos, Michelle, 와/과 Jon Cohen. 2015. “The power of print: Uncertainty shocks, markets, and the economy”. International Review of Economics & Finance 40 (11월): 8–28. https://doi.org/10.1016/j.iref.2015.02.002.\n\n\nAmaka, Ofunne, 와/과 Amber Thomas. 2021. “The Naked Truth: How the names of 6,816 complexion products can reveal bias in beauty”. The Pudding, 3월. https://pudding.cool/2021/03/foundation-names/.\n\n\nArel-Bundock, Vincent. 2024. tinytable: Simple and Configurable Tables in “HTML”, “LaTeX”, “Markdown”, “Word”, “PNG”, “PDF”, and “Typst” Formats. https://vincentarelbundock.github.io/tinytable/.\n\n\nBååth, Rasmus. 2018. beepr: Easily Play Notification Sounds on any Platform. https://CRAN.R-project.org/package=beepr.\n\n\nBenoit, Kenneth. 2020. “Text as data: An overview”. In The SAGE Handbook of Research Methods in Political Science and International Relations, 편집자： Luigi Curini 와/과 Robert Franzese, 461–97. London: SAGE Publishing. https://doi.org/10.4135/9781526486387.n29.\n\n\nBenoit, Kenneth, Kohei Watanabe, Haiyan Wang, Paul Nulty, Adam Obeng, Stefan Müller, 와/과 Akitaka Matsuo. 2018. “quanteda: An R package for the quantitative analysis of textual data”. Journal of Open Source Software 3 (30): 774. https://doi.org/10.21105/joss.00774.\n\n\nBlei, David. 2012. “Probabilistic Topic Models”. Communications of the ACM 55 (4): 77–84. https://doi.org/10.1145/2133806.2133826.\n\n\nGelfand, Sharla. 2022. astrologer: Chani Nicholas Weekly Horoscopes (2013-2017). http://github.com/sharlagelfand/astrologer.\n\n\nGrimmer, Justin, Margaret Roberts, 와/과 Brandon Stewart. 2022. Text As Data: A New Framework for Machine Learning and the Social Sciences. New Jersey: Princeton University Press.\n\n\nGronsbell, Jessica, Jessica Minnier, Sheng Yu, Katherine Liao, 와/과 Tianxi Cai. 2019. “Automated feature selection of predictors in electronic medical records data”. Biometrics 75 (1): 268–77. https://doi.org/10.1111/biom.12987.\n\n\nGrün, Bettina, 와/과 Kurt Hornik. 2011. “topicmodels: An R Package for Fitting Topic Models”. Journal of Statistical Software 40 (13): 1–30. https://doi.org/10.18637/jss.v040.i13.\n\n\nHassan, Mai. 2022. “New Insights On Africa’s Autocratic Past”. African Affairs 121 (483): 321–33. https://doi.org/10.1093/afraf/adac002.\n\n\nHester, Jim, Hadley Wickham, 와/과 Gábor Csárdi. 2021. fs: Cross-Platform File System Operations Based on “libuv”. https://CRAN.R-project.org/package=fs.\n\n\nHvitfeldt, Emil, 와/과 Julia Silge. 2021. Supervised Machine Learning for Text Analysis in R. 1st ed. Chapman; Hall/CRC. https://doi.org/10.1201/9781003093459.\n\n\nJohnston, Myfanwy, 와/과 David Robinson. 2022. gutenbergr: Download and Process Public Domain Works from Project Gutenberg. https://CRAN.R-project.org/package=gutenbergr.\n\n\nJurafsky, Dan, 와/과 James Martin. (2000년) 2023. Speech and Language Processing. 3rd ed. https://web.stanford.edu/~jurafsky/slp3/.\n\n\nKalamara, Eleni, Arthur Turrell, Chris Redl, George Kapetanios, 와/과 Sujit Kapadia. 2022. “Making text count: Economic forecasting using newspaper text”. Journal of Applied Econometrics 37 (5): 896–919. https://doi.org/10.1002/jae.2907.\n\n\nMiller, Michael, 와/과 Joseph Sutherland. 2022. “The Effect of Gender on Interruptions at Congressional Hearings”. American Political Science Review, 1–19. https://doi.org/10.1017/S0003055422000260.\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nRoberts, Margaret, Brandon Stewart, 와/과 Dustin Tingley. 2019. “stm: An R Package for Structural Topic Models”. Journal of Statistical Software 91 (2): 1–40. https://doi.org/10.18637/jss.v091.i02.\n\n\nSchofield, Alexandra, Måns Magnusson, 와/과 David Mimno. 2017. “Pulling Out the Stops: Rethinking Stopword Removal for Topic Models”. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, 432–36. Valencia, Spain: Association for Computational Linguistics. https://aclanthology.org/E17-2069.\n\n\nSilge, Julia, 와/과 David Robinson. 2016. “tidytext: Text Mining and Analysis Using Tidy Data Principles in R”. The Journal of Open Source Software 1 (3). https://doi.org/10.21105/joss.00037.\n\n\nSteyvers, Mark, 와/과 Tom Griffiths. 2006. “Probabilistic Topic Models”. In Latent Semantic Analysis: A Road to Meaning, 편집자： T. Landauer, D McNamara, S. Dennis, 와/과 W. Kintsch. https://cocosci.princeton.edu/tom/papers/SteyversGriffiths.pdf.\n\n\nWickham, Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, 기타. 2019. “Welcome to the Tidyverse”. Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.",
    "crumbs": [
      "적용",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>데이터로서의 텍스트</span>"
    ]
  },
  {
    "objectID": "17-concluding_ko.html",
    "href": "17-concluding_ko.html",
    "title": "18  Concluding remarks",
    "section": "",
    "text": "18.1 Concluding remarks\nPrerequisites\nThere is an old saying, something along the lines of “may you live in interesting times”. Maybe every generation feels this way, but we sure do live in interesting times. In this book, we have covered some essential skills for telling stories with data. And this is just the start.\nIn less than a generation, data science has gone from something that barely existed, to a defining part of academia and industry. The extent and pace of this change has many implications for those learning data science. For instance, it may imply that one should not just make decisions that optimize for what data science looks like right now, but also what could happen. While that is a little difficult, that is also one of the things that makes data science so exciting. That might mean choices like:\nOne of the most exciting times when you learn data science is realizing that you just love playing with data. A decade ago, this did not fit into any particular department or company. These days, it fits into almost any of them.\nData science needs to insist on diversity, both in terms of approaches and applications. It is increasingly the most important work in the world, and hegemonic approaches have no place. It is just such an exciting time to be enthusiastic about data and able to build things.\nThe central thesis of this book has been that a revolution is needed in data science, and we have proposed one view of what it could look like. This revolution builds on the long history of statistics, borrows heavily from computer science, and draws on other disciplines as needed, but is centered around reproducibility, workflows, and respect. When data science began it was nebulous and ill-defined. As it has matured, we now come to see it as able to stand on its own.\nThis book has been a reimagining of what data science is, and what it could be. In 장 1 we provided an informal definition of data science. We now revisit it. We consider data science to be the process of developing and applying a principled, tested, reproducible, end-to-end workflow that focuses on quantitative measures in and of themselves, and as a foundation to explore questions. We have known for a long-time what rigor looks like in mathematical and statistical theory: theorems are accompanied by proofs (Horton 기타 2022). And we increasingly know what rigor looks like in data science: claims that are accompanied by verified, tested, reproducible, code and data. Rigorous data science creates lasting understanding of the world.",
    "crumbs": [
      "적용",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Concluding remarks</span>"
    ]
  },
  {
    "objectID": "17-concluding_ko.html#결론",
    "href": "17-concluding_ko.html#결론",
    "title": "18  결론",
    "section": "",
    "text": "유행하는 응용 프로그램뿐만 아니라 기초에 대한 강좌를 수강합니다.\n유행하는 것뿐만 아니라 핵심 텍스트를 읽습니다. 그리고\n초전문화보다는 적어도 몇 가지 다른 분야의 교차점에 있으려고 노력합니다.",
    "crumbs": [
      "적용",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>결론</span>"
    ]
  },
  {
    "objectID": "17-concluding_ko.html#몇-가지-미해결-문제",
    "href": "17-concluding_ko.html#몇-가지-미해결-문제",
    "title": "18  결론",
    "section": "18.2 몇 가지 미해결 문제",
    "text": "18.2 몇 가지 미해결 문제\n데이터 과학에 대해 생각할 때 미해결된 많은 문제가 있습니다. 그것들은 명확한 답이 있는 유형의 문제가 아닙니다. 대신, 탐색하고 가지고 놀아야 할 질문입니다. 이 작업은 데이터 과학을 발전시키고, 더 중요하게는, 우리가 세상에 대한 더 나은 이야기를 하는 데 도움이 될 것입니다. 여기서 우리는 그 중 일부를 자세히 설명합니다.\n1. 효과적인 테스트를 어떻게 작성합니까?\n컴퓨터 과학은 테스트를 중심으로 철저한 기반을 구축했으며, 단위 및 기능 테스트의 중요성은 널리 받아들여지고 있습니다. 이 책의 혁신 중 하나는 데이터 과학 워크플로 전반에 걸쳐 테스트를 통합하는 것이었지만, 이것은 모든 것의 첫 번째 반복과 마찬가지로 상당한 개선과 개발이 필요합니다.\n데이터 과학을 통해 테스트를 철저히 통합해야 합니다. 그러나 이것이 어떤 모습이어야 하는지, 어떻게 해야 하는지, 그리고 최종 상태가 무엇인지는 불분명합니다. 데이터 과학에서 잘 테스트된 코드를 갖는다는 것은 무엇을 의미합니까? 테스트가 있는 코드 줄의 백분율을 측정하는 코드 커버리지는 데이터 과학에서 특히 의미가 없지만, 대신 무엇을 사용해야 합니까? 데이터 과학에서 테스트는 어떤 모습입니까? 어떻게 작성됩니까? 데이터 과학이 채택한 통계의 광범위한 시뮬레이션 사용은 기초를 제공하지만, 상당한 양의 작업과 투자가 필요합니다.\n2. 데이터 정리 및 준비 단계에서 무슨 일이 일어나고 있습니까?\n데이터 정리 및 준비가 추정치에 얼마나 영향을 미치는지에 대한 좋은 이해가 없습니다. Huntington-Klein 기타 (2021) 및 Breznau 기타 (2022) 등은 이 작업을 시작했습니다. 그들은 숨겨진 연구 결정이 후속 추정치에 큰 영향을 미치며, 때로는 표준 오차보다 더 크다는 것을 보여줍니다. 통계는 모델링이 추정치에 어떤 영향을 미치는지에 대한 좋은 이해를 제공하지만, 데이터 과학 워크플로의 초기 단계의 영향에 대한 더 많은 조사가 필요합니다. 더 구체적으로, 우리는 주요 실패 지점을 찾고 실패가 발생할 수 있는 방식을 이해해야 합니다.\n이것은 더 큰 데이터셋으로 확장할 때 특히 우려됩니다. 예를 들어, ImageNet은 1,400만 개의 이미지로 구성된 데이터셋이며, 수동으로 주석이 달려 있습니다. 시간과 비용 모두에서, 모든 이미지를 검토하여 레이블이 데이터셋의 각 사용자의 요구와 일치하는지 확인하는 것은 엄청나게 어렵습니다. 그러나 이것을 수행하지 않으면, 특히 명백하지 않은 경우, 후속 모델 예측에 대한 신뢰를 갖기 어렵습니다.\n3. 효과적인 이름을 어떻게 만듭니까?\n생물학의 최고 업적 중 하나는 이명법입니다. 이것은 18세기 의사인 카롤루스 린네우스가 확립한 이름에 대한 공식적인 체계적인 접근 방식입니다 (Morange 2016, p. 81). 각 종은 라틴어 문법 형태를 가진 두 단어로 지칭됩니다. 첫 번째는 속이고, 두 번째는 종을 특징짓는 형용사입니다. 표준화된 명명법을 보장하는 것은 생물학에서 적극적으로 고려됩니다. 예를 들어, 연구자들이 명명법 위원회를 사용하는 것이 권장됩니다 (McCarthy 기타 2023). ?sec-clean-and-prepare에서 논의했듯이, 이름은 데이터 과학에서 큰 마찰의 원천이며, 데이터 과학에서도 유사하게 표준화된 접근 방식이 필요합니다.\n이것이 매우 시급한 이유는 이해에 영향을 미치고, 이는 효율성에 영향을 미치기 때문입니다. 이명법은 단순한 참조가 아니라 진단 정보를 제공합니다 (Koerner 2000, p. 45). 이것은 특히 데이터 과학이 한 개인이 아닌 팀에서 수행될 때 그렇습니다. 효과적인 이름이 무엇인지에 대한 철저한 이해와 그것들을 장려하기 위한 인프라는 상당한 이익을 가져올 것입니다.\n4. 데이터 과학과 구성 요소 간의 적절한 관계는 무엇입니까?\n우리는 데이터 과학의 기원을 다양한 학문 분야로 설명했습니다. 앞으로 우리는 이러한 구성 요소, 특히 통계 및 컴퓨터 과학이 어떤 역할을 해야 하는지 고려해야 합니다. 더 일반적으로, 우리는 또한 데이터 과학이 계량 경제학, 응용 수학 및 계산 사회 과학과 어떻게 관련되고 상호 작용하는지 확립해야 합니다. 이것들은 자신의 학문 분야에서 질문에 답하기 위해 데이터 과학을 활용하지만, 통계 및 컴퓨터 과학과 마찬가지로 데이터 과학에도 다시 기여합니다. 예를 들어, 계산 사회 과학에서 기계 학습의 응용은 투명성, 해석 가능성, 불확실성 및 윤리에 초점을 맞춰야 하며, 이 모든 것은 다른 학문 분야에서 수행되는 더 이론적인 기계 학습 연구를 발전시킵니다 (Wallach 2018).\n통계학자로부터 통계를, 컴퓨터 과학자로부터 컴퓨터 과학을 계속 배우도록 주의해야 합니다. 이것을 하지 않는 것의 위험의 예는 p-값의 경우에 분명하며, 이 책에서는 많이 다루지 않았지만, 통계학자들이 수십 년 동안 그 오용에 대해 경고했음에도 불구하고 정량 분석을 지배합니다. 통계학자로부터 통계를 배우지 않는 한 가지 문제는 통계적 실천이 순진하게 따르는 레시피가 될 수 있다는 것입니다. 왜냐하면 그것이 가르치는 가장 쉬운 방법이기 때문입니다. 비록 그것이 통계학자들이 통계를 하는 방식이 아니더라도 말입니다.\n데이터 과학은 이러한 학문 분야와 깊이 연결되어 있어야 합니다. 데이터 과학이 나쁜 관행을 가져오지 않으면서 최상의 측면을 계속 갖도록 보장하는 방법은 특히 중요한 과제입니다. 그리고 이것은 기술적인 것뿐만 아니라 문화적인 것이기도 합니다 (Meng 2021). 데이터 과학이 포용적인 우수성 문화를 유지하도록 보장하는 것이 특히 중요합니다.\n5. 데이터 과학을 어떻게 가르칩니까?\n데이터 과학의 기초가 무엇인지에 대한 합의가 이루어지기 시작했습니다. 여기에는 계산적 사고, 표본 추출, 통계, 그래프, Git 및 GitHub, SQL, 명령줄, 지저분한 데이터 정리, R 및 파이썬을 포함한 몇 가지 언어, 윤리 및 글쓰기에 대한 편안함을 개발하는 것이 포함됩니다. 그러나 그것을 가장 잘 가르치는 방법에 대한 합의는 거의 없습니다. 부분적으로 이것은 데이터 과학 강사가 종종 다른 분야에서 오기 때문이지만, 부분적으로는 자원과 우선 순위의 차이이기도 합니다.\n문제를 복잡하게 만드는 것은 데이터 과학 기술에 대한 수요를 감안할 때, 학부생이 노동 시장에 진입할 때 해당 기술이 필요하기 때문에 데이터 과학 교육을 대학원생에게만 제한할 수 없다는 것입니다. 데이터 과학이 학부 수준에서 가르쳐지려면, 대규모 수업에서 가르칠 수 있을 만큼 견고해야 합니다. 확장 가능한 교육 도구를 개발하는 것이 중요합니다. 예를 들어, GitHub Actions를 사용하여 학생 코드를 확인하고 강사의 개입 없이 개선 사항을 제안할 수 있습니다. 그러나 학생들이 종종 매우 유용하다고 생각하는 사례 연구 스타일 수업을 확장하는 것은 특히 어렵습니다. 상당한 혁신이 필요합니다.\n6. 산업과 학계의 관계는 어떤 모습입니까?\n데이터 과학의 상당한 혁신은 산업계에서 일어나지만, 때로는 이 지식이 공유될 수 없으며, 공유될 수 있을 때에도 느리게 이루어지는 경향이 있습니다. 데이터 과학이라는 용어는 1960년대부터 학계에서 사용되었지만, 지난 10년 동안 인기를 얻게 된 것은 산업계 덕분입니다 (Irizarry 2020).\n학계와 산업계를 하나로 모으는 것은 데이터 과학의 핵심 과제이자 가장 쉽게 간과되는 것 중 하나입니다. 산업계에서 직면하는 문제의 성격, 예를 들어 고객의 요구 사항을 파악하고 대규모로 운영하는 것은 일반적인 학문적 관심사와는 거리가 멉니다. 학자들이 산업계에 한 발을 들여놓고 유지하지 않고, 산업계가 학계에 적극적으로 참여하도록 하지 않으면 학술 연구가 무의미해질 위험이 있습니다. 산업계 측면에서는 즉각적인 성과가 없다면 모범 사례를 신속하게 채택하는 것이 어려울 수 있습니다. 학술 채용 및 보조금 평가에서 산업 경험을 중요하게 여기고, 학계에서 기업가 정신을 장려하면 도움이 될 것입니다.",
    "crumbs": [
      "적용",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>결론</span>"
    ]
  },
  {
    "objectID": "17-concluding_ko.html#다음-단계",
    "href": "17-concluding_ko.html#다음-단계",
    "title": "18  결론",
    "section": "18.3 다음 단계",
    "text": "18.3 다음 단계\n이 책은 많은 내용을 다루었으며, 끝을 향해 가고 있지만, 가즈오 이시구로의 소설 남아있는 나날에서 집사 스티븐스가 듣는 것처럼:\n\n저녁은 하루 중 가장 좋은 부분입니다. 하루의 일을 마쳤습니다. 이제 발을 뻗고 즐길 수 있습니다.\nIshiguro (1989)\n\n당신이 쌓아온 기초를 바탕으로 더 탐구하고 싶은 측면이 있을 가능성이 있습니다. 그렇다면, 이 책은 그 목표를 달성한 것입니다.\n이 책을 시작할 때 데이터 과학에 처음이라면, 다음 단계는 우리가 건너뛴 것을 보충하는 것입니다. 데이터 과학: 첫 번째 소개 (Timbers, Campbell, 와/과 Lee 2022)로 시작하십시오. 그 후 R for Data Science (Wickham, Çetinkaya-Rundel, 와/과 Grolemund [2016년] 2023)를 살펴보십시오. 이 책에서는 R을 사용했고 SQL과 파이썬은 지나가듯이 언급했지만, 이러한 언어에 익숙해지는 것이 중요합니다. 데이터 과학자를 위한 SQL (Teate 2022), 데이터 분석을 위한 파이썬 (McKinney [2011년] 2022), 그리고 무료 Replit “100일 코딩” 파이썬 과정으로 시작하십시오.\n표본 추출은 데이터 과학의 중요하지만 간과하기 쉬운 측면입니다. 표본 추출: 설계 및 분석 (Lohr [1999년] 2022)을 살펴보는 것이 합리적일 것입니다. 설문 조사 및 실험에 대한 이해를 심화시키려면, 다음으로 현장 실험: 설계, 분석 및 해석 (Gerber 와/과 Green 2012) 및 신뢰할 수 있는 온라인 통제 실험 (Kohavi, Tang, 와/과 Xu 2020)으로 이동하십시오.\n더 나은 데이터 시각화 기술을 개발하려면, 데이터 스케치 (Bremer 와/과 Wu 2021)와 데이터 시각화 (Healy 2018)로 시작하십시오. 그 후, 그래픽의 문법 (Wilkinson 2005)과 같은 강력한 기초를 개발하십시오.\n모델링에 대해 더 배우고 싶다면, 다음 단계는 통계적 재고: R 및 스탠의 예제를 사용한 베이지안 과정 (McElreath [2015년] 2020)이며, 여기에는 훌륭한 동반 비디오 시리즈도 있습니다. 베이즈 규칙! R을 사용한 베이지안 모델링 입문 (Johnson, Ott, 와/과 Dogucu 2022) 및 회귀 및 기타 이야기 (Gelman, Hill, 와/과 Vehtari 2020)입니다. 또한 모든 통계 (Wasserman 2005)로 확률의 기초를 다지는 것이 가치가 있을 것입니다.\n기계 학습에 관심이 있다면, 다음 자연스러운 단계는 통계 학습 입문 (James 기타 [2013년] 2021)과 통계 학습의 요소 (Friedman, Tibshirani, 와/과 Hastie 2009)뿐입니다.\n인과 관계에 대해 더 배우려면, 인과 추론: 믹스테이프 (Cunningham 2021)와 효과: 연구 설계 및 인과성 입문 (Huntington-Klein 2021)을 통해 경제학적 관점으로 시작하십시오. 그런 다음 만약 (Hernán 와/과 Robins 2023)을 통해 보건 과학적 관점으로 전환하십시오.\n데이터로서의 텍스트에 대해서는, 데이터로서의 텍스트 (Grimmer, Roberts, 와/과 Stewart 2022)로 시작하십시오. 그런 다음 R에서의 텍스트 분석을 위한 지도 기계 학습 (Hvitfeldt 와/과 Silge 2021)으로 넘어가십시오.\n윤리 측면에서는 다양한 책이 있습니다. 이 책 전반에 걸쳐 많은 장을 다루었지만, 데이터 페미니즘 (D’Ignazio 와/과 Klein 2020)을 처음부터 끝까지 살펴보는 것이 유용할 것이며, AI의 아틀라스 (Crawford 2021)도 마찬가지입니다.\n그리고 마지막으로, 글쓰기에 관해서는, 내면으로 향하는 것이 가장 좋습니다. 한 달 동안 매일 글을 쓰도록 자신을 강요하십시오. 그런 다음 다시, 그리고 다시 하십시오. 당신은 더 나아질 것입니다. 그렇긴 하지만, 일하기 (Caro 2019)와 글쓰기에 관하여: 공예에 대한 회고록 (King 2000)을 포함하여 몇 가지 유용한 책이 있습니다.\n우리는 종종 “데이터가 말하게 하라”는 말을 듣습니다. 이것이 결코 일어나지 않는다는 것이 분명하기를 바랍니다. 우리가 할 수 있는 전부는 우리가 데이터를 사용하여 이야기를 하는 사람이라는 것을 인정하고, 그것들을 가치 있게 만들기 위해 노력하고 추구하는 것입니다.\n\n그녀의 목소리가 하늘이 사라질 때 가장 날카롭게 만들었다. 그녀는 그 고독을 시간까지 측정했다. 그녀는 세상의 유일한 창조자였다. 그녀가 노래한 곳에서. 그리고 그녀가 노래했을 때, 바다는, 어떤 자아를 가졌든, 그 자아가 되었다. 그것은 그녀의 노래였고, 그녀가 창조자였기 때문이다.\n“키웨스트에서의 질서의 개념”에서 발췌, (Stevens 1934)",
    "crumbs": [
      "적용",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>결론</span>"
    ]
  },
  {
    "objectID": "17-concluding_ko.html#연습-문제",
    "href": "17-concluding_ko.html#연습-문제",
    "title": "18  결론",
    "section": "18.4 연습 문제",
    "text": "18.4 연습 문제\n\n질문\n\n데이터 과학이란 무엇입니까?\n데이터는 누구에게 영향을 미치며, 무엇이 데이터에 영향을 미칩니까?\n모델에 “인종” 및/또는 “성적 지향”을 포함하는 것에 대해 논의하십시오.\n무엇이 이야기를 더 설득력 있게 또는 덜 설득력 있게 만듭니까?\n데이터를 다룰 때 윤리의 역할은 무엇입니까?\n\n\n\n수업 활동\n\nGitHub를 정리하십시오: 불필요한 리포지토리를 삭제하고, 최고의 리포지토리를 고정하고, 약력을 업데이트하고, 프로필 README를 추가하십시오.\n\n\n\n\n\nBremer, Nadieh, 와/과 Shirley Wu. 2021. Data Sketches. A K Peters/CRC Press. https://doi.org/10.1201/9780429445019.\n\n\nBreznau, Nate, Eike Mark Rinke, Alexander Wuttke, Hung HV Nguyen, Muna Adem, Jule Adriaans, Amalia Alvarez-Benjumea, 기타. 2022. “Observing many researchers using the same data and hypothesis reveals a hidden universe of uncertainty”. Proceedings of the National Academy of Sciences 119 (44): e2203150119. https://doi.org/10.1073/pnas.2203150119.\n\n\nCaro, Robert. 2019. Working. 1st ed. New York: Knopf.\n\n\nCrawford, Kate. 2021. Atlas of AI. 1st ed. New Haven: Yale University Press.\n\n\nCunningham, Scott. 2021. Causal Inference: The Mixtape. 1st ed. New Haven: Yale Press. https://mixtape.scunning.com.\n\n\nD’Ignazio, Catherine, 와/과 Lauren Klein. 2020. Data Feminism. Massachusetts: The MIT Press. https://data-feminism.mitpress.mit.edu.\n\n\nFriedman, Jerome, Robert Tibshirani, 와/과 Trevor Hastie. 2009. The Elements of Statistical Learning. 2nd ed. Springer. https://hastie.su.domains/ElemStatLearn/.\n\n\nGelman, Andrew, Jennifer Hill, 와/과 Aki Vehtari. 2020. Regression and Other Stories. Cambridge University Press. https://avehtari.github.io/ROS-Examples/.\n\n\nGerber, Alan, 와/과 Donald Green. 2012. Field Experiments: Design, Analysis, and Interpretation. New York: WW Norton.\n\n\nGray, Charles T., 와/과 Ben Marwick. 2019. “Truth, Proof, and Reproducibility: There’s No Counter-Attack for the Codeless”. In Communications in Computer and Information Science, 111–29. Springer Singapore. https://doi.org/10.1007/978-981-15-1960-4_8.\n\n\nGrimmer, Justin, Margaret Roberts, 와/과 Brandon Stewart. 2022. Text As Data: A New Framework for Machine Learning and the Social Sciences. New Jersey: Princeton University Press.\n\n\nHealy, Kieran. 2018. Data Visualization. New Jersey: Princeton University Press. https://socviz.co.\n\n\nHernán, Miguel, 와/과 James Robins. 2023. What If. 1st ed. Boca Raton: Chapman & Hall/CRC. https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/.\n\n\nHorton, Nicholas, Rohan Alexander, Micaela Parker, Aneta Piekut, 와/과 Colin Rundel. 2022. “The Growing Importance of Reproducibility and Responsible Workflow in the Data Science and Statistics Curriculum”. Journal of Statistics and Data Science Education 30 (3): 207–8. https://doi.org/10.1080/26939169.2022.2141001.\n\n\nHuntington-Klein, Nick. 2021. The Effect: An Introduction to Research Design and Causality. 1st ed. Chapman & Hall. https://theeffectbook.net.\n\n\nHuntington-Klein, Nick, Andreu Arenas, Emily Beam, Marco Bertoni, Jeffrey Bloem, Pralhad Burli, Naibin Chen, 기타. 2021. “The influence of hidden researcher decisions in applied microeconomics”. Economic Inquiry 59: 944–60. https://doi.org/10.1111/ecin.12992.\n\n\nHvitfeldt, Emil, 와/과 Julia Silge. 2021. Supervised Machine Learning for Text Analysis in R. 1st ed. Chapman; Hall/CRC. https://doi.org/10.1201/9781003093459.\n\n\nIrizarry, Rafael. 2020. “The Role of Academia in Data Science Education”. Harvard Data Science Review 2 (1). https://doi.org/10.1162/99608f92.dd363929.\n\n\nIshiguro, Kazuo. 1989. The Remains of the Day. 1st ed. Faber; Faber.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, 와/과 Robert Tibshirani. (2013년) 2021. An Introduction to Statistical Learning with Applications in R. 2nd ed. Springer. https://www.statlearning.com.\n\n\nJohnson, Alicia, Miles Ott, 와/과 Mine Dogucu. 2022. Bayes Rules! An Introduction to Bayesian Modeling with R. 1st ed. Chapman; Hall/CRC. https://www.bayesrulesbook.com.\n\n\nKing, Stephen. 2000. On Writing: A Memoir of the Craft. 1st ed. Scribner.\n\n\nKoerner, Lisbet. 2000. Linnaeus: Nature and Nation. Cambridge: Harvard University Press.\n\n\nKohavi, Ron, Diane Tang, 와/과 Ya Xu. 2020. Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing. Cambridge University Press.\n\n\nLeek, Jeff, Blakeley McShane, Andrew Gelman, David Colquhoun, Michèle Nuijten, 와/과 Steven Goodman. 2017. “Five ways to fix statistics”. Nature 551 (7682): 557–59. https://doi.org/10.1038/d41586-017-07522-z.\n\n\nLeonelli, Sabina. 2020. “Learning from Data Journeys”. In Data Journeys in the Sciences, 1–24. Springer International Publishing. https://doi.org/10.1007/978-3-030-37177-7_1.\n\n\nLohr, Sharon. (1999년) 2022. Sampling: Design and Analysis. 3rd ed. Chapman; Hall/CRC.\n\n\nMcCarthy, Fiona M., Tamsin E. M. Jones, Anne E. Kwitek, Cynthia L. Smith, Peter D. Vize, Monte Westerfield, 와/과 Elspeth A. Bruford. 2023. “The case for standardizing gene nomenclature in vertebrates”. Nature 614 (7948): E31–32. https://doi.org/10.1038/s41586-022-05633-w.\n\n\nMcElreath, Richard. (2015년) 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. 2nd ed. Chapman; Hall/CRC.\n\n\n———. 2020. “Science as Amateur Software Development”. YouTube, 9월. https://youtu.be/zwRdO9%5FGGhY.\n\n\nMcKinney, Wes. (2011년) 2022. Python for Data Analysis. 3rd ed. https://wesmckinney.com/book/.\n\n\nMeng, Xiao-Li. 2021. “What Are the Values of Data, Data Science, or Data Scientists?” Harvard Data Science Review 3 (1). https://doi.org/10.1162/99608f92.ee717cf7.\n\n\nMorange, Michel. 2016. A History of Biology. New Jersey: Princeton University Press.\n\n\nPerkel, Jeffrey. 2021. “Ten computer codes that transformed science”. Nature 589 (7842): 344–48. https://doi.org/10.1038/d41586-021-00075-2.\n\n\nStevens, Wallace. 1934. The Idea of Order at Key West. https://www.poetryfoundation.org/poems/43431/the-idea-of-order-at-key-west.\n\n\nTeate, Renée. 2022. SQL for Data Scientists. Wiley.\n\n\nTimbers, Tiffany, Trevor Campbell, 와/과 Melissa Lee. 2022. Data Science: A First Introduction. Chapman; Hall/CRC. https://datasciencebook.ca.\n\n\nWallach, Hanna. 2018. “Computational social science \\(\\ne\\) computer science + social data”. Communications of the ACM 61 (3): 42–44. https://doi.org/10.1145/3132698.\n\n\nWasserman, Larry. 2005. All of Statistics. Springer.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, 와/과 Garrett Grolemund. (2016년) 2023. R for Data Science. 2nd ed. O’Reilly Media. https://r4ds.hadley.nz.\n\n\nWilkinson, Leland. 2005. The Grammar of Graphics. 2nd ed. Springer.",
    "crumbs": [
      "적용",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>결론</span>"
    ]
  },
  {
    "objectID": "26-deploy_ko.html",
    "href": "26-deploy_ko.html",
    "title": "Online Appendix G — 생산",
    "section": "",
    "text": "G.1 소개\n선수 지식\n주요 개념 및 기술\n소프트웨어 및 패키지\n데이터셋을 개발하고 우리가 확신할 수 있는 모델로 탐색하는 작업을 마친 후, 우리는 그것을 우리 자신의 컴퓨터뿐만 아니라 더 널리 사용하고 싶을 수 있습니다. 이것을 하는 다양한 방법이 있습니다. 다음을 포함합니다.\n여기서 일반적인 아이디어는 우리가 전체 워크플로를 알고, 다른 사람들이 그것을 신뢰하도록 허용해야 한다는 것입니다. 이것이 지금까지 우리의 접근 방식이 가져온 것입니다. 그 후, 우리는 모델을 더 널리 사용하고 싶을 수 있습니다. 예를 들어, 웹사이트에서 일부 데이터를 스크랩하고, 그 혼돈에 질서를 부여하고, 일부 차트를 만들고, 적절하게 모델링하고, 이 모든 것을 작성했다고 가정해 봅시다. 대부분의 학술 환경에서는 그것으로 충분합니다. 그러나 많은 산업 환경에서는 모델을 사용하여 무언가를 하고 싶을 것입니다. 예를 들어, 여러 입력을 기반으로 모델을 사용하여 보험 견적을 생성할 수 있는 웹사이트를 설정하는 것입니다.\n이 장에서는 먼저 계산을 로컬 컴퓨터에서 클라우드로 이동하는 것으로 시작합니다. 그런 다음 모델 공유를 위한 R 패키지 및 Shiny 사용에 대해 설명합니다. 그것은 잘 작동하지만, 일부 환경에서는 다른 사용자가 우리가 초점을 맞추지 않는 방식으로 모델과 상호 작용하고 싶을 수 있습니다. 이것을 허용하는 한 가지 방법은 결과를 다른 컴퓨터에서 사용할 수 있도록 하는 것이며, 이를 위해 API를 만들고 싶을 것입니다. 따라서 API를 만드는 방법인 plumber (Schloerke 와/과 Allen 2022)를 소개합니다.",
    "crumbs": [
      "부록",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>생산</span>"
    ]
  },
  {
    "objectID": "26-deploy_ko.html#소개",
    "href": "26-deploy_ko.html#소개",
    "title": "Online Appendix G — 생산",
    "section": "",
    "text": "클라우드 사용;\nR 패키지 생성;\nshiny 응용 프로그램 만들기; 그리고\nplumber를 사용하여 API 만들기.",
    "crumbs": [
      "부록",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>생산</span>"
    ]
  },
  {
    "objectID": "26-deploy_ko.html#아마존-웹-서비스",
    "href": "26-deploy_ko.html#아마존-웹-서비스",
    "title": "Online Appendix G — 생산",
    "section": "G.2 아마존 웹 서비스",
    "text": "G.2 아마존 웹 서비스\n전설에 따르면 클라우드는 다른 사람의 컴퓨터에 대한 또 다른 이름일 뿐입니다. 그리고 그것이 어느 정도 사실이지만, 우리의 목적에는 그것으로 충분합니다. 다른 사람의 컴퓨터를 사용하는 방법을 배우는 것은 여러 가지 이유로 훌륭할 수 있습니다. 다음을 포함합니다.\n\n확장성: 새 컴퓨터를 구입하는 것은 상당히 비쌀 수 있습니다. 특히 가끔씩만 실행해야 하는 경우 더욱 그렇습니다. 그러나 클라우드를 사용하면 몇 시간 또는 며칠 동안만 임대할 수 있습니다. 이것은 우리가 이 비용을 상각하고 구매를 결정하기 전에 실제로 필요한 것이 무엇인지 파악할 수 있게 해줍니다. 또한 수요가 갑자기 크게 증가하는 경우 계산 규모를 쉽게 늘리거나 줄일 수 있습니다.\n이식성: 분석 워크플로를 로컬 컴퓨터에서 클라우드로 옮길 수 있다면, 이는 재현성 및 이식성 측면에서 좋은 일을 하고 있음을 시사합니다. 적어도 코드는 로컬과 클라우드 모두에서 실행될 수 있으며, 이는 재현성 측면에서 큰 진전입니다.\n설정 및 잊기: 시간이 오래 걸리는 작업을 수행하는 경우, 자신의 컴퓨터가 밤새 실행될 필요가 없다는 점은 훌륭할 수 있습니다. 또한, 많은 클라우드 옵션에서 R 및 파이썬과 같은 오픈 소스 통계 소프트웨어는 이미 사용 가능하거나 비교적 쉽게 설정할 수 있습니다.\n\n그렇긴 하지만, 다음과 같은 단점도 있습니다.\n\n비용: 대부분의 클라우드 옵션은 저렴하지만, 무료인 경우는 거의 없습니다. 비용에 대한 아이디어를 제공하자면, 잘 갖춰진 AWS 인스턴스를 며칠 동안 사용하는 데 몇 달러가 들 수 있습니다. 또한, 특히 처음에는 무언가를 실수로 잊어버리고 예상치 못한 큰 청구서가 발생하는 것도 쉽습니다.\n공개: 실수를 저지르고 실수로 모든 것을 공개하는 것이 쉬울 수 있습니다.\n시간: 클라우드에서 설정하고 익숙해지는 데 시간이 걸립니다.\n\n클라우드를 사용할 때, 우리는 일반적으로 “가상 머신”(VM)에서 코드를 실행합니다. 이것은 특정 기능을 가진 컴퓨터처럼 작동하도록 설계된 더 큰 컴퓨터 모음의 일부인 할당입니다. 예를 들어, 가상 머신에 8GB RAM, 128GB 저장 공간, 4개의 CPU가 있다고 지정할 수 있습니다. 그러면 VM은 해당 사양을 가진 컴퓨터처럼 작동합니다. 클라우드 옵션 사용 비용은 VM 사양에 따라 증가합니다.\n어떤 의미에서, 우리는 장 2 에서 Posit Cloud를 사용하도록 처음 권장한 것을 통해 클라우드 옵션으로 시작했으며, 부록 A 에서 로컬 컴퓨터로 이동했습니다. 그 클라우드 옵션은 특히 초보자를 위해 설계되었습니다. 이제 더 일반적인 클라우드 옵션인 아마존 웹 서비스(AWS)를 소개합니다. 종종 특정 비즈니스는 Google, AWS 또는 Azure와 같은 특정 클라우드 옵션을 사용하지만, 하나에 익숙해지면 다른 것을 더 쉽게 사용할 수 있습니다.\n아마존 웹 서비스는 아마존의 클라우드 서비스입니다. 시작하려면 여기에서 AWS 개발자 계정을 만들어야 합니다(그림 G.1 (a)).\n\n\n\n\n\n\n\n\n\n\n\n(a) AWS 개발자 웹사이트\n\n\n\n\n\n\n\n\n\n\n\n(b) AWS 개발자 콘솔\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) AWS 인스턴스 시작\n\n\n\n\n\n\n\n\n\n\n\n(d) 키 페어 설정\n\n\n\n\n\n\n\n그림 G.1: 아마존 AWS 설정 개요\n\n\n\n계정을 만든 후, 액세스할 컴퓨터가 위치할 지역을 선택해야 합니다. 그 후, EC2로 “가상 머신 시작”을 원합니다(그림 G.1 (b)).\n첫 번째 단계는 Amazon Machine Image(AMI)를 선택하는 것입니다. 이것은 사용할 컴퓨터의 세부 정보를 제공합니다. 예를 들어, 로컬 컴퓨터는 Monterey를 실행하는 MacBook일 수 있습니다. 루이스 애슬렛은 RStudio 및 기타 많은 것이 이미 설정된 AMI를 여기에서 제공합니다. 등록한 지역의 AMI를 검색하거나 애슬렛의 웹사이트에서 관련 링크를 클릭할 수 있습니다. 예를 들어, 캐나다 중앙 지역에 설정된 AMI를 사용하려면 “ami-0bdd24fd36f07b638”을 검색합니다. 이러한 AMI를 사용하는 이점은 RStudio를 위해 특별히 설정되었다는 것이지만, 2020년 8월에 컴파일되었기 때문에 약간 오래되었다는 단점이 있습니다.\n다음 단계에서는 컴퓨터의 성능을 선택할 수 있습니다. 무료 티어는 기본 컴퓨터이지만, 필요할 때 더 좋은 것을 선택할 수 있습니다. 이 시점에서 거의 인스턴스를 시작할 수 있습니다(그림 G.1 (c)). AWS를 더 진지하게 사용하기 시작하면, 특히 계정 보안과 관련하여 다른 옵션을 선택할 수 있습니다. AWS는 키 페어에 의존합니다. 따라서 Privacy Enhanced Mail(PEM)을 만들고 로컬에 저장해야 합니다(그림 G.1 (d)). 그런 다음 인스턴스를 시작할 수 있습니다.\n몇 분 후, 인스턴스가 실행될 것입니다. “공개 DNS”를 브라우저에 붙여넣어 사용할 수 있습니다. 사용자 이름은 “rstudio”이고 비밀번호는 인스턴스 ID입니다.\nRStudio가 실행 중이어야 합니다. 이것은 흥미로운 일입니다. 가장 먼저 할 일은 인스턴스의 지침에 따라 기본 비밀번호를 변경하는 것입니다.\n예를 들어, tidyverse를 설치할 필요가 없습니다. 대신 라이브러리를 호출하고 계속 진행할 수 있습니다. 이것은 이 AMI에 많은 패키지가 이미 설치되어 있기 때문입니다. installed.packages()를 사용하여 설치된 패키지 목록을 볼 수 있습니다. 예를 들어, rstan은 이미 설치되어 있으며, 필요한 경우 GPU가 있는 인스턴스를 설정할 수 있습니다.\n아마도 AWS 인스턴스를 시작할 수 있는 것만큼 중요한 것은 그것을 중지할 수 있다는 것입니다(청구되지 않도록). 무료 티어는 유용하지만, 꺼야 합니다. 인스턴스를 중지하려면 AWS 인스턴스 페이지에서 그것을 선택한 다음 “작업 -&gt; 인스턴스 상태 -&gt; 종료”를 선택하십시오.",
    "crumbs": [
      "부록",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>생산</span>"
    ]
  },
  {
    "objectID": "26-deploy_ko.html#plumber-및-모델-api",
    "href": "26-deploy_ko.html#plumber-및-모델-api",
    "title": "Online Appendix G — 생산",
    "section": "G.3 Plumber 및 모델 API",
    "text": "G.3 Plumber 및 모델 API\nplumber 패키지 (Schloerke 와/과 Allen 2022)의 기본 아이디어는 모델을 훈련시키고 예측을 원할 때 호출할 수 있는 API를 통해 사용할 수 있도록 하는 것입니다. 장 7 에서 데이터 수집의 맥락에서 API를 사람이 아닌 다른 컴퓨터가 액세스할 수 있도록 설정된 웹사이트로 비공식적으로 정의했음을 기억하십시오. 여기서 우리는 데이터를 모델을 포함하도록 확장합니다.\n작동하는 것을 만들기 위해, 출력에 관계없이 “Hello Toronto”를 반환하는 함수를 만들어 봅시다. 새 R 파일을 열고 다음을 추가한 다음 “plumber.R”로 저장하십시오(아직 plumber 패키지를 설치하지 않았다면 설치해야 할 수 있습니다).\n\n#* @get /print_toronto\nprint_toronto &lt;- function() {\n  result &lt;- \"Hello Toronto\"\n  return(result)\n}\n\n저장한 후, 편집기 오른쪽 상단에 “API 실행” 버튼이 나타나야 합니다. 그것을 클릭하면 API가 로드됩니다. API 주변에 GUI를 제공하는 “Swagger” 응용 프로그램이 될 것입니다. GET 메서드를 확장한 다음 “시도해 보기”를 클릭하고 “실행”을 클릭하십시오. 응답 본문에 “Hello Toronto”가 나타나야 합니다.\n이것이 컴퓨터를 위해 설계된 API라는 사실을 더 밀접하게 반영하기 위해, “요청 URL”을 브라우저에 복사/붙여넣기하면 “Hello Toronto”가 반환되어야 합니다.\n\nG.3.0.1 로컬 모델\n이제 API를 업데이트하여 입력이 주어지면 모델 출력을 제공하도록 할 것입니다. 이것은 (buhrplumber를?) 따릅니다.\n이 시점에서 새 R 프로젝트를 시작해야 합니다. 시작하려면 일부 데이터를 시뮬레이션한 다음 그것에 대해 모델을 훈련시켜 봅시다. 이 경우, 우리는 아기가 오후 낮잠을 얼마나 잤는지 알 때 밤새 얼마나 오래 잠을 잘 수 있는지 예측하는 데 관심이 있습니다.\n\nset.seed(853)\n\nnumber_of_observations &lt;- 1000\n\nbaby_sleep &lt;-\n  tibble(\n    afternoon_nap_length = rnorm(number_of_observations, 120, 5) |&gt; abs(),\n    noise = rnorm(number_of_observations, 0, 120),\n    night_sleep_length = afternoon_nap_length * 4 + noise,\n  )\n\nbaby_sleep |&gt;\n  ggplot(aes(x = afternoon_nap_length, y = night_sleep_length)) +\n  geom_point(alpha = 0.5) +\n  labs(\n    x = \"Baby's afternoon nap length (minutes)\",\n    y = \"Baby's overnight sleep length (minutes)\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\n\n\n이제 tidymodels를 사용하여 빠르게 모델을 만들어 봅시다.\n\nset.seed(853)\n\nbaby_sleep_split &lt;- initial_split(baby_sleep, prop = 0.80)\nbaby_sleep_train &lt;- training(baby_sleep_split)\nbaby_sleep_test &lt;- testing(baby_sleep_split)\n\nmodel &lt;-\n  linear_reg() |&gt;\n  set_engine(engine = \"lm\") |&gt;\n  fit(\n    night_sleep_length ~ afternoon_nap_length,\n    data = baby_sleep_train\n  )\n\nwrite_rds(x = model, file = \"baby_sleep.rds\")\n\n이 시점에서 모델이 있습니다. 익숙할 수 있는 것과 다른 점은 모델을 “.rds” 파일로 저장했다는 것입니다. 그것을 읽어들일 것입니다.\n이제 모델이 있으므로 API를 사용하여 액세스할 파일에 넣고 싶습니다. 이 파일도 “plumber.R”이라고 부릅니다. 그리고 API를 설정하는 파일도 필요하며, “server.R”이라고 부릅니다. “server.R”이라는 R 스크립트를 만들고 다음 내용을 추가하십시오.\n\nlibrary(plumber)\n\nserve_model &lt;- plumb(\"plumber.R\")\nserve_model$run(port = 8000)\n\n그런 다음 “plumber.R”에 다음 내용을 추가하십시오.\n\nlibrary(plumber)\nlibrary(tidyverse)\n\nmodel &lt;- readRDS(\"baby_sleep.rds\")\n\nversion_number &lt;- \"0.0.1\"\n\nvariables &lt;-\n  list(\n    afternoon_nap_length = \"A value in minutes, likely between 0 and 240.\",\n    night_sleep_length = \"A forecast, in minutes, likely between 0 and 1000.\"\n  )\n\n#* @param afternoon_nap_length\n#* @get /survival\npredict_sleep &lt;- function(afternoon_nap_length = 0) {\n  afternoon_nap_length &lt;- as.integer(afternoon_nap_length)\n\n  payload &lt;- data.frame(afternoon_nap_length = afternoon_nap_length)\n\n  prediction &lt;- predict(model, payload)\n\n  result &lt;- list(\n    input = list(payload),\n    response = list(\"estimated_night_sleep\" = prediction),\n    status = 200,\n    model_version = version_number\n  )\n\n  return(result)\n}\n\n다시, “plumber.R” 파일을 저장한 후 “API 실행” 옵션이 나타나야 합니다. 그것을 클릭하면 이전과 동일한 방식으로 로컬에서 API를 테스트할 수 있습니다. 이 경우, “시도해 보기”를 클릭한 다음 오후 낮잠 시간을 분 단위로 입력하십시오. 응답 본문에는 우리가 설정한 데이터와 모델을 기반으로 한 예측이 포함될 것입니다.\n\n\nG.3.0.2 클라우드 모델\n이 시점까지, 우리는 우리 자신의 컴퓨터에서 API를 작동시켰지만, 우리가 정말로 하고 싶은 것은 API가 누구에게나 액세스할 수 있도록 컴퓨터에서 작동시키는 것입니다. 이를 위해 DigitalOcean을 사용할 것입니다. 유료 서비스이지만, 계정을 만들면 200달러의 크레딧이 제공되어 시작하기에 충분할 것입니다.\n이 설정 과정은 시간이 좀 걸리겠지만, 한 번만 하면 됩니다. 여기에 도움이 될 두 가지 추가 패키지는 plumberDeploy (Allen 2021)와 analogsea (Chamberlain 기타 2022)입니다(GitHub에서 설치해야 합니다: install_github(\"sckott/analogsea\")).\n이제 로컬 컴퓨터를 DigitalOcean 계정과 연결해야 합니다.\n\naccount()\n\n이제 연결을 인증해야 하며, 이는 SSH 공개 키를 사용하여 수행됩니다.\n\nkey_create()\n\n컴퓨터에 “.pub” 파일이 있어야 합니다. 그런 다음 해당 파일의 공개 키 부분을 복사하여 계정 보안 설정의 SSH 키 섹션에 추가하십시오. 로컬 컴퓨터에 키가 있으면 ssh를 사용하여 확인할 수 있습니다.\n\nssh_key_info()\n\n다시 말하지만, 이 모든 것을 검증하는 데 시간이 좀 걸릴 것입니다. DigitalOcean은 우리가 시작하는 모든 컴퓨터를 “droplet”이라고 부릅니다. 세 대의 컴퓨터를 시작하면 세 개의 droplet을 시작한 것입니다. 실행 중인 droplet을 확인할 수 있습니다.\n\ndroplets()\n\n모든 것이 제대로 설정되었다면, 계정과 연결된 모든 droplet에 대한 정보가 인쇄될 것입니다(이 시점에서는 아마도 없을 것입니다). 먼저 droplet을 만들어야 합니다.\n\nid &lt;- do_provision(example = FALSE)\n\n그런 다음 SSH 암호를 요청받고, 그러면 많은 것들이 설정될 것입니다. 그 후에는 droplet에 많은 것들을 설치해야 할 것입니다.\n\ninstall_r_package(\n  droplet = id,\n  c(\n    \"plumber\",\n    \"remotes\",\n    \"here\"\n  )\n)\n\ndebian_apt_get_install(\n  id,\n  \"libssl-dev\",\n  \"libsodium-dev\",\n  \"libcurl4-openssl-dev\"\n)\n\ndebian_apt_get_install(\n  id,\n  \"libxml2-dev\"\n)\n\ninstall_r_package(\n  id,\n  c(\n    \"config\",\n    \"httr\",\n    \"urltools\",\n    \"plumber\"\n  )\n)\n\ninstall_r_package(id, c(\"xml2\"))\ninstall_r_package(id, c(\"tidyverse\"))\ninstall_r_package(id, c(\"tidymodels\"))\n\n그리고 마침내 설정이 완료되면(약 30분 정도 걸릴 것입니다) API를 배포할 수 있습니다.\n\ndo_deploy_api(\n  droplet = id,\n  path = \"example\",\n  localPath = getwd(),\n  port = 8000,\n  docs = TRUE,\n  overwrite = TRUE\n)",
    "crumbs": [
      "부록",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>생산</span>"
    ]
  },
  {
    "objectID": "26-deploy_ko.html#연습-문제",
    "href": "26-deploy_ko.html#연습-문제",
    "title": "Online Appendix G — 생산",
    "section": "G.4 연습 문제",
    "text": "G.4 연습 문제\n\n스케일\n\n(계획)\n(시뮬레이션)\n(수집)\n(탐색)\n(전달)\n\n\n\n질문\n\n\n튜토리얼\n\n\n\n\nAllen, Jeff. 2021. plumberDeploy: Plumber Deployment. https://CRAN.R-project.org/package=plumberDeploy.\n\n\nBlair, James. 2019. Democratizing R with Plumber APIs. https://posit.co/resources/videos/democratizing-r-with-plumber-apis/.\n\n\nChamberlain, Scott, Hadley Wickham, Winston Chang, 와/과 Mauricio Vargas. 2022. analogsea: Interface to “Digital Ocean”. https://CRAN.R-project.org/package=analogsea.\n\n\nCsárdi, Gábor, Jim Hester, Hadley Wickham, Winston Chang, Martin Morgan, 와/과 Dan Tenenbaum. 2021. remotes: R Package Installation from Remote Repositories, Including “GitHub”. https://CRAN.R-project.org/package=remotes.\n\n\nGentemann, Chelle Leigh, Chris Holdgraf, Ryan Abernathey, Daniel Crichton, James Colliander, Edward Joseph Kearns, Yuvi Panda, 와/과 Richard Signell. 2021. “Science Storms the Cloud”. AGU Advances 2 (2). https://doi.org/10.1029/2020av000354.\n\n\nHuyen, Chip. 2020. “Machine learning is going real-time”, 12월. https://huyenchip.com/2020/12/27/real-time-machine-learning.html.\n\n\nKuhn, Max, 와/과 Hadley Wickham. 2020. tidymodels: a collection of packages for modeling and machine learning using tidyverse principles. https://www.tidymodels.org.\n\n\nOoms, Jeroen. 2022. ssh: Secure Shell (SSH) Client for R. https://CRAN.R-project.org/package=ssh.\n\n\nSchloerke, Barret, 와/과 Jeff Allen. 2022. plumber: An API Generator for R. https://CRAN.R-project.org/package=plumber.\n\n\nShankar, Shreya, Rolando Garcia, Joseph Hellerstein, 와/과 Aditya Parameswaran. 2022. “Operationalizing Machine Learning: An Interview Study”. arXiv. https://doi.org/10.48550/ARXIV.2209.09125.\n\n\nWickham, Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, 기타. 2019. “Welcome to the Tidyverse”. Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.",
    "crumbs": [
      "부록",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>생산</span>"
    ]
  },
  {
    "objectID": "98-cocktails_ko.html",
    "href": "98-cocktails_ko.html",
    "title": "Online Appendix H — 칵테일",
    "section": "",
    "text": "각 챕터는 모니카가 만든 칵테일에서 영감을 받았습니다. (대부분의) 이름은 라디오헤드 노래입니다.\n1장—데이터로 이야기하기\n모든 것이 제자리에\n바카디 슈페리어 1 oz 라임 주스 1/2 oz 망고 시럽 1/2 oz 샴페인 1 oz\n2장—소방 호스에서 마시기\n강에 뛰어들었는데 무엇을 보았을까?\n헤네시 2 oz 레몬 주스 1 oz 딸기 시럽 1 oz 소다수 채우기\n3장—재현 가능한 워크플로우\n알람도 놀라움도 없이\n엠프레스 진 1 1/2 oz 생 제르맹 1/2 oz 라임 주스 1/2 oz 라즈베리 시럽 1/4 oz 앙고스투라 비터스 한 방울\n4장—연구 글쓰기\n감상적으로 되지 마라, 항상 헛소리로 끝난다\n헤네시 1 oz 그랑 마르니에 1/2 oz 아마로 노니노 1/2 oz 레몬 주스 1/4 oz 꿀 시럽 1/4 oz 체리 가니쉬\n5장—정적 커뮤니케이션\n제자리를 찾아가는 직소 퍼즐\n불릿 라이 1 oz 레몬 주스 1 oz 그린 샤르트뢰즈 1 oz 생 제르맹 1/2 oz 룩사르도 마라스키노 오리지널 1/2 oz 앙고스투라 비터스 한 방울 레모네이드 채우기\n6장—데이터 수집 (Farm data)\n네온사인 속에서; 위아래로 스크롤\n불릿 라이 1 1/4 oz 그랑 마르니에 1/2 oz 레몬 주스 3/4 oz 오렌지 주스 3/4 oz 그레나딘 1/2 oz 라즈베리 시럽 1/4 oz\n7장—데이터 수집 (Gather data)\n1월에 4월 소나기\n버팔로 트레이스 버번 2 oz 갈리아노 바닐라 1 oz 라즈베리 시럽 1 oz 레몬 주스 1 oz 앙고스투라 비터스 한 방울\n8장—데이터 사냥 (Hunt data)\n여기 나는 살아있다, 항상 모든 것이\n릴레 2 oz 코인트로 1 oz 토닉 2-3 oz\n9장—데이터 정리 및 준비\n인프라가 붕괴될 것이다\n체리 주입 보드카 (아이스버그) 1 1/2 oz 스위트 베르무트 1 oz 라즈베리 시럽 1 oz 라임 주스 1 oz 앙고스투라 비터스 한 방울\n10장—저장 및 공유\n그리고 당신은 잘못된 곳을 보고 있다는 것을 깨닫는다\n포 로지스 버번 1 1/4 oz 사과 사이다 1 oz 그랑 마르니에 1/2 oz 아마레토 1/4 oz 레몬 주스 1/2 oz 생강 시럽 1/2 oz 앙고스투라 비터스 한 방울\n11장—탐색적 데이터 분석\n느낀다고 해서 그것이 거기에 있다는 의미는 아니다\n글린네반 캐나다 위스키 1 1/2 oz 스위트 베르무트 3/4 oz 베네딕틴 3/4 oz 압생트 한 방울 페이쇼 비터스 두 방울 앙고스투라 비터스 두 방울\n12장—선형 모델\n최선을 다할 수 있다면, 그 최선으로 충분하다\n불릿 라이 2 oz 스위트 베르무트 1 oz 블랙베리 시럽 1/2 oz 앙고스투라 비터스 한 방울\n13장—일반화 선형 모델\n그들은 우리를 대변하지 않는다\n캡틴 모건 다크 럼 1 1/2 oz 꿀 시럽 1 oz 레몬 주스 1 oz 생강 시럽 1/2 oz 피 브라더스 올드 패션드 비터스 한 방울 체리 가니쉬\n14장—관찰 데이터에서 인과 관계\n우리는 언제든 일어날 수 있는 사고이다\n헤네시 1 1/2 oz 코인트로 3/4 oz 베르주 3/4 oz 심플 시럽 1 바스푼 레몬 가니쉬\n15장—사후 계층화 다단계 회귀\n조금씩, 어떻게든\n캡틴 모건 다크 럼 1 1/2 oz 사탕수수 시럽 3/4 oz 레몬 주스 1/2 oz 오렌지 주스 1/2 oz 생강 시럽 1/2 oz 올드 패션드 비터스 한 방울\n16장—데이터로서의 텍스트\n단어는 무딘 도구이다; 단어는 잘린 산탄총이다\n버팔로 트레이스 버번 2 oz 룩사르도 마라스키노 오리지널 1/4 oz 설탕 조각 1개 앙고스투라 비터스 한 방울 압생트 한 방울 체리 가니쉬\n17장—결론\n나는 다음 막, 무대 뒤에서 기다리고 있다\n플리머스 진 1 oz 레몬 주스 1/2 oz 생 제르맹 1/2 oz 앙고스투라 비터스 한 방울 샴페인 채우기\n부록 A—R 필수\n중력은 항상 이긴다\n버팔로 트레이스 버번 2 oz 스위트 베르무트 1 oz 체리 시럽 1 1/2 oz 레몬 주스 1 1/2 oz 피 브라더스 올드 패션드 비터스 한 방울\n부록 B—데이터셋\nTBD\n부록 C—R 마크다운*\nTBD\n부록 D—논문*\nTBD\n부록 E—상호작용*\nTBD\n부록 F—데이터시트*\nTBD\n부록 G—SQL*\nTBD\n부록 H—예측*\n2+2는 항상 5가 된다\nTBD\n부록 I—생산*\n사랑의 관심은 필요 없다\n봄베이 사파이어 진 1 3/4 oz 으깬 오이에 10분간 얼음과 함께 담가둔 릴레 블랑 1 oz 베네딕틴 1/4 oz 앙고스투라 2 방울 저어주고 걸러냄 선택 사항: 신선한 보리지로 장식\n부록 J—수업 활동*\nTBD",
    "crumbs": [
      "부록",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>칵테일</span>"
    ]
  },
  {
    "objectID": "99-references.html",
    "href": "99-references.html",
    "title": "References",
    "section": "",
    "text": "Abelson, Harold, and Gerald Jay Sussman. 1996. Structure and\nInterpretation of Computer Programs. Cambridge: The MIT Press.\n\n\nAcemoglu, Daron, Simon Johnson, and James Robinson. 2001. “The\nColonial Origins of Comparative Development: An Empirical\nInvestigation.” American Economic Review 91\n(5): 1369–1401. https://doi.org/10.1257/aer.91.5.1369.\n\n\nAchen, Christopher. 1978. “Measuring Representation.”\nAmerican Journal of Political Science 22 (3): 475–510. https://doi.org/10.2307/2110458.\n\n\nAkerlof, George. 1970. “The Market for ‘Lemons’:\nQuality Uncertainty and the Market Mechanism.” The Quarterly\nJournal of Economics 84 (3): 488–500. https://doi.org/10.2307/1879431.\n\n\nAlexander, Monica. 2019. “Analyzing Name Changes After Marriage\nUsing a Non-Representative Survey,” August. https://www.monicaalexander.com/posts/2019-08-07-mrp/.\n\n\n———. 2021. “Overcoming Barriers to Sharing Code.”\nYouTube, February. https://youtu.be/yvM2C6aZ94k.\n\n\nAlexander, Rohan, and Paul Hodgetts. 2021.\nAustralianPoliticians: Provides Datasets About Australian\nPoliticians. https://CRAN.R-project.org/package=AustralianPoliticians.\n\n\nAlexander, Rohan, and A Mahfouz. 2021. heapsofpapers: Easily Download Heaps of PDF and CSV\nFiles. https://CRAN.R-project.org/package=heapsofpapers.\n\n\nAlexopoulos, Michelle, and Jon Cohen. 2015. “The power of print: Uncertainty shocks, markets, and the\neconomy.” International Review of Economics\n& Finance 40 (November): 8–28. https://doi.org/10.1016/j.iref.2015.02.002.\n\n\nAllen, Jeff. 2021. plumberDeploy: Plumber\nDeployment. https://CRAN.R-project.org/package=plumberDeploy.\n\n\nAlsan, Marcella, and Amy Finkelstein. 2021. “Beyond Causality:\nAdditional Benefits of Randomized Controlled Trials for Improving Health\nCare Delivery.” The Milbank Quarterly 99 (4): 864–81. https://doi.org/10.1111/1468-0009.12521.\n\n\nAmaka, Ofunne, and Amber Thomas. 2021. “The Naked Truth: How the\nNames of 6,816 Complexion Products Can Reveal Bias in Beauty.”\nThe Pudding, March. https://pudding.cool/2021/03/foundation-names/.\n\n\nAmerican Medical Association and New York Academy of Medicine. 1848.\nCode of Medical Ethics. Academy of Medicine. https://hdl.handle.net/2027/chi.57108026.\n\n\nAndersen, Robert, and David Armstrong. 2021. Presenting Statistical\nResults Effectively. London: Sage.\n\n\nAnderson, Margo, and Stephen Fienberg. 1999. Who Counts?: The Politics of Census-Taking in\nContemporary America. Russell Sage Foundation. http://www.jstor.org/stable/10.7758/9781610440059.\n\n\nAngelucci, Charles, and Julia Cagé. 2019. “Newspapers in Times of\nLow Advertising Revenues.” American Economic Journal:\nMicroeconomics 11 (3): 319–64. https://doi.org/10.1257/mic.20170306.\n\n\nAngrist, Joshua, and Jörn-Steffen Pischke. 2010. “The Credibility\nRevolution in Empirical Economics: How Better Research Design Is Taking\nthe Con Out of Econometrics.” Journal of Economic\nPerspectives 24 (2): 3–30. https://doi.org/10.1257/jep.24.2.3.\n\n\nAnnas, George. 2003. “HIPAA Regulations: A New Era of\nMedical-Record Privacy?” New England Journal of Medicine\n348 (15): 1486–90. https://doi.org/10.1056/NEJMlim035027.\n\n\nAnsolabehere, Stephen, Brian Schaffner, and Sam Luks. 2021. “Guide to the 2020 Cooperative Election\nStudy.” https://doi.org/10.7910/DVN/E9N6PH.\n\n\nArel-Bundock, Vincent. 2021. WDI: World\nDevelopment Indicators and Other World Bank Data. https://CRAN.R-project.org/package=WDI.\n\n\n———. 2022. “modelsummary: Data and\nModel Summaries in R.” Journal of Statistical\nSoftware 103 (1): 1–23. https://doi.org/10.18637/jss.v103.i01.\n\n\n———. 2023. marginaleffects: Predictions,\nComparisons, Slopes, Marginal Means, and Hypothesis Tests.\nhttps://vincentarelbundock.github.io/marginaleffects/.\n\n\n———. 2024. tinytable: Simple and Configurable\nTables in “HTML,” “LaTeX,”\n“Markdown,” “Word,” “PNG,”\n“PDF,” and “Typst” Formats. https://vincentarelbundock.github.io/tinytable/.\n\n\nArmstrong, Zan. 2022. “Stop Aggregating Away the Signal in Your\nData.” The Overflow, March. https://stackoverflow.blog/2022/03/03/stop-aggregating-away-the-signal-in-your-data/.\n\n\nArnold, Jeffrey. 2021. ggthemes: Extra Themes,\nScales and Geoms for “ggplot2”. https://CRAN.R-project.org/package=ggthemes.\n\n\nAsher, Sam, Tobias Lunt, Ryu Matsuura, and Paul Novosad. 2021.\n“Development Research at High Geographic Resolution: An Analysis\nof Night Lights, Firms, and Poverty in India Using the SHRUG Open Data\nPlatform.” World Bank Economic Review 35 (4). https://shrug-assets-ddl.s3.amazonaws.com/static/main/assets/other/almn-shrug.pdf.\n\n\nAu, Randy. 2020. “Data Cleaning IS Analysis, Not Grunt\nWork,” September. https://counting.substack.com/p/data-cleaning-is-analysis-not-grunt.\n\n\nBååth, Rasmus. 2018. beepr: Easily Play\nNotification Sounds on any Platform. https://CRAN.R-project.org/package=beepr.\n\n\nBache, Stefan Milton, and Hadley Wickham. 2022. magrittr: A Forward-Pipe Operator for R. https://CRAN.R-project.org/package=magrittr.\n\n\nBackus, John. 1981. “The History of FORTRAN\nI, II, and III.” In History of Programming\nLanguages, edited by Richard Wexelblat, 25–74. Academic Press.\n\n\nBailey, Rosemary. 2008. Design of Comparative Experiments.\nCambridge: Cambridge University Press. https://doi.org/10.1017/CBO9780511611483.\n\n\nBaker, Reg, Michael Brick, Nancy Bates, Mike Battaglia, Mick Couper,\nJill Dever, Krista Gile, and Roger Tourangeau. 2013. “Summary Report of the AAPOR Task Force on Non-Probability\nSampling.” Journal of Survey Statistics and\nMethodology 1 (2): 90–143. https://doi.org/10.1093/jssam/smt008.\n\n\nBandy, John, and Nicholas Vincent. 2021. “Addressing\n‘Documentation Debt’ in Machine Learning: A Retrospective\nDatasheet for BookCorpus.” In Proceedings of the Neural\nInformation Processing Systems Track on Datasets and Benchmarks,\nedited by J. Vanschoren and S. Yeung. Vol. 1. https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/file/54229abfcfa5649e7003b83dd4755294-Paper-round1.pdf.\n\n\nBanerjee, Abhijit, and Esther Duflo. 2011. Poor Economics: A Radical\nRethinking of the Way to Fight Global Poverty. New York:\nPublicAffairs.\n\n\nBarba, Lorena. 2018. “Terminologies for Reproducible\nResearch.” https://arxiv.org/abs/1802.03311.\n\n\nBarrett, Malcolm. 2021a. Data Science as an Atomic Habit. https://malco.io/articles/2021-01-04-data-science-as-an-atomic-habit.\n\n\n———. 2021b. ggdag: Analyze and Create Elegant\nDirected Acyclic Graphs. https://CRAN.R-project.org/package=ggdag.\n\n\nBarron, Alexander, Jenny Huang, Rebecca Spang, and Simon DeDeo. 2018.\n“Individuals, Institutions, and Innovation in the Debates of the\nFrench Revolution.” Proceedings of the National Academy of\nSciences 115 (18): 4607–12. https://doi.org/10.1073/pnas.1717729115.\n\n\nBaumgartner, Jason, Savvas Zannettou, Brian Keegan, Megan Squire, and\nJeremy Blackburn. 2020. “The Pushshift Reddit Dataset.”\narXiv. https://doi.org/10.48550/arxiv.2001.08435.\n\n\nBecker, Richard, Allan Wilks, Ray Brownrigg, Thomas Minka, and Alex\nDeckmyn. 2022. maps: Draw Geographical\nMaps. https://CRAN.R-project.org/package=maps.\n\n\nBegley, Glenn, and Lee Ellis. 2012. “Raise Standards for\nPreclinical Cancer Research.” Nature 483 (7391):\n531--533. https://doi.org/10.1038/483531a.\n\n\nBengtsson, Henrik. 2021. “A Unifying\nFramework for Parallel and Distributed Processing in R using\nFutures.” The R Journal 13 (2): 208–27. https://doi.org/10.32614/RJ-2021-048.\n\n\nBenoit, Kenneth. 2020. “Text as Data: An Overview.” In\nThe SAGE Handbook of Research Methods in Political Science and\nInternational Relations, edited by Luigi Curini and Robert\nFranzese, 461–97. London: SAGE Publishing. https://doi.org/10.4135/9781526486387.n29.\n\n\nBenoit, Kenneth, Kohei Watanabe, Haiyan Wang, Paul Nulty, Adam Obeng,\nStefan Müller, and Akitaka Matsuo. 2018. “quanteda: An R package for the quantitative analysis of\ntextual data.” Journal of Open Source Software 3\n(30): 774. https://doi.org/10.21105/joss.00774.\n\n\nBensinger, Greg. 2020. “Google Redraws the Borders on Maps\nDepending on Who’s Looking.” The Washington Post,\nFebruary. https://www.washingtonpost.com/technology/2020/02/14/google-maps-political-borders/.\n\n\nBerdine, Gilbert, Vincent Geloso, and Benjamin Powell. 2018.\n“Cuban Infant Mortality and Longevity: Health Care or\nRepression?” Health Policy and Planning 33 (6): 755–57.\nhttps://doi.org/10.1093/heapol/czy033.\n\n\nBerkson, Joseph. 1946. “Limitations of the Application of Fourfold\nTable Analysis to Hospital Data.” Biometrics Bulletin 2\n(3): 47–53. https://doi.org/10.2307/3002000.\n\n\nBerners-Lee, Timothy. 1989. “Information Management: A\nProposal.” https://www.w3.org/History/1989/proposal.html.\n\n\nBickel, Peter, Eugene Hammel, and William O’Connell. 1975. “Sex\nBias in Graduate Admissions: Data from Berkeley: Measuring Bias Is\nHarder Than Is Usually Assumed, and the Evidence Is Sometimes Contrary\nto Expectation.” Science 187 (4175): 398–404. https://doi.org/10.1126/science.187.4175.398.\n\n\nBiderman, Stella, Kieran Bicheno, and Leo Gao. 2022. “Datasheet\nfor the Pile.” https://arxiv.org/abs/2201.07311.\n\n\nBirkmeyer, John, Jonathan Finks, Amanda O’Reilly, Mary Oerline, Arthur\nCarlin, Andre Nunn, Justin Dimick, Mousumi Banerjee, and Nancy\nBirkmeyer. 2013. “Surgical Skill and Complication Rates After\nBariatric Surgery.” New England Journal of Medicine 369\n(15): 1434–42. https://doi.org/10.1056/nejmsa1300625.\n\n\nBlair, Graeme, Jasper Cooper, Alexander Coppock, and Macartan Humphreys.\n2019. “Declaring and Diagnosing Research Designs.”\nAmerican Political Science Review 113 (3): 838–59. https://doi.org/10.1017/S0003055419000194.\n\n\nBlair, Graeme, Jasper Cooper, Alexander Coppock, Macartan Humphreys, and\nLuke Sonnet. 2021. estimatr: Fast Estimators\nfor Design-Based Inference. https://CRAN.R-project.org/package=estimatr.\n\n\nBlair, James. 2019. Democratizing R with\nPlumber APIs. https://posit.co/resources/videos/democratizing-r-with-plumber-apis/.\n\n\nBland, Martin, and Douglas Altman. 1986. “Statistical Methods for\nAssessing Agreement Between Two Methods of Clinical Measurement.”\nThe Lancet 327 (8476): 307–10. https://doi.org/10.1016/S0140-6736(86)90837-8.\n\n\nBlei, David. 2012. “Probabilistic Topic Models.”\nCommunications of the ACM 55 (4): 77–84. https://doi.org/10.1145/2133806.2133826.\n\n\nBloom, Howard, Andrew Bell, and Kayla Reiman. 2020. “Using Data\nfrom Randomized Trials to Assess the Likely Generalizability of\nEducational Treatment-Effect Estimates from Regression Discontinuity\nDesigns.” Journal of Research on Educational\nEffectiveness 13 (3): 488–517. https://doi.org/10.1080/19345747.2019.1634169.\n\n\nBlumenthal, Mark. 2014. “Polls, Forecasts, and\nAggregators.” PS: Political Science & Politics 47\n(02): 297–300. https://doi.org/10.1017/s1049096514000055.\n\n\nBoland, Philip. 1984. “A Biographical Glimpse of William Sealy\nGosset.” The American Statistician 38 (3): 179–83. https://doi.org/10.2307/2683648.\n\n\nBolker, Ben, and David Robinson. 2022. broom.mixed: Tidying Methods for Mixed\nModels. https://CRAN.R-project.org/package=broom.mixed.\n\n\nBorer, Elizabeth T., Eric W. Seabloom, Matthew B. Jones, and Mark\nSchildhauer. 2009. “Some Simple Guidelines for Effective Data\nManagement.” Bulletin of the Ecological Society of\nAmerica 90 (2): 205–14. https://doi.org/10.1890/0012-9623-90.2.205.\n\n\nBorghi, John, and Ana Van Gulick. 2022. “Promoting Open Science\nThrough Research Data Management.” Harvard Data Science\nReview 4 (3). https://doi.org/10.1162/99608f92.9497f68e.\n\n\nBowen, Claire McKay. 2022. Protecting Your\nPrivacy in a Data-Driven World. 1st ed. Chapman; Hall/CRC.\nhttps://doi.org/10.1201/9781003122043.\n\n\nBowers, Jake, and Maarten Voors. 2016. “How to Improve Your\nRelationship with Your Future Self.” Revista de Ciencia\nPolı́tica 36 (3): 829–48. https://doi.org/10.4067/S0718-090X2016000300011.\n\n\nBowley, Arthur Lyon. 1901. Elements of Statistics. London: P.\nS. King.\n\n\n———. 1913. “Working-Class Households in Reading.”\nJournal of the Royal Statistical Society 76 (7): 672–701. https://doi.org/10.2307/2339708.\n\n\nBox, George E. P. 1976. “Science and Statistics.”\nJournal of the American Statistical Association 71 (356):\n791–99. https://doi.org/10.1080/01621459.1976.10480949.\n\n\nBoykis, Vicki. 2019. “A Deep Dive on Python Type Hints,”\nJuly. https://vickiboykis.com/2019/07/08/a-deep-dive-on-python-type-hints/.\n\n\nBoysel, Sam, and Davis Vaughan. 2021. fredr: An\nR Client for the “FRED” API. https://CRAN.R-project.org/package=fredr.\n\n\nBradley, Valerie, Shiro Kuriwaki, Michael Isakov, Dino Sejdinovic,\nXiao-Li Meng, and Seth Flaxman. 2021. “Unrepresentative Big\nSurveys Significantly Overestimated US Vaccine\nUptake.” Nature 600 (7890): 695–700. https://doi.org/10.1038/s41586-021-04198-4.\n\n\nBraginsky, Mika. 2020. wordbankr: Accessing the\nWordbank Database. https://CRAN.R-project.org/package=wordbankr.\n\n\nBremer, Nadieh, and Shirley Wu. 2021. Data Sketches. A K\nPeters/CRC Press. https://doi.org/10.1201/9780429445019.\n\n\nBrewer, Cynthia. 2015. Designing Better Maps: A Guide for GIS\nUsers. 2nd ed.\n\n\nBrewer, Ken. 2013. “Three Controversies in the History of Survey\nSampling.” Survey Methodology 39 (2): 249–63.\n\n\nBreznau, Nate, Eike Mark Rinke, Alexander Wuttke, Hung HV Nguyen, Muna\nAdem, Jule Adriaans, Amalia Alvarez-Benjumea, et al. 2022.\n“Observing Many Researchers Using the Same Data and Hypothesis\nReveals a Hidden Universe of Uncertainty.” Proceedings of the\nNational Academy of Sciences 119 (44): e2203150119. https://doi.org/10.1073/pnas.2203150119.\n\n\nBrokowski, Carolyn, and Mazhar Adli. 2019. “CRISPR Ethics: Moral\nConsiderations for Applications of a Powerful Tool.” Journal\nof Molecular Biology 431 (1): 88–101. https://doi.org/10.1016/j.jmb.2018.05.044.\n\n\nBronner, Laura. 2021. “Quantitative Editing.”\nYouTube, June. https://youtu.be/LI5m9RzJgWc.\n\n\nBrontë, Charlotte. 1847. Jane Eyre. https://www.gutenberg.org/files/1260/1260-h/1260-h.htm.\n\n\n———. 1857. The Professor. https://www.gutenberg.org/files/1028/1028-h/1028-h.htm.\n\n\nBrown, Zack. 2018. “A Git Origin Story.” Linux\nJournal, July. https://www.linuxjournal.com/content/git-origin-story.\n\n\nBryan, Jenny. 2018a. “Excuse Me, Do You Have a Moment to Talk\nabout Version Control?” The American Statistician 72\n(1): 20–27. https://doi.org/10.1080/00031305.2017.1399928.\n\n\n———. 2018b. “Code Smells and Feels.” YouTube,\nJuly. https://youtu.be/7oyiPBjLAWY.\n\n\n———. 2020. Happy Git and GitHub for the\nuseR. https://happygitwithr.com.\n\n\nBryan, Jenny, and Jim Hester. 2020. What They\nForgot to Teach You About R. https://rstats.wtf/index.html.\n\n\nBryan, Jenny, Jim Hester, David Robinson, Hadley Wickham, and Christophe\nDervieux. 2022. reprex: Prepare Reproducible\nExample Code via the Clipboard. https://CRAN.R-project.org/package=reprex.\n\n\nBryan, Jenny, and Hadley Wickham. 2021. gh:\nGitHub API. https://CRAN.R-project.org/package=gh.\n\n\nBueno de Mesquita, Ethan, and Anthony Fowler. 2021. Thinking Clearly\nwith Data: A Guide to Quantitative Reasoning and Analysis. New\nJersey: Princeton University Press.\n\n\nBuja, Andreas, Dianne Cook, Heike Hofmann, Michael Lawrence, Eun-Kyung\nLee, Deborah F. Swayne, and Hadley Wickham. 2009. “Statistical\nInference for Exploratory Data Analysis and Model Diagnostics.”\nPhilosophical Transactions of the Royal Society A:\nMathematical, Physical and Engineering Sciences 367 (1906):\n4361–83. https://doi.org/10.1098/rsta.2009.0120.\n\n\nBuja, Andreas, Dianne Cook, and Deborah Swayne. 1996. “Interactive\nHigh-Dimensional Data Visualization.” Journal of\nComputational and Graphical Statistics 5 (1): 78–99. https://doi.org/10.2307/1390754.\n\n\nBuneman, Peter, Sanjeev Khanna, and Tan Wang-Chiew. 2001. “Why and\nWhere: A Characterization of Data Provenance.” In Database\nTheory  ICDT 2001, 316–30. Springer\nBerlin Heidelberg. https://doi.org/10.1007/3-540-44503-x_20.\n\n\nBush, Vannevar. 1945. “As We May Think.” The Atlantic\nMonthly, July. https://www.theatlantic.com/magazine/archive/1945/07/as-we-may-think/303881/.\n\n\nByrd, James Brian, Anna Greene, Deepashree Venkatesh Prasad, Xiaoqian\nJiang, and Casey Greene. 2020. “Responsible, Practical Genomic\nData Sharing That Accelerates Research.” Nature Reviews\nGenetics 21 (10): 615–29. https://doi.org/10.1038/s41576-020-0257-5.\n\n\nCalonico, Sebastian, Matias Cattaneo, Max Farrell, and Rocio Titiunik.\n2021. rdrobust: Robust Data-Driven Statistical\nInference in Regression-Discontinuity Designs. https://CRAN.R-project.org/package=rdrobust.\n\n\nCambon, Jesse, and Christopher Belanger. 2021. “tidygeocoder: Geocoding Made Easy.” Zenodo.\nhttps://doi.org/10.5281/zenodo.3981510.\n\n\nCanty, Angelo, and B. D. Ripley. 2021. boot:\nBootstrap R (S-Plus) Functions.\n\n\nCarl, Sebastian, Ben Baldwin, Lee Sharpe, Tan Ho, and John Edwards.\n2023. Nflverse: Easily Install and Load the ’Nflverse’. https://CRAN.R-project.org/package=nflverse.\n\n\nCaro, Robert. 2019. Working. 1st ed. New York: Knopf.\n\n\nCarpenter, Christopher, and Carlos Dobkin. 2014. “Replication data for: The Minimum Legal Drinking Age and\nCrime.” https://doi.org/10.7910/DVN/27070.\n\n\nCarroll, Lewis. 1871. Through the Looking-Glass. Macmillan. https://www.gutenberg.org/files/12/12-h/12-h.htm.\n\n\nChamberlain, Scott, Hadley Wickham, Winston Chang, and Mauricio Vargas.\n2022. Analogsea: Interface to “Digital Ocean”. https://CRAN.R-project.org/package=analogsea.\n\n\nChamberlin, Donald. 2012. “Early History of\nSQL.” IEEE Annals of the History of\nComputing 34 (4): 78–82. https://doi.org/10.1109/mahc.2012.61.\n\n\nChambliss, Daniel. 1989. “The Mundanity of Excellence: An\nEthnographic Report on Stratification and Olympic Swimmers.”\nSociological Theory 7 (1): 70–86. https://doi.org/10.2307/202063.\n\n\nChambru, Cédric, and Paul Maneuvrier-Hervieu. 2022. “Introducing HiSCoD: A new gateway for the study of\nhistorical social conflict.” Working Paper Series,\nDepartment of Economics, University of Zurich. https://doi.org/10.5167/uzh-217109.\n\n\nChan, Duo. 2021. “Combining Statistical, Physical, and Historical\nEvidence to Improve Historical Sea-Surface Temperature Records.”\nHarvard Data Science Review 3 (1). https://doi.org/10.1162/99608f92.edcee38f.\n\n\nChang, Winston, Joe Cheng, JJ Allaire, Carson Sievert, Barret Schloerke,\nYihui Xie, Jeff Allen, Jonathan McPherson, Alan Dipert, and Barbara\nBorges. 2021. shiny: Web Application Framework\nfor R. https://CRAN.R-project.org/package=shiny.\n\n\nChase, William. 2020. “The Glamour of Graphics.”\nRStudio Conference, January. https://posit.co/resources/videos/the-glamour-of-graphics/.\n\n\nChen, Heng, Marie-Hélène Felt, and Christopher Henry. 2018. “2017\nMethods-of-Payment Survey: Sample Calibration and Variance\nEstimation.” Bank of Canada. https://doi.org/10.34989/tr-114.\n\n\nChen, Wei, Xilu Chen, Chang-Tai Hsieh, and Zheng Song. 2019. “A\nForensic Examination of China’s National Accounts.” Brookings\nPapers on Economic Activity, 77–127. https://www.jstor.org/stable/26798817.\n\n\nCheng, Joe, Bhaskar Karambelkar, and Yihui Xie. 2021. leaflet: Create Interactive Web Maps with the JavaScript\n“Leaflet” Library. https://CRAN.R-project.org/package=leaflet.\n\n\nCheriet, Mohamed, Nawwaf Kharma, Cheng-Lin Liu, and Ching Suen. 2007.\nCharacter Recognition Systems: A Guide for Students and\nPractitioner. Wiley.\n\n\nChouldechova, Alexandra, Diana Benavides-Prado, Oleksandr Fialko, and\nRhema Vaithianathan. 2018. “A Case Study of Algorithm-Assisted\nDecision Making in Child Maltreatment Hotline Screening\nDecisions.” In Proceedings of the 1st Conference on Fairness,\nAccountability and Transparency, edited by Sorelle Friedler and\nChristo Wilson, 81:134–48. Proceedings of Machine Learning Research. https://proceedings.mlr.press/v81/chouldechova18a.html.\n\n\nChrétien, Jean. 2007. My Years as Prime Minister. 1st ed.\nToronto: Knopf Canada.\n\n\nChristensen, Garret, Jeremy Freese, and Edward Miguel. 2019.\nTransparent and Reproducible Social Science Research.\nCalifornia: University of California Press.\n\n\nChristian, Brian. 2012. “The A/B Test: Inside\nthe Technology That’s Changing the Rules of Business.”\nWired, April. https://www.wired.com/2012/04/ff-abtesting/.\n\n\nCirone, Alexandra, and Arthur Spirling. 2021. “Turning History\ninto Data: Data Collection, Measurement, and Inference in HPE.”\nJournal of Historical Political Economy 1 (1): 127–54. https://doi.org/10.1561/115.00000005.\n\n\nCity of Toronto. 2021. 2021 Street Needs Assessment. https://www.toronto.ca/city-government/data-research-maps/research-reports/housing-and-homelessness-research-and-reports/.\n\n\nCleveland, William. (1985) 1994. The Elements of Graphing Data.\n2nd ed. New Jersey: Hobart Press.\n\n\nCohen, Glenn, and Michelle Mello. 2018. “HIPAA and\nProtecting Health Information in the 21st Century.”\nJAMA 320 (3): 231. https://doi.org/10.1001/jama.2018.5630.\n\n\nCohen, Jason, Steven Teleki, and Eric Brown. 2006. Best Kept Secrets\nof Peer Code Review. Smart Bear Incorporated.\n\n\nCohn, Alain. 2019. “Data and code for: Civic\nHonesty Around the Globe.” Harvard Dataverse. https://doi.org/10.7910/dvn/ykbodn.\n\n\nCohn, Alain, Michel André Maréchal, David Tannenbaum, and Christian\nLukas Zünd. 2019a. “Civic Honesty Around the Globe.”\nScience 365 (6448): 70–73. https://doi.org/10.1126/science.aau8712.\n\n\n———. 2019b. “Supplementary Materials for: Civic Honesty Around the\nGlobe.” Science 365 (6448): 70–73.\n\n\nCohn, Nate. 2016. “We Gave Four Good Pollsters the Same Raw Data.\nThey Had Four Different Results.” The New York Times,\nSeptember. https://www.nytimes.com/interactive/2016/09/20/upshot/the-error-the-polling-world-rarely-talks-about.html.\n\n\nColombo, Tommaso, Holger Fröning, Pedro Javier Garcı̀a, and Wainer\nVandelli. 2016. “Optimizing the Data-Collection Time of a\nLarge-Scale Data-Acquisition System Through a Simulation\nFramework.” The Journal of Supercomputing 72 (12):\n4546–72. https://doi.org/10.1007/s11227-016-1764-1.\n\n\nCongelio, Bradley. 2024. Introduction to NFL\nAnalytics with R. 1st ed. Chapman; Hall/CRC. https://bradcongelio.com/nfl-analytics-with-r-book/.\n\n\nCook, Dianne, Andreas Buja, Javier Cabrera, and Catherine Hurley. 1995.\n“Grand Tour and Projection\nPursuit.” Journal of Computational and Graphical\nStatistics 4 (3): 155–72. https://doi.org/10.1080/10618600.1995.10474674.\n\n\nCook, Dianne, Nancy Reid, and Emi Tanaka. 2021. “The Foundation Is\nAvailable for Thinking about Data Visualization Inferentially.”\nHarvard Data Science Review 3 (3). https://doi.org/10.1162/99608f92.8453435d.\n\n\nCook, Dianne, and Deborah Swayne. 2007. Interactive and Dynamic Graphics for Data Analysis: With\nR and GGobi. 1st ed. Springer.\n\n\nCooley, David. 2020. mapdeck: Interactive Maps\nUsing “Mapbox GL JS” and\n“Deck.gl”. https://CRAN.R-project.org/package=mapdeck.\n\n\nCouncil of European Union. 2016. “General Data Protection\nRegulation 2016/679.” https://eur-lex.europa.eu/eli/reg/2016/679/oj.\n\n\nCowen, Tyler. 2021. “Episode 132: Amia Srinivasan on Utopian\nFeminism.” Conversations with Tyler, September. https://conversationswithtyler.com/episodes/amia-srinivasan/.\n\n\n———. 2023. “Episode 168: Katherine Rundell on the Art of\nWords.” Conversations with Tyler, January. https://conversationswithtyler.com/episodes/katherine-rundell/.\n\n\nCox, Murray. 2021. “Inside Airbnb—Toronto\nData.” http://insideairbnb.com/get-the-data.html.\n\n\nCoyle, Edward, Andrew Coggan, Mari Hopper, and Thomas Walters. 1988.\n“Determinants of Endurance in Well-Trained\nCyclists.” Journal of Applied Physiology 64 (6):\n2622–30. https://doi.org/10.1152/jappl.1988.64.6.2622.\n\n\nCraiu, Radu. 2019. “The Hiring Gambit: In Search of the Twofer\nData Scientist.” Harvard Data Science Review 1 (1). https://doi.org/10.1162/99608f92.440445cb.\n\n\nCrawford, Kate. 2021. Atlas of AI.\n1st ed. New Haven: Yale University Press.\n\n\nCrosby, Alfred. 1997. The Measure of Reality: Quantification in\nWestern Europe, 1250-1600. Cambridge: Cambridge University Press.\n\n\nCsárdi, Gábor. 2022. gitcreds: Query\n“git” Credentials from “R”. https://CRAN.R-project.org/package=gitcreds.\n\n\nCsárdi, Gábor, Jim Hester, Hadley Wickham, Winston Chang, Martin Morgan,\nand Dan Tenenbaum. 2021. remotes: R Package\nInstallation from Remote Repositories, Including\n“GitHub”. https://CRAN.R-project.org/package=remotes.\n\n\nCunningham, Scott. 2021. Causal Inference: The Mixtape. 1st ed.\nNew Haven: Yale Press. https://mixtape.scunning.com.\n\n\nD’Ignazio, Catherine, and Lauren Klein. 2020. Data Feminism.\nMassachusetts: The MIT Press. https://data-feminism.mitpress.mit.edu.\n\n\nDagan, Noa, Noam Barda, Eldad Kepten, Oren Miron, Shay Perchik, Mark\nKatz, Miguel Hernán, Marc Lipsitch, Ben Reis, and Ran Balicer. 2021.\n“BNT162b2 mRNA Covid-19 Vaccine in a Nationwide Mass Vaccination\nSetting.” New England Journal of Medicine 384 (15):\n1412–23. https://doi.org/10.1056/NEJMoa2101765.\n\n\nData and Justice Criminology Lab, Institute of Criminology and Criminal\nJustice, Carleton University; The Centre for Research & Innovation\nfor Black Survivors of Homicide Victims (The CRIB), at the\nFactor-Inwentash Faculty of Social Work, University of Toronto; Canadian\nCivil Liberties Association; Ethics and Technology Lab, Queen’s\nUniversity. 2022. “Tracking (in)justice: A Living Data Set\nTracking Canadian Police-Involved Deaths.” https://trackinginjustice.ca.\n\n\nDattani, Saloni. 2024. “The Rise in Reported Maternal Mortality\nRates in the US Is Largely Due to a Change in Measurement.”\nOur World in Data.\n\n\nDavies, Rhian, Steph Locke, and Lucy D’Agostino McGowan. 2022. datasauRus: Datasets from the Datasaurus\nDozen. https://CRAN.R-project.org/package=datasauRus.\n\n\nDavis, Darren. 1997. “Nonrandom Measurement Error and Race of\nInterviewer Effects Among African Americans.” The Public\nOpinion Quarterly 61 (1): 183–207. https://doi.org/10.1086/297792.\n\n\nDavison, A. C., and D. V. Hinkley. 1997. Bootstrap Methods and Their\nApplications. Cambridge: Cambridge University Press. http://statwww.epfl.ch/davison/BMA/.\n\n\nDe Jonge, Edwin, and Mark van der Loo. 2013. An\nintroduction to data cleaning with R. Statistics Netherlands\nHeerlen. https://cran.r-project.org/doc/contrib/de%5FJonge+van%5Fder%5FLoo-Introduction%5Fto%5Fdata%5Fcleaning%5Fwith%5FR.pdf.\n\n\nDeaton, Angus. 2010. “Instruments, Randomization, and Learning\nabout Development.” Journal of Economic Literature 48\n(2): 424–55. https://doi.org/10.1257/jel.48.2.424.\n\n\nDeWitt, Helen. 2000. The Last Samurai. 1st ed. United States:\nTalk Mirimax Books.\n\n\nDoggers, Peter. 2021. “Carlsen Wins Game 6, Longest World Chess\nChampionship Game of All Time,” December. https://www.chess.com/news/view/fide-world-chess-championship-2021-game-6.\n\n\nDolatsara, Hamidreza Ahady, Ying-Ju Chen, Robert Leonard, Fadel Megahed,\nand Allison Jones-Farmer. 2021. “Explaining Predictive Model\nPerformance: An Experimental Study of Data Preparation and Model\nChoice.” Big Data, October. https://doi.org/10.1089/big.2021.0067.\n\n\nDoll, Richard, and Bradford Hill. 1950. “Smoking and Carcinoma of\nthe Lung.” British Medical Journal 2 (4682): 739–48. https://doi.org/10.1136/bmj.2.4682.739.\n\n\nDruckman, James, and Donald Green. 2021. “A New Era of\nExperimental Political Science.” In Advances in Experimental\nPolitical Science, 1–16. Cambridge: Cambridge University Press. https://doi.org/10.1017/9781108777919.002.\n\n\nDuflo, Esther. 2020. “Field Experiments and the Practice of\nPolicy.” American Economic Review 110 (7): 1952–73. https://doi.org/10.1257/aer.110.7.1952.\n\n\nDwork, Cynthia, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006.\n“Calibrating Noise to Sensitivity in Private Data\nAnalysis.” In Theory of Cryptography Conference, 265–84.\nSpringer. https://doi.org/10.1007/11681878_14.\n\n\nDwork, Cynthia, and Aaron Roth. 2013. “The Algorithmic Foundations\nof Differential Privacy.” Foundations and Trends in\nTheoretical Computer Science 9 (3-4): 211–407. https://doi.org/10.1561/0400000042.\n\n\nEdgeworth, Francis Ysidro. 1885. “Methods of Statistics.”\nJournal of the Statistical Society of London, 181–217.\n\n\nEdwards, Jonathan. 2017. “PACE team response\nshows a disregard for the principles of science.”\nJournal of Health Psychology 22 (9): 1155–58. https://doi.org/10.1177/1359105317700886.\n\n\nEghbal, Nadia. 2020. Working in Public: The Making and Maintenance\nof Open Source Software. California: Stripe Press.\n\n\nEisenstein, Michael. 2022. “Need Web Data? Here’s How to Harvest\nThem.” Nature 607: 200–201. https://doi.org/10.1038/d41586-022-01830-9.\n\n\nFirke, Sam. 2023. janitor: Simple Tools for\nExamining and Cleaning Dirty Data. https://CRAN.R-project.org/package=janitor.\n\n\nFisher, Ronald. (1925) 1928. Statistical Methods for Research\nWorkers. 2nd ed. London: Oliver; Boyd.\n\n\nFlynn, Michael. 2022. troopdata: Tools for\nAnalyzing Cross-National Military Deployment and Basing\nData. https://CRAN.R-project.org/package=troopdata.\n\n\nFord, Paul. 2015. “What Is Code?” Bloomberg\nBusinessweek, June. https://www.bloomberg.com/graphics/2015-paul-ford-what-is-code/.\n\n\nForster, Edward Morgan. 1927. Aspects of the Novel. London:\nEdward Arnold.\n\n\nFourcade, Marion, and Kieran Healy. 2017. “Seeing Like a\nMarket.” Socio-Economic Review 15 (1): 9–29. https://doi.org/10.1093/ser/mww033.\n\n\nFowler, Martin, and Kent Beck. 2018. Refactoring: Improving the Design of Existing\nCode. 2nd ed. New York: Addison-Wesley Professional.\n\n\nFox, John, Sanford Weisberg, and Brad Price. 2022. carData:\nCompanion to Applied Regression Data Sets. https://CRAN.R-project.org/package=carData.\n\n\nFranconeri, Steven, Lace Padilla, Priti Shah, Jeffrey Zacks, and Jessica\nHullman. 2021. “The Science of Visual Data Communication: What\nWorks.” Psychological Science in the Public Interest 22\n(3): 110–61. https://doi.org/10.1177/15291006211051956.\n\n\nFrandell, Ashlee, Mary Feeney, Timothy Johnson, Eric Welch, Lesley\nMichalegko, and Heyjie Jung. 2021. “The Effects of Electronic\nAlert Letters for Internet Surveys of Academic Scientists.”\nScientometrics 126 (8): 7167–81. https://doi.org/10.1007/s11192-021-04029-3.\n\n\nFranklin, Laura. 2005. “Exploratory Experiments.”\nPhilosophy of Science 72 (5): 888–99. https://doi.org/10.1086/508117.\n\n\nFrick, Hannah, Fanny Chow, Max Kuhn, Michael Mahoney, Julia Silge, and\nHadley Wickham. 2022. rsample: General\nResampling Infrastructure. https://CRAN.R-project.org/package=rsample.\n\n\nFriedman, Jerome, Robert Tibshirani, and Trevor Hastie. 2009. The\nElements of Statistical Learning. 2nd ed. Springer. https://hastie.su.domains/ElemStatLearn/.\n\n\nFriendly, Michael. 2021. HistData: Data Sets from the History of\nStatistics and Data Visualization. https://CRAN.R-project.org/package=HistData.\n\n\nFriendly, Michael, and Howard Wainer. 2021. A History of Data\nVisualization and Graphic Communication. 1st ed. Massachusetts:\nHarvard University Press.\n\n\nFry, Hannah. 2020. “Big Tech Is Testing You.” The New\nYorker, February, 61–65. https://www.newyorker.com/magazine/2020/03/02/big-tech-is-testing-you.\n\n\nFryzlewicz, Piotr. 2024. “Telling Stories\nwith Data: With Applications in R.” The American\nStatistician, April, 1–5. https://doi.org/10.1080/00031305.2024.2339562.\n\n\nFunkhouser, Gray. 1937. “Historical Development of the Graphical\nRepresentation of Statistical Data.” Osiris 3: 269–404.\nhttps://doi.org/10.1086/368480.\n\n\nGagolewski, Marek. 2022. “stringi:\nFast and Portable Character String Processing in\nR.” Journal of Statistical Software 103\n(2): 1–59. https://doi.org/10.18637/jss.v103.i02.\n\n\nGalef, Julia. 2020. “Episode 248: Are Democrats Being Irrational?\n(David Shor).” Rationally Speaking, December. http://rationallyspeakingpodcast.org/248-are-democrats-being-irrational-david-shor/.\n\n\nGao, Lucy, Jacob Bien, and Daniela Witten. 2022. “Selective\nInference for Hierarchical Clustering.” Journal of the\nAmerican Statistical Association, October, 1–11. https://doi.org/10.1080/01621459.2022.2116331.\n\n\nGarfinkel, Irwin, Lee Rainwater, and Timothy Smeeding. 2006. “A\nRe-Examination of Welfare States and Inequality in Rich Nations: How\nin-Kind Transfers and Indirect Taxes Change the Story.”\nJournal of Policy Analysis and Management 25 (4): 897–919. https://doi.org/10.1002/pam.20213.\n\n\nGarnier, Simon, Noam Ross, Robert Rudis, Antônio Camargo, Marco Sciaini,\nand Cédric Scherer. 2021. viridis –\nColorblind-Friendly Color Maps for R. https://doi.org/10.5281/zenodo.4679424.\n\n\nGazeley, Ursula, Georges Reniers, Hallie Eilerts-Spinelli, Julio Romero\nPrieto, Momodou Jasseh, Sammy Khagayi, and Veronique Filippi. 2022.\n“Women’s Risk of Death Beyond 42 Days Post Partum: A Pooled\nAnalysis of Longitudinal Health and Demographic Surveillance System Data\nin Sub-Saharan Africa.” The Lancet Global Health 10\n(11): e1582–89. https://doi.org/10.1016/s2214-109x(22)00339-4.\n\n\nGebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman\nVaughan, Hanna Wallach, Hal Daumé III, and Kate Crawford. 2021.\n“Datasheets for Datasets.” Communications of the\nACM 64 (12): 86–92. https://doi.org/10.1145/3458723.\n\n\nGelfand, Sharla. 2021. “Make a ReprEx... Please.”\nYouTube, February. https://youtu.be/G5Nm-GpmrLw.\n\n\n———. 2022a. Astrologer: Chani Nicholas Weekly Horoscopes\n(2013-2017). http://github.com/sharlagelfand/astrologer.\n\n\n———. 2022b. opendatatoronto: Access the City of\nToronto Open Data Portal. https://CRAN.R-project.org/package=opendatatoronto.\n\n\nGelman, Andrew. 2016. “What has happened down\nhere is the winds have changed,” September. https://statmodeling.stat.columbia.edu/2016/09/21/what-has-happened-down-here-is-the-winds-have-changed/.\n\n\n———. 2019. “Another Regression Discontinuity Disaster and What Can\nWe Learn from It,” June. https://statmodeling.stat.columbia.edu/2019/06/25/another-regression-discontinuity-disaster-and-what-can-we-learn-from-it/.\n\n\n———. 2020. “Statistical Models of Election Outcomes.”\nYouTube, August. https://youtu.be/7gjDnrbLQ4k.\n\n\nGelman, Andrew, John Carlin, Hal Stern, David Dunson, Aki Vehtari, and\nDonald Rubin. (1995) 2014. Bayesian Data Analysis. 3rd ed.\nChapman; Hall/CRC.\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using\nRegression and Multilevel/Hierarchical Models. 1st ed. Cambridge\nUniversity Press.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and\nOther Stories. Cambridge University Press. https://avehtari.github.io/ROS-Examples/.\n\n\nGelman, Andrew, and Guido Imbens. 2019. “Why High-Order\nPolynomials Should Not Be Used in Regression Discontinuity\nDesigns.” Journal of Business & Economic Statistics\n37 (3): 447–56. https://doi.org/10.1080/07350015.2017.1366909.\n\n\nGelman, Andrew, and Eric Loken. 2013. “The Garden of Forking\nPaths: Why Multiple Comparisons Can Be a Problem, Even When There Is No\n‘Fishing Expedition’ or ‘p-Hacking’ and the\nResearch Hypothesis Was Posited Ahead of Time.” Department of\nStatistics, Columbia University. http://www.stat.columbia.edu/~gelman/research/unpublished/p%5Fhacking.pdf.\n\n\nGelman, Andrew, Cristian Pasarica, and Rahul Dodhia. 2002. “Let’s\nPractice What We Preach: Turning Tables into Graphs.” The\nAmerican Statistician 56 (2): 121–30. https://doi.org/10.1198/000313002317572790.\n\n\nGelman, Andrew, and Aki Vehtari. 2024. Active\nStatistics: Stories, Games, Problems, and Hands-on Demonstrations for\nApplied Regression and Causal Inference. Cambridge\nUniversity Press. https://doi.org/10.1017/9781009436243.\n\n\nGelman, Andrew, Aki Vehtari, Daniel Simpson, Charles Margossian, Bob\nCarpenter, Yuling Yao, Lauren Kennedy, Jonah Gabry, Paul-Christian\nBürkner, and Martin Modrák. 2020. “Bayesian Workflow.”\narXiv. https://doi.org/10.48550/arXiv.2011.01808.\n\n\nGentemann, Chelle Leigh, Chris Holdgraf, Ryan Abernathey, Daniel\nCrichton, James Colliander, Edward Joseph Kearns, Yuvi Panda, and\nRichard Signell. 2021. “Science Storms the Cloud.”\nAGU Advances 2 (2). https://doi.org/10.1029/2020av000354.\n\n\nGerber, Alan, and Donald Green. 2012. Field Experiments: Design,\nAnalysis, and Interpretation. New York: WW Norton.\n\n\nGertler, Paul, Sebastian Martinez, Patrick Premand, Laura Rawlings, and\nChristel Vermeersch. 2016. Impact Evaluation in Practice. 2nd\ned. The World Bank. https://doi.org/10.1596/978-1-4648-0779-4.\n\n\nGibney, Elizabeth. 2022. “The leap second’s\ntime is up: world votes to stop pausing clocks.”\nNature 612 (7938): 18–18. https://doi.org/10.1038/d41586-022-03783-5.\n\n\nGodfrey, Ernest. 1918. “History and Development of Statistics in\nCanada.” In The History of Statistics–Their Development and\nProgress in Many Countries. New York: Macmillan, edited by John\nKoren, 179–98. Macmillan Company of New York.\n\n\nGoodrich, Ben, Jonah Gabry, Imad Ali, and Sam Brilleman. 2023.\n“rstanarm: Bayesian applied\nregression modeling via Stan.” https://mc-stan.org/rstanarm.\n\n\nGoogle. 2022. “What to Look for in a Code Review.” Google\nEngineering Practices Documentation. https://google.github.io/eng-practices/review/reviewer/looking-for.html.\n\n\nGordon, Brett, Robert Moakler, and Florian Zettelmeyer. 2022.\n“Close Enough? A Large-Scale Exploration of Non-Experimental\nApproaches to Advertising Measurement.” Marketing\nScience, November. https://doi.org/10.1287/mksc.2022.1413.\n\n\nGordon, Brett, Florian Zettelmeyer, Neha Bhargava, and Dan Chapsky.\n2019. “A Comparison of Approaches to Advertising Measurement:\nEvidence from Big Field Experiments at Facebook.” Marketing\nScience 38 (2): 193–225. https://doi.org/10.1287/mksc.2018.1135.\n\n\nGould, Elliot, Hannah Fraser, Timothy Parker, Shinichi Nakagawa, Simon\nGriffith, Peter Vesk, and Fiona Fidler. 2023. “Same Data,\nDifferent Analysts: Variation in Effect Sizes Due to Analytical\nDecisions in Ecology and Evolutionary Biology,” October. https://doi.org/10.32942/x2gg62.\n\n\nGraham, Paul. 2020. “How to Write Usefully,” February. http://paulgraham.com/useful.html.\n\n\nGray, Charles T., and Ben Marwick. 2019. “Truth, Proof, and\nReproducibility: There’s No Counter-Attack for the Codeless.” In\nCommunications in Computer and Information Science, 111–29.\nSpringer Singapore. https://doi.org/10.1007/978-981-15-1960-4_8.\n\n\nGreen, Donald, Terence Leong, Holger Kern, Alan Gerber, and Christopher\nLarimer. 2009. “Testing the Accuracy of Regression Discontinuity\nAnalysis Using Experimental Benchmarks.” Political\nAnalysis 17 (4): 400–417. https://doi.org/10.1093/pan/mpp018.\n\n\nGreen, Eric. 2020. “Nivi Research: Mister P\nhelps us understand vaccine hesitancy,” December. https://research.nivi.io/posts/2020-12-08-mister-p-helps-us-understand-vaccine-hesitancy/.\n\n\nGreenberg, Bernard, Abdel-Latif Abul-Ela, Walt Simmons, and Daniel\nHorvitz. 1969. “The Unrelated Question Randomized Response Model:\nTheoretical Framework.” Journal of the American Statistical\nAssociation 64 (326): 520–39. https://doi.org/10.1080/01621459.1969.10500991.\n\n\nGreenland, Sander, Stephen Senn, Kenneth Rothman, John Carlin, Charles\nPoole, Steven Goodman, and Douglas Altman. 2016. “Statistical Tests, P values, Confidence Intervals, and\nPower: A Guide to Misinterpretations.” European\nJournal of Epidemiology 31 (4): 337–50. https://doi.org/10.1007/s10654-016-0149-3.\n\n\nGreifer, Noah. 2021. “Why Do We Do Matching for Causal Inference\nVs Regressing on Confounders?” Cross Validated,\nSeptember. https://stats.stackexchange.com/q/544958.\n\n\nGrimmer, Justin, Margaret Roberts, and Brandon Stewart. 2022. Text As Data: A New Framework for Machine Learning and\nthe Social Sciences. New Jersey: Princeton University Press.\n\n\nGrolemund, Garrett, and Hadley Wickham. 2011. “Dates and Times\nMade Easy with lubridate.”\nJournal of Statistical Software 40 (3): 1–25. https://doi.org/10.18637/jss.v040.i03.\n\n\nGronsbell, Jessica, Jessica Minnier, Sheng Yu, Katherine Liao, and\nTianxi Cai. 2019. “Automated Feature Selection of Predictors in\nElectronic Medical Records Data.” Biometrics 75 (1):\n268–77. https://doi.org/10.1111/biom.12987.\n\n\nGrün, Bettina, and Kurt Hornik. 2011. “topicmodels: An R Package for Fitting\nTopic Models.” Journal of Statistical Software 40 (13):\n1–30. https://doi.org/10.18637/jss.v040.i13.\n\n\nGustafsson, Karl, and Linus Hagström. 2017. “What Is the Point?\nTeaching Graduate Students How to Construct Political Science Research\nPuzzles.” European Political Science 17 (4): 634–48. https://doi.org/10.1057/s41304-017-0130-y.\n\n\nGutman, Robert. 1958. “Birth and Death Registration in\nMassachusetts: II. The Inauguration of a Modern System,\n1800-1849.” The Milbank Memorial Fund Quarterly 36 (4):\n373–402.\n\n\nHackett, Robert. 2016. “Researchers Caused an\nUproar By Publishing Data From 70,000 OkCupid Users.”\nFortune, May. https://fortune.com/2016/05/18/okcupid-data-research/.\n\n\nHalberstam, David. 1972. The Best and the\nBrightest. 1st ed. New York: Random House.\n\n\nHamming, Richard. (1997) 2020. The Art of Doing\nScience and Engineering. 2nd ed. Stripe Press.\n\n\nHand, David. 2018. “Statistical Challenges of Administrative and\nTransaction Data.” Journal of the Royal Statistical Society:\nSeries A (Statistics in Society) 181 (3): 555–605. https://doi.org/10.1111/rssa.12315.\n\n\nHao, Karen. 2019. “This is How AI Bias Really\nHappens—And Why It’s So Hard To Fix.” MIT Technology\nReview, February. https://www.technologyreview.com/2019/02/04/137602/this-is-how-ai-bias-really-happensand-why-its-so-hard-to-fix/.\n\n\nHart, Edmund, Pauline Barmby, David LeBauer, François Michonneau, Sarah\nMount, Patrick Mulrooney, Timothée Poisot, Kara Woo, Naupaka Zimmerman,\nand Jeffrey Hollister. 2016. “Ten Simple Rules for Digital Data\nStorage.” PLOS Computational Biology 12\n(10): e1005097. https://doi.org/10.1371/journal.pcbi.1005097.\n\n\nHartocollis, Anemona. 2022. “U.S. News Ranked\nColumbia No. 2, but a Math Professor Has His Doubts.”\nThe New York Times, March. https://www.nytimes.com/2022/03/17/us/columbia-university-rank.html.\n\n\nHassan, Mai. 2022. “New Insights on Africa’s Autocratic\nPast.” African Affairs 121 (483): 321–33. https://doi.org/10.1093/afraf/adac002.\n\n\nHawes, Michael. 2020. “Implementing Differential\nPrivacy: Seven Lessons From the\n2020 United States\nCensus.” Harvard Data Science Review 2 (2).\nhttps://doi.org/10.1162/99608f92.353c6f99.\n\n\nHayot, Eric. 2014. The Elements of Academic Style. New York:\nColumbia University Press.\n\n\nHealy, Kieran. 2018. Data Visualization. New Jersey: Princeton\nUniversity Press. https://socviz.co.\n\n\n———. 2020. “The Kitchen Counter Observatory,” May. https://kieranhealy.org/blog/archives/2020/05/21/the-kitchen-counter-observatory/.\n\n\nHeil, Benjamin, Michael Hoffman, Florian Markowetz, Su-In Lee, Casey\nGreene, and Stephanie Hicks. 2021. “Reproducibility Standards for\nMachine Learning in the Life Sciences.” Nature Methods\n18 (10): 1132–35. https://doi.org/10.1038/s41592-021-01256-7.\n\n\nHermans, Felienne. 2017. “Peter Hilton on Naming.” IEEE\nSoftware 34 (3): 117–20. https://doi.org/10.1109/MS.2017.81.\n\n\n———. 2021. The Programmer’s Brain: What Every Programmer Needs to\nKnow about Cognition. 1st ed. New York: Simon; Schuster. https://www.manning.com/books/the-programmers-brain.\n\n\nHernán, Miguel, and James Robins. 2023. What If. 1st ed. Boca\nRaton: Chapman & Hall/CRC. https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/.\n\n\nHester, Jim, Florent Angly, Russ Hyde, Michael Chirico, Kun Ren,\nAlexander Rosenstock, and Indrajeet Patil. 2022. lintr: A “Linter” for R Code. https://CRAN.R-project.org/package=lintr.\n\n\nHester, Jim, Hadley Wickham, and Gábor Csárdi. 2021. fs: Cross-Platform File System Operations Based on\n“libuv”. https://CRAN.R-project.org/package=fs.\n\n\nHill, Austin Bradford. 1965. “The Environment and Disease:\nAssociation or Causation?” Proceedings of the Royal Society\nof Medicine 58 (5): 295–300.\n\n\nHillel, Wayne. 2017. How Do We Trust Our Science Code? https://www.hillelwayne.com/how-do-we-trust-science-code/.\n\n\nHo, Daniel, Kosuke Imai, Gary King, and Elizabeth Stuart. 2011.\n“MatchIt: Nonparametric Preprocessing for Parametric\nCausal Inference.” Journal of Statistical Software 42\n(8): 1–28. https://doi.org/10.18637/jss.v042.i08.\n\n\nHodgetts, Paul. 2022. “The Negative Space of Data,” March.\nhttps://hodgettsp.netlify.app/post/data-negativespace/.\n\n\nHofmeister, Johannes, Janet Siegmund, and Daniel Holt. 2017.\n“Shorter Identifier Names Take Longer to Comprehend.” In\n2017 IEEE 24th International Conference on Software Analysis,\nEvolution and Reengineering (SANER), 217–27. https://doi.org/10.1109/saner.2017.7884623.\n\n\nHolland, Paul. 1986. “Statistics and Causal Inference.”\nJournal of the American Statistical Association 81 (396):\n945–60. https://doi.org/10.2307/2289064.\n\n\nHorst, Allison Marie, Alison Presmanes Hill, and Kristen Gorman. 2020.\npalmerpenguins: Palmer Archipelago (Antarctica)\npenguin data. https://doi.org/10.5281/zenodo.3960218.\n\n\nHorton, Nicholas, Rohan Alexander, Micaela Parker, Aneta Piekut, and\nColin Rundel. 2022. “The Growing Importance of Reproducibility and\nResponsible Workflow in the Data Science and Statistics\nCurriculum.” Journal of Statistics and Data Science\nEducation 30 (3): 207–8. https://doi.org/10.1080/26939169.2022.2141001.\n\n\nHorton, Nicholas, and Stuart Lipsitz. 2001. “Multiple Imputation\nin Practice.” The American Statistician 55 (3): 244–54.\nhttps://doi.org/10.1198/000313001317098266.\n\n\nHotz, Joseph, Christopher Bollinger, Tatiana Komarova, Charles Manski,\nRobert Moffitt, Denis Nekipelov, Aaron Sojourner, and Bruce Spencer.\n2022. “Balancing Data Privacy and Usability in the Federal\nStatistical System.” Proceedings of the National Academy of\nSciences 119 (31): 1–10. https://doi.org/10.1073/pnas.2104906119.\n\n\nHowes, Adam. 2022. “Representing Uncertainty Using Significant\nFigures,” April. https://athowes.github.io/posts/2022-04-24-representing-uncertainty-using-significant-figures/.\n\n\nHulley, Stephen, Steven Cummings, Warren Browner, Deborah Grady, and\nThomas Newman. 2007. Designing Clinical Research. 3rd ed.\nLippincott Williams & Wilkins.\n\n\nHullman, Jessica, and Andrew Gelman. 2021. “Designing for\nInteractive Exploratory Data Analysis Requires Theories of Graphical\nInference.” Harvard Data Science Review 3 (3). https://doi.org/10.1162/99608f92.3ab8a587.\n\n\nHuntington-Klein, Nick. 2021. The Effect: An Introduction to\nResearch Design and Causality. 1st ed. Chapman & Hall. https://theeffectbook.net.\n\n\n———. 2022. “Library of Statistical Techniques.” https://lost-stats.github.io.\n\n\nHuntington-Klein, Nick, Andreu Arenas, Emily Beam, Marco Bertoni,\nJeffrey Bloem, Pralhad Burli, Naibin Chen, et al. 2021. “The\nInfluence of Hidden Researcher Decisions in Applied\nMicroeconomics.” Economic Inquiry 59: 944–60. https://doi.org/10.1111/ecin.12992.\n\n\nHuyen, Chip. 2020. “Machine Learning Is Going Real-Time,”\nDecember. https://huyenchip.com/2020/12/27/real-time-machine-learning.html.\n\n\nHvitfeldt, Emil, and Julia Silge. 2021. Supervised Machine Learning for Text Analysis in\nR. 1st ed. Chapman; Hall/CRC. https://doi.org/10.1201/9781003093459.\n\n\nHyman, Michael, Luca Sartore, and Linda J Young. 2021. “Capture-Recapture Estimation of Characteristics of U.S.\nLocal Food Farms Using a Web-Scraped List Frame.”\nJournal of Survey Statistics and Methodology 10 (4): 979–1004.\nhttps://doi.org/10.1093/jssam/smab008.\n\n\nHyndman, Rob, Timothy Hyndman, Charles Gray, Sayani Gupta, and Jacquie\nTran. 2022. cricketdata: International Cricket\nData. https://CRAN.R-project.org/package=cricketdata.\n\n\nIannone, Richard. 2022. DiagrammeR: Graph/Network\nVisualization. https://CRAN.R-project.org/package=DiagrammeR.\n\n\nIannone, Richard, and Mauricio Vargas. 2022. pointblank: Data Validation and Organization of Metadata\nfor Local and Remote Tables. https://CRAN.R-project.org/package=pointblank.\n\n\nInternational Organization Of Legal Metrology. 2007. International\nVocabulary of Metrology – Basic and General Concepts and Associated\nTerms. 3rd ed. https://www.oiml.org/en/files/pdf%5Fv/v002-200-e07.pdf.\n\n\nIoannidis, John. 2005. “Why Most Published Research Findings Are\nFalse.” PLOS Medicine 2 (8): e124. https://doi.org/10.1371/journal.pmed.0020124.\n\n\nIrizarry, Rafael. 2020. “The Role of Academia\nin Data Science Education.” Harvard Data Science\nReview 2 (1). https://doi.org/10.1162/99608f92.dd363929.\n\n\nIrving, Damien, Kate Hertweck, Luke Johnston, Joel Ostblom, Charlotte\nWickham, and Greg Wilson. 2021. Research Software Engineering with\nPython. Chapman; Hall/CRC.\n\n\nIsaacson, Walter. 2011. Steve Jobs. 1st ed. Simon &\nSchuster.\n\n\nIshiguro, Kazuo. 1989. The Remains of the Day. 1st ed. Faber;\nFaber.\n\n\nIzrailev, Sergei. 2022. tictoc: Functions for\nTiming R Scripts, as Well as Implementations of “Stack” and\n“List” Structures. https://CRAN.R-project.org/package=tictoc.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n(2013) 2021. An Introduction to Statistical\nLearning with Applications in R. 2nd ed. Springer. https://www.statlearning.com.\n\n\nJet Propulsion Laboratory. 2009. “JPL\nInstitutional Coding Standard for the C Programming\nLanguage.” Document Number D-60411, March. https://web.archive.org/web/20111015064908/http://lars-lab.jpl.nasa.gov/JPL_Coding_Standard_C.pdf.\n\n\nJohnson, Alicia, Miles Ott, and Mine Dogucu. 2022. Bayes Rules! An Introduction to Bayesian Modeling with\nR. 1st ed. Chapman; Hall/CRC. https://www.bayesrulesbook.com.\n\n\nJohnson, Kaneesha. 2021. “Two Regimes of Prison Data\nCollection.” Harvard Data Science Review 3 (3). https://doi.org/10.1162/99608f92.72825001.\n\n\nJohnston, Myfanwy, and David Robinson. 2022. gutenbergr: Download and Process Public Domain Works from\nProject Gutenberg. https://CRAN.R-project.org/package=gutenbergr.\n\n\nJordan, Michael. 2019. “Artificial\nIntelligence–The Revolution Hasn’t Happened Yet.”\nHarvard Data Science Review 1 (1). https://doi.org/10.1162/99608f92.f06c6e61.\n\n\nJoyner, Michael. 1991. “Modeling: Optimal Marathon Performance on\nthe Basis of Physiological Factors.” Journal of Applied\nPhysiology 70 (2): 683–87. https://doi.org/10.1152/jappl.1991.70.2.683.\n\n\nJurafsky, Dan, and James Martin. (2000) 2023. Speech and Language\nProcessing. 3rd ed. https://web.stanford.edu/~jurafsky/slp3/.\n\n\nKahan, Brennan, Suzie Cro, Fan Li, and Michael Harhay. 2023.\n“Eliminating Ambiguous Treatment Effects Using Estimands.”\nAmerican Journal of Epidemiology, February. https://doi.org/10.1093/aje/kwad036.\n\n\nKahan, Brennan, Joanna Hindley, Mark Edwards, Suzie Cro, and Tim Morris.\n2024. “The estimands framework: a primer on\nthe ICH E9(R1) addendum.” BMJ, January, e076316.\nhttps://doi.org/10.1136/bmj-2023-076316.\n\n\nKahan, Brennan, Fan Li, Andrew Copas, and Michael Harhay. 2022.\n“Estimands in Cluster-Randomized Trials: Choosing Analyses That\nAnswer the Right Question.” International Journal of\nEpidemiology, July. https://doi.org/10.1093/ije/dyac131.\n\n\nKahle, David, and Hadley Wickham. 2013. “ggmap: Spatial Visualization with ggplot2.”\nThe R Journal 5 (1): 144–61. http://journal.r-project.org/archive/2013-1/kahle-wickham.pdf.\n\n\nKalamara, Eleni, Arthur Turrell, Chris Redl, George Kapetanios, and\nSujit Kapadia. 2022. “Making text count:\nEconomic forecasting using newspaper text.”\nJournal of Applied Econometrics 37 (5): 896–919.\nhttps://doi.org/10.1002/jae.2907.\n\n\nKalgin, Alexander. 2014. “Implementation of\nPerformance Management in Regional Government in Russia: Evidence of\nData Manipulation.” Public Management Review 18\n(1): 110–38. https://doi.org/10.1080/14719037.2014.965271.\n\n\nKarsten, Karl. 1923. Charts and Graphs. New York:\nPrentice-Hall.\n\n\nKay, Matthew. 2022. tidybayes: Tidy Data\nand Geoms for Bayesian Models. https://doi.org/10.5281/zenodo.1308151.\n\n\nKennedy, Lauren, Katharine Khanna, Daniel Simpson, Andrew Gelman, Yajun\nJia, and Julien Teitler. 2022. “He, She, They: Using Sex and\nGender in Survey Adjustment.” https://arxiv.org/abs/2009.14401.\n\n\nKenny, Christopher T., Shiro Kuriwaki, Cory McCartan, Evan T. R.\nRosenman, Tyler Simko, and Kosuke Imai. 2021. “The use of differential privacy for census data and its\nimpact on redistricting: The case of the 2020 U.S.\nCensus.” Science Advances 7 (41). https://doi.org/10.1126/sciadv.abk3283.\n\n\n———. 2023. “Comment: The Essential Role of Policy Evaluation for\nthe 2020 Census Disclosure Avoidance System.” Harvard Data\nScience Review, no. Special Issue 2. https://doi.org/10.1162/99608f92.abc2c765.\n\n\nKent, William. 1993. “My Height: A Model for Numeric\nInformation.” https://www.bkent.net/Doc/myheight.htm.\n\n\nKeshav, Srinivasan. 2007. “How to Read a Paper.”\nACM SIGCOMM Computer Communication\nReview 37 (3): 83–84. https://doi.org/10.1145/1273445.1273458.\n\n\nKeyes, Os. 2019. “Counting the Countless.” Real\nLife. https://reallifemag.com/counting-the-countless/.\n\n\nKharecha, Pushker, and James Hansen. 2013. “Prevented Mortality\nand Greenhouse Gas Emissions from Historical and Projected Nuclear\nPower.” Environmental Science & Technology 47 (9):\n4889–95. https://doi.org/10.1021/es3051197.\n\n\nKing, Gary. 2006. “Publication, Publication.” PS:\nPolitical Science & Politics 39 (1): 119–25. https://doi.org/10.1017/S1049096506060252.\n\n\nKing, Gary, and Richard Nielsen. 2019. “Why Propensity Scores\nShould Not Be Used for Matching.” Political Analysis 27\n(4): 435–54. https://doi.org/10.1017/pan.2019.11.\n\n\nKing, Stephen. 2000. On Writing: A Memoir of the Craft. 1st ed.\nScribner.\n\n\nKleiber, Christian, and Achim Zeileis. 2008. Applied Econometrics\nwith R. New York: Springer-Verlag. https://CRAN.R-project.org/package=AER.\n\n\nKnuth, Donald. 1984. “Literate Programming.” The\nComputer Journal 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.\n\n\n———. 1998. Art of Computer Programming, Volume 2: Seminumerical\nAlgorithms. 2nd ed.\n\n\nKoenker, Roger, and Achim Zeileis. 2009. “On Reproducible\nEconometric Research.” Journal of Applied Econometrics\n24 (5): 833–47. https://doi.org/10.1002/jae.1083.\n\n\nKoerner, Lisbet. 2000. Linnaeus: Nature and Nation. Cambridge:\nHarvard University Press.\n\n\nKohavi, Ron, Alex Deng, Brian Frasca, Roger Longbotham, Toby Walker, and\nYa Xu. 2012. “Trustworthy Online Controlled Experiments.”\nIn Proceedings of the 18th ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining -\nKDD 12, 1st ed. ACM Press.\nhttps://doi.org/10.1145/2339530.2339653.\n\n\nKohavi, Ron, Diane Tang, and Ya Xu. 2020. Trustworthy Online Controlled Experiments: A Practical\nGuide to A/B Testing. Cambridge University Press.\n\n\nKoitsalu, Marie, Martin Eklund, Jan Adolfsson, Henrik Grönberg, and\nYvonne Brandberg. 2018. “Effects of Pre-Notification, Invitation\nLength, Questionnaire Length and Reminder on Participation Rate: A\nQuasi-Randomised Controlled Trial.” BMC Medical Research\nMethodology 18 (3): 1–5. https://doi.org/10.1186/s12874-017-0467-5.\n\n\nKrantz, Sebastian. 2023. collapse: Advanced and\nFast Data Transformation. https://CRAN.R-project.org/package=collapse.\n\n\nKuhn, Max. 2022. tune: Tidy Tuning\nTools. https://CRAN.R-project.org/package=tune.\n\n\nKuhn, Max, and Hannah Frick. 2022. poissonreg:\nModel Wrappers for Poisson Regression. https://CRAN.R-project.org/package=poissonreg.\n\n\nKuhn, Max, and Davis Vaughan. 2022. parsnip: A\nCommon API to Modeling and Analysis Functions. https://CRAN.R-project.org/package=parsnip.\n\n\nKuhn, Max, Davis Vaughan, and Emil Hvitfeldt. 2022. yardstick: Tidy Characterizations of Model\nPerformance. https://CRAN.R-project.org/package=yardstick.\n\n\nKuhn, Max, and Hadley Wickham. 2020. tidymodels: a collection of packages for modeling and\nmachine learning using tidyverse principles. https://www.tidymodels.org.\n\n\n———. 2022. recipes: Preprocessing and Feature\nEngineering Steps for Modeling. https://CRAN.R-project.org/package=recipes.\n\n\nKuriwaki, Shiro, Will Beasley, and Thomas Leeper. 2023. dataverse: R Client for Dataverse 4+\nRepositories.\n\n\nKuznets, Simon, Lillian Epstein, and Elizabeth Jenks. 1941. National Income and Its Composition,\n1919-1938. National Bureau of Economic Research.\n\n\nLamott, Anne. 1994. Bird by Bird: Some Instructions on Writing and\nLife. Anchor Books.\n\n\nLandau, William Michael. 2021. “The targets R\nPackage: A Dynamic Make-Like Function-Oriented Pipeline Toolkit for\nReproducibility and High-Performance Computing.”\nJournal of Open Source Software 6 (57): 2959. https://doi.org/10.21105/joss.02959.\n\n\nLane, Nick. 2015. “The Unseen World: Reflections on Leeuwenhoek\n(1677) ‘Concerning Little Animals’.”\nPhilosophical Transactions of the Royal Society B: Biological\nSciences 370 (1666): 20140344. https://doi.org/10.1098/rstb.2014.0344.\n\n\nLarmarange, Joseph. 2023. labelled:\nManipulating Labelled Data. https://CRAN.R-project.org/package=labelled.\n\n\nLatour, Bruno. 1996. “On Actor-Network Theory: A Few\nClarifications.” Soziale Welt 47 (4): 369–81. http://www.jstor.org/stable/40878163.\n\n\nLauderdale, Benjamin, Delia Bailey, Jack Blumenau, and Douglas Rivers.\n2020. “Model-Based Pre-Election Polling for National and\nSub-National Outcomes in the US and UK.” International\nJournal of Forecasting 36 (2): 399–413. https://doi.org/10.1016/j.ijforecast.2019.05.012.\n\n\nLeek, Jeff, Blakeley McShane, Andrew Gelman, David Colquhoun, Michèle\nNuijten, and Steven Goodman. 2017. “Five Ways to Fix\nStatistics.” Nature 551 (7682): 557–59. https://doi.org/10.1038/d41586-017-07522-z.\n\n\nLeonelli, Sabina. 2020. “Learning from Data Journeys.” In\nData Journeys in the Sciences, 1–24. Springer International\nPublishing. https://doi.org/10.1007/978-3-030-37177-7_1.\n\n\nLetterman, Clark. 2021. “Q&A: How Pew\nResearch Center surveyed nearly 30,000 people in India,”\nJuly. https://medium.com/pew-research-center-decoded/q-a-how-pew-research-center-surveyed-nearly-30-000-people-in-india-7c778f6d650e.\n\n\nLevine, Judah, Patrizia Tavella, and Martin Milton. 2022. “Towards\na Consensus on a Continuous Coordinated Universal Time.”\nMetrologia 60 (1): 014001. https://doi.org/10.1088/1681-7575/ac9da5.\n\n\nLewis, Crystal. 2024. Data Management in Large-Scale Education\nResearch. 1st ed. Chapman; Hall/CRC. https://datamgmtinedresearch.com/index.html.\n\n\nLight, Richard, Judith Singer, and John Willett. 1990. By Design: Planning Research on Higher\nEducation. 1st ed. Cambridge: Harvard University Press.\n\n\nLima, Renato de, Oliver Phillips, Alvaro Duque, Sebastian Tello, Stuart\nDavies, Alexandre Adalardo de Oliveira, Sandra Muller, et al. 2022.\n“Making Forest Data Fair and Open.” Nature Ecology\n& Evolution 6 (April): 656–58. https://doi.org/10.1038/s41559-022-01738-7.\n\n\nLin, Herbert. 2014. “A Proposal to Reduce Government\nOverclassification of Information Related to National Security.”\nJournal of National Security Law and Policy 7: 443–63.\n\n\nLin, Sarah, Ibraheem Ali, and Greg Wilson. 2021. “Ten Quick Tips\nfor Making Things Findable.” PLOS Computational Biology\n16 (12): 1–10. https://doi.org/10.1371/journal.pcbi.1008469.\n\n\nLips, Hilary. 2020. Sex and Gender: An Introduction. 7th ed.\nIllinois: Waveland Press.\n\n\nLittle, Roderick, and Roger Lewis. 2021. “Estimands, Estimators,\nand Estimates.” JAMA 326 (10): 967. https://doi.org/10.1001/jama.2021.2886.\n\n\nLiu, Emily, Lenny Bronner, and Jeremy Bowers. 2022. “What the\nWashington Post Elections Engineering Team Had to Learn about Election\nData.” Washington Post Engineering, April. https://washpost.engineering/what-the-washington-post-elections-engineering-team-had-to-learn-about-election-data-a41603daf9ca.\n\n\nLockheed Martin. 2005. “Joint Strike Fighter Air Vehicle C++\nCoding Standards For The System Development And Demonstration\nProgram.” Document Number 2RDU00001 Rev C,\nDecember. https://www.stroustrup.com/JSF-AV-rules.pdf.\n\n\nLohr, Sharon. (1999) 2022. Sampling: Design and Analysis. 3rd\ned. Chapman; Hall/CRC.\n\n\nLoken, Meredith, and Hilary Matfess. 2023. “Introducing the\nWomen’s Activities in Armed Rebellion (WAAR) Project, 1946-2015.”\nJournal of Peace Research.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019. Geocomputation with R. 1st ed. Chapman;\nHall/CRC. https://geocompr.robinlovelace.net.\n\n\nLucas, Jack, Reed Merrill, Kelly Blidook, Sandra Breux, Laura Conrad,\nGabriel Eidelman, Royce Koop, et al. 2020. “Canadian\nMunicipal Elections Database.” Scholars Portal Dataverse.\nhttps://doi.org/10.5683/sp2/4mzjpq.\n\n\nLucas, Robert. 1978. “Asset Prices in an Exchange Economy.”\nEconometrica 46 (6): 1429–45. https://doi.org/10.2307/1913837.\n\n\nLuebke, David Martin, and Sybil Milton. 1994. “Locating the\nVictim: An Overview of Census-Taking, Tabulation Technology, and\nPersecution in Nazi Germany.” IEEE Annals of the History of\nComputing 16 (3): 25–39. https://doi.org/10.1109/MAHC.1994.298418.\n\n\nLumley, Thomas. 2020. “survey: analysis of\ncomplex survey samples.” https://cran.r-project.org/web/packages/survey/index.html.\n\n\nLundberg, Ian, Rebecca Johnson, and Brandon Stewart. 2021. “What\nIs Your Estimand? Defining the Target Quantity Connects Statistical\nEvidence to Theory.” American Sociological Review 86\n(3): 532–65. https://doi.org/10.1177/00031224211004187.\n\n\nLuscombe, Alex, Kevin Dick, and Kevin Walby. 2021. “Algorithmic\nThinking in the Public Interest: Navigating Technical, Legal, and\nEthical Hurdles to Web Scraping in the Social Sciences.”\nQuality & Quantity 56 (3): 1–22. https://doi.org/10.1007/s11135-021-01164-0.\n\n\nLyman, Frank. 1981. “The Responsive Classroom Discussion: The\nInclusion of All Students.” Mainstreaming Digest 109:\n109–13.\n\n\nMacDorman, Marian, and Eugene Declercq. 2018. “The Failure of\nUnited States Maternal Mortality Reporting and Its Impact on Women’s\nLives.” Birth 45 (2): 105–8. https://doi.org/1111/birt.12333.\n\n\nMaher, Michael. 1982. “Modelling Association Football\nScores.” Statistica Neerlandica 36 (3): 109–18. https://doi.org/10.1111/j.1467-9574.1982.tb00782.x.\n\n\nMaier, Maximilian, František Bartoš, Tom Stanley, David Shanks, Adam\nHarris, and Eric-Jan Wagenmakers. 2022. “No Evidence for Nudging\nAfter Adjusting for Publication Bias.” Proceedings of the\nNational Academy of Sciences 119 (31): e2200300119. https://doi.org/10.1073/pnas.2200300119.\n\n\nMammoliti, Anthony, Petr Smirnov, Minoru Nakano, Zhaleh Safikhani,\nChristopher Eeles, Heewon Seo, Sisira Kadambat Nair, et al. 2021.\n“Orchestrating and Sharing Large Multimodal Data for Transparent\nand Reproducible Research.” Nature Communications 12\n(1). https://doi.org/10.1038/s41467-021-25974-w.\n\n\nManski, Charles. 2022. “Inference with Imputed Data: The Allure of\nMaking Stuff Up.” arXiv. https://doi.org/10.48550/arXiv.2205.07388.\n\n\nMarchese, David. 2022. “Her Discovery Changed the World. How Does\nShe Think We Should Use It?” The New York Times, August.\nhttps://www.nytimes.com/interactive/2022/08/15/magazine/jennifer-doudna-crispr-interview.html.\n\n\nMartin, Charles, and Ben Popper. 2021. “Don’t Push That Button:\nExploring the Software That Flies SpaceX Rockets and Starships.”\nThe Overflow, December. https://stackoverflow.blog/2021/12/27/dont-push-that-button-exploring-the-software-that-flies-spacex-starships/.\n\n\nMatsumoto, Yukihiro. 2007. “Treating Code as\nan Essay.” In Beautiful Code, edited by Andy Oram\nand Greg Wilson, 477–81. O’Reilly.\n\n\nMattson, Greggor. 2017. “Artificial Intelligence Discovers\nGayface. Sigh.” https://greggormattson.com/2017/09/09/artificial-intelligence-discovers-gayface/amp/.\n\n\nMcCarthy, Fiona M., Tamsin E. M. Jones, Anne E. Kwitek, Cynthia L.\nSmith, Peter D. Vize, Monte Westerfield, and Elspeth A. Bruford. 2023.\n“The Case for Standardizing Gene Nomenclature in\nVertebrates.” Nature 614 (7948): E31–32. https://doi.org/10.1038/s41586-022-05633-w.\n\n\nMcElreath, Richard. (2015) 2020. Statistical\nRethinking: A Bayesian Course with Examples in R and Stan.\n2nd ed. Chapman; Hall/CRC.\n\n\n———. 2020. “Science as Amateur Software Development.”\nYouTube, September. https://youtu.be/zwRdO9%5FGGhY.\n\n\nMcIlroy, Doug, Ray Brownrigg, Thomas Minka, and Roger Bivand. 2023.\nmapproj: Map Projections. https://CRAN.R-project.org/package=mapproj.\n\n\nMcKinney, Wes. (2011) 2022. Python for Data Analysis. 3rd ed.\nhttps://wesmckinney.com/book/.\n\n\nMcPhee, John. 2017. Draft No. 4. 1st ed. Farrar, Straus;\nGiroux.\n\n\nMeng, Xiao-Li. 1994. “Multiple-Imputation Inferences with\nUncongenial Sources of Input.” Statistical Science 9\n(4): 538–58. https://doi.org/10.1214/ss/1177010269.\n\n\n———. 2012. “You Want Me to Analyze Data i Don’t Have? Are You\nInsane?” Shanghai Archives of Psychiatry 24 (5):\n297–301. https://doi.org/10.3969/j.issn.1002-0829.2012.05.011.\n\n\n———. 2018. “Statistical Paradises and Paradoxes in Big Data (i):\nLaw of Large Populations, Big Data Paradox, and the 2016 US Presidential\nElection.” The Annals of Applied Statistics 12 (2):\n685–726. https://doi.org/10.1214/18-AOAS1161SF.\n\n\n———. 2021. “What Are the Values of Data, Data Science, or Data\nScientists?” Harvard Data Science Review 3 (1). https://doi.org/10.1162/99608f92.ee717cf7.\n\n\nMerali, Zeeya. 2010. “Computational Science:... Error.”\nNature 467 (7317): 775–77. https://doi.org/10.1038/467775a.\n\n\nMichener, William. 2015. “Ten Simple Rules for Creating a Good\nData Management Plan.” PLOS Computational Biology 11\n(10): e1004525. https://doi.org/10.1371/journal.pcbi.1004525.\n\n\nMill, James. 1817. The History of British India. 1st ed. https://books.google.ca/books?id=Orw_AAAAcAAJ.\n\n\nMiller, Greg. 2014. “The Cartographer Who’s\nTransforming Map Design.” Wired, October. https://www.wired.com/2014/10/cindy-brewer-map-design/.\n\n\nMiller, Michael, and Joseph Sutherland. 2022. “The Effect of\nGender on Interruptions at Congressional Hearings.” American\nPolitical Science Review, 1–19. https://doi.org/10.1017/S0003055422000260.\n\n\nMindell, David. 2008. Digital Apollo: Human and\nMachine in Spaceflight. 1st ed. New York: The MIT Press.\n\n\nMinsky, Yaron. 2011. “OCaml for the\nmasses.” Communications of the ACM 54 (11):\n53–58. https://doi.org/10.1145/2018396.2018413.\n\n\n———. 2015. “Automated Trading and OCaml with Yaron Minsky.”\nHackers — Software Engineering Daily, November. https://softwareengineeringdaily.com/2015/11/09/automated-trading-and-ocaml-with-yaron-minsky/.\n\n\nMitchell, Alanna. 2022a. “Get Ready for the New, Improved\nSecond.” The New York Times, April. https://www.nytimes.com/2022/04/25/science/time-second-measurement.html.\n\n\n———. 2022b. “Time Has Run Out for the Leap Second.” The\nNew York Times, November. https://www.nytimes.com/2022/11/14/science/time-leap-second.html.\n\n\nMitchell, Margaret, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy\nVasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and\nTimnit Gebru. 2019. “Model Cards for Model Reporting.”\nProceedings of the Conference on Fairness, Accountability, and\nTransparency, January. https://doi.org/10.1145/3287560.3287596.\n\n\nMiyakawa, Tsuyoshi. 2020. “No Raw Data, No Science: Another\nPossible Source of the Reproducibility Crisis.” Molecular\nBrain 13 (1): 1–6. https://doi.org/10.1186/s13041-020-0552-2.\n\n\nMolanphy, Chris. 2012. “100 & Single: Three Rules to Define\nthe Term ‘One-Hit Wonder’ in 2012.” The Village\nVoice, September. https://www.villagevoice.com/2012/09/10/100-single-three-rules-to-define-the-term-one-hit-wonder-in-2012/.\n\n\nMorange, Michel. 2016. A History of Biology. New Jersey:\nPrinceton University Press.\n\n\nMoyer, Brian, and Abe Dunn. 2020. “Measuring the\nGross Domestic Product\n(GDP): The Ultimate Data\nScience Project.” Harvard Data\nScience Review 2 (1). https://doi.org/10.1162/99608f92.414caadb.\n\n\nMullard, Asher. 2021. “Half of Top Cancer Studies Fail\nHigh-Profile Reproducibility Effort.” Nature 600 (7889):\n368--369. https://doi.org/10.1038/d41586-021-03691-0.\n\n\nMüller, Kirill. 2020. here: A Simpler Way to\nFind Your Files. https://CRAN.R-project.org/package=here.\n\n\nMüller, Kirill, Tobias Schieferdecker, and Patrick Schratz. 2019.\nVisualization, Transformation and Reporting with the Tidyverse.\nhttps://krlmlr.github.io/vistransrep/.\n\n\nMüller, Kirill, and Lorenz Walthert. 2022. styler: Non-Invasive Pretty Printing of R\nCode. https://CRAN.R-project.org/package=styler.\n\n\nMüller, Kirill, and Hadley Wickham. 2022. tibble: Simple Data Frames. https://CRAN.R-project.org/package=tibble.\n\n\nNational Academies of Sciences, Engineering, and Medicine. 2019.\nReproducibility and Replicability in Science. 1st ed. National\nAcademies Press. https://doi.org/10.17226/25303.\n\n\nNelder, John. 1999. “From Statistics to Statistical\nScience.” Journal of the Royal Statistical Society: Series D\n(The Statistician) 48 (2): 257–69. https://doi.org/10.1111/1467-9884.00187.\n\n\nNeufeld, Anna, and Daniela Witten. 2021. “Discussion of Breiman’s\n\"Two Cultures\": From Two Cultures to One.” Observational\nStudies 7 (1): 171–74. https://doi.org/10.1353/obs.2021.0004.\n\n\nNeufeld, Michael. 2002. “Wernher von Braun, the SS, and\nConcentration Camp Labor: Questions of Moral, Political, and Criminal\nResponsibility.” German Studies Review 25 (1): 57–78. https://doi.org/10.2307/1433245.\n\n\nNeuwirth, Erich. 2022. RColorBrewer: ColorBrewer\nPalettes. https://CRAN.R-project.org/package=RColorBrewer.\n\n\nNewman, Daniel. 2014. “Missing Data: Five Practical\nGuidelines.” Organizational Research Methods 17 (4):\n372–411. https://doi.org/10.1177/1094428114548590.\n\n\nNeyman, Jerzy. 1934. “On the Two Different Aspects of the\nRepresentative Method: The Method of Stratified Sampling and the Method\nof Purposive Selection.” Journal of the Royal Statistical\nSociety 97 (4): 558–625. https://doi.org/10.2307/2342192.\n\n\nNobles, Melissa. 2002. “Racial Categorization and\nCensuses.” In Census and Identity: The Politics of Race,\nEthnicity, and Language in National Censuses, edited by David\nKertzer and Dominique Arel, 43–70. Cambridge: Cambridge University\nPress. https://doi.org/10.1017/CBO9780511606045.003.\n\n\nOberski, Daniel, and Frauke Kreuter. 2020. “Differential Privacy\nand Social Science: An Urgent\nPuzzle.” Harvard Data Science Review 2 (1).\nhttps://doi.org/10.1162/99608f92.63a22079.\n\n\nOECD. 2014. “The Essential Macroeconomic Aggregates.” In\nUnderstanding National Accounts, 13–46. OECD. https://doi.org/10.1787/9789264214637-2-en.\n\n\n———. 2022. Quarterly GDP. https://data.oecd.org/gdp/quarterly-gdp.htm.\n\n\nOoms, Jeroen. 2014. “The jsonlite Package: A\nPractical and Consistent Mapping Between JSON Data and R\nObjects.” arXiv:1403.2805 [Stat.CO]. https://arxiv.org/abs/1403.2805.\n\n\n———. 2022a. openssl: Toolkit for Encryption,\nSignatures and Certificates Based on OpenSSL. https://CRAN.R-project.org/package=openssl.\n\n\n———. 2022b. pdftools: Text Extraction,\nRendering and Converting of PDF Documents. https://CRAN.R-project.org/package=pdftools.\n\n\n———. 2022c. ssh: Secure Shell (SSH) Client for\nR. https://CRAN.R-project.org/package=ssh.\n\n\n———. 2022d. tesseract: Open Source OCR\nEngine. https://CRAN.R-project.org/package=tesseract.\n\n\nOpen Science Collaboration. 2015. “Estimating the Reproducibility\nof Psychological Science.” Science 349 (6251): aac4716.\nhttps://doi.org/10.1126/science.aac4716.\n\n\nOsborne, Jason. 2012. Best Practices in Data\nCleaning: A Complete Guide to Everything You Need to Do Before and After\nCollecting Your Data. SAGE Publications.\n\n\nPalmer Station Antarctica LTER, and Gorman, Kristen. 2020.\n“Structural Size Measurements and Isotopic Signatures of Foraging\nAmong Adult Male and Female Adélie Penguins (Pygoscelis Adeliae) Nesting\nAlong the Palmer Archipelago Near Palmer Station, 2007-2009.” https://doi.org/10.6073/PASTA/98B16D7D563F265CB52372C8CA99E60F.\n\n\nPasek, Josh. 2015. “Predicting Elections:\nConsidering Tools to Pool the Polls.” Public Opinion\nQuarterly 79 (2): 594–619. https://doi.org/10.1093/poq/nfu060.\n\n\nPatki, Neha, Roy Wedge, and Kalyan Veeramachaneni. 2016. “The\nSynthetic Data Vault.” In 2016 IEEE International Conference\non Data Science and Advanced Analytics (DSAA), 399–410. https://doi.org/10.1109/DSAA.2016.49.\n\n\nPaullada, Amandalynne, Inioluwa Deborah Raji, Emily Bender, Emily\nDenton, and Alex Hanna. 2021. “Data and Its (Dis)contents: A\nSurvey of Dataset Development and Use in Machine Learning\nResearch.” Patterns 2 (11): 100336. https://doi.org/10.1016/j.patter.2021.100336.\n\n\nPedersen, Thomas Lin. 2022. patchwork: The\nComposer of Plots. https://CRAN.R-project.org/package=patchwork.\n\n\nPenrose, Carly. 2024. “Deadly Fires: Risk of Death, Injury Highest\nin Toronto’s Poor Neighbourhoods.” CBC News, April. https://www.cbc.ca/news/canada/toronto/fatal-fires-lower-income-1.7177356.\n\n\nPerepolkin, Dmytro. 2022. polite: Be Nice on\nthe Web. https://CRAN.R-project.org/package=polite.\n\n\nPerkel, Jeffrey. 2021. “Ten Computer Codes That Transformed\nScience.” Nature 589 (7842): 344–48. https://doi.org/10.1038/d41586-021-00075-2.\n\n\n———. 2023. “The Sleight-of-Hand Trick That Can Simplify Scientific\nComputing.” Nature 617 (7959): 212--213. https://doi.org/10.1038/d41586-023-01469-0.\n\n\nPhillips, Alban. 1958. “The Relation Between Unemployment and the\nRate of Change of Money Wage Rates in the United Kingdom,\n1861-1957.” Economica 25 (100): 283–99. https://doi.org/10.1111/j.1468-0335.1958.tb00003.x.\n\n\nPineau, Joelle, Philippe Vincent-Lamarre, Koustuv Sinha, Vincent\nLarivière, Alina Beygelzimer, Florence d’Alché-Buc, Emily Fox, and Hugo\nLarochelle. 2021. “Improving Reproducibility in Machine Learning\nResearch (a Report from the NeurIPS 2019 Reproducibility\nProgram).” Journal of Machine Learning Research 22\n(164): 1–20. http://jmlr.org/papers/v22/20-303.html.\n\n\nPitman, Jim. 1993. Probability. 1st ed. New York: Springer. https://doi.org/10.1007/978-1-4612-4374-8.\n\n\nPlant, Anne, and Robert Hanisch. 2020. “Reproducibility in\nScience: A Metrology Perspective.” Harvard Data Science\nReview 2 (4). https://doi.org/10.1162/99608f92.eb6ddee4.\n\n\nPodlogar, Tim, Peter Leo, and James Spragg. 2022. “Using VO2max as a marker of training status in\nathletes—Can we do better?” Journal of Applied\nPhysiology 133 (6): 144–47. https://doi.org/10.1152/japplphysiol.00723.2021.\n\n\nPrévost, Jean-Guy, and Jean-Pierre Beaud. 2015. Statistics, Public\nDebate and the State, 1800–1945: A Social, Political and Intellectual\nHistory of Numbers. Routledge.\n\n\nPython Software Foundation. 2024. Python\nLanguage Reference, version 3.13.0. https://docs.python.org/3/index.html.\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical\nComputing. Vienna, Austria: R Foundation for Statistical Computing.\nhttps://www.R-project.org/.\n\n\nR Special Interest Group on Databases (R-SIG-DB), Hadley Wickham, and\nKirill Müller. 2022. DBI: R Database Interface. https://CRAN.R-project.org/package=DBI.\n\n\nRegister, Yim. 2020a. “Introduction to Sampling and\nRandomization.” YouTube, November. https://youtu.be/U272FFxG8LE.\n\n\n———. 2020b. “Data Science Ethics in 6 Minutes.”\nYouTube, December. https://youtu.be/mA4gypAiRYU.\n\n\nRehaag, Sean. 2023. “Supreme Court of Canada Bulk Decisions\nDataset.” Refugee Law Laboratory. https://refugeelab.ca/bulk-data/scc.\n\n\nRichardson, Neal, Ian Cook, Nic Crane, Dewey Dunnington, Romain\nFrançois, Jonathan Keane, Dragoș Moldovan-Grünfeld, Jeroen Ooms, and\nApache Arrow. 2023. arrow: Integration to\nApache Arrow. https://CRAN.R-project.org/package=arrow.\n\n\nRiederer, Emily. 2020. “Column Names as Contracts,”\nSeptember. https://emilyriederer.netlify.app/post/column-name-contracts/.\n\n\n———. 2021. “Causal Design Patterns for Data Analysts,”\nJanuary. https://emilyriederer.netlify.app/post/causal-design-patterns/.\n\n\nRiffe, Tim, Enrique Acosta, Enrique José Acosta, Diego Manuel Aburto,\nAnna Alburez-Gutierrez, Ainhoa Altová, Ugofilippo Alustiza, et al. 2021.\n“Data Resource Profile: COVerAGE-DB: A\nGlobal Demographic Database of COVID-19 Cases and\nDeaths.” International Journal of Epidemiology 50 (2):\n390–390f. https://doi.org/10.1093/ije/dyab027.\n\n\nRilke, Rainer Maria. (1929) 2014. Letters to a Young Poet.\nPenguin Classics.\n\n\nRoberts, Margaret, Brandon Stewart, and Dustin Tingley. 2019.\n“stm: An R Package for\nStructural Topic Models.” Journal of Statistical\nSoftware 91 (2): 1–40. https://doi.org/10.18637/jss.v091.i02.\n\n\nRobinson, David, Alex Hayes, and Simon Couch. 2022. broom: Convert Statistical Objects into Tidy\nTibbles. https://CRAN.R-project.org/package=broom.\n\n\nRobinson, Emily, and Jacqueline Nolis. 2020. Build a Career in Data\nScience. Shelter Island: Manning Publications. https://livebook.manning.com/book/build-a-career-in-data-science.\n\n\nRosenau, James N. 1999. “A Transformed Observer in a Transforming\nWorld.” Studia Diplomatica 52 (1/2): 5–14. http://www.jstor.org/stable/44838096.\n\n\nRoss, Casey. 2022. “How a Decades-Old Database Became a Hugely\nProfitable Dossier on the Health of 270 Million Americans.”\nStat, February. https://www.statnews.com/2022/02/01/ibm-watson-health-marketscan-data/.\n\n\nRubinstein, Benjamin, and Francesco Alda. 2017. “Pain-Free Random\nDifferential Privacy with Sensitivity Sampling.” In 34th\nInternational Conference on Machine Learning (ICML’2017).\n\n\nRudis, Bob. 2020. hrbrthemes: Additional\nThemes, Theme Components and Utilities for\n“ggplot2”. https://CRAN.R-project.org/package=hrbrthemes.\n\n\nRuggles, Steven, Catherine Fitch, Diana Magnuson, and Jonathan\nSchroeder. 2019. “Differential Privacy and Census Data:\nImplications for Social and Economic Research.” AEA Papers\nand Proceedings 109 (May): 403–8. https://doi.org/10.1257/pandp.20191107.\n\n\nRyan, Philip. 2015. “Keeping a Lab Notebook.”\nYouTube, May. https://youtu.be/-MAIuaOL64I.\n\n\nSadowski, Caitlin, Emma Söderberg, Luke Church, Michal Sipko, and\nAlberto Bacchelli. 2018. “Modern Code Review: A Case Study at\nGoogle.” In Proceedings of the 40th International Conference\non Software Engineering: Software Engineering in Practice, 181–90.\nICSE-SEIP ’18. New York, NY, USA: Association for Computing Machinery.\nhttps://doi.org/10.1145/3183519.3183525.\n\n\nSalganik, Matthew. 2018. Bit by Bit: Social Research in the Digital\nAge. New Jersey: Princeton University Press.\n\n\nSalganik, Matthew, and Douglas Heckathorn. 2004. “Sampling and\nEstimation in Hidden Populations Using Respondent-Driven\nSampling.” Sociological Methodology 34 (1): 193–240. https://doi.org/10.1111/j.0081-1750.2004.00152.x.\n\n\nSambasivan, Nithya, Shivani Kapania, Hannah Highfill, Diana Akrong,\nPraveen Paritosh, and Lora Aroyo. 2021. “‘Everyone Wants to\nDo the Model Work, Not the Data Work’: Data Cascades in\nHigh-Stakes AI.” In Proceedings of the 2021\nCHI Conference on Human Factors in Computing Systems.\nACM. https://doi.org/10.1145/3411764.3445518.\n\n\nSamuel, Arthur. 1959. “Some Studies in Machine Learning Using the\nGame of Checkers.” IBM Journal of Research and\nDevelopment 3 (3): 210–29. https://doi.org/10.1147/rd.33.0210.\n\n\nSavage, Van, and Pamela Yeh. 2019. “Novelist Cormac\nMcCarthy’s Tips on How to Write a Great Science\nPaper.” Nature 574 (7778): 441–42. https://doi.org/10.1038/d41586-019-02918-5.\n\n\nSchaffner, Brian, Stephen Ansolabehere, and Sam Luks. 2021.\n“Cooperative Election Study Common Content,\n2020.” Harvard Dataverse. https://doi.org/10.7910/DVN/E9N6PH.\n\n\nSchloerke, Barret, and Jeff Allen. 2022. plumber: An API Generator for R. https://CRAN.R-project.org/package=plumber.\n\n\nSchofield, Alexandra, Måns Magnusson, and David Mimno. 2017.\n“Pulling Out the Stops: Rethinking Stopword Removal for Topic\nModels.” In Proceedings of the 15th Conference of the\nEuropean Chapter of the Association for Computational\nLinguistics: Volume 2, Short Papers, 432–36. Valencia, Spain:\nAssociation for Computational Linguistics. https://aclanthology.org/E17-2069.\n\n\nScott, James. 1998. Seeing Like a State. Yale University Press.\n\n\nSekhon, Jasjeet, and Rocío Titiunik. 2017. “Understanding\nRegression Discontinuity Designs as Observational Studies.”\nObservational Studies 3 (2): 174–82. https://doi.org/10.1353/obs.2017.0005.\n\n\nSen, Amartya. 1980. “Description as\nChoice.” Oxford Economic Papers 32 (3): 353–69.\nhttps://doi.org/10.1093/oxfordjournals.oep.a041484.\n\n\nShankar, Shreya, Rolando Garcia, Joseph Hellerstein, and Aditya\nParameswaran. 2022. “Operationalizing Machine Learning: An\nInterview Study.” arXiv. https://doi.org/10.48550/ARXIV.2209.09125.\n\n\nSilberzahn, Raphael, Eric Uhlmann, Daniel Martin, Pasquale Anselmi,\nFrederik Aust, Eli Awtrey, Štěpán Bahnı́k, et al. 2018. “Many\nAnalysts, One Data Set: Making Transparent How Variations in Analytic\nChoices Affect Results.” Advances in Methods and Practices in\nPsychological Science 1 (3): 337–56. https://doi.org/10.1177/2515245917747646.\n\n\nSilge, Julia, and David Robinson. 2016. “tidytext: Text Mining and Analysis Using Tidy Data\nPrinciples in R.” The Journal of Open Source\nSoftware 1 (3). https://doi.org/10.21105/joss.00037.\n\n\nSilver, Nate. 2020. “We Fixed an Issue with How Our Primary\nForecast Was Calculating Candidates’ Demographic Strengths.”\nFiveThirtyEight, February. https://fivethirtyeight.com/features/we-fixed-a-mistake-in-how-our-primary-forecast-was-calculating-candidates-demographic-strengths/.\n\n\nSimonsohn, Uri. 2013. “Just Post It: The Lesson from Two Cases of\nFabricated Data Detected by Statistics Alone.” Psychological\nScience 24 (10): 1875–88. https://doi.org/10.1177/0956797613480366.\n\n\nSimpkinson, Scott. 1971. “Testing to Ensure\nMission Success.” In What Made Apollo a Success,\nedited by NASA, 21–29.\n\n\nSimpson, Edward. 1951. “The Interpretation of Interaction in\nContingency Tables.” Journal of the Royal Statistical\nSociety: Series B (Methodological) 13 (2): 238–41. https://doi.org/10.1111/j.2517-6161.1951.tb00088.x.\n\n\nSmith, Richard. 2002. “A Statistical Assessment of Buchanan’s Vote\nin Palm Beach County.” Statistical Science 17 (4):\n441–57. https://doi.org/10.1214/ss/1049993203.\n\n\nSobek, Matthew, and Steven Ruggles. 1999. “The IPUMS Project: An\nUpdate.” Historical Methods: A Journal of Quantitative and\nInterdisciplinary History 32 (3): 102–10. https://doi.org/10.1080/01615449909598930.\n\n\nSomers, James. 2015. “Toolkits for the\nMind.” MIT Technology Review, April. https://www.technologyreview.com/2015/04/02/168469/toolkits-for-the-mind/.\n\n\n———. 2017. “Torching the Modern-Day Library of Alexandria.”\nThe Atlantic, April. https://www.theatlantic.com/technology/archive/2017/04/the-tragedy-of-google-books/523320/.\n\n\nSpear, Mary Eleanor. 1952. Charting Statistics. https://archive.org/details/ChartingStatistics_201801/.\n\n\nSprint, Gina, and Jason Conci. 2019. “Mining GitHub Classroom\nCommit Behavior in Elective and Introductory Computer Science\nCourses.” Journal of Computing Sciences in Colleges 35\n(1): 76–84.\n\n\nStaicu, Ana-Maria. 2017. “Interview with Nancy Reid.”\nInternational Statistical Review 85 (3): 381–403. https://doi.org/10.1111/insr.12237.\n\n\nStaniak, Mateusz, and Przemysław Biecek. 2019. “The Landscape of R Packages for Automated Exploratory\nData Analysis.” The R Journal 11\n(2): 347–69. https://doi.org/10.32614/RJ-2019-033.\n\n\nStantcheva, Stefanie. 2023. “How to Run Surveys: A Guide to\nCreating Your Own Identifying Variation and Revealing the\nInvisible.” Annual Review of Economics 15 (1): 205–34.\nhttps://doi.org/10.1146/annurev-economics-091622-010157.\n\n\nStatistics Canada. 2023. “Guide to the Census of Population,\n2021.” Statistics Canada. https://www12.statcan.gc.ca/census-recensement/2021/ref/98-304/98-304-x2021001-eng.pdf.\n\n\nSteckel, Richard. 1991. “The Quality of Census Data for Historical\nInquiry: A Research Agenda.” Social Science History 15\n(4): 579–99. https://doi.org/10.2307/1171470.\n\n\nStevens, Wallace. 1934. The Idea of Order at Key West. https://www.poetryfoundation.org/poems/43431/the-idea-of-order-at-key-west.\n\n\nSteyvers, Mark, and Tom Griffiths. 2006. “Probabilistic Topic\nModels.” In Latent Semantic Analysis: A Road to Meaning,\nedited by T. Landauer, D McNamara, S. Dennis, and W. Kintsch. https://cocosci.princeton.edu/tom/papers/SteyversGriffiths.pdf.\n\n\nStigler, Stephen. 1978. “Francis Ysidro Edgeworth,\nStatistician.” Journal of the Royal Statistical\nSociety. Series A (General) 141 (3): 287–322. https://doi.org/10.2307/2344804.\n\n\n———. 1986. The History of Statistics. Massachusetts: Belknap\nHarvard.\n\n\nStock, James, and Francesco Trebbi. 2003. “Retrospectives: Who\nInvented Instrumental Variable Regression?” Journal of\nEconomic Perspectives 17 (3): 177–94. https://doi.org/10.1257/089533003769204416.\n\n\nStoler, Ann Laura. 2002. “Colonial Archives and the Arts of\nGovernance.” Archival Science 2 (March): 87–109. https://doi.org/10.1007/bf02435632.\n\n\nStommes, Drew, P. M. Aronow, and Fredrik Sävje. 2023. “On the\nReliability of Published Findings Using the Regression Discontinuity\nDesign in Political Science.” Research & Politics 10\n(2). https://doi.org/https://doi.org/10.1177/2053168023116645.\n\n\nStudent. 1908. “The Probable Error of a Mean.”\nBiometrika 6 (1): 1–25. https://doi.org/10.2307/2331554.\n\n\nSunstein, Cass, and Lucia Reisch. 2017. The Economics of Nudge.\nRoutledge.\n\n\nSylvester, Christine, Anastasia Ershova, Aleksandra Khokhlova, Nikoleta\nYordanova, and Zachary Greene. 2023. “ParlEE\nplenary speeches V2 data set: Annotated full-text of 15.1 million\nsentence-level plenary speeches of six EU legislative\nchambers.” Harvard Dataverse. https://doi.org/10.7910/DVN/VOPK0E.\n\n\nSzaszi, Barnabas, Anthony Higney, Aaron Charlton, Andrew Gelman, Ignazio\nZiano, Balazs Aczel, Daniel Goldstein, David Yeager, and Elizabeth\nTipton. 2022. “No Reason to Expect Large and Consistent Effects of\nNudge Interventions.” Proceedings of the National Academy of\nSciences 119 (31): e2200732119. https://doi.org/10.1073/pnas.2200732119.\n\n\nTaddy, Matt. 2019. Business Data Science. 1st ed. McGraw Hill.\n\n\nTal, Eran. 2020. “Measurement in\nScience.” In The Stanford Encyclopedia of\nPhilosophy, edited by Edward Zalta, Fall 2020. https://plato.stanford.edu/archives/fall2020/entries/measurement-science/;\nMetaphysics Research Lab, Stanford University.\n\n\nTang, John. 2015. “Pollution havens and the\ntrade in toxic chemicals: Evidence from U.S. trade flows.”\nEcological Economics 112 (April): 150–60. https://doi.org/10.1016/j.ecolecon.2015.02.022.\n\n\nTaylor, Adam. 2015. “New Zealand Says No to Jedis.” The\nWashington Post, September. https://www.washingtonpost.com/news/worldviews/wp/2015/09/29/new-zealand-says-no-to-jedis/.\n\n\nTeate, Renée. 2022. SQL for Data Scientists. Wiley.\n\n\nThe Washington Post. 2023. “Fatal Force Database.” https://github.com/washingtonpost/data-police-shootings.\n\n\nThieme, Nick. 2018. “R Generation.” Significance\n15 (4): 14–19. https://doi.org/10.1111/j.1740-9713.2018.01169.x.\n\n\nThompson, Charlie, Daniel Antal, Josiah Parry, Donal Phipps, and Tom\nWolff. 2022. spotifyr: R Wrapper for the\n“Spotify” Web API. https://CRAN.R-project.org/package=spotifyr.\n\n\nThomson-DeVeaux, Amelia, Laura Bronner, and Damini Sharma. 2021.\n“Cities Spend Millions On Police Misconduct\nEvery Year. Here’s Why It’s So Difficult to Hold Departments\nAccountable.” FiveThirtyEight, February. https://fivethirtyeight.com/features/police-misconduct-costs-cities-millions-every-year-but-thats-where-the-accountability-ends/.\n\n\nThornhill, John. 2021. “Lunch with the FT: Mathematician Hannah\nFry.” Financial Times, July. https://www.ft.com/content/a5e33e5a-99b9-4bbc-948f-8a527c7675c3.\n\n\nTierney, Nicholas, Di Cook, Miles McBain, and Colin Fay. 2021. naniar: Data Structures, Summaries, and Visualisations\nfor Missing Data. https://CRAN.R-project.org/package=naniar.\n\n\nTierney, Nicholas, and Karthik Ram. 2020. “A Realistic Guide to\nMaking Data Available Alongside Code to Improve Reproducibility.”\nhttps://arxiv.org/abs/2002.11626.\n\n\nTimbers, Tiffany. 2020. canlang: Canadian\nCensus language data. https://ttimbers.github.io/canlang/.\n\n\nTimbers, Tiffany, Trevor Campbell, and Melissa Lee. 2022. Data\nScience: A First Introduction. Chapman; Hall/CRC. https://datasciencebook.ca.\n\n\nTourangeau, Roger, Lance Rips, and Kenneth Rasinski. 2000. The\nPsychology of Survey Response. 1st ed. Cambridge University Press.\nhttps://doi.org/10.1017/CBO9780511819322.003.\n\n\nTukey, John. 1962. “The Future of Data Analysis.” The\nAnnals of Mathematical Statistics 33 (1): 1–67. https://doi.org/10.1214/aoms/1177704711.\n\n\nUN IGME. 2021. “Levels and Trends in Child Mortality,\n2021.” https://childmortality.org/wp-content/uploads/2021/12/UNICEF-2021-Child-Mortality-Report.pdf.\n\n\nUshey, Kevin. 2022. renv: Project\nEnvironments. https://CRAN.R-project.org/package=renv.\n\n\nvan Buuren, Stef, and Karin Groothuis-Oudshoorn. 2011. “mice: Multivariate Imputation by Chained Equations in\nR.” Journal of Statistical Software 45 (3): 1–67.\nhttps://doi.org/10.18637/jss.v045.i03.\n\n\nVan den Broeck, Jan, Solveig Argeseanu Cunningham, Roger Eeckels, and\nKobus Herbst. 2005. “Data Cleaning: Detecting, Diagnosing, and\nEditing Data Abnormalities.” PLOS Medicine 2 (10): e267.\nhttps://doi.org/10.1371/journal.pmed.0020267.\n\n\nvan der Loo, Mark, and Edwin De Jonge. 2021. “Data Validation Infrastructure for R.”\nJournal of Statistical Software 97 (10): 1–33. https://doi.org/10.18637/jss.v097.i10.\n\n\nVanderplas, Susan, Dianne Cook, and Heike Hofmann. 2020. “Testing\nStatistical Charts: What Makes a Good Graph?” Annual Review\nof Statistics and Its Application 7: 61–88. https://doi.org/10.1146/annurev-statistics-031219-041252.\n\n\nVanhoenacker, Mark. 2015. Skyfaring: A Journey with a Pilot.\n1st ed. Alfred A. Knopf.\n\n\nVavreck, Lynn, and Chris Tausanovitch. 2021. “Democracy Fund\n+ UCLA Nationscape Project User Guide.” https://www.voterstudygroup.org/data/nationscape.\n\n\nVidoni, Melina. 2021. “Evaluating Unit\nTesting Practices in R Packages.” In 2021 IEEE/ACM\n43rd International Conference on Software Engineering (ICSE),\n1523–34. https://doi.org/10.1109/ICSE43902.2021.00136.\n\n\nvon Bergmann, Jens, Dmitry Shkolnik, and Aaron Jacobs. 2021. cancensus: R package to access, retrieve, and work with\nCanadian Census data and geography. https://mountainmath.github.io/cancensus/.\n\n\nWalby, Kevin, and Alex Luscombe. 2019. Freedom of Information and\nSocial Science Research Design. Routledge.\n\n\nWalker, Kyle, and Matt Herman. 2022. tidycensus: Load US Census Boundary and Attribute Data as\n“tidyverse” and “sf”-Ready Data\nFrames. https://CRAN.R-project.org/package=tidycensus.\n\n\nWallach, Hanna. 2018. “Computational Social Science ≠ Computer Science + Social Data.”\nCommunications of the ACM 61 (3): 42–44. https://doi.org/10.1145/3132698.\n\n\nWan, Mengting, and Julian J. McAuley. 2018. “Item Recommendation\non Monotonic Behavior Chains.” In Proceedings of the 12th\nACM Conference on Recommender Systems, RecSys 2018,\nVancouver, BC, Canada, October 2-7, 2018, edited by Sole Pera,\nMichael D. Ekstrand, Xavier Amatriain, and John O’Donovan, 86–94.\nACM. https://doi.org/10.1145/3240323.3240369.\n\n\nWan, Mengting, Rishabh Misra, Ndapa Nakashole, and Julian J. McAuley.\n2019. “Fine-Grained Spoiler Detection from Large-Scale Review\nCorpora.” In Proceedings of the 57th Conference of the\nAssociation for Computational Linguistics, ACL 2019,\nFlorence, Italy, July 28- August 2, 2019, Volume 1: Long Papers,\nedited by Anna Korhonen, David R. Traum, and Lluı́s Màrquez, 2605–10.\nAssociation for Computational Linguistics. https://doi.org/10.18653/v1/p19-1248.\n\n\nWang, Wei, David Rothschild, Sharad Goel, and Andrew Gelman. 2015.\n“Forecasting Elections with Non-Representative Polls.”\nInternational Journal of Forecasting 31 (3): 980–91. https://doi.org/10.1016/j.ijforecast.2014.06.001.\n\n\nWang, Yilun, and Michal Kosinski. 2018. “Deep Neural Networks Are\nMore Accurate Than Humans at Detecting Sexual Orientation from Facial\nImages.” Journal of Personality and Social Psychology\n114 (2): 246–57. https://doi.org/10.1037/pspa0000098.\n\n\nWardrop, Robert. 1995. “Simpson’s Paradox and the Hot Hand in\nBasketball.” The American Statistician 49 (1): 24–28. https://doi.org/10.2307/2684806.\n\n\nWare, James. 1989. “Investigating Therapies of Potentially Great\nBenefit: ECMO.” Statistical Science 4 (4): 298–306. https://doi.org/10.1214/ss/1177012384.\n\n\nWasserman, Larry. 2005. All of Statistics. Springer.\n\n\nWei, Eugene. 2017. Remove the Legend to Become One. https://www.eugenewei.com/blog/2017/11/13/remove-the-legend.\n\n\nWeinberg, Gerald. 1971. The Psychology of Computer Programming.\nNew York: Van Nostrand Reinhold Company.\n\n\nWeissgerber, Tracey, Natasa Milic, Stacey Winham, and Vesna Garovic.\n2015. “Beyond Bar and Line Graphs: Time for a New Data\nPresentation Paradigm.” PLoS Biology 13 (4): e1002128.\nhttps://doi.org/10.1371/journal.pbio.1002128.\n\n\nWhitby, Andrew. 2020. The Sum of the\nPeople. New York: Basic Books.\n\n\nWhitelaw, James. 1805. An Essay on the Population of Dublin. Being\nthe Result of an Actual Survey Taken in 1798, with Great Care and\nPrecision, and Arranged in a Manner Entirely New. Graisberry;\nCampbell.\n\n\nWickham, Hadley. 2009. “Manipulating Data.” In ggplot2, 157–75. Springer New York. https://doi.org/10.1007/978-0-387-98141-3_9.\n\n\n———. 2011. “testthat: Get Started with\nTesting.” The R Journal 3: 5–10. https://journal.r-project.org/archive/2011-1/RJournal%5F2011-1%5FWickham.pdf.\n\n\n———. 2014. “Tidy Data.” Journal of Statistical\nSoftware 59 (1): 1–23. https://doi.org/10.18637/jss.v059.i10.\n\n\n———. 2016. ggplot2: Elegant Graphics for Data\nAnalysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\n———. 2017. tidyverse: Easily Install and Load\nthe “Tidyverse”. https://CRAN.R-project.org/package=tidyverse.\n\n\n———. 2018. “Whole Game.” YouTube, January. https://youtu.be/go5Au01Jrvs.\n\n\n———. 2019. Advanced R. 2nd ed. Chapman; Hall/CRC.\nhttps://adv-r.hadley.nz.\n\n\n———. 2020. Tidyverse. https://www.tidyverse.org/.\n\n\n———. 2021a. babynames: US Baby Names\n1880-2017. https://CRAN.R-project.org/package=babynames.\n\n\n———. 2021b. Mastering Shiny. 1st ed. O’Reilly Media. https://mastering-shiny.org.\n\n\n———. 2021c. The Tidyverse Style Guide. https://style.tidyverse.org/index.html.\n\n\n———. 2022a. R Packages. 2nd ed. O’Reilly Media. https://r-pkgs.org.\n\n\n———. 2022b. rvest: Easily Harvest (Scrape) Web\nPages. https://CRAN.R-project.org/package=rvest.\n\n\n———. 2022c. stringr: Simple, Consistent\nWrappers for Common String Operations. https://CRAN.R-project.org/package=stringr.\n\n\n———. 2023a. forcats: Tools for Working with\nCategorical Variables (Factors). https://CRAN.R-project.org/package=forcats.\n\n\n———. 2023b. httr: Tools for Working with URLs\nand HTTP. https://CRAN.R-project.org/package=httr.\n\n\nWickham, Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019.\n“Welcome to the Tidyverse.”\nJournal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, and Jennifer Bryan. 2023. readxl: Read Excel Files. https://CRAN.R-project.org/package=readxl.\n\n\nWickham, Hadley, Jennifer Bryan, and Malcolm Barrett. 2022. usethis: Automate Package and Project Setup.\nhttps://CRAN.R-project.org/package=usethis.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. (2016)\n2023. R for Data Science. 2nd ed. O’Reilly Media. https://r4ds.hadley.nz.\n\n\nWickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2022.\ndplyr: A Grammar of Data\nManipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, Maximilian Girlich, and Edgar Ruiz. 2022. dbplyr: A “dplyr” Back End for\nDatabases. https://CRAN.R-project.org/package=dbplyr.\n\n\nWickham, Hadley, and Lionel Henry. 2022. purrr:\nFunctional Programming Tools. https://CRAN.R-project.org/package=purrr.\n\n\nWickham, Hadley, Jim Hester, and Jenny Bryan. 2022. readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr.\n\n\nWickham, Hadley, Jim Hester, Winston Chang, and Jenny Bryan. 2022.\ndevtools: Tools to Make Developing R Packages\nEasier. https://CRAN.R-project.org/package=devtools.\n\n\nWickham, Hadley, Jim Hester, and Jeroen Ooms. 2021. xml2: Parse XML. https://CRAN.R-project.org/package=xml2.\n\n\nWickham, Hadley, Evan Miller, and Danny Smith. 2023. haven: Import and Export “SPSS”\n“Stata” and “SAS” Files. https://CRAN.R-project.org/package=haven.\n\n\nWickham, Hadley, and Dana Seidel. 2022. scales:\nScale Functions for Visualization. https://CRAN.R-project.org/package=scales.\n\n\nWickham, Hadley, and Lisa Stryjewski. 2011. “40 Years of\nBoxplots,” November. https://vita.had.co.nz/papers/boxplots.pdf.\n\n\nWickham, Hadley, Davis Vaughan, and Maximilian Girlich. 2023. tidyr: Tidy Messy Data. https://CRAN.R-project.org/package=tidyr.\n\n\nWiessner, Polly. 2014. “Embers of Society: Firelight Talk Among\nthe Ju/’hoansi Bushmen.” Proceedings of the National Academy\nof Sciences 111 (39): 14027–35. https://doi.org/10.1073/pnas.1404212111.\n\n\nWilde, Oscar. 1891. The Picture of Dorian Gray. https://www.gutenberg.org/files/174/174-h/174-h.htm.\n\n\nWilford, John Noble. 1977. “Wernher von Braun, Rocket Pioneer,\nDies.” The New York Times, June. https://www.nytimes.com/1977/06/18/archives/wernher-von-braun-rocket-pioneer-dies-wernher-von-braun-pioneer-in.html.\n\n\nWilkinson, Leland. 2005. The Grammar of Graphics. 2nd ed.\nSpringer.\n\n\nWilkinson, Mark, Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle\nAppleton, Myles Axton, Arie Baak, Niklas Blomberg, et al. 2016.\n“The FAIR Guiding Principles for Scientific Data Management and\nStewardship.” Scientific Data 3 (1): 1–9. https://doi.org/10.1038/sdata.2016.18.\n\n\nWilson, Greg, Jenny Bryan, Karen Cranston, Justin Kitzes, Lex\nNederbragt, and Tracy Teal. 2017. “Good Enough Practices in\nScientific Computing.” PLOS Computational Biology 13\n(6): 1–20. https://doi.org/10.1371/journal.pcbi.1005510.\n\n\nWood, Simon. 2015. Core Statistics. Cambridge University Press.\nhttps://www.maths.ed.ac.uk/\\%7Eswood34/core-statistics.pdf.\n\n\nWorld Health Organization. 2019. “Trends in Maternal Mortality\n2000 to 2017: Estimates by WHO, UNICEF, UNFPA, World Bank Group and the\nUnited Nations Population Division.” https://apps.who.int/iris/handle/10665/327596.\n\n\nWu, Changbao, and Mary Thompson. 2020. Sampling Theory and\nPractice. Springer.\n\n\nXie, Yihui. 2019. “TinyTeX: A lightweight,\ncross-platform, and easy-to-maintain LaTeX distribution based on TeX\nLive.” TUGboat, no. 1: 30–32. https://tug.org/TUGboat/Contents/contents40-1.html.\n\n\n———. 2023. knitr: A General-Purpose Package for\nDynamic Report Generation in R. https://yihui.org/knitr/.\n\n\nXu, Ya. 2020. “Causal Inference Challenges in Industry: A\nPerspective from Experiences at LinkedIn.” YouTube,\nJuly. https://youtu.be/OoKsLAvyIYA.\n\n\nYoshioka, Alan. 1998. “Use of Randomisation in the Medical\nResearch Council’s Clinical Trial of Streptomycin in Pulmonary\nTuberculosis in the 1940s.” BMJ 317 (7167): 1220–23. https://doi.org/10.1136/bmj.317.7167.1220.\n\n\nZhang, Ping, XunPeng Shi, YongPing Sun, Jingbo Cui, and Shuai Shao.\n2019. “Have China’s provinces achieved their\ntargets of energy intensity reduction? Reassessment based on nighttime\nlighting data.” Energy Policy 128 (May): 276–83.\nhttps://doi.org/10.1016/j.enpol.2019.01.014.\n\n\nZinsser, William. 1976. On Writing Well. New York:\nHarperCollins.",
    "crumbs": [
      "부록",
      "References"
    ]
  },
  {
    "objectID": "index.html#독자-및-예상-배경-지식",
    "href": "index.html#독자-및-예상-배경-지식",
    "title": "데이터로 이야기하기",
    "section": "독자 및 예상 배경 지식",
    "text": "독자 및 예상 배경 지식\n이 책을 읽는 일반적인 독자는 1학년 학부 통계학에 대한 약간의 지식이 있습니다. 예를 들어 회귀 분석을 실행해 본 경험이 있을 것입니다. 그러나 특정 수준을 대상으로 하지 않고, 거의 모든 양적 과정에 관련된 측면을 제공합니다. 저는 이 책을 학부, 대학원 및 전문 수준에서 가르쳤습니다. 모든 사람은 고유한 요구 사항이 있지만, 이 책의 어떤 측면이 여러분에게 도움이 되기를 바랍니다.\n열정과 관심은 사람들을 멀리 이끌었습니다. 그것들이 있다면 다른 것에 대해 너무 걱정하지 마십시오. 가장 성공적인 학생들 중 일부는 양적 또는 코딩 배경이 없는 학생들이었습니다.\n이 책은 많은 내용을 다루지만, 특정 측면에 대해 깊이 있게 다루지는 않습니다. 따라서 데이터 과학: 첫 번째 소개 (Timbers, Campbell, 와/과 Lee 2022), 데이터 과학을 위한 R (Wickham, Çetinkaya-Rundel, 와/과 Grolemund [2016년] 2023), 통계 학습 입문 (James 기타 [2013년] 2021), 통계적 재고 (McElreath [2015년] 2020)와 같은 더 자세한 책들을 특히 보완합니다. 이 책들에 관심이 있다면, 이 책이 좋은 시작점이 될 수 있습니다.",
    "crumbs": [
      "서문"
    ]
  },
  {
    "objectID": "index.html#구성-및-내용",
    "href": "index.html#구성-및-내용",
    "title": "데이터로 이야기하기",
    "section": "구성 및 내용",
    "text": "구성 및 내용\n이 책은 여섯 부분으로 구성되어 있습니다: I) 기초, II) 커뮤니케이션, III) 획득, IV) 준비, V) 모델링, VI) 응용.\n파트 I—기초—는 이 책으로 무엇을 달성하려는지, 왜 이 책을 읽어야 하는지에 대한 개요를 제공하는 1  데이터로 이야기하기으로 시작합니다. ?sec-fire-hose는 세 가지 예제를 통해 설명합니다. 이 예제들의 의도는 이 책에서 권장하는 전체 워크플로우를 무엇이 일어나는지에 대한 세부 사항에 너무 신경 쓰지 않고 경험할 수 있도록 하는 것입니다. 그 워크플로우는 계획, 시뮬레이션, 획득, 모델링, 그리고 커뮤니케이션입니다. 이 장의 모든 것을 처음부터 따르지 못하는 것은 정상적이지만, 직접 코드를 입력하고 실행하면서 진행해야 합니다. 이 책에서 한 장만 읽을 시간이 있다면, 저는 이 장을 추천합니다. ?sec-reproducible-workflows는 제가 옹호하는 워크플로우에서 사용되는 재현성을 위한 몇 가지 핵심 도구를 소개합니다. 여기에는 Quarto, R 프로젝트, Git 및 GitHub, 그리고 R을 실제로 사용하는 것과 같은 측면이 포함됩니다.\n파트 II—커뮤니케이션—은 서면 및 정적 커뮤니케이션을 다룹니다. ?sec-on-writing은 양적 글쓰기가 가져야 할 특징과 명확한 양적 연구 논문을 작성하는 방법을 자세히 설명합니다. ?sec-static-communication의 정적 커뮤니케이션은 그래프, 표, 지도와 같은 기능을 소개합니다.\n파트 III—획득—은 우리 세상을 데이터로 바꾸는 데 중점을 둡니다. ?sec-farm-data는 측정으로 시작하여 데이터에 대한 우리의 접근 방식을 지배하는 샘플링의 필수 개념을 단계별로 설명합니다. 그런 다음 인구 조사 및 기타 정부 통계와 같이 데이터로 사용하기 위해 명시적으로 제공되는 데이터셋을 고려합니다. 이들은 일반적으로 깨끗하고 잘 문서화되어 있으며 미리 패키지화된 데이터셋입니다. ?sec-gather-data는 API(응용 프로그래밍 인터페이스) 사용, 데이터 스크래핑, PDF에서 데이터 가져오기, OCR(광학 문자 인식)과 같은 측면을 다룹니다. 데이터는 사용 가능하지만 반드시 데이터셋으로 설계된 것은 아니며, 우리가 직접 가져와야 한다는 생각입니다. 마지막으로 ?sec-hunt-data는 우리에게 더 많은 것이 요구되는 측면을 다룹니다. 예를 들어, 실험을 수행하거나 A/B 테스트를 실행하거나 설문 조사를 해야 할 수도 있습니다.\n파트 IV—준비—는 원본, 편집되지 않은 데이터를 탐색하고 공유할 수 있는 형태로 존중하며 변환하는 방법을 다룹니다. ?sec-clean-and-prepare는 데이터 정리 및 준비 작업에 접근할 때 따라야 할 몇 가지 원칙을 자세히 설명한 다음, 취해야 할 특정 단계와 구현할 검사를 설명합니다. ?sec-store-and-share는 R 데이터 패키지 및 파케트 사용을 포함하여 이러한 데이터셋을 저장하고 검색하는 방법에 중점을 둡니다. 그런 다음 데이터셋이 기반으로 하는 사람들을 존중하면서 가능한 한 광범위하게 데이터셋을 배포하고자 할 때 고려해야 할 사항과 취해야 할 단계를 계속 설명합니다.\n파트 V—모델링—은 ?sec-exploratory-data-analysis의 탐색적 데이터 분석으로 시작합니다. 이것은 데이터셋을 이해하는 중요한 과정이지만, 일반적으로 최종 제품에 포함되지 않는 것입니다. 이 과정 자체로 목적이 있습니다. ?sec-its-just-a-linear-model에서는 데이터를 탐색하기 위한 선형 모델 사용이 소개됩니다. 그리고 ?sec-its-just-a-generalized-linear-model에서는 로지스틱, 포아송, 음이항 회귀를 포함한 일반화 선형 모델을 다룹니다. 또한 다단계 모델링도 소개합니다.\n파트 VI—응용—는 모델링의 세 가지 응용을 제공합니다. ?sec-causality-from-observational-data는 관찰 데이터에서 인과 관계 주장을 하는 데 중점을 두며, 차이-차이, 회귀 불연속성, 도구 변수와 같은 접근 방식을 다룹니다. ?sec-multilevel-regression-with-post-stratification은 사후 계층화 다단계 회귀를 소개하는데, 이는 통계 모델을 사용하여 알려진 편향에 대해 샘플을 조정하는 것입니다. ?sec-text-as-data는 텍스트-데이터에 중점을 둡니다.\n?sec-concluding-remarks는 몇 가지 결론적인 언급을 제공하고, 몇 가지 미해결 문제를 자세히 설명하며, 다음 단계를 제안합니다.\n온라인 부록은 페이지 크기 제약으로 인해 다루기 어렵거나 인쇄된 책에 비해 더 자주 업데이트해야 할 가능성이 있는 중요한 측면을 제공합니다. Online Appendix A — R 필수 사항는 이 책에서 사용되는 통계 프로그래밍 언어인 R의 필수 작업을 설명합니다. 참고 자료 장이 될 수 있으며, 일부 학생들은 책의 나머지 부분을 진행하면서 이 장으로 돌아오기도 합니다. Online Appendix D — 데이터셋는 평가에 유용할 수 있는 데이터셋 목록을 제공합니다. 이 책의 핵심은 Quarto를 중심으로 하지만, 그 전신인 R Markdown은 아직 사용 중단되지 않았으며 많은 자료가 있습니다. 따라서 Online Appendix E — R 마크다운은 ?sec-reproducible-workflows의 Quarto 관련 측면에 대한 R Markdown 동등물을 포함합니다. 일련의 논문이 Online Appendix F — 논문에 포함되어 있습니다. 이 논문들을 작성하면 관심 있는 주제에 대한 독창적인 연구를 수행하게 될 것입니다. 개방형 연구가 여러분에게 생소할 수 있지만, 자신의 질문을 개발하고, 양적 방법을 사용하여 탐색하고, 발견한 내용을 전달할 수 있는 정도가 이 책의 성공을 측정하는 척도입니다. Online Appendix C — SQL 필수 사항은 SQL 필수 사항에 대한 간략한 개요를 제공합니다. Online Appendix G — 생산은 모델 추정치 및 예측을 더 널리 사용할 수 있도록 하는 방법을 다룹니다.",
    "crumbs": [
      "서문"
    ]
  },
  {
    "objectID": "index.html#감사",
    "href": "index.html#감사",
    "title": "데이터로 이야기하기",
    "section": "감사",
    "text": "감사\n많은 분들이 이 책을 개발하는 데 도움이 된 코드, 데이터, 예제, 지침, 기회, 생각, 시간을 아낌없이 주셨습니다.\n데이비드 그럽스, 커티스 힐, 로빈 로이드-스타크스, 그리고 테일러 앤 프랜시스 팀에게 이 책을 편집하고 출판하며 귀중한 지침과 지원을 제공해 주셔서 감사합니다. 이 책을 철저히 편집해 준 에리카 올로프에게 감사합니다. 이 책의 초기 초안을 철저히 검토하고 개선에 도움이 되는 상세한 피드백을 제공해 준 이사벨라 게멘트에게 감사합니다.\n이 책의 모든 단어를 검토하고 많은 부분을 개선하며, 이 책에 다루어진 많은 내용에 대한 저의 생각을 날카롭게 하는 데 도움을 준 애니 콜린스에게 감사합니다. 가르치는 즐거움 중 하나는 애니와 같이 재능 있는 사람들과 그들의 경력을 시작할 때 함께 일할 기회를 얻는 것입니다.\n이 책의 초기 계획에 대해 상세한 의견을 제공해 준 에밀리 리더러에게 감사합니다. 그녀는 초안이 작성된 후 원고로 돌아와 세부적으로 검토했습니다. 그녀의 사려 깊은 의견은 이 책을 크게 개선했습니다. 더 나아가 그녀의 작업은 이 책의 많은 내용에 대한 저의 생각을 바꾸었습니다.\n저는 전체 장을 읽어준 많은 검토자들을 만날 수 있었던 행운을 누렸습니다. 때로는 두세 번, 심지어 그 이상 읽어주기도 했습니다. 그들은 기대 이상으로 훌륭한 제안을 제공하여 이 책을 개선하는 데 큰 도움이 되었습니다. 이에 대해 저는 알버트 랩, 알렉스 헤이즈, 알렉스 루스콤(경찰 폭력 “오, 당신은…” 항목도 제안), 아리엘 문도, 벤자민 하이브-케인스, 댄 라이언, 에릭 드라이스데일, 플로렌스 발레-뒤부아, 잭 베일리, 재 해트릭-심퍼스, 존 칸, 조나단 킨(파케트 전문 지식을 아낌없이 공유), 로렌 케네디(MRP에 대한 저의 생각을 발전시키기 위해 코드, 데이터, 전문 지식을 아낌없이 공유), 리암 웰시, 리자 볼튼(이 책을 가르치는 방법에 대한 저의 아이디어를 발전시키는 데 도움), 루이스 코레이아, 맷 라토, 마티아스 베르거, 마이클 문, 로베르토 렌티니, 라이언 브릭스, 그리고 테일러 라이트에게 감사드립니다.\n많은 분들이 구체적인 제안을 해주셔서 많은 것이 개선되었습니다. 이 모든 분들은 이 책이 기반으로 하는 오픈 소스 프로그래밍 언어 커뮤니티를 특징짓는 관대함의 정신에 기여합니다. 이 모든 분들께 감사드립니다. 아 마푸즈는 포아송 회귀를 다루는 것이 중요하다고 깨닫게 해주었습니다. 아론 밀러는 FINER 프레임워크를 제안했습니다. 앨리슨 프레스메인즈 힐은 워드뱅크를 제안했습니다. 크리스 워쇼는 민주주의 기금 유권자 연구 그룹 설문조사 데이터를 제안했습니다. 크리스티나 웨이는 많은 코드 오류를 지적했습니다. 클레어 배터실은 글쓰기에 대한 많은 책을 추천해 주었습니다. 엘라 케이는 Quarto로 전환할 것을 제안하고 정당하게 주장했습니다. 파리아 칸다커는 “R 필수” 장이 된 것을 제안했습니다. 하림 나비드는 자신의 산업 경험을 아낌없이 공유했습니다. 히스 프리슨은 토론토 노숙자 데이터에 대한 도움을 주었습니다. 제시카 그론스벨은 통계 실습에 대한 귀중한 제안을 해주었습니다. 켈리 치우는 텍스트-데이터의 중요성을 강조했습니다. 레슬리 루트는 “오, 우리가 그 데이터에 대해 좋은 데이터를 가지고 있다고 생각하는군요!”라는 아이디어를 내놓았습니다. 마이클 청은 EDA에 대한 저의 접근 방식을 형성했습니다. 마이클 도넬리, 피터 헵번, 레오 레이몬드-벨질은 제가 몰랐던 정치학, 사회학, 통계학의 고전 논문에 대한 정보를 제공했습니다. 닉 호튼은 ?sec-exploratory-data-analysis의 해들리 위컴 비디오를 제안했습니다. 폴 호지츠는 R 패키지를 만드는 방법을 가르쳐 주었고 이 책의 표지 그림을 만들었습니다. 라두 크라이우는 샘플링이 적절한 위치를 차지하도록 했습니다. 샤를라 겔판드는 R을 사용하는 방법에 대한 저의 접근 방식을 옹호했습니다. 토마스 윌리엄 로젠탈은 Shiny의 잠재력을 깨닫게 해주었습니다. 톰 카르도소와 제인 슈워츠는 언론인들이 수집한 훌륭한 데이터 소스였습니다. 얀보 탕은 낸시 리드의 “거인의 어깨” 항목을 도왔습니다. 마지막으로 크리스 매디슨과 마이아 발린트는 마지막 시를 제안했습니다.\n저의 박사 학위 지도 교수님인 존 탕, 마르틴 마리오티, 팀 해튼, 자크 워드에게 감사드립니다. 그들은 제가 관심 있는 지적 공간을 탐색할 자유를 주었고, 그러한 관심사를 추구할 수 있도록 지원했으며, 모든 것이 구체적인 결과로 이어지도록 지도해 주었습니다. 그 시절에 배운 것이 이 책의 토대가 되었습니다.\n이 책은 크리스 베일, 스콧 커닝햄, 앤드류 헤이스(이 책이 나오기 훨씬 전에 이 책과 같은 이름의 강의를 독립적으로 가르쳤음), 리사 렌드웨이, 그랜트 맥더모트, 네이선 마티아스, 데이비드 밈노, 에드 루빈을 포함하여 온라인에서 무료로 제공되는 다른 사람들의 노트와 교육 자료로부터 큰 도움을 받았습니다. 이 모든 분들께 감사드립니다. 학자들이 자료를 온라인에서 무료로 제공하는 변화된 규범은 훌륭한 것이며, 여기에서 제공되는 이 책의 무료 온라인 버전이 기여하기를 바랍니다.\n일부 평가 항목을 개발하는 데 도움을 준 사만다-조 카에타노에게 감사합니다. 그리고 루브릭의 일부 측면을 적용할 수 있도록 허락해 준 리사 롬키와 앨런 청에게도 감사합니다. 4  연구 논문 작성 튜토리얼의 일부 측면의 촉매제는 McPhee (2017, p. 186)와 첼시 팔렛-펠레리티였습니다. “대화형 커뮤니케이션” 튜토리얼의 아이디어는 마우리시오 바르가스 세풀베다(“파차”)와 앤드류 휘트비의 작업이었습니다.\n다음 분들의 수정에 감사드립니다:\n\n에이미 패로우, 아르쉬 라칸팔, 세자르 빌라레알 구즈만, 클로이 티어스타인, 핀 코롤-오드와이어, 플라비아 로페즈, 그레고리 파워, 홍 시,제이든 정, 존 헤이즈, 조이스 쉬안, 로라 클라인, 로레나 알마라즈 데 라 가르자, 매튜 로버트슨, 미카엘라 드루일라드, 모우니카 타남, 림 알라사디, 롭 짐머만, 타예자 치쿰비리케, 위즈단 타리크, 양 우, 그리고 한예원.\n\n켈리 라이온스는 지원, 지도, 멘토링, 그리고 우정을 제공했습니다. 그녀는 매일 학자가 어떤 모습이어야 하는지, 그리고 더 나아가 한 사람으로서 어떤 사람이 되기를 열망해야 하는지를 보여줍니다.\n그렉 윌슨은 가르침에 대해 생각할 구조를 제공하고 “스케일” 스타일 연습을 제안했습니다. 그는 이 책의 촉매제였으며, 초안에 대한 유용한 의견을 제공했습니다. 그는 매일 지적 공동체에 기여하는 방법을 보여줍니다.\n팬데믹 기간 동안 첫째, 그리고 둘째 아이를 돌보며 이 책을 쓸 수 있도록 해준 엘르 코테에게 감사합니다.\n2021년 크리스마스 현재 이 책은 부분적으로 완성된 노트들의 흩어진 모음이었습니다. 모든 것을 내려놓고 두 달 동안 지구 반대편에서 건너와 모든 것을 다시 쓰고 응집력 있는 초안을 만들 기회를 준 엄마와 아빠에게 감사합니다.\n마리야 타플라가와 ANU 호주 정치 연구 센터(정치 및 국제 관계 학부)에 캔버라에서 2주간의 “글쓰기 휴양” 자금을 지원해 주셔서 감사합니다.\n마지막으로 모니카 알렉산더에게 감사합니다. 당신이 없었다면 저는 책을 쓰지 못했을 것입니다. 심지어 가능하다고 생각조차 하지 못했을 것입니다. 이 책의 최고의 아이디어 중 많은 부분이 당신의 것이며, 그렇지 않은 것들도 당신이 여러 번 읽어주면서 더 좋게 만들었습니다. 이 책을 쓰는 데 헤아릴 수 없는 도움을 주시고, 이 책이 기반으로 하는 토대를 제공해 주시고(도서관에서 R에서 특정 행을 가져오는 방법을 여러 번 보여주셨던 것을 기억합니다!), 제가 글을 쓰는 데 필요한 시간을 주시고, 책을 쓰는 것이 전날 완벽했던 것을 끝없이 다시 쓰는 것을 의미한다는 것이 밝혀졌을 때 격려해 주시고, 이 책의 모든 것을 여러 번 읽어주시고, 적절하게 커피나 칵테일을 만들어 주시고, 아이들을 돌봐주시고, 그 외에도 많은 것을 해주셔서 감사합니다.\n저에게 연락하실 수 있습니다: rohan.alexander@utoronto.ca.\n\n로한 알렉산더 캐나다 토론토 2023년 5월\n\n\n\n\n\nDeWitt, Helen. 2000. The Last Samurai. 1st ed. United States: Talk Mirimax Books.\n\n\nHamming, Richard. (1997년) 2020. The Art of Doing Science and Engineering. 2nd ed. Stripe Press.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, 와/과 Robert Tibshirani. (2013년) 2021. An Introduction to Statistical Learning with Applications in R. 2nd ed. Springer. https://www.statlearning.com.\n\n\nLyman, Frank. 1981. “The responsive classroom discussion: The inclusion of all students”. Mainstreaming Digest 109: 109–13.\n\n\nMcElreath, Richard. (2015년) 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. 2nd ed. Chapman; Hall/CRC.\n\n\nMcPhee, John. 2017. Draft No. 4. 1st ed. Farrar, Straus; Giroux.\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nRobinson, Emily, 와/과 Jacqueline Nolis. 2020. Build a Career in Data Science. Shelter Island: Manning Publications. https://livebook.manning.com/book/build-a-career-in-data-science.\n\n\nTimbers, Tiffany, Trevor Campbell, 와/과 Melissa Lee. 2022. Data Science: A First Introduction. Chapman; Hall/CRC. https://datasciencebook.ca.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, 와/과 Garrett Grolemund. (2016년) 2023. R for Data Science. 2nd ed. O’Reilly Media. https://r4ds.hadley.nz.\n\n\nWiessner, Polly. 2014. “Embers of society: Firelight talk among the Ju/’hoansi Bushmen”. Proceedings of the National Academy of Sciences 111 (39): 14027–35. https://doi.org/10.1073/pnas.1404212111.",
    "crumbs": [
      "서문"
    ]
  },
  {
    "objectID": "17-concluding_ko.html#concluding-remarks",
    "href": "17-concluding_ko.html#concluding-remarks",
    "title": "18  Concluding remarks",
    "section": "",
    "text": "taking courses on fundamentals, not just fashionable applications;\nreading core texts, not just whatever is trending; and\ntrying to be at the intersection of at least a few different areas, rather than hyper-specialized.",
    "crumbs": [
      "적용",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Concluding remarks</span>"
    ]
  },
  {
    "objectID": "17-concluding_ko.html#some-outstanding-issues",
    "href": "17-concluding_ko.html#some-outstanding-issues",
    "title": "18  Concluding remarks",
    "section": "18.2 Some outstanding issues",
    "text": "18.2 Some outstanding issues\nThere are many issues that are outstanding as we think about data science. They are not the type of issues with a definitive answer. Instead, they are questions to be explored and played with. This work will move data science forward and, more importantly, help us tell better stories about the world. Here we detail some of them.\n1. How do we write effective tests?\nComputer science has built a thorough foundation around testing and the importance of unit and functional tests is broadly accepted. One of the innovations of this book has been to integrate testing throughout the data science workflow, but this, like the first iteration of anything, needs considerable improvement and development.\nWe need to thoroughly integrate testing through data science. But it is unclear what this should look like, how we should do it, and what is the end-state. What does it mean to have well-tested code in data science? Code coverage, which is a measure of the percentage of lines of code that have tests, is not especially meaningful in data science, but what should we use instead? What do tests look like in data science? How are they written? The extensive use of simulation in statistics, which data science has adopted, provides groundwork, but there is a significant amount of work and investment that is needed.\n2. What is happening at the data cleaning and preparation stage?\nWe do not have a good understanding how much data cleaning and preparation is driving estimates. Huntington-Klein 기타 (2021), and Breznau 기타 (2022), among others, have begun this work. They show that hidden research decisions have a big effect on subsequent estimates, sometimes greater than the standard errors. Statistics provides a good understanding of how modeling affects estimates, but we need more investigation of the influence of the earlier stages of the data science workflow. More specifically, we need to look for key points of failure and understand the ways in which failure can happen.\nThis is especially concerning as we scale to larger datasets. For instance, ImageNet is a dataset of 14 million images, which were hand-annotated. The cost, in both time and money, makes it prohibitively difficult to go through every image to ensure the label is consistent with the needs of each user of the dataset. Yet without undertaking this it is difficult to have much faith in subsequent model forecasts, especially in non-obvious cases.\n3. How do we create effective names?\nOne of the crowning achievements of biology is the binomial nomenclature. This is the formal systematic approach to names, established by Carolus Linnaeus, the eighteenth century physician (Morange 2016, p. 81). Each species is referred to by two words with Latin grammatical form: the first is its genus, and the second is an adjective to characterize the species. Ensuring standardized nomenclature is given active consideration in biology. For instance, the use of nomenclature committees by researchers is recommended (McCarthy 기타 2023). As discussed in 장 9, names are a large source of friction in data science, and a standardized approach is similarly needed in data science.\nThe reason this is so pressing is that it affects understanding, which impacts efficiency. The binomial nomenclature provides diagnostic information, not just a casual reference (Koerner 2000, p. 45). This is particularly the case when data science is conducted in a team, rather than just one individual. A thorough understanding of what makes an effective name and then infrastructure to encourage them would bring significant dividends.\n4. What is the appropriate relationship for data science with the constituent parts?\nWe have described the origins of data science as being various disciplines. Moving forward we need to consider what role these constituent parts, especially statistics and computer science, should play. More generally, we also need to establish how data science relates to, and interacts with, econometrics, applied mathematics, and computational social science. These draw on data science to answer questions in their own discipline, but like statistics and computer science, they also contribute back to data science. For instance, applications of machine learning in computational social science need to focus on transparency, interpretability, uncertainty, and ethics, and this all advances the more theoretical machine learning research done in other disciplines (Wallach 2018).\nWe must be careful to continue to learn statistics from statisticians, computer science from computer scientists, etc. An example of the danger of not doing this is clear in the case of p-values, which we have not made much of in this book, but which dominate quantitative analysis even though statisticians have warned about their misapplication for decades. One issue with not learning statistics from statisticians is that statistical practice can become a recipe that is naively followed, because that is the easiest way to teach it, even though that is not how statisticians do statistics.\nData science must remain deeply connected to these disciplines. How we continue to ensure that data science has the best aspects, without also bringing bad practice, is an especially significant challenge. And this is not just technical, but also cultural (Meng 2021). It is particularly important to ensure that data science maintains an inclusive culture of excellence.\n5. How do we teach data science?\nWe are beginning to have agreement on what the foundations of data science are. It involves developing comfort with: computational thinking, sampling, statistics, graphs, Git and GitHub, SQL, command line, cleaning messy data, a few languages including R and Python, ethics, and writing. But we have very little agreement on how best to teach it. Partly this is because data science instructors often come from different fields, but it is also partly a difference in resources and priorities.\nComplicating matters is that given the demand for data science skills we cannot limit data science education to graduate students because undergraduate students need those skills when they enter the workforce. If data science is to be taught at the undergraduate level, then it needs to be robust enough to be taught in large classes. Developing teaching tools that scale is critical. For instance, GitHub Actions could be used to run checks of student code and suggest improvements without instructor involvement. However, it is especially difficult to scale case studies style classes, which students often find so useful. Substantial innovation is needed.\n6. What does the relationship between industry and academia look like?\nConsiderable innovation in data science occurs in industry, but sometimes this knowledge cannot be shared, and when it can it tends to be done slowly. The term data science has been used in academia since the 1960s, but it is because of industry that it has become popular in the past decade or so (Irizarry 2020).\nBringing academia and industry together is both a key challenge for data science and one of the easiest to overlook. The nature of the problems faced in industry, for instance scoping the needs of a client, and operating at scale, are removed from typical academic concerns. There is a danger that academic research could be rendered moot unless academics establish and maintain one foot in industry, and enable industry to actively participate in academia. From the industry side, ensuring that best practice is quickly adopted can be challenging if there is no immediate payoff. Ensuring that industry experience is valued in academic hiring and grant evaluation would help, as would encouraging entrepreneurship in academia.",
    "crumbs": [
      "적용",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Concluding remarks</span>"
    ]
  },
  {
    "objectID": "17-concluding_ko.html#next-steps",
    "href": "17-concluding_ko.html#next-steps",
    "title": "18  Concluding remarks",
    "section": "18.3 Next steps",
    "text": "18.3 Next steps\nThis book has covered much ground, and while we are toward the end of it, as the butler Stevens is told in the novel The Remains of the Day by Kazuo Ishiguro:\n\nThe evening’s the best part of the day. You’ve done your day’s work. Now you can put your feet up and enjoy it.\nIshiguro (1989)\n\nChances are there are aspects that you want to explore further, building on the foundation that you have established. If so, then the book has accomplished its aim.\nIf you were new to data science at the start of this book, then the next step would be to backfill what we skipped over. Begin with Data Science: A First Introduction (Timbers, Campbell, 와/과 Lee 2022). After that go through R for Data Science (Wickham, Çetinkaya-Rundel, 와/과 Grolemund [2016년] 2023). We used R in this book and only mentioned SQL and Python in passing, but it is important to develop comfort in these languages. Start with SQL for Data Scientists (Teate 2022), Python for Data Analysis (McKinney [2011년] 2022), and the free Replit “100 Days of Code” Python course.\nSampling is a critical, but easy to overlook, aspect of data science. It would be sensible to go through Sampling: Design and Analysis (Lohr [1999년] 2022). To deepen your understanding of surveys and experiments, go next to Field Experiments: Design, Analysis, and Interpretation (Gerber 와/과 Green 2012) and Trustworthy online controlled experiments (Kohavi, Tang, 와/과 Xu 2020).\nFor developing better data visualization skills, begin by turning to Data Sketches (Bremer 와/과 Wu 2021) and Data Visualization (Healy 2018). After that, develop strong foundations, such as The Grammar of Graphics (Wilkinson 2005).\nIf you are interested to learn more about modeling, then the next steps are Statistical Rethinking: A Bayesian Course with Examples in R and Stan (McElreath [2015년] 2020), which additionally has an excellent series of accompanying videos, Bayes Rules! An Introduction to Bayesian Modeling with R (Johnson, Ott, 와/과 Dogucu 2022), and Regression and Other Stories (Gelman, Hill, 와/과 Vehtari 2020). It would also be worthwhile to establish a foundation of probability with All of Statistics (Wasserman 2005).\nThere is only one next natural step if you are interested in machine learning and that is An Introduction to Statistical Learning (James 기타 [2013년] 2021) followed by The Elements of Statistical Learning (Friedman, Tibshirani, 와/과 Hastie 2009).\nTo learn more about causality start with the economics perspective by going through Causal Inference: The Mixtape (Cunningham 2021) and The Effect: An Introduction to Research Design and Causality (Huntington-Klein 2021). Then turn to the health sciences perspective by going through What If (Hernán 와/과 Robins 2023).\nFor text as data, start with Text As Data (Grimmer, Roberts, 와/과 Stewart 2022). Then turn to Supervised Machine Learning for Text Analysis in R (Hvitfeldt 와/과 Silge 2021).\nIn terms of ethics, there are a variety of books. We have covered many chapters of it, throughout this book, but going through Data Feminism (D’Ignazio 와/과 Klein 2020) end-to-end would be useful, as would Atlas of AI (Crawford 2021).\nAnd finally, for writing, it would be best to turn inward. Force yourself to write every day for a month. Then do it again and again. You will get better. That said, there are some useful books, including Working (Caro 2019) and On Writing: A Memoir of the Craft (King 2000).\nWe often hear the phrase “let the data speak”. Hopefully it is clear this never happens. All that we can do is to acknowledge that we are the ones using data to tell stories, and strive and seek to make them worthy.\n\nIt was her voice that made\nThe sky acutest at its vanishing.\nShe measured to the hour its solitude.\nShe was the single artificer of the world\nIn which she sang. And when she sang, the sea,\nWhatever self it had, became the self\nThat was her song, for she was the maker.\nExtract from “The Idea of Order at Key West”, (Stevens 1934)",
    "crumbs": [
      "적용",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Concluding remarks</span>"
    ]
  },
  {
    "objectID": "17-concluding_ko.html#exercises",
    "href": "17-concluding_ko.html#exercises",
    "title": "18  Concluding remarks",
    "section": "18.4 Exercises",
    "text": "18.4 Exercises\n\nQuestions\n\nWhat is data science?\nWho does data affect, and what affects data?\nDiscuss the inclusion of “race” and/or “sexuality” in a model.\nWhat makes a story more or less convincing?\nWhat is the role of ethics when dealing with data?\n\n\n\nClass activities\n\nClean up your GitHub: delete unnecessary repos, pin your best ones, update your bio, add a profile README.\n\n\n\n\n\nBremer, Nadieh, 와/과 Shirley Wu. 2021. Data Sketches. A K Peters/CRC Press. https://doi.org/10.1201/9780429445019.\n\n\nBreznau, Nate, Eike Mark Rinke, Alexander Wuttke, Hung HV Nguyen, Muna Adem, Jule Adriaans, Amalia Alvarez-Benjumea, 기타. 2022. “Observing many researchers using the same data and hypothesis reveals a hidden universe of uncertainty”. Proceedings of the National Academy of Sciences 119 (44): e2203150119. https://doi.org/10.1073/pnas.2203150119.\n\n\nCaro, Robert. 2019. Working. 1st ed. New York: Knopf.\n\n\nCrawford, Kate. 2021. Atlas of AI. 1st ed. New Haven: Yale University Press.\n\n\nCunningham, Scott. 2021. Causal Inference: The Mixtape. 1st ed. New Haven: Yale Press. https://mixtape.scunning.com.\n\n\nD’Ignazio, Catherine, 와/과 Lauren Klein. 2020. Data Feminism. Massachusetts: The MIT Press. https://data-feminism.mitpress.mit.edu.\n\n\nFriedman, Jerome, Robert Tibshirani, 와/과 Trevor Hastie. 2009. The Elements of Statistical Learning. 2nd ed. Springer. https://hastie.su.domains/ElemStatLearn/.\n\n\nGelman, Andrew, Jennifer Hill, 와/과 Aki Vehtari. 2020. Regression and Other Stories. Cambridge University Press. https://avehtari.github.io/ROS-Examples/.\n\n\nGerber, Alan, 와/과 Donald Green. 2012. Field Experiments: Design, Analysis, and Interpretation. New York: WW Norton.\n\n\nGray, Charles T., 와/과 Ben Marwick. 2019. “Truth, Proof, and Reproducibility: There’s No Counter-Attack for the Codeless”. In Communications in Computer and Information Science, 111–29. Springer Singapore. https://doi.org/10.1007/978-981-15-1960-4_8.\n\n\nGrimmer, Justin, Margaret Roberts, 와/과 Brandon Stewart. 2022. Text As Data: A New Framework for Machine Learning and the Social Sciences. New Jersey: Princeton University Press.\n\n\nHealy, Kieran. 2018. Data Visualization. New Jersey: Princeton University Press. https://socviz.co.\n\n\nHernán, Miguel, 와/과 James Robins. 2023. What If. 1st ed. Boca Raton: Chapman & Hall/CRC. https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/.\n\n\nHorton, Nicholas, Rohan Alexander, Micaela Parker, Aneta Piekut, 와/과 Colin Rundel. 2022. “The Growing Importance of Reproducibility and Responsible Workflow in the Data Science and Statistics Curriculum”. Journal of Statistics and Data Science Education 30 (3): 207–8. https://doi.org/10.1080/26939169.2022.2141001.\n\n\nHuntington-Klein, Nick. 2021. The Effect: An Introduction to Research Design and Causality. 1st ed. Chapman & Hall. https://theeffectbook.net.\n\n\nHuntington-Klein, Nick, Andreu Arenas, Emily Beam, Marco Bertoni, Jeffrey Bloem, Pralhad Burli, Naibin Chen, 기타. 2021. “The influence of hidden researcher decisions in applied microeconomics”. Economic Inquiry 59: 944–60. https://doi.org/10.1111/ecin.12992.\n\n\nHvitfeldt, Emil, 와/과 Julia Silge. 2021. Supervised Machine Learning for Text Analysis in R. 1st ed. Chapman; Hall/CRC. https://doi.org/10.1201/9781003093459.\n\n\nIrizarry, Rafael. 2020. “The Role of Academia in Data Science Education”. Harvard Data Science Review 2 (1). https://doi.org/10.1162/99608f92.dd363929.\n\n\nIshiguro, Kazuo. 1989. The Remains of the Day. 1st ed. Faber; Faber.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, 와/과 Robert Tibshirani. (2013년) 2021. An Introduction to Statistical Learning with Applications in R. 2nd ed. Springer. https://www.statlearning.com.\n\n\nJohnson, Alicia, Miles Ott, 와/과 Mine Dogucu. 2022. Bayes Rules! An Introduction to Bayesian Modeling with R. 1st ed. Chapman; Hall/CRC. https://www.bayesrulesbook.com.\n\n\nKing, Stephen. 2000. On Writing: A Memoir of the Craft. 1st ed. Scribner.\n\n\nKoerner, Lisbet. 2000. Linnaeus: Nature and Nation. Cambridge: Harvard University Press.\n\n\nKohavi, Ron, Diane Tang, 와/과 Ya Xu. 2020. Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing. Cambridge University Press.\n\n\nLeek, Jeff, Blakeley McShane, Andrew Gelman, David Colquhoun, Michèle Nuijten, 와/과 Steven Goodman. 2017. “Five ways to fix statistics”. Nature 551 (7682): 557–59. https://doi.org/10.1038/d41586-017-07522-z.\n\n\nLeonelli, Sabina. 2020. “Learning from Data Journeys”. In Data Journeys in the Sciences, 1–24. Springer International Publishing. https://doi.org/10.1007/978-3-030-37177-7_1.\n\n\nLohr, Sharon. (1999년) 2022. Sampling: Design and Analysis. 3rd ed. Chapman; Hall/CRC.\n\n\nMcCarthy, Fiona M., Tamsin E. M. Jones, Anne E. Kwitek, Cynthia L. Smith, Peter D. Vize, Monte Westerfield, 와/과 Elspeth A. Bruford. 2023. “The case for standardizing gene nomenclature in vertebrates”. Nature 614 (7948): E31–32. https://doi.org/10.1038/s41586-022-05633-w.\n\n\nMcElreath, Richard. (2015년) 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. 2nd ed. Chapman; Hall/CRC.\n\n\n———. 2020. “Science as Amateur Software Development”. YouTube, 9월. https://youtu.be/zwRdO9%5FGGhY.\n\n\nMcKinney, Wes. (2011년) 2022. Python for Data Analysis. 3rd ed. https://wesmckinney.com/book/.\n\n\nMeng, Xiao-Li. 2021. “What Are the Values of Data, Data Science, or Data Scientists?” Harvard Data Science Review 3 (1). https://doi.org/10.1162/99608f92.ee717cf7.\n\n\nMorange, Michel. 2016. A History of Biology. New Jersey: Princeton University Press.\n\n\nPerkel, Jeffrey. 2021. “Ten computer codes that transformed science”. Nature 589 (7842): 344–48. https://doi.org/10.1038/d41586-021-00075-2.\n\n\nStevens, Wallace. 1934. The Idea of Order at Key West. https://www.poetryfoundation.org/poems/43431/the-idea-of-order-at-key-west.\n\n\nTeate, Renée. 2022. SQL for Data Scientists. Wiley.\n\n\nTimbers, Tiffany, Trevor Campbell, 와/과 Melissa Lee. 2022. Data Science: A First Introduction. Chapman; Hall/CRC. https://datasciencebook.ca.\n\n\nWallach, Hanna. 2018. “Computational social science \\(\\ne\\) computer science + social data”. Communications of the ACM 61 (3): 42–44. https://doi.org/10.1145/3132698.\n\n\nWasserman, Larry. 2005. All of Statistics. Springer.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, 와/과 Garrett Grolemund. (2016년) 2023. R for Data Science. 2nd ed. O’Reilly Media. https://r4ds.hadley.nz.\n\n\nWilkinson, Leland. 2005. The Grammar of Graphics. 2nd ed. Springer.",
    "crumbs": [
      "적용",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Concluding remarks</span>"
    ]
  },
  {
    "objectID": "20-r_essentials_ko.html",
    "href": "20-r_essentials_ko.html",
    "title": "Online Appendix A — R 필수 사항",
    "section": "",
    "text": "A.1 서론\n선수 지식\n핵심 개념 및 기술\n소프트웨어 및 패키지\n이 장에서는 통계 프로그래밍 언어 R(R Core Team 2024)을 사용하여 데이터로 이야기를 하는 데 필요한 기초적인 기술에 초점을 맞춥니다. 처음에는 이해가 안 될 수도 있지만, 이것들은 우리가 자주 사용할 기술과 접근 방식입니다. 처음에는 이 장을 빠르게 훑어보고 이해가 안 되는 부분을 메모하십시오. 그런 다음 책의 나머지 부분을 계속 진행하면서 이 장으로 돌아오십시오. 그렇게 하면 다양한 부분이 어떻게 맥락에 맞는지 알 수 있을 것입니다.\nR은 통계 프로그래밍을 위한 오픈 소스 언어입니다. 종합 R 아카이브 네트워크(CRAN)에서 R을 무료로 다운로드할 수 있습니다. RStudio는 R을 더 쉽게 사용할 수 있도록 하는 R용 통합 개발 환경(IDE)이며, Posit 여기에서 무료로 다운로드할 수 있습니다.\n지난 10여 년은 tidyverse의 사용 증가로 특징지어졌습니다. 이것은 “…데이터 과학을 위해 설계된 R 패키지의 의견이 반영된 컬렉션입니다. 모든 패키지는 기본 설계 철학, 문법 및 데이터 구조를 공유합니다”(Wickham 2020). 세 가지 구별을 명확히 해야 합니다. 즉, 일반적으로 “base”라고 불리는 원본 R 언어; base 위에 구축된 일관된 패키지 모음인 tidyverse; 그리고 다른 패키지입니다.\n본질적으로 tidyverse에서 할 수 있는 모든 것은 base에서도 할 수 있습니다. 그러나 tidyverse는 특히 데이터 과학을 위해 구축되었기 때문에 특히 학습할 때 사용하기가 더 쉽습니다. 또한 tidyverse에서 할 수 있는 대부분의 모든 것은 다른 패키지로도 할 수 있습니다. 그러나 tidyverse는 일관된 패키지 모음이므로 특히 학습할 때 사용하기가 더 쉽습니다. 결국 tidyverse의 편리함과 일관성을 base, 다른 패키지 또는 언어의 일부 기능과 교환하는 것이 합리적인 경우가 있습니다. 실제로, 장 10 에서 SQL을 데이터 작업 시 상당한 효율성 향상의 한 가지 원천으로 소개합니다. 예를 들어, tidyverse는 느릴 수 있으므로 수천 개의 CSV를 가져와야 하는 경우 read_csv()에서 벗어나는 것이 합리적일 수 있습니다. 특정 솔루션에 대한 독단적인 고집보다는 base 및 비-tidyverse 패키지 또는 언어의 적절한 사용은 지적 성숙의 신호입니다.\n통계 프로그래밍 언어 R을 사용하는 데 있어 핵심은 데이터이며, 우리가 사용하는 대부분의 데이터는 인간을 중심으로 할 것입니다. 때로는 이러한 방식으로 인간 중심 데이터를 다루는 것이 무감각한 효과를 가져와 과도한 일반화 및 잠재적으로 문제가 있는 작업으로 이어질 수 있습니다. 지적 성숙의 또 다른 신호는 반대 효과를 가져와 의사 결정 과정과 그 결과에 대한 인식을 높이는 것입니다.",
    "crumbs": [
      "부록",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R 필수 사항</span>"
    ]
  },
  {
    "objectID": "20-r_essentials_ko.html#서론",
    "href": "20-r_essentials_ko.html#서론",
    "title": "Online Appendix A — R 필수 사항",
    "section": "",
    "text": "실제로, 나는 의미에 대한 질문에서 멀어지는 것이 아니라, 정량적 데이터가 당신에게 그것들을 직면하도록 강요한다는 것을 발견합니다. 숫자가 당신을 끌어들입니다. 이런 식으로 데이터를 다루는 것은 겸손의 끝없는 연습이며, 당신이 무엇을 보고 무엇을 볼 수 없는지 끊임없이 생각하게 하고, 측정값이 실제로 무엇을 포착하는지, 즉 그것들이 무엇을 의미하고 누구를 위한 것인지 이해하도록 끊임없이 초대합니다.\nHealy (2020)",
    "crumbs": [
      "부록",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R 필수 사항</span>"
    ]
  },
  {
    "objectID": "20-r_essentials_ko.html#r-rstudio-그리고-posit-cloud",
    "href": "20-r_essentials_ko.html#r-rstudio-그리고-posit-cloud",
    "title": "Online Appendix A — R 필수 사항",
    "section": "A.2 R, RStudio, 그리고 Posit Cloud",
    "text": "A.2 R, RStudio, 그리고 Posit Cloud\nR과 RStudio는 상호 보완적이지만, 동일한 것은 아닙니다. Müller, Schieferdecker, 와/과 Schratz (2019) 은 R은 엔진과 같고 RStudio는 자동차와 같다고 비유하여 그들의 관계를 설명합니다. 우리는 엔진을 다양한 상황에서 사용할 수 있으며, 자동차에만 국한되지 않지만, 이 조합은 특히 유용합니다.\n\nA.2.1 R\nR은 일반 통계에 중점을 둔 오픈 소스 무료 프로그래밍 언어입니다. 이 맥락에서 무료는 가격이 0이라는 것을 의미하는 것이 아니라, 개발자들이 사용자에게 원하는 대로 자유롭게 사용할 수 있도록 허용한다는 것을 의미합니다(물론 가격도 0입니다). 이는 Python과 같이 범용으로 설계된 오픈 소스 프로그래밍 언어 또는 Stan과 같이 확률에 중점을 둔 오픈 소스 프로그래밍 언어와 대조됩니다. 1990년대 오클랜드 대학교의 로스 이아카와 로버트 젠틀맨이 만들었으며, 1970년대 벨 연구소에서 개발된 S에서 유래했습니다. R Core Team이 유지 관리하며, 이 “base” 코드의 변경은 체계적으로 이루어지며 다양한 우선 순위를 고려합니다.\n많은 사람들이 이 안정적인 base 위에 구축하여 R의 기능을 더 잘, 더 빠르게 자신의 필요에 맞게 확장합니다. 그들은 패키지를 생성하여 이를 수행합니다. 일반적으로, 항상 그런 것은 아니지만, 패키지는 R 코드, 주로 함수 모음이며, 이를 통해 원하는 작업을 더 쉽게 수행할 수 있습니다. 이러한 패키지는 CRAN 및 Bioconductor와 같은 저장소에서 관리됩니다.\n패키지를 사용하려면 먼저 컴퓨터에 설치해야 하고, 사용하고 싶을 때 로드해야 합니다. 모나쉬 대학교의 비즈니스 분석 교수인 디 쿡 박사는 이를 전구에 비유합니다. 집에 불을 켜고 싶다면 먼저 전구를 끼워야 하고, 그런 다음 스위치를 켜야 합니다. install.packages(\"tidyverse\")와 같이 패키지를 설치하는 것은 전구를 소켓에 끼우는 것과 같습니다. 각 전구에 대해 한 번만 하면 됩니다. 그러나 불을 켜고 싶을 때마다 스위치를 켜야 합니다. R 패키지의 경우, library(tidyverse)와 같이 라이브러리를 불러오는 것을 의미합니다.\n\n\n\n\n\n\n거인의 어깨 위에 서서\n\n\n\n디 쿡 박사는 모나쉬 대학교의 통계학 특훈 교수입니다. 1993년 럿거스 대학교에서 통계 그래픽에 중점을 둔 통계학 박사 학위를 받은 후, 아이오와 주립 대학교에서 조교수로 임명되었고, 2005년 정교수로 승진했으며, 2015년 모나쉬로 옮겼습니다. 그녀의 연구 분야 중 하나는 데이터 시각화, 특히 대화형 및 동적 그래픽입니다. Buja, Cook, 와/과 Swayne (1996) 는 대화형 데이터 시각화의 분류법과 관련 소프트웨어 XGobi를 제안하며, 이는 Cook 와/과 Swayne (2007) 의 초점입니다. Cook 기타 (1995) 는 탐색적 데이터 분석을 위한 동적 그래픽 도구의 개발 및 탐색을 다루고, Buja 기타 (2009) 는 시각적 통계 방법을 평가하기 위한 프레임워크를 개발하며, 여기서 플롯과 인간 인지는 각각 테스트 통계량과 통계 테스트를 대신합니다. 그녀는 미국 통계 학회 펠로우입니다.\n\n\n컴퓨터에 패키지를 설치하려면(다시, 컴퓨터당 한 번만 하면 됩니다) install.packages()를 사용합니다.\n\ninstall.packages(\"tidyverse\")\n\n그리고 패키지를 사용하고 싶을 때는 library()를 사용합니다.\n\nlibrary(tidyverse)\n\n다운로드한 후 R을 열고 직접 사용할 수 있습니다. 주로 명령줄을 통해 상호 작용하도록 설계되었습니다. 이것은 기능적이지만, 명령줄이 제공하는 것보다 더 풍부한 환경을 갖는 것이 유용할 수 있습니다. 특히, 자주 사용될 다양한 비트와 조각을 모아주는 애플리케이션인 통합 개발 환경(IDE)을 설치하는 것이 유용할 수 있습니다. R에 대한 일반적인 IDE 중 하나는 RStudio이지만, Visual Studio와 같은 다른 IDE도 사용됩니다.\n\n\nA.2.2 RStudio\nRStudio는 R과 구별되며, 서로 다른 개체입니다. RStudio는 R 위에 구축되어 R을 더 쉽게 사용할 수 있도록 합니다. 이는 명령줄에서 인터넷을 사용할 수 있지만, 대부분의 사람들이 Chrome, Firefox 또는 Safari와 같은 브라우저를 사용하는 것과 같은 방식입니다.\nRStudio는 우리가 비용을 지불하지 않는다는 의미에서 무료입니다. 또한 코드를 가져와 수정하고 배포할 수 있다는 의미에서도 무료입니다. 그러나 RStudio의 제작사인 Posit은 B Corp이긴 하지만 회사이므로 현재 상황이 변경될 수 있습니다. Posit 여기에서 다운로드할 수 있습니다.\nRStudio를 열면 그림 A.1 와 같이 보일 것입니다.\n\n\n\n\n\n\n그림 A.1: RStudio를 처음 열었을 때\n\n\n\n왼쪽 창은 콘솔로, R 코드를 한 줄씩 입력하고 실행할 수 있습니다. 프롬프트 “&gt;” 옆을 클릭하고 “2+2”를 입력한 다음 “return/enter”를 눌러 2+2를 시도해 보십시오.\n\n2 + 2\n\n[1] 4\n\n\n오른쪽 상단 창에는 환경에 대한 정보가 있습니다. 예를 들어, 변수를 만들면 변수 이름과 일부 속성 목록이 거기에 나타납니다. 프롬프트 옆에 다음 코드를 입력하고, Rohan을 자신의 이름으로 바꾼 다음, 다시 엔터를 누르십시오.\n\nmy_name &lt;- \"로한\"\n\n장 2 에서 언급했듯이 &lt;- 또는 “할당 연산자”는 \"로한\"을 “my_name”이라는 객체에 할당합니다. 변수 이름과 값이 있는 환경 창에 새 값이 나타나는 것을 알 수 있을 것입니다.\n오른쪽 하단 창은 파일 관리자입니다. 현재는 R 기록 파일과 R 프로젝트 파일 두 개만 있어야 합니다. 나중에 이것들이 무엇인지 알아볼 것이지만, 지금은 파일을 만들고 저장할 것입니다.\n자세한 내용은 너무 걱정하지 말고 다음 코드를 실행하십시오. 파일 목록에 새 “.rds” 파일이 나타나는 것을 볼 수 있을 것입니다.\n\nsaveRDS(object = my_name, file = \"my_first_file.rds\")\n\n\n\nA.2.3 Posit Cloud\nRStudio를 자신의 컴퓨터에 다운로드해야 하지만, 처음에는 Posit Cloud를 사용하는 것을 권장합니다. 이것은 Posit에서 제공하는 RStudio의 온라인 버전입니다. 우리는 이것을 사용하여 R과 RStudio에 일관된 환경에서 익숙해질 수 있도록 할 것입니다. 이렇게 하면 컴퓨터나 설치 권한 등에 대해 걱정할 필요가 없습니다.\nPosit Cloud의 무료 버전은 재정적 비용이 없다는 의미에서 무료입니다. 단점은 강력하지 않고 때로는 느리다는 것이지만, 시작하는 데는 충분합니다.",
    "crumbs": [
      "부록",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R 필수 사항</span>"
    ]
  },
  {
    "objectID": "20-r_essentials_ko.html#dplyr-동사",
    "href": "20-r_essentials_ko.html#dplyr-동사",
    "title": "Online Appendix A — R 필수 사항",
    "section": "A.4 dplyr 동사",
    "text": "A.4 dplyr 동사\n데이터 조작 측면에서 우리가 사용할 핵심 패키지 중 하나는 tidyverse(Wickham 기타 2019)입니다. tidyverse는 실제로 패키지 모음이며, 이는 tidyverse를 설치할 때 실제로 많은 다른 패키지를 설치한다는 것을 의미합니다. 데이터 조작 측면에서 tidyverse의 핵심 패키지는 dplyr(Wickham 기타 2022)입니다.\n정기적으로 사용되는 다섯 가지 dplyr 함수가 있으며, 이제 각각을 살펴보겠습니다. 이들은 일반적으로 dplyr 동사라고 불립니다.\n\nselect()\nfilter()\narrange()\nmutate()\nsummarise() 또는 summarize()\n\n또한 밀접하게 관련된 .by와 count()도 다룰 것입니다.\ntidyverse를 이미 설치했으므로 로드하기만 하면 됩니다.\nlibrary(tidyverse)\n그리고 AustralianPoliticians 패키지(Alexander 와/과 Hodgetts 2021)의 호주 정치인에 대한 데이터를 다시 사용하는 것으로 시작하겠습니다.\n\nlibrary(AustralianPoliticians)\n\naustralian_politicians &lt;-\n  get_auspol(\"all\")\n\nhead(australian_politicians)\n\n# A tibble: 6 × 20\n  uniqueID   surname allOtherNames          firstName commonName displayName    \n  &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;                  &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;          \n1 Abbott1859 Abbott  Richard Hartley Smith  Richard   &lt;NA&gt;       Abbott, Richard\n2 Abbott1869 Abbott  Percy Phipps           Percy     &lt;NA&gt;       Abbott, Percy  \n3 Abbott1877 Abbott  Macartney              Macartney Mac        Abbott, Mac    \n4 Abbott1886 Abbott  Charles Lydiard Aubrey Charles   Aubrey     Abbott, Aubrey \n5 Abbott1891 Abbott  Joseph Palmer          Joseph    &lt;NA&gt;       Abbott, Joseph \n6 Abbott1957 Abbott  Anthony John           Anthony   Tony       Abbott, Tony   \n# ℹ 14 more variables: earlierOrLaterNames &lt;chr&gt;, title &lt;chr&gt;, gender &lt;chr&gt;,\n#   birthDate &lt;date&gt;, birthYear &lt;dbl&gt;, birthPlace &lt;chr&gt;, deathDate &lt;date&gt;,\n#   member &lt;dbl&gt;, senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;, wikidataID &lt;chr&gt;,\n#   wikipedia &lt;chr&gt;, adb &lt;chr&gt;, comments &lt;chr&gt;\n\n\n\nA.4.1 select()\n데이터셋의 특정 열을 선택하려면 select()를 사용합니다. 예를 들어, “firstName” 열을 선택하고 싶을 수 있습니다.\n\naustralian_politicians |&gt;\n  select(firstName)\n\n# A tibble: 1,783 × 1\n   firstName\n   &lt;chr&gt;    \n 1 Richard  \n 2 Percy    \n 3 Macartney\n 4 Charles  \n 5 Joseph   \n 6 Anthony  \n 7 John     \n 8 Eric     \n 9 Judith   \n10 Dick     \n# ℹ 1,773 more rows\n\n\nR에는 작업을 수행하는 여러 가지 방법이 있습니다. 때로는 동일한 작업을 수행하는 다른 방법이 있고, 때로는 거의 동일한 작업을 수행하는 다른 방법이 있습니다. 예를 들어, 데이터셋의 특정 열을 선택하는 또 다른 방법은 “추출” 연산자 $를 사용하는 것입니다. 이것은 select()와 달리 base에서 온 것입니다.\n\naustralian_politicians$firstName |&gt;\n  head()\n\n[1] \"Richard\"   \"Percy\"     \"Macartney\" \"Charles\"   \"Joseph\"    \"Anthony\"  \n\n\n두 가지는 비슷해 보입니다. 둘 다 “firstName” 열을 선택하지만, 반환하는 것의 클래스가 다릅니다. select()는 티블을 반환하고 $는 벡터를 반환합니다. 완전성을 위해 select()와 pull()을 결합하면 추출 연산자를 사용한 것과 동일한 클래스의 출력, 즉 벡터를 얻습니다.\n\naustralian_politicians |&gt;\n  select(firstName) |&gt;\n  pull() |&gt;\n  head()\n\n[1] \"Richard\"   \"Percy\"     \"Macartney\" \"Charles\"   \"Joseph\"    \"Anthony\"  \n\n\n열 이름을 부정하여 select()를 사용하여 열을 제거할 수도 있습니다.\n\naustralian_politicians |&gt;\n  select(-firstName)\n\n# A tibble: 1,783 × 19\n   uniqueID   surname allOtherNames   commonName displayName earlierOrLaterNames\n   &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;           &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;              \n 1 Abbott1859 Abbott  Richard Hartle… &lt;NA&gt;       Abbott, Ri… &lt;NA&gt;               \n 2 Abbott1869 Abbott  Percy Phipps    &lt;NA&gt;       Abbott, Pe… &lt;NA&gt;               \n 3 Abbott1877 Abbott  Macartney       Mac        Abbott, Mac &lt;NA&gt;               \n 4 Abbott1886 Abbott  Charles Lydiar… Aubrey     Abbott, Au… &lt;NA&gt;               \n 5 Abbott1891 Abbott  Joseph Palmer   &lt;NA&gt;       Abbott, Jo… &lt;NA&gt;               \n 6 Abbott1957 Abbott  Anthony John    Tony       Abbott, To… &lt;NA&gt;               \n 7 Abel1939   Abel    John Arthur     &lt;NA&gt;       Abel, John  &lt;NA&gt;               \n 8 Abetz1958  Abetz   Eric            &lt;NA&gt;       Abetz, Eric &lt;NA&gt;               \n 9 Adams1943  Adams   Judith Anne     &lt;NA&gt;       Adams, Jud… nee Bird           \n10 Adams1951  Adams   Dick Godfrey H… &lt;NA&gt;       Adams, Dick &lt;NA&gt;               \n# ℹ 1,773 more rows\n# ℹ 13 more variables: title &lt;chr&gt;, gender &lt;chr&gt;, birthDate &lt;date&gt;,\n#   birthYear &lt;dbl&gt;, birthPlace &lt;chr&gt;, deathDate &lt;date&gt;, member &lt;dbl&gt;,\n#   senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;, wikidataID &lt;chr&gt;, wikipedia &lt;chr&gt;,\n#   adb &lt;chr&gt;, comments &lt;chr&gt;\n\n\n마지막으로, 조건을 기반으로 select()할 수 있습니다. 예를 들어, “birth”로 시작하는 모든 열을 select()할 수 있습니다.\n\naustralian_politicians |&gt;\n  select(starts_with(\"birth\"))\n\n# A tibble: 1,783 × 3\n   birthDate  birthYear birthPlace  \n   &lt;date&gt;         &lt;dbl&gt; &lt;chr&gt;       \n 1 NA              1859 Bendigo     \n 2 1869-05-14        NA Hobart      \n 3 1877-07-03        NA Murrurundi  \n 4 1886-01-04        NA St Leonards \n 5 1891-10-18        NA North Sydney\n 6 1957-11-04        NA London      \n 7 1939-06-25        NA Sydney      \n 8 1958-01-25        NA Stuttgart   \n 9 1943-04-11        NA Picton      \n10 1951-04-29        NA Launceston  \n# ℹ 1,773 more rows\n\n\nstarts_with(), ends_with(), contains()를 포함한 다양한 유사한 “선택 도우미”가 있습니다. 이에 대한 자세한 정보는 ?select()를 실행하여 접근할 수 있는 select()의 도움말 페이지에서 확인할 수 있습니다.\n이 시점에서 select()를 사용하여 데이터셋의 너비를 줄일 것입니다.\n\naustralian_politicians &lt;-\n  australian_politicians |&gt;\n  select(\n    uniqueID,\n    surname,\n    firstName,\n    gender,\n    birthDate,\n    birthYear,\n    deathDate,\n    member,\n    senator,\n    wasPrimeMinister\n  )\n\naustralian_politicians\n\n# A tibble: 1,783 × 10\n   uniqueID   surname firstName gender birthDate  birthYear deathDate  member\n   &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;  &lt;date&gt;         &lt;dbl&gt; &lt;date&gt;      &lt;dbl&gt;\n 1 Abbott1859 Abbott  Richard   male   NA              1859 1940-02-28      0\n 2 Abbott1869 Abbott  Percy     male   1869-05-14        NA 1940-09-09      1\n 3 Abbott1877 Abbott  Macartney male   1877-07-03        NA 1960-12-30      0\n 4 Abbott1886 Abbott  Charles   male   1886-01-04        NA 1975-04-30      1\n 5 Abbott1891 Abbott  Joseph    male   1891-10-18        NA 1965-05-07      1\n 6 Abbott1957 Abbott  Anthony   male   1957-11-04        NA NA              1\n 7 Abel1939   Abel    John      male   1939-06-25        NA NA              1\n 8 Abetz1958  Abetz   Eric      male   1958-01-25        NA NA              0\n 9 Adams1943  Adams   Judith    female 1943-04-11        NA 2012-03-31      0\n10 Adams1951  Adams   Dick      male   1951-04-29        NA NA              1\n# ℹ 1,773 more rows\n# ℹ 2 more variables: senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;\n\n\nR을 처음 사용하는 사람들을 혼란스럽게 하는 한 가지는 출력이 객체에 할당되지 않으면 “저장”되지 않는다는 것입니다. 예를 들어, 여기서는 첫 줄이 australian_politicians &lt;- australian_politicians |&gt;이고, 그 다음 select()가 사용됩니다. australian_politicians |&gt;와 비교됩니다. 이렇게 하면 select()에 의해 발생한 변경 사항이 객체에 적용되므로, 코드의 나중에 해당 수정된 버전이 사용됩니다.\n\n\nA.4.2 filter()\n데이터셋의 특정 행을 선택하려면 filter()를 사용합니다. 예를 들어, 총리가 된 정치인에게만 관심이 있을 수 있습니다.\n\naustralian_politicians |&gt;\n  filter(wasPrimeMinister == 1)\n\n# A tibble: 30 × 10\n   uniqueID    surname firstName gender birthDate  birthYear deathDate  member\n   &lt;chr&gt;       &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;  &lt;date&gt;         &lt;dbl&gt; &lt;date&gt;      &lt;dbl&gt;\n 1 Abbott1957  Abbott  Anthony   male   1957-11-04        NA NA              1\n 2 Barton1849  Barton  Edmund    male   1849-01-18        NA 1920-01-07      1\n 3 Bruce1883   Bruce   Stanley   male   1883-04-15        NA 1967-08-25      1\n 4 Chifley1885 Chifley Joseph    male   1885-09-22        NA 1951-06-13      1\n 5 Cook1860    Cook    Joseph    male   1860-12-07        NA 1947-07-30      1\n 6 Curtin1885  Curtin  John      male   1885-01-08        NA 1945-07-05      1\n 7 Deakin1856  Deakin  Alfred    male   1856-08-03        NA 1919-10-07      1\n 8 Fadden1894  Fadden  Arthur    male   1894-04-13        NA 1973-04-21      1\n 9 Fisher1862  Fisher  Andrew    male   1862-08-29        NA 1928-10-22      1\n10 Forde1890   Forde   Francis   male   1890-07-18        NA 1983-01-28      1\n# ℹ 20 more rows\n# ℹ 2 more variables: senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;\n\n\nfilter()에 두 가지 조건을 줄 수도 있습니다. 예를 들어, 총리가 되었고 이름이 조셉인 정치인을 “and” 연산자 &를 사용하여 찾을 수 있습니다.\n\naustralian_politicians |&gt;\n  filter(wasPrimeMinister == 1 & firstName == \"Joseph\")\n\n# A tibble: 3 × 10\n  uniqueID    surname firstName gender birthDate  birthYear deathDate  member\n  &lt;chr&gt;       &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;  &lt;date&gt;         &lt;dbl&gt; &lt;date&gt;      &lt;dbl&gt;\n1 Chifley1885 Chifley Joseph    male   1885-09-22        NA 1951-06-13      1\n2 Cook1860    Cook    Joseph    male   1860-12-07        NA 1947-07-30      1\n3 Lyons1879   Lyons   Joseph    male   1879-09-15        NA 1939-04-07      1\n# ℹ 2 more variables: senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;\n\n\n앰퍼샌드 대신 쉼표를 사용해도 동일한 결과를 얻습니다.\n\naustralian_politicians |&gt;\n  filter(wasPrimeMinister == 1, firstName == \"Joseph\")\n\n# A tibble: 3 × 10\n  uniqueID    surname firstName gender birthDate  birthYear deathDate  member\n  &lt;chr&gt;       &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;  &lt;date&gt;         &lt;dbl&gt; &lt;date&gt;      &lt;dbl&gt;\n1 Chifley1885 Chifley Joseph    male   1885-09-22        NA 1951-06-13      1\n2 Cook1860    Cook    Joseph    male   1860-12-07        NA 1947-07-30      1\n3 Lyons1879   Lyons   Joseph    male   1879-09-15        NA 1939-04-07      1\n# ℹ 2 more variables: senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;\n\n\n마찬가지로, “or” 연산자 |를 사용하여 마일스 또는 루스라는 이름의 정치인을 찾을 수 있습니다.\n\naustralian_politicians |&gt;\n  filter(firstName == \"Myles\" | firstName == \"Ruth\")\n\n# A tibble: 3 × 10\n  uniqueID     surname  firstName gender birthDate  birthYear deathDate  member\n  &lt;chr&gt;        &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;  &lt;date&gt;         &lt;dbl&gt; &lt;date&gt;      &lt;dbl&gt;\n1 Coleman1931  Coleman  Ruth      female 1931-09-27        NA 2008-03-27      0\n2 Ferricks1875 Ferricks Myles     male   1875-11-12        NA 1932-08-20      0\n3 Webber1965   Webber   Ruth      female 1965-03-24        NA NA              0\n# ℹ 2 more variables: senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;\n\n\n결과를 파이프할 수도 있습니다. 예를 들어, filter()에서 select()로 파이프할 수 있습니다.\n\naustralian_politicians |&gt;\n  filter(firstName == \"Ruth\" | firstName == \"Myles\") |&gt;\n  select(firstName, surname)\n\n# A tibble: 3 × 2\n  firstName surname \n  &lt;chr&gt;     &lt;chr&gt;   \n1 Ruth      Coleman \n2 Myles     Ferricks\n3 Ruth      Webber  \n\n\n관심 있는 특정 행 번호를 알고 있다면 해당 행만 filter()할 수 있습니다. 예를 들어, 853번 행에 관심이 있다고 가정해 봅시다.\n\naustralian_politicians |&gt;\n  filter(row_number() == 853)\n\n# A tibble: 1 × 10\n  uniqueID     surname  firstName gender birthDate  birthYear deathDate member\n  &lt;chr&gt;        &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;  &lt;date&gt;         &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt;\n1 Jakobsen1947 Jakobsen Carolyn   female 1947-09-11        NA NA             1\n# ℹ 2 more variables: senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;\n\n\n이를 수행하는 전용 함수도 있습니다. 즉, slice()입니다.\n\naustralian_politicians |&gt;\n  slice(853)\n\n# A tibble: 1 × 10\n  uniqueID     surname  firstName gender birthDate  birthYear deathDate member\n  &lt;chr&gt;        &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;  &lt;date&gt;         &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt;\n1 Jakobsen1947 Jakobsen Carolyn   female 1947-09-11        NA NA             1\n# ℹ 2 more variables: senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;\n\n\n이것은 다소 난해해 보일 수 있지만, 부정을 사용하여 특정 행을 제거하거나 특정 행을 복제하려는 경우 특히 유용합니다. 예를 들어, 첫 번째 행을 제거할 수 있습니다.\n\naustralian_politicians |&gt;\n  slice(-1)\n\n# A tibble: 1,782 × 10\n   uniqueID    surname firstName gender birthDate  birthYear deathDate  member\n   &lt;chr&gt;       &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;  &lt;date&gt;         &lt;dbl&gt; &lt;date&gt;      &lt;dbl&gt;\n 1 Abbott1869  Abbott  Percy     male   1869-05-14        NA 1940-09-09      1\n 2 Abbott1877  Abbott  Macartney male   1877-07-03        NA 1960-12-30      0\n 3 Abbott1886  Abbott  Charles   male   1886-01-04        NA 1975-04-30      1\n 4 Abbott1891  Abbott  Joseph    male   1891-10-18        NA 1965-05-07      1\n 5 Abbott1957  Abbott  Anthony   male   1957-11-04        NA NA              1\n 6 Abel1939    Abel    John      male   1939-06-25        NA NA              1\n 7 Abetz1958   Abetz   Eric      male   1958-01-25        NA NA              0\n 8 Adams1943   Adams   Judith    female 1943-04-11        NA 2012-03-31      0\n 9 Adams1951   Adams   Dick      male   1951-04-29        NA NA              1\n10 Adamson1857 Adamson John      male   1857-02-18        NA 1922-05-02      0\n# ℹ 1,772 more rows\n# ℹ 2 more variables: senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;\n\n\n또한, 예를 들어, 처음 세 행만 유지할 수도 있습니다.\n\naustralian_politicians |&gt;\n  slice(1:3)\n\n# A tibble: 3 × 10\n  uniqueID   surname firstName gender birthDate  birthYear deathDate  member\n  &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;  &lt;date&gt;         &lt;dbl&gt; &lt;date&gt;      &lt;dbl&gt;\n1 Abbott1859 Abbott  Richard   male   NA              1859 1940-02-28      0\n2 Abbott1869 Abbott  Percy     male   1869-05-14        NA 1940-09-09      1\n3 Abbott1877 Abbott  Macartney male   1877-07-03        NA 1960-12-30      0\n# ℹ 2 more variables: senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;\n\n\n마지막으로, 처음 두 행을 복제할 수 있으며, 이는 현재 그룹 크기를 제공하는 n()을 활용합니다.\n\naustralian_politicians |&gt;\n  slice(1:2, 1:n())\n\n# A tibble: 1,785 × 10\n   uniqueID   surname firstName gender birthDate  birthYear deathDate  member\n   &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;  &lt;date&gt;         &lt;dbl&gt; &lt;date&gt;      &lt;dbl&gt;\n 1 Abbott1859 Abbott  Richard   male   NA              1859 1940-02-28      0\n 2 Abbott1869 Abbott  Percy     male   1869-05-14        NA 1940-09-09      1\n 3 Abbott1859 Abbott  Richard   male   NA              1859 1940-02-28      0\n 4 Abbott1869 Abbott  Percy     male   1869-05-14        NA 1940-09-09      1\n 5 Abbott1877 Abbott  Macartney male   1877-07-03        NA 1960-12-30      0\n 6 Abbott1886 Abbott  Charles   male   1886-01-04        NA 1975-04-30      1\n 7 Abbott1891 Abbott  Joseph    male   1891-10-18        NA 1965-05-07      1\n 8 Abbott1957 Abbott  Anthony   male   1957-11-04        NA NA              1\n 9 Abel1939   Abel    John      male   1939-06-25        NA NA              1\n10 Abetz1958  Abetz   Eric      male   1958-01-25        NA NA              0\n# ℹ 1,775 more rows\n# ℹ 2 more variables: senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;\n\n\n\n\nA.4.3 arrange()\n특정 열의 값을 기반으로 데이터셋의 순서를 변경하려면 arrange()를 사용합니다. 예를 들어, 정치인들을 생년월일순으로 정렬할 수 있습니다.\n\naustralian_politicians |&gt;\n  arrange(birthYear)\n\n# A tibble: 1,783 × 10\n   uniqueID    surname firstName gender birthDate birthYear deathDate  member\n   &lt;chr&gt;       &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;  &lt;date&gt;        &lt;dbl&gt; &lt;date&gt;      &lt;dbl&gt;\n 1 Edwards1842 Edwards Richard   male   NA             1842 1915-10-29      1\n 2 Sawers1844  Sawers  William   male   NA             1844 1916-05-19      1\n 3 Barker1846  Barker  Stephen   male   NA             1846 1924-06-21      0\n 4 Corser1852  Corser  Edward    male   NA             1852 1928-07-31      1\n 5 Lee1856     Lee     Henry     male   NA             1856 1927-08-12      1\n 6 Grant1857   Grant   John      male   NA             1857 1928-05-19      0\n 7 Abbott1859  Abbott  Richard   male   NA             1859 1940-02-28      0\n 8 Palmer1859  Palmer  Albert    male   NA             1859 1919-08-14      1\n 9 Riley1859   Riley   Edward    male   NA             1859 1943-07-21      1\n10 Kennedy1860 Kennedy Thomas    male   NA             1860 1929-02-16      1\n# ℹ 1,773 more rows\n# ℹ 2 more variables: senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;\n\n\ndesc()를 사용하여 arrange()를 수정하여 오름차순에서 내림차순으로 변경할 수 있습니다.\n\naustralian_politicians |&gt;\n  arrange(desc(birthYear))\n\n# A tibble: 1,783 × 10\n   uniqueID      surname firstName gender birthDate  birthYear deathDate  member\n   &lt;chr&gt;         &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;  &lt;date&gt;         &lt;dbl&gt; &lt;date&gt;      &lt;dbl&gt;\n 1 McBain1982    McBain  Kristy    female 1982-09-29      1982 NA              1\n 2 Cox1977       Cox     Dorinda   female NA              1977 NA              0\n 3 Thorpe1973    Thorpe  Lidia     female 1973-08-18      1973 NA              0\n 4 McLachlan1966 McLach… Andrew    male   1966-01-14      1966 NA              0\n 5 Wortley1959   Wortley Dana      female NA              1959 NA              0\n 6 Baker1903     Baker   Francis   male   NA              1903 1939-03-28      1\n 7 Clay1900      Clay    Lionel    male   NA              1900 1965-04-16      1\n 8 Breen1898     Breen   John      male   NA              1898 1966-02-05      1\n 9 Clasby1891    Clasby  John      male   NA              1891 1932-01-15      1\n10 Gander1888    Gander  Joseph    male   NA              1888 1954-11-22      1\n# ℹ 1,773 more rows\n# ℹ 2 more variables: senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;\n\n\n빼기 기호를 사용해도 동일한 결과를 얻을 수 있습니다.\n\naustralian_politicians |&gt;\n  arrange(-birthYear)\n\n# A tibble: 1,783 × 10\n   uniqueID      surname firstName gender birthDate  birthYear deathDate  member\n   &lt;chr&gt;         &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;  &lt;date&gt;         &lt;dbl&gt; &lt;date&gt;      &lt;dbl&gt;\n 1 McBain1982    McBain  Kristy    female 1982-09-29      1982 NA              1\n 2 Cox1977       Cox     Dorinda   female NA              1977 NA              0\n 3 Thorpe1973    Thorpe  Lidia     female 1973-08-18      1973 NA              0\n 4 McLachlan1966 McLach… Andrew    male   1966-01-14      1966 NA              0\n 5 Wortley1959   Wortley Dana      female NA              1959 NA              0\n 6 Baker1903     Baker   Francis   male   NA              1903 1939-03-28      1\n 7 Clay1900      Clay    Lionel    male   NA              1900 1965-04-16      1\n 8 Breen1898     Breen   John      male   NA              1898 1966-02-05      1\n 9 Clasby1891    Clasby  John      male   NA              1891 1932-01-15      1\n10 Gander1888    Gander  Joseph    male   NA              1888 1954-11-22      1\n# ℹ 1,773 more rows\n# ℹ 2 more variables: senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;\n\n\n그리고 하나 이상의 열을 기준으로 정렬할 수도 있습니다. 예를 들어, 두 정치인이 동일한 이름을 가지고 있다면, 생년월일을 기준으로도 정렬할 수 있습니다.\n\naustralian_politicians |&gt;\n  arrange(firstName, birthYear)\n\n# A tibble: 1,783 × 10\n   uniqueID      surname firstName gender birthDate  birthYear deathDate  member\n   &lt;chr&gt;         &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;  &lt;date&gt;         &lt;dbl&gt; &lt;date&gt;      &lt;dbl&gt;\n 1 Blain1894     Blain   Adair     male   1894-11-21        NA 1983-04-28      1\n 2 Armstrong1909 Armstr… Adam      male   1909-07-01        NA 1982-02-22      1\n 3 Bandt1972     Bandt   Adam      male   1972-03-11        NA NA              1\n 4 Dein1889      Dein    Adam      male   1889-03-04        NA 1969-05-09      1\n 5 Ridgeway1962  Ridgew… Aden      male   1962-09-18        NA NA              0\n 6 Bennett1933   Bennett Adrian    male   1933-01-21        NA 2006-05-09      1\n 7 Gibson1935    Gibson  Adrian    male   1935-11-03        NA 2015-04-30      1\n 8 Wynne1850     Wynne   Agar      male   1850-01-15        NA 1934-05-12      1\n 9 Robertson1882 Robert… Agnes     female 1882-07-31        NA 1968-01-29      0\n10 Bird1906      Bird    Alan      male   1906-09-28        NA 1962-07-21      1\n# ℹ 1,773 more rows\n# ℹ 2 more variables: senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;\n\n\n두 개의 arrange() 인스턴스 사이에 파이프를 사용하여 동일한 결과를 얻을 수 있습니다.\n\naustralian_politicians |&gt;\n  arrange(birthYear) |&gt;\n  arrange(firstName)\n\n# A tibble: 1,783 × 10\n   uniqueID      surname firstName gender birthDate  birthYear deathDate  member\n   &lt;chr&gt;         &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;  &lt;date&gt;         &lt;dbl&gt; &lt;date&gt;      &lt;dbl&gt;\n 1 Blain1894     Blain   Adair     male   1894-11-21        NA 1983-04-28      1\n 2 Armstrong1909 Armstr… Adam      male   1909-07-01        NA 1982-02-22      1\n 3 Bandt1972     Bandt   Adam      male   1972-03-11        NA NA              1\n 4 Dein1889      Dein    Adam      male   1889-03-04        NA 1969-05-09      1\n 5 Ridgeway1962  Ridgew… Aden      male   1962-09-18        NA NA              0\n 6 Bennett1933   Bennett Adrian    male   1933-01-21        NA 2006-05-09      1\n 7 Gibson1935    Gibson  Adrian    male   1935-11-03        NA 2015-04-30      1\n 8 Wynne1850     Wynne   Agar      male   1850-01-15        NA 1934-05-12      1\n 9 Robertson1882 Robert… Agnes     female 1882-07-31        NA 1968-01-29      0\n10 Bird1906      Bird    Alan      male   1906-09-28        NA 1962-07-21      1\n# ℹ 1,773 more rows\n# ℹ 2 more variables: senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;\n\n\narrange()를 사용할 때는 우선순위를 명확히 해야 합니다. 예를 들어, 생년월일 다음에 이름을 기준으로 변경하면 다른 정렬이 됩니다.\n\naustralian_politicians |&gt;\n  arrange(birthYear, firstName)\n\n# A tibble: 1,783 × 10\n   uniqueID    surname firstName gender birthDate birthYear deathDate  member\n   &lt;chr&gt;       &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;  &lt;date&gt;        &lt;dbl&gt; &lt;date&gt;      &lt;dbl&gt;\n 1 Edwards1842 Edwards Richard   male   NA             1842 1915-10-29      1\n 2 Sawers1844  Sawers  William   male   NA             1844 1916-05-19      1\n 3 Barker1846  Barker  Stephen   male   NA             1846 1924-06-21      0\n 4 Corser1852  Corser  Edward    male   NA             1852 1928-07-31      1\n 5 Lee1856     Lee     Henry     male   NA             1856 1927-08-12      1\n 6 Grant1857   Grant   John      male   NA             1857 1928-05-19      0\n 7 Palmer1859  Palmer  Albert    male   NA             1859 1919-08-14      1\n 8 Riley1859   Riley   Edward    male   NA             1859 1943-07-21      1\n 9 Abbott1859  Abbott  Richard   male   NA             1859 1940-02-28      0\n10 Kennedy1860 Kennedy Thomas    male   NA             1860 1929-02-16      1\n# ℹ 1,773 more rows\n# ℹ 2 more variables: senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;\n\n\n다양한 열을 기준으로 정렬하는 좋은 방법은 across()를 사용하는 것입니다. 이는 select()와 관련하여 언급된 starts_with()와 같은 “선택 도우미”를 사용할 수 있게 해줍니다.\n\naustralian_politicians |&gt;\n  arrange(across(c(firstName, birthYear)))\n\n# A tibble: 1,783 × 10\n   uniqueID      surname firstName gender birthDate  birthYear deathDate  member\n   &lt;chr&gt;         &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;  &lt;date&gt;         &lt;dbl&gt; &lt;date&gt;      &lt;dbl&gt;\n 1 Blain1894     Blain   Adair     male   1894-11-21        NA 1983-04-28      1\n 2 Armstrong1909 Armstr… Adam      male   1909-07-01        NA 1982-02-22      1\n 3 Bandt1972     Bandt   Adam      male   1972-03-11        NA NA              1\n 4 Dein1889      Dein    Adam      male   1889-03-04        NA 1969-05-09      1\n 5 Ridgeway1962  Ridgew… Aden      male   1962-09-18        NA NA              0\n 6 Bennett1933   Bennett Adrian    male   1933-01-21        NA 2006-05-09      1\n 7 Gibson1935    Gibson  Adrian    male   1935-11-03        NA 2015-04-30      1\n 8 Wynne1850     Wynne   Agar      male   1850-01-15        NA 1934-05-12      1\n 9 Robertson1882 Robert… Agnes     female 1882-07-31        NA 1968-01-29      0\n10 Bird1906      Bird    Alan      male   1906-09-28        NA 1962-07-21      1\n# ℹ 1,773 more rows\n# ℹ 2 more variables: senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;\n\naustralian_politicians |&gt;\n  arrange(across(starts_with(\"birth\")))\n\n# A tibble: 1,783 × 10\n   uniqueID     surname  firstName gender birthDate  birthYear deathDate  member\n   &lt;chr&gt;        &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;  &lt;date&gt;         &lt;dbl&gt; &lt;date&gt;      &lt;dbl&gt;\n 1 Braddon1829  Braddon  Edward    male   1829-06-11        NA 1904-02-02      1\n 2 Ferguson1830 Ferguson John      male   1830-03-15        NA 1906-03-30      0\n 3 Zeal1830     Zeal     William   male   1830-12-05        NA 1912-03-11      0\n 4 Fraser1832   Fraser   Simon     male   1832-08-21        NA 1919-07-30      0\n 5 Groom1833    Groom    William   male   1833-03-09        NA 1901-08-08      1\n 6 Sargood1834  Sargood  Frederick male   1834-05-30        NA 1903-01-02      0\n 7 Fysh1835     Fysh     Philip    male   1835-03-01        NA 1919-12-20      1\n 8 Playford1837 Playford Thomas    male   1837-11-26        NA 1915-04-19      0\n 9 Solomon1839  Solomon  Elias     male   1839-09-02        NA 1909-05-23      1\n10 McLean1840   McLean   Allan     male   1840-02-03        NA 1911-07-13      1\n# ℹ 1,773 more rows\n# ℹ 2 more variables: senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;\n\n\n\n\nA.4.4 mutate()\n새 열을 만들고 싶을 때 mutate()를 사용합니다. 예를 들어, 어떤 사람이 의원이자 상원 의원인 경우 1이고 그렇지 않으면 0인 새 열을 만들고 싶을 수 있습니다. 즉, 새 열은 상원과 하원 모두에서 봉사한 정치인을 나타냅니다.\n\naustralian_politicians &lt;-\n  australian_politicians |&gt;\n  mutate(was_both = if_else(member == 1 & senator == 1, 1, 0))\n\naustralian_politicians |&gt;\n  select(member, senator, was_both)\n\n# A tibble: 1,783 × 3\n   member senator was_both\n    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n 1      0       1        0\n 2      1       1        1\n 3      0       1        0\n 4      1       0        0\n 5      1       0        0\n 6      1       0        0\n 7      1       0        0\n 8      0       1        0\n 9      0       1        0\n10      1       0        0\n# ℹ 1,773 more rows\n\n\n덧셈과 뺄셈과 같은 수학을 mutate()와 함께 사용할 수 있습니다. 예를 들어, 정치인들의 2022년 나이(또는 나이였을 것)를 계산할 수 있습니다.\n\nlibrary(lubridate)\n\naustralian_politicians &lt;-\n  australian_politicians |&gt;\n  mutate(age = 2022 - year(birthDate))\n\naustralian_politicians |&gt;\n  select(uniqueID, age)\n\n# A tibble: 1,783 × 2\n   uniqueID     age\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 Abbott1859    NA\n 2 Abbott1869   153\n 3 Abbott1877   145\n 4 Abbott1886   136\n 5 Abbott1891   131\n 6 Abbott1957    65\n 7 Abel1939      83\n 8 Abetz1958     64\n 9 Adams1943     79\n10 Adams1951     71\n# ℹ 1,773 more rows\n\n\n새 열을 구성할 때 특히 유용한 다양한 함수가 있습니다. 여기에는 자연 로그를 계산하는 log(), 값을 한 행 위로 가져오는 lead(), 값을 한 행 아래로 내리는 lag(), 그리고 열의 누적 합계를 생성하는 cumsum()이 포함됩니다.\n\naustralian_politicians |&gt;\n  select(uniqueID, age) |&gt;\n  mutate(log_age = log(age))\n\n# A tibble: 1,783 × 3\n   uniqueID     age log_age\n   &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n 1 Abbott1859    NA   NA   \n 2 Abbott1869   153    5.03\n 3 Abbott1877   145    4.98\n 4 Abbott1886   136    4.91\n 5 Abbott1891   131    4.88\n 6 Abbott1957    65    4.17\n 7 Abel1939      83    4.42\n 8 Abetz1958     64    4.16\n 9 Adams1943     79    4.37\n10 Adams1951     71    4.26\n# ℹ 1,773 more rows\n\naustralian_politicians |&gt;\n  select(uniqueID, age) |&gt;\n  mutate(lead_age = lead(age))\n\n# A tibble: 1,783 × 3\n   uniqueID     age lead_age\n   &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n 1 Abbott1859    NA      153\n 2 Abbott1869   153      145\n 3 Abbott1877   145      136\n 4 Abbott1886   136      131\n 5 Abbott1891   131       65\n 6 Abbott1957    65       83\n 7 Abel1939      83       64\n 8 Abetz1958     64       79\n 9 Adams1943     79       71\n10 Adams1951     71      165\n# ℹ 1,773 more rows\n\naustralian_politicians |&gt;\n  select(uniqueID, age) |&gt;\n  mutate(lag_age = lag(age))\n\n# A tibble: 1,783 × 3\n   uniqueID     age lag_age\n   &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n 1 Abbott1859    NA      NA\n 2 Abbott1869   153      NA\n 3 Abbott1877   145     153\n 4 Abbott1886   136     145\n 5 Abbott1891   131     136\n 6 Abbott1957    65     131\n 7 Abel1939      83      65\n 8 Abetz1958     64      83\n 9 Adams1943     79      64\n10 Adams1951     71      79\n# ℹ 1,773 more rows\n\naustralian_politicians |&gt;\n  select(uniqueID, age) |&gt;\n  drop_na(age) |&gt;\n  mutate(cumulative_age = cumsum(age))\n\n# A tibble: 1,718 × 3\n   uniqueID      age cumulative_age\n   &lt;chr&gt;       &lt;dbl&gt;          &lt;dbl&gt;\n 1 Abbott1869    153            153\n 2 Abbott1877    145            298\n 3 Abbott1886    136            434\n 4 Abbott1891    131            565\n 5 Abbott1957     65            630\n 6 Abel1939       83            713\n 7 Abetz1958      64            777\n 8 Adams1943      79            856\n 9 Adams1951      71            927\n10 Adamson1857   165           1092\n# ℹ 1,708 more rows\n\n\n이전 예시에서와 마찬가지로 mutate()를 across()와 함께 사용할 수도 있습니다. 여기에는 선택 도우미의 잠재적 사용이 포함됩니다. 예를 들어, 이름과 성의 문자 수를 동시에 셀 수 있습니다.\n\naustralian_politicians |&gt;\n  mutate(across(c(firstName, surname), str_count)) |&gt;\n  select(uniqueID, firstName, surname)\n\n# A tibble: 1,783 × 3\n   uniqueID   firstName surname\n   &lt;chr&gt;          &lt;int&gt;   &lt;int&gt;\n 1 Abbott1859         7       6\n 2 Abbott1869         5       6\n 3 Abbott1877         9       6\n 4 Abbott1886         7       6\n 5 Abbott1891         6       6\n 6 Abbott1957         7       6\n 7 Abel1939           4       4\n 8 Abetz1958          4       5\n 9 Adams1943          6       5\n10 Adams1951          4       5\n# ℹ 1,773 more rows\n\n\n마지막으로, 두 개 이상의 조건문(첫 번째 mutate() 예시의 if_else()와 대조적으로)을 기반으로 새 열을 만들어야 할 때 case_when()을 사용합니다. 예를 들어, 몇 년이 있고 그것들을 10년 단위로 그룹화하고 싶을 수 있습니다.\n```{ library(lubridate)\naustralian_politicians |&gt; mutate( year_of_birth = year(birthDate), decade_of_birth = case_when( year_of_birth &lt;= 1929 ~ “1929년 이전”, year_of_birth &lt;= 1939 ~ “1930년대”, year_of_birth &lt;= 1949 ~ “1940년대”, year_of_birth &lt;= 1959 ~ “1950년대”, year_of_birth &lt;= 1969 ~ “1960년대”, year_of_birth &lt;= 1979 ~ “1970년대”, year_of_birth &lt;= 1989 ~ “1980년대”, year_of_birth &lt;= 1999 ~ “1990년대”, TRUE ~ “알 수 없음 또는 오류” ) ) |&gt; select(uniqueID, year_of_birth, decade_of_birth)\n\n일련의 중첩된 `if_else()` 문으로 이를 달성할 수 있지만, `case_when()`이 더 명확합니다. 사례는 순서대로 평가되며, 일치하는 것이 있으면 `case_when()`은 나머지 사례로 계속 진행하지 않습니다. 코드가 그곳에 도달할 경우 우리가 알고 싶을 수 있는 잠재적인 문제를 알리는 포괄적인 것을 끝에 두는 것이 유용할 수 있습니다.\n\n\n### `summarise()`\n\n새롭고 압축된 요약 변수를 만들고 싶을 때 `summarise()`를 사용합니다. 예를 들어, 어떤 열의 최소값, 평균값, 최대값을 알고 싶을 수 있습니다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\naustralian_politicians |&gt;\n  summarise(\n    youngest = min(age, na.rm = TRUE),\n    oldest = max(age, na.rm = TRUE),\n    average = mean(age, na.rm = TRUE)\n  )\n\n# A tibble: 1 × 3\n  youngest oldest average\n     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1       28    193    101.\n\n:::\n여담으로, summarise()와 summarize()는 동일하며 둘 중 하나를 사용할 수 있습니다. 이 책에서는 summarise()를 사용합니다.\n\naustralian_politicians |&gt;\n  summarize(\n    youngest = min(age, na.rm = TRUE),\n    oldest = max(age, na.rm = TRUE),\n    average = mean(age, na.rm = TRUE)\n  )\n\n# A tibble: 1 × 3\n  youngest oldest average\n     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1       28    193    101.\n\n\n기본적으로 summarise()는 전체 데이터셋에 대해 한 행의 출력을 제공합니다. 예를 들어, 이전 예시에서 우리는 모든 정치인에 대한 최연소, 최고령 및 평균을 찾았습니다. 그러나 함수 내에서 .by를 사용하여 데이터셋에 더 많은 그룹을 만들 수 있습니다. 그룹을 기반으로 많은 함수를 사용할 수 있지만, summarise() 함수는 .by와 함께 특히 강력합니다. 예를 들어, 성별로 그룹화한 다음 연령 기반 요약 통계를 얻을 수 있습니다.\n\naustralian_politicians |&gt;\n  summarise(\n    youngest = min(age, na.rm = TRUE),\n    oldest = max(age, na.rm = TRUE),\n    average = mean(age, na.rm = TRUE),\n    .by = gender\n  )\n\n# A tibble: 2 × 4\n  gender youngest oldest average\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 male         28    193   106. \n2 female       32    140    66.0\n\n\n마찬가지로, 성별별 사망 시 최연소, 최고령 및 평균 연령을 살펴볼 수 있습니다.\n\naustralian_politicians |&gt;\n  mutate(days_lived = deathDate - birthDate) |&gt;\n  drop_na(days_lived) |&gt;\n  summarise(\n    min_days = min(days_lived),\n    mean_days = mean(days_lived) |&gt; round(),\n    max_days = max(days_lived),\n    .by = gender\n  )\n\n# A tibble: 2 × 4\n  gender min_days   mean_days  max_days  \n  &lt;chr&gt;  &lt;drtn&gt;     &lt;drtn&gt;     &lt;drtn&gt;    \n1 male   12380 days 27376 days 36416 days\n2 female 14856 days 28857 days 35560 days\n\n\n따라서 여성 의원들이 남성 의원들보다 평균적으로 약간 더 오래 살았다는 것을 알 수 있습니다.\n하나 이상의 그룹을 기준으로 .by를 사용할 수 있습니다. 예를 들어, 성별 및 하원 또는 상원 의원 여부에 따라 평균 생존 일수를 살펴볼 수 있습니다.\n\naustralian_politicians |&gt;\n  mutate(days_lived = deathDate - birthDate) |&gt;\n  drop_na(days_lived) |&gt;\n  summarise(\n    min_days = min(days_lived),\n    mean_days = mean(days_lived) |&gt; round(),\n    max_days = max(days_lived),\n    .by = c(gender, member)\n  )\n\n# A tibble: 4 × 5\n  gender member min_days   mean_days  max_days  \n  &lt;chr&gt;   &lt;dbl&gt; &lt;drtn&gt;     &lt;drtn&gt;     &lt;drtn&gt;    \n1 male        1 12380 days 27496 days 36328 days\n2 male        0 13619 days 27133 days 36416 days\n3 female      0 21746 days 29517 days 35560 days\n4 female      1 14856 days 27538 days 33442 days\n\n\ncount()를 사용하여 그룹별 개수를 생성할 수 있습니다. 예를 들어, 성별별 정치인 수입니다.\n\naustralian_politicians |&gt;\n  count(gender)\n\n# A tibble: 2 × 2\n  gender     n\n  &lt;chr&gt;  &lt;int&gt;\n1 female   240\n2 male    1543\n\n\ncount() 외에도 비율을 계산할 수 있습니다.\n\naustralian_politicians |&gt;\n  count(gender) |&gt;\n  mutate(proportion = n / (sum(n)))\n\n# A tibble: 2 × 3\n  gender     n proportion\n  &lt;chr&gt;  &lt;int&gt;      &lt;dbl&gt;\n1 female   240      0.135\n2 male    1543      0.865\n\n\ncount()를 사용하는 것은 summarise() 내에서 n()와 함께 .by를 사용하는 것과 본질적으로 동일하며, 그렇게 하면 동일한 결과를 얻습니다.\n\naustralian_politicians |&gt;\n  summarise(n = n(),\n            .by = gender)\n\n# A tibble: 2 × 2\n  gender     n\n  &lt;chr&gt;  &lt;int&gt;\n1 male    1543\n2 female   240\n\n\n그리고 mutate()와 유사하게 도움이 되는 함수가 있습니다. 즉, add_count()입니다. 차이점은 숫자가 새 열에 추가된다는 것입니다.\n\naustralian_politicians |&gt;\n  add_count(gender) |&gt;\n  select(uniqueID, gender, n)\n\n# A tibble: 1,783 × 3\n   uniqueID   gender     n\n   &lt;chr&gt;      &lt;chr&gt;  &lt;int&gt;\n 1 Abbott1859 male    1543\n 2 Abbott1869 male    1543\n 3 Abbott1877 male    1543\n 4 Abbott1886 male    1543\n 5 Abbott1891 male    1543\n 6 Abbott1957 male    1543\n 7 Abel1939   male    1543\n 8 Abetz1958  male    1543\n 9 Adams1943  female   240\n10 Adams1951  male    1543\n# ℹ 1,773 more rows",
    "crumbs": [
      "부록",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R 필수 사항</span>"
    ]
  },
  {
    "objectID": "20-r_essentials_ko.html#base-r",
    "href": "20-r_essentials_ko.html#base-r",
    "title": "Online Appendix A — R 필수 사항",
    "section": "A.5 Base R",
    "text": "A.5 Base R\ntidyverse는 데이터 과학을 돕기 위해 비교적 최근에 설립되었지만, R은 그 훨씬 이전부터 존재했습니다. R에는 프로그래밍 및 통계학자의 핵심 요구 사항을 중심으로 구축된 많은 기능이 있습니다.\n특히 다음을 다룰 것입니다.\n\nclass().\n데이터 시뮬레이션.\nfunction(), for(), 그리고 apply().\n\n이러한 기능은 R에 포함되어 있으므로 추가 패키지를 설치하거나 로드할 필요가 없습니다.\n\nA.5.1 class()\n일상적인 사용에서 “a, b, c, …”는 글자이고 “1, 2, 3,…”은 숫자입니다. 그리고 우리는 글자와 숫자를 다르게 사용합니다. 예를 들어, 글자를 더하거나 빼지 않습니다. 마찬가지로, R은 다른 클래스의 내용을 구별하고 각 클래스가 가지는 속성, 즉 “어떻게 동작하고 다른 유형의 객체와 어떻게 관련되는지”를 정의하는 방법이 필요합니다(Wickham 2019).\n클래스에는 계층 구조가 있습니다. 예를 들어, 우리는 “인간”이며, 그 자체로 “동물”입니다. 모든 “인간”은 “동물”이지만, 모든 “동물”이 “인간”인 것은 아닙니다. 마찬가지로, 모든 정수는 숫자이지만, 모든 숫자가 정수는 아닙니다. R에서 객체의 클래스를 class()로 확인할 수 있습니다.\n\na_number &lt;- 8\nclass(a_number)\n\n[1] \"numeric\"\n\na_letter &lt;- \"a\"\nclass(a_letter)\n\n[1] \"character\"\n\n\n여기서 다루는 클래스는 “numeric”, “character”, “factor”, “date”, “data.frame”입니다.\n가장 먼저 알아야 할 것은 개구리가 왕자가 될 수 있는 것처럼, R에서 객체의 클래스를 변경할 수 있다는 것입니다. 이를 “캐스팅”이라고 합니다. 예를 들어, “numeric”으로 시작하여 as.character()로 “character”로 변경한 다음, as.factor()로 “factor”로 변경할 수 있습니다. 그러나 as.Date()로 “date”로 만들려고 하면 오류가 발생합니다. 왜냐하면 모든 숫자가 날짜가 되는 데 필요한 속성을 가지고 있지 않기 때문입니다.\n\na_number &lt;- 8\na_number\n\n[1] 8\n\nclass(a_number)\n\n[1] \"numeric\"\n\na_number &lt;- as.character(a_number)\na_number\n\n[1] \"8\"\n\nclass(a_number)\n\n[1] \"character\"\n\na_number &lt;- as.factor(a_number)\na_number\n\n[1] 8\nLevels: 8\n\nclass(a_number)\n\n[1] \"factor\"\n\n\n“numeric” 및 “character” 클래스와 비교하여 “factor” 클래스는 덜 익숙할 수 있습니다. “factor”는 특정 값만 취할 수 있는 범주형 데이터에 사용됩니다(Wickham 2019). 예를 들어, “factor” 변수의 일반적인 사용은 “낮” 또는 “밤”과 같은 이진 변수입니다. 또한 “18-29”, “30-44”, “45-60”, “60+”과 같은 연령대(종종 “numeric”인 연령과 대조적으로)에도 자주 사용되며, 때로는 교육 수준: “고등학교 미만”, “고등학교”, “대학”, “학부 학위”, “대학원 학위”에도 사용됩니다. levels()를 사용하여 “factor”에 허용되는 수준을 찾을 수 있습니다.\n\nage_groups &lt;- factor(\n  c(\"18-29\", \"30-44\", \"45-60\", \"60+\")\n)\nage_groups\n\n[1] 18-29 30-44 45-60 60+  \nLevels: 18-29 30-44 45-60 60+\n\nclass(age_groups)\n\n[1] \"factor\"\n\nlevels(age_groups)\n\n[1] \"18-29\" \"30-44\" \"45-60\" \"60+\"  \n\n\n날짜는 특히 까다로운 클래스이며 빠르게 복잡해집니다. 그럼에도 불구하고, 기초적인 수준에서 as.Date()를 사용하여 “날짜”처럼 보이는 문자를 실제 “날짜”로 변환할 수 있습니다. 이를 통해 “character”로는 할 수 없는 덧셈과 뺄셈을 수행할 수 있습니다.\n\nlooks_like_a_date_but_is_not &lt;- \"2022-01-01\"\nlooks_like_a_date_but_is_not\n\n[1] \"2022-01-01\"\n\nclass(looks_like_a_date_but_is_not)\n\n[1] \"character\"\n\nis_a_date &lt;- as.Date(looks_like_a_date_but_is_not)\nis_a_date\n\n[1] \"2022-01-01\"\n\nclass(is_a_date)\n\n[1] \"Date\"\n\nis_a_date + 3\n\n[1] \"2022-01-04\"\n\n\n여기서 다루는 마지막 클래스는 “data.frame”입니다. 이것은 스프레드시트처럼 보이며, 우리가 분석할 데이터를 저장하는 데 일반적으로 사용됩니다. 공식적으로 “데이터 프레임은 길이가 같은 벡터의 목록”입니다(Wickham 2019). 열 이름과 행 이름을 가질 것이며, colnames()와 rownames()를 사용하여 볼 수 있지만, 종종 행 이름은 숫자일 뿐입니다.\n이를 설명하기 위해 AER(Kleiber 와/과 Zeileis 2008)의 “ResumeNames” 데이터셋을 사용합니다. 이 패키지는 CRAN의 다른 패키지와 동일한 방식으로 설치할 수 있습니다. 이 데이터셋은 이력서 내용, 특히 이력서에 사용된 이름에 대한 교차 단면 데이터와 4,870개의 가상 이력서에 대한 콜백 여부 정보를 포함합니다. 이 데이터셋은 (bertrand2004emily가?) 보스턴과 시카고의 구인 광고에 가상 이력서를 보냈는데, 이력서에 “매우 아프리카계 미국인처럼 들리는 이름” 또는 “매우 백인처럼 들리는 이름”이 할당되었는지 여부에 따라 달라졌습니다. 그들은 “백인 이름이 인터뷰 콜백을 50% 더 많이 받는다”는 상당한 차별을 발견했습니다. (hangartner2021monitoring은?) 온라인 스위스 플랫폼을 사용하여 이를 일반화하고, 이민자와 소수 민족 그룹은 채용 담당자로부터 연락을 덜 받고, 직업이 남성 지배적인 경우 여성도 마찬가지라는 것을 발견했습니다.\n\ninstall.packages(\"AER\")\n\n\nlibrary(AER)\ndata(\"ResumeNames\", package = \"AER\")\n\n\nResumeNames |&gt;\n  head()\n\n     name gender ethnicity quality call    city jobs experience honors\n1 Allison female      cauc     low   no chicago    2          6     no\n2 Kristen female      cauc    high   no chicago    3          6     no\n3 Lakisha female      afam     low   no chicago    1          6     no\n4 Latonya female      afam    high   no chicago    4          6     no\n5  Carrie female      cauc    high   no chicago    3         22     no\n6     Jay   male      cauc     low   no chicago    2          6    yes\n  volunteer military holes school email computer special college minimum equal\n1        no       no   yes     no    no      yes      no     yes       5   yes\n2       yes      yes    no    yes   yes      yes      no      no       5   yes\n3        no       no    no    yes    no      yes      no     yes       5   yes\n4       yes       no   yes     no   yes      yes     yes      no       5   yes\n5        no       no    no    yes   yes      yes      no      no    some   yes\n6        no       no    no     no    no       no     yes     yes    none   yes\n      wanted requirements reqexp reqcomm reqeduc reqcomp reqorg\n1 supervisor          yes    yes      no      no     yes     no\n2 supervisor          yes    yes      no      no     yes     no\n3 supervisor          yes    yes      no      no     yes     no\n4 supervisor          yes    yes      no      no     yes     no\n5  secretary          yes    yes      no      no     yes    yes\n6      other           no     no      no      no      no     no\n                          industry\n1                    manufacturing\n2                    manufacturing\n3                    manufacturing\n4                    manufacturing\n5 health/education/social services\n6                            trade\n\nclass(ResumeNames)\n\n[1] \"data.frame\"\n\ncolnames(ResumeNames)\n\n [1] \"name\"         \"gender\"       \"ethnicity\"    \"quality\"      \"call\"        \n [6] \"city\"         \"jobs\"         \"experience\"   \"honors\"       \"volunteer\"   \n[11] \"military\"     \"holes\"        \"school\"       \"email\"        \"computer\"    \n[16] \"special\"      \"college\"      \"minimum\"      \"equal\"        \"wanted\"      \n[21] \"requirements\" \"reqexp\"       \"reqcomm\"      \"reqeduc\"      \"reqcomp\"     \n[26] \"reqorg\"       \"industry\"    \n\n\n열 이름으로 지정하여 데이터 프레임을 구성하는 벡터, 즉 열의 클래스를 검사할 수 있습니다.\n\nclass(ResumeNames$name)\n\n[1] \"factor\"\n\nclass(ResumeNames$jobs)\n\n[1] \"integer\"\n\n\n때로는 많은 열의 클래스를 한 번에 변경할 수 있는 것이 도움이 됩니다. mutate()와 across()를 사용하여 이를 수행할 수 있습니다.\n\nclass(ResumeNames$name)\n\n[1] \"factor\"\n\nclass(ResumeNames$gender)\n\n[1] \"factor\"\n\nclass(ResumeNames$ethnicity)\n\n[1] \"factor\"\n\nResumeNames &lt;- ResumeNames |&gt;\n  mutate(across(c(name, gender, ethnicity), as.character)) |&gt;\n  head()\n\nclass(ResumeNames$name)\n\n[1] \"character\"\n\nclass(ResumeNames$gender)\n\n[1] \"character\"\n\nclass(ResumeNames$ethnicity)\n\n[1] \"character\"\n\n\n코드가 실행되지 않는 많은 방법이 있지만, 클래스 문제가 항상 가장 먼저 확인해야 할 사항 중 하나입니다. 일반적인 문제는 “character” 또는 “numeric”이어야 한다고 생각하는 변수가 실제로는 “factor”인 경우입니다. 그리고 “numeric”이어야 한다고 생각하는 변수가 실제로는 “character”인 경우입니다.\n마지막으로, 벡터의 클래스는 내용의 클래스라는 점을 지적할 가치가 있습니다. Python 및 다른 언어에서 벡터와 유사한 데이터 구조는 “list”입니다. “list”는 자체 클래스이며, “list”의 객체는 자체 클래스를 가집니다(예를 들어, [\"a\", 1]은 “string” 및 “int” 클래스의 항목을 가진 “list” 클래스의 객체입니다). R을 다른 언어에서 온 경우 벡터가 자체 클래스가 아니라는 것을 보는 것이 직관적이지 않을 수 있습니다.\n\n\nA.5.2 데이터 시뮬레이션\n데이터 시뮬레이션은 데이터로 믿을 수 있는 이야기를 하는 데 핵심적인 기술입니다. 데이터를 시뮬레이션하려면 통계 분포 및 기타 컬렉션에서 무작위로 표본을 추출할 수 있어야 합니다. R에는 이를 더 쉽게 만드는 다양한 함수가 있습니다. 즉, 정규 분포 rnorm(), 균일 분포 runif(), 포아송 분포 rpois(), 이항 분포 rbinom() 등입니다. 항목 컬렉션에서 무작위로 표본을 추출하려면 sample()을 사용할 수 있습니다.\n무작위성을 다룰 때, 재현성의 필요성은 역설적으로 무작위성이 반복 가능해야 한다는 것을 중요하게 만듭니다. 즉, 다른 사람이 우리가 추출하는 무작위 숫자를 추출할 수 있어야 합니다. set.seed()를 사용하여 무작위 추출에 대한 시드를 설정하여 이를 수행합니다.\n표준 정규 분포에서 관측치를 얻고 그것들을 데이터 프레임에 넣을 수 있습니다.\n\nset.seed(853)\n\nnumber_of_observations &lt;- 5\n\nsimulated_data &lt;-\n  data.frame(\n    person = c(1:number_of_observations),\n    std_normal_observations = rnorm(\n      n = number_of_observations,\n      mean = 0,\n      sd = 1\n    )\n  )\n\nsimulated_data\n\n  person std_normal_observations\n1      1             -0.35980342\n2      2             -0.04064753\n3      3             -1.78216227\n4      4             -1.12242282\n5      5             -1.00278400\n\n\ncbind()를 사용하여 원본 데이터셋의 열과 새 데이터셋의 열을 함께 가져와 균일, 포아송, 이항 분포에서 표본을 추가할 수 있습니다.\n\nsimulated_data &lt;-\n  simulated_data |&gt;\n  cbind() |&gt;\n  data.frame(\n    uniform_observations =\n      runif(n = number_of_observations, min = 0, max = 10),\n    poisson_observations =\n      rpois(n = number_of_observations, lambda = 100),\n    binomial_observations =\n      rbinom(n = number_of_observations, size = 2, prob = 0.5)\n  )\n\nsimulated_data\n\n  person std_normal_observations uniform_observations poisson_observations\n1      1             -0.35980342            9.6219155                   81\n2      2             -0.04064753            7.2269016                   91\n3      3             -1.78216227            0.8252921                   84\n4      4             -1.12242282            1.0379810                  100\n5      5             -1.00278400            3.0942004                   97\n  binomial_observations\n1                     2\n2                     1\n3                     1\n4                     1\n5                     1\n\n\n마지막으로, sample()을 사용하여 각 관측치에 좋아하는 색상을 추가할 것입니다.\n\nsimulated_data &lt;-\n  data.frame(\n    favorite_color = sample(\n      x = c(\"파란색\", \"흰색\"),\n      size = number_of_observations,\n      replace = TRUE\n    )\n  ) |&gt;\n  cbind(simulated_data)\n\nsimulated_data\n\n  favorite_color person std_normal_observations uniform_observations\n1         파란색      1             -0.35980342            9.6219155\n2         파란색      2             -0.04064753            7.2269016\n3         파란색      3             -1.78216227            0.8252921\n4           흰색      4             -1.12242282            1.0379810\n5         파란색      5             -1.00278400            3.0942004\n  poisson_observations binomial_observations\n1                   81                     2\n2                   91                     1\n3                   84                     1\n4                  100                     1\n5                   97                     1\n\n\n“replace” 옵션을 “TRUE”로 설정한 이유는 두 항목 중에서만 선택하지만, 선택할 때마다 둘 중 하나를 선택할 가능성이 있기 때문입니다. 시뮬레이션에 따라 “replace”를 “TRUE” 또는 “FALSE”로 설정해야 할지 생각해야 할 수 있습니다. sample()의 또 다른 유용한 선택적 인수는 각 항목이 추출될 확률을 조정하는 것입니다. 기본값은 모든 옵션이 동일하게 가능하지만, “prob”를 사용하여 원하는 경우 특정 확률을 지정할 수 있습니다. 함수와 마찬가지로, 도움말 파일에서 더 많은 정보를 찾을 수 있습니다. 예를 들어 ?sample.\n\n\nA.5.3 function(), for(), 그리고 apply()\nR은 “함수형 프로그래밍 언어”입니다(Wickham 2019). 이는 우리가 특정 작업을 수행하는 코드 모음인 함수를 기본적으로 작성하고, 사용하고, 구성한다는 것을 의미합니다.\nR에는 다른 사람들이 작성한 많은 함수가 있으며, 우리는 그것들을 사용할 수 있습니다. 우리가 수행해야 할 거의 모든 일반적인 통계 또는 데이터 과학 작업은 이미 다른 사람이 작성하여 base R 설치 또는 패키지의 일부로 우리에게 제공된 함수를 가지고 있을 가능성이 높습니다. 그러나 때때로, 특히 더 구체적인 작업을 위해 우리 자신의 함수를 작성해야 할 것입니다. function()을 사용하여 함수를 정의한 다음 이름을 할당합니다. 함수에 일부 입력과 출력을 포함해야 할 것입니다. 입력은 괄호 안에 지정됩니다. 함수가 수행할 특정 작업은 중괄호 안에 들어갑니다.\n\nprint_names &lt;- function(some_names) {\n  print(some_names)\n}\n\nprint_names(c(\"로한\", \"모니카\"))\n\n[1] \"로한\"   \"모니카\"\n\n\n함수를 사용하는 사람이 입력을 제공하지 않는 경우를 대비하여 입력에 대한 기본값을 지정할 수 있습니다.\n\nprint_names &lt;- function(some_names = c(\"에드워드\", \"휴고\")) {\n  print(some_names)\n}\n\nprint_names()\n\n[1] \"에드워드\" \"휴고\"    \n\n\n일반적인 시나리오는 함수를 여러 번 적용하려는 경우입니다. 많은 프로그래밍 언어와 마찬가지로 for() 루프를 사용할 수 있습니다. R에서 for() 루프의 모양은 function()과 유사합니다. 즉, 괄호 안에 반복할 대상을 정의하고, 중괄호 안에 적용할 함수를 정의합니다.\n\n\n\n\n\n\n\n\n\n\n\nR은 통계에 중점을 둔 프로그래밍 언어이므로, 우리는 종종 배열이나 행렬에 관심이 있습니다. apply()를 사용하여 행(“MARGIN = 1”) 또는 열(“MARGIN = 2”)에 함수를 적용합니다.\n\nsimulated_data\n\n  favorite_color person std_normal_observations uniform_observations\n1         파란색      1             -0.35980342            9.6219155\n2         파란색      2             -0.04064753            7.2269016\n3         파란색      3             -1.78216227            0.8252921\n4           흰색      4             -1.12242282            1.0379810\n5         파란색      5             -1.00278400            3.0942004\n  poisson_observations binomial_observations\n1                   81                     2\n2                   91                     1\n3                   84                     1\n4                  100                     1\n5                   97                     1\n\napply(X = simulated_data, MARGIN = 2, FUN = unique)\n\n$favorite_color\n[1] \"파란색\" \"흰색\"  \n\n$person\n[1] \"1\" \"2\" \"3\" \"4\" \"5\"\n\n$std_normal_observations\n[1] \"-0.35980342\" \"-0.04064753\" \"-1.78216227\" \"-1.12242282\" \"-1.00278400\"\n\n$uniform_observations\n[1] \"9.6219155\" \"7.2269016\" \"0.8252921\" \"1.0379810\" \"3.0942004\"\n\n$poisson_observations\n[1] \" 81\" \" 91\" \" 84\" \"100\" \" 97\"\n\n$binomial_observations\n[1] \"2\" \"1\"",
    "crumbs": [
      "부록",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R 필수 사항</span>"
    ]
  },
  {
    "objectID": "20-r_essentials_ko.html#ggplot2로-그래프-만들기",
    "href": "20-r_essentials_ko.html#ggplot2로-그래프-만들기",
    "title": "Online Appendix A — R 필수 사항",
    "section": "A.6 ggplot2로 그래프 만들기",
    "text": "A.6 ggplot2로 그래프 만들기\n데이터 조작 측면에서 tidyverse의 핵심 패키지가 dplyr(Wickham 기타 2022)이라면, 그래프 생성 측면에서 tidyverse의 핵심 패키지는 ggplot2(Wickham 2016)입니다. 장 5 에서 그래프에 대해 더 자세히 다룰 것이지만, 여기서는 몇 가지 필수 사항을 간략하게 소개합니다. ggplot2는 “그래픽 문법”(따라서 “gg”)을 기반으로 그래프를 형성하는 레이어를 정의하여 작동합니다. 파이프 연산자(|&gt;) 대신 ggplot2는 더하기 연산자 +를 사용합니다. tidyverse 패키지 모음의 일부이므로 tidyverse가 로드되면 ggplot2는 명시적으로 설치하거나 로드할 필요가 없습니다.\nggplot2로 그래프를 만들려면 세 가지 핵심 측면을 지정해야 합니다.\n\n데이터;\n미학 / 매핑; 그리고\n유형.\n\n시작하려면 경제 협력 개발 기구(OECD) 국가의 GDP 데이터를 가져올 것입니다(OECD 2022).\n\nlibrary(tidyverse)\n\noecd_gdp &lt;-\n  read_csv(\"https://stats.oecd.org/sdmx-json/data/DP_LIVE/.QGDP.../OECD?contentType=csv&detail=code&separator=comma&csv-lang=en\")\n\nwrite_csv(oecd_gdp, \"inputs/data/oecd_gdp.csv\")\n\n\n\n# A tibble: 6 × 8\n  LOCATION INDICATOR SUBJECT MEASURE  FREQUENCY TIME  Value `Flag Codes`\n  &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;       \n1 OECD     QGDP      TOT     PC_CHGPP A         1962   5.70 &lt;NA&gt;        \n2 OECD     QGDP      TOT     PC_CHGPP A         1963   5.20 &lt;NA&gt;        \n3 OECD     QGDP      TOT     PC_CHGPP A         1964   6.38 &lt;NA&gt;        \n4 OECD     QGDP      TOT     PC_CHGPP A         1965   5.35 &lt;NA&gt;        \n5 OECD     QGDP      TOT     PC_CHGPP A         1966   5.75 &lt;NA&gt;        \n6 OECD     QGDP      TOT     PC_CHGPP A         1967   3.96 &lt;NA&gt;        \n\n\n우리는 먼저 2021년 3분기 OECD 10개국(호주, 캐나다, 칠레, 인도네시아, 독일, 영국, 뉴질랜드, 남아프리카 공화국, 스페인, 미국)의 GDP 변화에 대한 막대 차트를 만들고자 합니다.\n\noecd_gdp_2021_q3 &lt;-\n  oecd_gdp |&gt;\n  filter(\n    TIME == \"2021-Q3\",\n    SUBJECT == \"TOT\",\n    LOCATION %in% c(\n      \"AUS\",\n      \"CAN\",\n      \"CHL\",\n      \"DEU\",\n      \"GBR\",\n      \"IDN\",\n      \"ESP\",\n      \"NZL\",\n      \"USA\",\n      \"ZAF\"\n    ),\n    MEASURE == \"PC_CHGPY\"\n  ) |&gt;\n  mutate(\n    european = if_else(\n      LOCATION %in% c(\"DEU\", \"GBR\", \"ESP\"),\n      \"European\",\n      \"Not european\"\n    ),\n    hemisphere = if_else(\n      LOCATION %in% c(\"CAN\", \"DEU\", \"GBR\", \"ESP\", \"USA\"),\n      \"Northern Hemisphere\",\n      \"Southern Hemisphere\"\n    ),\n  )\n\nggplot()으로 시작하여 매핑/미학을 지정합니다. 이 경우 x축과 y축을 지정하는 것을 의미합니다. ggplot()의 첫 번째 인수는 시각화하려는 데이터이므로, 평소처럼 파이프 연산자를 사용할 수 있습니다.\n\noecd_gdp_2021_q3 |&gt;\n  ggplot(mapping = aes(x = LOCATION, y = Value))\n\n\n\n\n\n\n\n\n이제 관심 있는 그래프 유형을 지정해야 합니다. 이 경우 막대 차트를 원하며, +를 사용하여 geom_bar()를 추가하여 이를 수행합니다.\n\noecd_gdp_2021_q3 |&gt;\n  ggplot(mapping = aes(x = LOCATION, y = Value)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\n\n“fill”이라는 또 다른 미학을 추가하여 국가가 유럽 국가인지 여부에 따라 막대에 색상을 지정할 수 있습니다.\n\noecd_gdp_2021_q3 |&gt;\n  ggplot(mapping = aes(x = LOCATION, y = Value, fill = european)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\n\n마지막으로, labs()로 레이블을 추가하고, scale_fill_brewer()로 색상을 변경하고, theme_classic()로 배경을 변경하여 더 멋지게 만들 수 있습니다.\n\noecd_gdp_2021_q3 |&gt;\n  ggplot(mapping = aes(x = LOCATION, y = Value, fill = european)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    title = \"2021년 3분기 OECD 10개국의 분기별 GDP 변화\",\n    x = \"국가\",\n    y = \"변화 (%)\",\n    fill = \"유럽입니까?\"\n  ) +\n  theme_classic() +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n\n패싯을 사용하면 데이터의 특정 측면에 초점을 맞춘 하위 플롯을 만들 수 있습니다. 3D 그래프를 만들 필요 없이 그래프에 다른 변수를 추가할 수 있으므로 매우 유용합니다. facet_wrap()을 사용하여 패싯을 추가하고 패싯할 변수를 지정합니다. 이 경우 반구별로 패싯합니다.\n\noecd_gdp_2021_q3 |&gt;\n  ggplot(mapping = aes(x = LOCATION, y = Value, fill = european)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    title = \"2021년 3분기 OECD 10개국의 분기별 GDP 변화\",\n    x = \"국가\",\n    y = \"변화 (%)\",\n    fill = \"유럽입니까?\"\n  ) +\n  theme_classic() +\n  scale_fill_brewer(palette = \"Set1\") +\n  facet_wrap(\n    ~hemisphere,\n    scales = \"free_x\"\n  )",
    "crumbs": [
      "부록",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R 필수 사항</span>"
    ]
  },
  {
    "objectID": "20-r_essentials_ko.html#연습-문제",
    "href": "20-r_essentials_ko.html#연습-문제",
    "title": "Online Appendix A — R 필수 사항",
    "section": "A.8 연습 문제",
    "text": "A.8 연습 문제\n\n연습\n\n(계획) 다음 시나리오를 고려하십시오: 어떤 사람이 왕이나 왕비가 더 오래 사는지에 관심이 있으며, 모든 군주에 대해 그들이 얼마나 오래 살았는지에 대한 데이터를 수집합니다. 데이터셋이 어떻게 생겼을지 스케치한 다음, 모든 관측치를 보여주기 위해 만들 수 있는 그래프를 스케치하십시오.\n(시뮬레이션) 설명된 시나리오를 더 고려하고, 1,000명의 군주가 있다고 가정하고, 다음 중 어떤 것을 사용하여 상황을 시뮬레이션할 수 있는지 결정하십시오(모두 선택하십시오)?\n\nrunif(n = 1000, min = 1, max = 110) |&gt; floor()\nrpois(n = 1000, lambda = 65)\nrnorm(n = 1000) |&gt; floor()\nsample(x = sunspot.month, size = 1000, replace = TRUE) |&gt; floor()\n\n(수집) 군주가 얼마나 오래 살았는지에 대한 실제 데이터의 가능한 출처를 하나 식별하십시오.\n(탐색) tidyverse가 로드되어 있고, 데이터셋 “monarchs”에 “years” 열이 있다고 가정하십시오. 다음 중 어떤 것이 35년 이상 산 군주만 결과로 나타낼까요(하나 선택)?\n\nmonarchs |&gt; arrange(years &gt; 35)\nmonarchs |&gt; select(years &gt; 35)\nmonarchs |&gt; filter(years &gt; 35)\nmonarchs |&gt; mutate(years &gt; 35)\n\n(소통) 해당 출처에서 데이터를 수집하고 그래프를 만들었다고 가정하고 두 단락을 작성하십시오. 단락에 포함된 정확한 세부 정보는 사실일 필요는 없습니다(즉, 실제로 데이터를 얻거나 그래프를 만들 필요는 없습니다).\n\n\n\n퀴즈\n\nR은 무엇입니까 (하나 선택)?\n\n오픈 소스 통계 프로그래밍 언어\n귀도 반 로섬이 만든 프로그래밍 언어\n폐쇄 소스 통계 프로그래밍 언어\n통합 개발 환경 (IDE)\n\nR의 세 가지 장점은 무엇입니까? 세 가지 단점은 무엇입니까?\nRStudio는 무엇입니까?\n\n통합 개발 환경 (IDE).\n폐쇄 소스 유료 프로그램.\n귀도 반 로섬이 만든 프로그래밍 언어\n통계 프로그래밍 언어.\n\n2 + 2의 출력 클래스는 무엇입니까 (하나 선택)?\n\ncharacter\nfactor\nnumeric\ndate\n\nmy_name &lt;- \"로한\"을 실행했다고 가정해 봅시다. print(my_name)을 실행한 결과는 무엇입니까 (하나 선택)?\n\n“에드워드”\n“모니카”\n“휴고”\n“로한”\n\n“name”과 “age”라는 두 개의 열이 있는 데이터셋이 있다고 가정해 봅시다. “name”만 선택하려면 어떤 동사를 사용해야 합니까 (하나 선택)?\n\nselect()\nmutate()\nfilter()\nrename()\n\nAustralianPoliticians와 tidyverse를 로드한 다음 다음 코드를 실행했다고 가정해 봅시다: australian_politicians &lt;- get_auspol(\"all\"). “Name”으로 끝나는 모든 열을 어떻게 선택할 수 있습니까 (하나 선택)?\n\naustralian_politicians |&gt; select(contains(\"Name\"))\naustralian_politicians |&gt; select(starts_with(\"Name\"))\naustralian_politicians |&gt; select(matches(\"Name\"))\naustralian_politicians |&gt; select(ends_with(\"Name\"))\n\n열 이름 측면에서, 위 질문에서 contains()를 사용하는 것이 ends_with()를 사용하는 것과 다른 결과를 줄 수 있는 상황은 무엇입니까?\n다음 중 tidyverse 동사가 아닌 것은 무엇입니까 (하나 선택)?\n\nselect()\nfilter()\narrange()\nmutate()\nvisualize()\n\n새 열을 만드는 함수는 무엇입니까 (하나 선택)?\n\nselect()\nfilter()\narrange()\nmutate()\nvisualize()\n\n특정 행에 초점을 맞추는 함수는 무엇입니까 (하나 선택)?\n\nselect()\nfilter()\narrange()\nmutate()\nsummarise()\n\n성별별 데이터셋의 평균을 제공할 수 있는 두 가지 조합은 무엇입니까 (두 가지 선택)?\n\nsummarise()\nfilter()\narrange()\nmutate()\n.by\n\n“age”라는 변수가 정수라고 가정해 봅시다. 다음 코드 줄 중 어떤 것이 그 지수를 나타내는 열을 생성할까요 (하나 선택)?\n\ngenerate(exp_age = exp(age))\nchange(exp_age = exp(age))\nmake(exp_age = exp(age))\nmutate(exp_age = exp(age))\n\n“age”라는 열이 있다고 가정해 봅시다. 다음 코드 줄 중 어떤 것이 5행 위에서 값을 포함하는 열을 생성할 수 있을까요?\n\nmutate(five_before = lag(age))\nmutate(five_before = lead(age))\nmutate(five_before = lag(age, n = 5))\nmutate(five_before = lead(age, n = 5))\n\nclass(\"edward\")의 출력은 무엇입니까 (하나 선택)?\n\n“numeric”\n“character”\n“data.frame”\n“vector”\n\n“파란색, 흰색, 빨간색” 세 가지 옵션에서 한 번 추출하고, “파란색”과 “흰색”에 10% 확률, 나머지는 “빨간색”에 확률을 부여하는 함수는 무엇입니까?\n\nsample(c(\"파란색\", \"흰색\", \"빨간색\"), prob = c(0.1, 0.1, 0.8))\nsample(c(\"파란색\", \"흰색\", \"빨간색\"), size = 1)\nsample(c(\"파란색\", \"흰색\", \"빨간색\"), size = 1, prob = c(0.8, 0.1, 0.1))\nsample(c(\"파란색\", \"흰색\", \"빨간색\"), size = 1, prob = c(0.1, 0.1, 0.8))\n\n평균 27, 표준 편차 3인 정규 분포에서 10,000개의 표본을 시뮬레이션하는 코드는 무엇입니까 (하나 선택)?\n\nrnorm(10000, mean = 27, sd = 3)\nrnorm(27, mean = 10000, sd = 3)\nrnorm(3, mean = 10000, sd = 27)\nrnorm(27, mean = 3, sd = 1000)\n\n그래픽 문법의 세 가지 핵심 측면은 무엇입니까 (모두 선택하십시오)?\n\n데이터\n미학\n유형\ngeom_histogram()\n\n\n\n\n활동\n\n우리는 데이터에 끌릴 때 의심해야 한다. 즉, 매우, 매우 얇고 약한 데이터가 역사상 많은 사회에서 큰 통용력을 가졌던 신념을 정당화하는 것처럼 보일 때, 이는 인구의 많은 부분을 억압하는 데 도움이 되는 방식으로 이루어진다.\n아미아 스리니바산 (Cowen 2021)\n\n옥스포드 올 소울스 칼리지의 사회 및 정치 이론 치첼레 교수인 아미아 스리니바산의 인용문과 D’Ignazio 와/과 Klein (2020), 특히 6장을 되돌아보십시오.\n의미 있는 이름과 적절한 구조를 가진 GitHub 리포지토리를 만들고, 재현 가능한 Quarto 파일을 사용하여 최소 두 페이지(참고 문헌 제외)의 PDF를 생성하여 익숙한 데이터셋과 관련하여 해당 인용문을 논의하십시오. PDF 파일 링크(예: https://github.com/RohanAlexander/starter_folder/blob/main/outputs/paper/paper.pdf)와 PDF 자체를 제출하십시오.\n\n\n\n\nAlexander, Rohan, 와/과 Paul Hodgetts. 2021. AustralianPoliticians: Provides Datasets About Australian Politicians. https://CRAN.R-project.org/package=AustralianPoliticians.\n\n\nBache, Stefan Milton, 와/과 Hadley Wickham. 2022. magrittr: A Forward-Pipe Operator for R. https://CRAN.R-project.org/package=magrittr.\n\n\nBuja, Andreas, Dianne Cook, Heike Hofmann, Michael Lawrence, Eun-Kyung Lee, Deborah F. Swayne, 와/과 Hadley Wickham. 2009. “Statistical inference for exploratory data analysis and model diagnostics”. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 367 (1906): 4361–83. https://doi.org/10.1098/rsta.2009.0120.\n\n\nBuja, Andreas, Dianne Cook, 와/과 Deborah Swayne. 1996. “Interactive high-dimensional data visualization”. Journal of computational and graphical statistics 5 (1): 78–99. https://doi.org/10.2307/1390754.\n\n\nCook, Dianne, Andreas Buja, Javier Cabrera, 와/과 Catherine Hurley. 1995. “Grand Tour and Projection Pursuit”. Journal of Computational and Graphical Statistics 4 (3): 155–72. https://doi.org/10.1080/10618600.1995.10474674.\n\n\nCook, Dianne, 와/과 Deborah Swayne. 2007. Interactive and Dynamic Graphics for Data Analysis: With R and GGobi. 1st ed. Springer.\n\n\nCowen, Tyler. 2021. “Episode 132: Amia Srinivasan on Utopian Feminism”. Conversations with Tyler, 9월. https://conversationswithtyler.com/episodes/amia-srinivasan/.\n\n\nD’Ignazio, Catherine, 와/과 Lauren Klein. 2020. Data Feminism. Massachusetts: The MIT Press. https://data-feminism.mitpress.mit.edu.\n\n\nFirke, Sam. 2023. janitor: Simple Tools for Examining and Cleaning Dirty Data. https://CRAN.R-project.org/package=janitor.\n\n\nGrolemund, Garrett, 와/과 Hadley Wickham. 2011. “Dates and Times Made Easy with lubridate”. Journal of Statistical Software 40 (3): 1–25. https://doi.org/10.18637/jss.v040.i03.\n\n\nHealy, Kieran. 2020. “The Kitchen Counter Observatory”, 5월. https://kieranhealy.org/blog/archives/2020/05/21/the-kitchen-counter-observatory/.\n\n\nKleiber, Christian, 와/과 Achim Zeileis. 2008. Applied Econometrics with R. New York: Springer-Verlag. https://CRAN.R-project.org/package=AER.\n\n\nLarmarange, Joseph. 2023. labelled: Manipulating Labelled Data. https://CRAN.R-project.org/package=labelled.\n\n\nMüller, Kirill, Tobias Schieferdecker, 와/과 Patrick Schratz. 2019. Visualization, transformation and reporting with the tidyverse. https://krlmlr.github.io/vistransrep/.\n\n\nMüller, Kirill, 와/과 Hadley Wickham. 2022. tibble: Simple Data Frames. https://CRAN.R-project.org/package=tibble.\n\n\nOECD. 2022. Quarterly GDP. https://data.oecd.org/gdp/quarterly-gdp.htm.\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nThieme, Nick. 2018. “R generation”. Significance 15 (4): 14–19. https://doi.org/10.1111/j.1740-9713.2018.01169.x.\n\n\nWickham, Hadley. 2009. “Manipulating data”. In ggplot2, 157–75. Springer New York. https://doi.org/10.1007/978-0-387-98141-3_9.\n\n\n———. 2014. “Tidy data”. Journal of statistical software 59 (1): 1–23. https://doi.org/10.18637/jss.v059.i10.\n\n\n———. 2016. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\n———. 2019. Advanced R. 2nd ed. Chapman; Hall/CRC. https://adv-r.hadley.nz.\n\n\n———. 2020. Tidyverse. https://www.tidyverse.org/.\n\n\n———. 2022. stringr: Simple, Consistent Wrappers for Common String Operations. https://CRAN.R-project.org/package=stringr.\n\n\n———. 2023. forcats: Tools for Working with Categorical Variables (Factors). https://CRAN.R-project.org/package=forcats.\n\n\nWickham, Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, 기타. 2019. “Welcome to the Tidyverse”. Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, 와/과 Garrett Grolemund. (2016년) 2023. R for Data Science. 2nd ed. O’Reilly Media. https://r4ds.hadley.nz.\n\n\nWickham, Hadley, Romain François, Lionel Henry, 와/과 Kirill Müller. 2022. dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, Jim Hester, 와/과 Jenny Bryan. 2022. readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr.\n\n\nWickham, Hadley, Evan Miller, 와/과 Danny Smith. 2023. haven: Import and Export “SPSS” “Stata” and “SAS” Files. https://CRAN.R-project.org/package=haven.\n\n\nWickham, Hadley, Davis Vaughan, 와/과 Maximilian Girlich. 2023. tidyr: Tidy Messy Data. https://CRAN.R-project.org/package=tidyr.",
    "crumbs": [
      "부록",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R 필수 사항</span>"
    ]
  },
  {
    "objectID": "21-python_essentials_ko.html",
    "href": "21-python_essentials_ko.html",
    "title": "Online Appendix B — Python 필수 사항",
    "section": "",
    "text": "B.1 서론\n선수 지식\n핵심 개념 및 기술\n소프트웨어 및 패키지\nPython은 귀도 반 로섬이 만든 범용 프로그래밍 언어입니다. Python 버전 0.9.0은 1991년 2월에 출시되었고, 현재 버전인 3.13은 2024년 10월에 출시되었습니다. 몬티 파이튼의 날아다니는 서커스의 이름을 따서 Python이라고 명명되었습니다.\nPython은 기계 학습에서 인기 있는 언어이지만, 더 일반적인 소프트웨어 응용 프로그램을 위해 설계되었으며 더 일반적으로 사용됩니다. 이는 데이터 과학을 위해 Python을 사용할 때 특히 패키지에 의존할 것임을 의미합니다. 이 책에서 Python의 사용은 개발된 다른 더 일반적인 용도보다는 데이터 과학에 중점을 둡니다.\nR을 알면 데이터 과학을 위한 Python을 빠르게 익힐 수 있습니다. 주요 데이터 과학 패키지는 동일한 기본 문제를 해결해야 합니다.",
    "crumbs": [
      "부록",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Python 필수 사항</span>"
    ]
  },
  {
    "objectID": "21-python_essentials_ko.html#python-vs-code-그리고-uv",
    "href": "21-python_essentials_ko.html#python-vs-code-그리고-uv",
    "title": "Online Appendix B — Python 필수 사항",
    "section": "B.2 Python, VS Code, 그리고 uv",
    "text": "B.2 Python, VS Code, 그리고 uv\nRStudio 내에서 Python을 사용할 수도 있지만, 더 널리 사용되는 VS Code를 사용하는 것도 한 가지 옵션입니다. 여기에서 VS Code를 무료로 다운로드하여 설치할 수 있습니다. 이 과정에서 어려움이 있다면, Posit Cloud에서 시작하여 로컬 머신으로 전환했던 것과 마찬가지로, 여기에서 Google Colab을 처음 사용할 수 있습니다.\nVS Code를 열고(그림 B.1 (a)) 새 터미널을 엽니다: 터미널 -&gt; 새 터미널(그림 B.1 (b)). 그런 다음 Python 패키지 관리자인 uv를 설치할 수 있습니다. 터미널에 curl -LsSf https://astral.sh/uv/install.sh | sh를 입력하고 엔터를 누릅니다(그림 B.1 (c)). 마지막으로, Python을 설치하려면 터미널에 uv python install을 입력하고 엔터를 누릅니다(그림 B.1 (d)).\n\n\n\n\n\n\n\n\n\n\n\n(a) VS Code 열기\n\n\n\n\n\n\n\n\n\n\n\n(b) VS Code에서 터미널 열기\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) uv 설치\n\n\n\n\n\n\n\n\n\n\n\n(d) Python 설치\n\n\n\n\n\n\n\n그림 B.1: VS Code를 열고 새 터미널을 연 다음 uv와 Python 설치",
    "crumbs": [
      "부록",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Python 필수 사항</span>"
    ]
  },
  {
    "objectID": "21-python_essentials_ko.html#시작하기",
    "href": "21-python_essentials_ko.html#시작하기",
    "title": "Online Appendix B — Python 필수 사항",
    "section": "B.3 시작하기",
    "text": "B.3 시작하기\n\nB.3.1 프로젝트 설정\nOpen Data Toronto에서 데이터를 다운로드하는 예시로 시작하겠습니다. 시작하려면 모든 코드가 자체 포함될 수 있도록 프로젝트를 만들어야 합니다.\nVS Code를 열고 새 터미널을 엽니다: “터미널” -&gt; “새 터미널”. 그런 다음 Unix 셸 명령을 사용하여 폴더를 만들고 싶은 곳으로 이동합니다. 예를 들어, ls를 사용하여 현재 디렉토리의 모든 폴더를 나열한 다음, cd와 폴더 이름을 사용하여 폴더로 이동합니다. 한 단계 위로 이동해야 하는 경우 ..를 사용합니다.\n이 새 폴더를 만들고 싶은 곳이 만족스러우면 터미널에서 uv init를 사용하여 이를 수행하고, 엔터를 누릅니다(그 다음 cd는 새 폴더 “shelter_usage”로 이동합니다).\n\nuv init shelter_usage\ncd shelter_usage\n\n기본적으로 예시 폴더에 스크립트가 있을 것입니다. uv run을 사용하여 해당 스크립트를 실행하여 프로젝트 환경을 만들 것입니다.\n\nuv run hello.py\n\n프로젝트 환경은 해당 프로젝트에만 해당됩니다. numpy 패키지를 사용하여 데이터를 시뮬레이션할 것입니다. 이 패키지를 uv add를 사용하여 환경에 추가해야 합니다.\n\nuv add numpy\n\n그런 다음 hello.py를 수정하여 numpy를 사용하여 정규 분포에서 시뮬레이션할 수 있습니다.\n\nimport numpy as np\n\ndef main():\n    np.random.seed(853)\n\n    mu, sigma = 0, 1\n    sample_sizes = [10, 100, 1000, 10000]\n    differences = []\n\n    for size in sample_sizes:\n        sample = np.random.normal(mu, sigma, size)\n        sample_mean = np.mean(sample)\n        diff = abs(mu - sample_mean)\n        differences.append(diff)\n        print(f\"표본 크기: {size}\")\n        print(f\"  표본과 모집단 평균 간의 차이: {round(diff, 3)}\")\n\nif __name__ == \"__main__\":\n    main()\n\nhello.py를 수정하고 저장한 후, 이전과 동일한 방식으로 uv run으로 실행할 수 있습니다.\n이 시점에서 VS Code를 닫아야 합니다. 프로젝트 환경이 제대로 작동하는지 확인하기 위해 다시 열고 싶습니다. VS Code에서 프로젝트는 자체 포함된 폴더입니다. “파일” -&gt; “폴더 열기…”로 폴더를 연 다음, 이 경우 “shelter_usage”와 같은 관련 폴더를 선택할 수 있습니다. 그런 다음 uv run hello.py를 다시 실행할 수 있어야 하며, 작동해야 합니다.\n\n\nB.3.2 계획\n이 데이터셋을 장 2 에서 처음 사용했지만, 다시 말하지만, 매일 각 쉼터에 대해 쉼터를 사용한 사람들의 수가 있습니다. 따라서 우리가 시뮬레이션하고 싶은 데이터셋은 그림 B.2 (a) 와 같고, 그림 B.2 (b) 과 같이 매월 평균 일일 점유 침대 수 표를 만들고 싶습니다.\n\n\n\n\n\n\n\n\n\n\n\n(a) 데이터셋의 빠른 스케치\n\n\n\n\n\n\n\n\n\n\n\n(b) 매월 평균 점유 침대 수 표의 빠른 스케치\n\n\n\n\n\n\n\n그림 B.2: 토론토의 쉼터 사용량과 관련된 데이터셋 및 표의 스케치\n\n\n\n\n\nB.3.3 시뮬레이션\n관심 있는 데이터셋을 더 철저히 시뮬레이션하고 싶습니다. 시뮬레이션된 결과를 저장할 데이터프레임을 제공하기 위해 polars를 사용할 것이므로, uv add를 사용하여 환경에 추가해야 합니다.\n\nuv add polars\n\n00-simulate_data.py라는 새 Python 파일을 만듭니다.\n\n#### 서문 ####\n# 목적: 일일 쉼터 사용량 데이터셋 시뮬레이션\n# 저자: Rohan Alexander\n# 날짜: 2024년 11월 12일\n# 연락처: rohan.alexander@utoronto.ca\n# 라이선스: MIT\n# 전제 조건:\n# - `polars` 추가: uv add polars\n# - `numpy` 추가: uv add numpy\n# - `datetime` 추가: uv add datetime\n\n\n#### 작업 공간 설정 ####\nimport polars as pl\nimport numpy as np\nfrom datetime import date\n\nrng = np.random.default_rng(seed=853)\n\n\n#### 데이터 시뮬레이션 ####\n# 쉼터 10개와 설정된 용량 시뮬레이션\nshelters_df = pl.DataFrame(\n    {\n        \"Shelters\": [f\"쉼터 {i}\" for i in range(1, 11)],\n        \"Capacity\": rng.integers(low=10, high=100, size=10),\n    }\n)\n\n# 날짜 데이터프레임 생성\ndates = pl.date_range(\n    start=date(2024, 1, 1), end=date(2024, 12, 31), interval=\"1d\", eager=True\n).alias(\"날짜\")\n\n# 날짜를 데이터프레임으로 변환\ndates_df = pl.DataFrame(dates)\n\n# 날짜와 쉼터 결합\ndata = dates_df.join(shelters_df, how=\"cross\")\n\n# 포아송 추출로 사용량 추가\npoisson_draw = rng.poisson(lam=data[\"Capacity\"])\nusage = np.minimum(poisson_draw, data[\"Capacity\"])\n\ndata = data.with_columns([pl.Series(\"사용량\", usage)])\n\ndata.write_parquet(\"simulated_data.parquet\")\n\n이 시뮬레이션된 데이터를 기반으로 실제 데이터에 적용할 테스트를 작성하고 싶습니다. 이를 위해 pydantic을 사용할 것이므로, uv add를 사용하여 환경에 추가해야 합니다.\n\nuv add pydantic\n\n00-test_simulated_data.py라는 새 Python 파일을 만듭니다. 첫 번째 단계는 pydantic에서 제공하는 BaseModel의 하위 클래스인 ShelterData를 정의하는 것입니다.\n\nfrom pydantic import BaseModel, Field, ValidationError, field_validator\nfrom datetime import date\n\n# Pydantic 모델 정의\nclass ShelterData(BaseModel):\n    Dates: date  # 날짜 형식 유효성 검사 (예: 'YYYY-MM-DD')\n    Shelters: str  # 문자열이어야 함\n    Capacity: int = Field(..., ge=0)  # 음수가 아닌 정수여야 함\n    Usage: int = Field(..., ge=0)  # 음수가 아니어야 함\n\n    # 사용량이 용량을 초과하지 않도록 필드 유효성 검사기 추가\n    @field_validator(\"Usage\")\n    def check_usage_not_exceed_capacity(cls, usage, info):\n        capacity = info.data.get(\"Capacity\")\n        if capacity is not None and usage &gt; capacity:\n            raise ValueError(f\"사용량 ({usage})이 용량 ({capacity})을 초과합니다.\")\n        return usage\n\n날짜가 유효한지, 쉼터가 올바른 유형인지, 용량과 사용량이 모두 음수가 아닌 정수인지 테스트하는 데 관심이 있습니다. 추가적인 문제는 사용량이 용량을 초과해서는 안 된다는 것입니다. 이를 위한 테스트를 작성하기 위해 field_validator를 사용합니다.\n그런 다음 시뮬레이션된 데이터셋을 가져와 테스트할 수 있습니다.\n\nimport polars as pl\n\ndf = pl.read_parquet(\"simulated_data.parquet\")\n\n# Polars DataFrame을 유효성 검사를 위한 딕셔너리 목록으로 변환\ndata_dicts = df.to_dicts()\n\n# 데이터셋을 일괄적으로 유효성 검사\nvalidated_data = []\nerrors = []\n\n# 일괄 유효성 검사\nfor i, row in enumerate(data_dicts):\n    try:\n        validated_row = ShelterData(**row)  # 각 행 유효성 검사\n        validated_data.append(validated_row)\n    except ValidationError as e:\n        errors.append((i, e))\n\n# 유효성 검사된 데이터를 Polars DataFrame으로 다시 변환\nvalidated_df = pl.DataFrame([row.dict() for row in validated_data])\n\n# 결과 표시\nprint(\"유효성 검사된 행:\")\nprint(validated_df)\n\nif errors:\n    print(\"\\n오류:\")\n    for i, error in errors:\n        print(f\"행 {i}: {error}\")\n\n오류가 있었다면 어떻게 되었을지 확인하기 위해 두 가지 오류가 포함된 더 작은 데이터셋을 고려할 수 있습니다. 즉, 잘못 형식화된 날짜 하나와 사용량이 용량을 초과하는 상황 하나입니다.\n\nimport polars as pl\nfrom pydantic import BaseModel, Field, ValidationError, field_validator\nfrom datetime import date\n\n# Pydantic 모델 정의\nclass ShelterData(BaseModel):\n    Dates: date  # 날짜 형식 유효성 검사 (예: 'YYYY-MM-DD')\n    Shelters: str  # 문자열이어야 함\n    Capacity: int = Field(..., ge=0)  # 음수가 아닌 정수여야 함\n    Usage: int = Field(..., ge=0)  # 음수가 아니어야 함\n\n    # 사용량이 용량을 초과하지 않도록 필드 유효성 검사기 추가\n    @field_validator(\"Usage\")\n    def check_usage_not_exceed_capacity(cls, usage, info):\n        capacity = info.data.get(\"Capacity\")\n        if capacity is not None and usage &gt; capacity:\n            raise ValueError(f\"사용량 ({usage})이 용량 ({capacity})을 초과할 수 없습니다.\")\n        return usage\n\n# 데이터셋 정의\ndf = [\n    {\"Dates\": \"2024-01-01\", \"Shelters\": \"쉼터 1\", \"Capacity\": 23, \"Usage\": 22},\n    {\"Dates\": \"로한\", \"Shelters\": \"쉼터 2\", \"Capacity\": 62, \"Usage\": 62},\n    {\"Dates\": \"2024-01-01\", \"Shelters\": \"쉼터 3\", \"Capacity\": 93, \"Usage\": 88},\n    # 테스트를 위한 잘못된 행 추가\n    {\"Dates\": \"2024-01-01\", \"Shelters\": \"쉼터 4\", \"Capacity\": 50, \"Usage\": 55},\n]\n\n# 데이터셋을 일괄적으로 유효성 검사\nvalidated_data = []\nerrors = []\n\n# 일괄 유효성 검사\nfor i, row in enumerate(df):\n    try:\n        validated_row = ShelterData(**row)  # 각 행 유효성 검사\n        validated_data.append(validated_row)\n    except ValidationError as e:\n        errors.append((i, e))\n\n# 유효성 검사된 데이터를 Polars DataFrame으로 다시 변환\nvalidated_df = pl.DataFrame([row.dict() for row in validated_data])\n\n# 결과 표시\nprint(\"유효성 검사된 행:\")\nprint(validated_df)\n\nif errors:\n    print(\"\\n오류:\")\n    for i, error in errors:\n        print(f\"행 {i}: {error}\")\n\n다음 메시지가 나타납니다.\n오류:\n행 1: ShelterData에 대한 1개의 유효성 검사 오류\nDates\n  입력은 유효한 날짜 또는 datetime이어야 합니다. 입력이 너무 짧습니다 [type=date_from_datetime_parsing, input_value='로한', input_type=str]\n    자세한 내용은 https://errors.pydantic.dev/2.9/v/date_from_datetime_parsing을 참조하십시오.\n행 3: ShelterData에 대한 1개의 유효성 검사 오류\nUsage\n  값 오류, 사용량 (55)이 용량 (50)을 초과할 수 없습니다. [type=value_error, input_value=55, input_type=int]\n    자세한 내용은 https://errors.pydantic.dev/2.9/v/value_error을 참조하십시오.\n\n\nB.3.4 획득\n이전과 동일한 출처 사용: https://ckan0.cf.opendata.inter.prod-toronto.ca/dataset/21c83b32-d5a8-4106-a54f-010dbe49f6f2/resource/ffd20867-6e3c-4074-8427-d63810edf231/download/Daily%20shelter%20overnight%20occupancy.csv\n\nimport polars as pl\n\n# CSV 파일 URL\nurl = \"https://ckan0.cf.opendata.inter.prod-toronto.ca/dataset/21c83b32-d5a8-4106-a54f-010dbe49f6f2/resource/ffd20867-6e3c-4074-8427-d63810edf231/download/Daily%20shelter%20overnight%20occupancy.csv\"\n\n# CSV 파일을 Polars DataFrame으로 읽기\ndf = pl.read_csv(url)\n\n# 원시 데이터 저장\ndf.write_parquet(\"shelter_usage.parquet\")\n\n몇 개의 열과 데이터가 있는 행에만 관심이 있을 것입니다.\n\nimport polars as pl\n\ndf = pl.read_parquet(\"shelter_usage.parquet\")\n\n# 특정 열 선택\nselected_columns = [\"OCCUPANCY_DATE\", \"SHELTER_ID\", \"OCCUPIED_BEDS\", \"CAPACITY_ACTUAL_BED\"]\n\nselected_df = df.select(selected_columns)\n\n# 데이터가 있는 행만 필터링\nfiltered_df = selected_df.filter(df[\"OCCUPIED_BEDS\"].is_not_null())\n\nprint(filtered_df.head())\n\nrenamed_df = filtered_df.rename({\"OCCUPANCY_DATE\": \"date\",\n                                 \"SHELTER_ID\": \"Shelters\",\n                                 \"CAPACITY_ACTUAL_BED\": \"Capacity\",\n                                 \"OCCUPIED_BEDS\": \"Usage\"\n                                 })\n\nprint(renamed_df.head())\n\nrenamed_df.write_parquet(\"cleaned_shelter_usage.parquet\")\n\n그런 다음 실제 데이터셋에 테스트를 적용하고 싶을 수 있습니다.\n\nimport polars as pl\nfrom pydantic import BaseModel, Field, ValidationError, field_validator\nfrom datetime import date\n\n# Pydantic 모델 정의\nclass ShelterData(BaseModel):\n    Dates: date  # 날짜 형식 유효성 검사 (예: 'YYYY-MM-DD')\n    Shelters: str  # 문자열이어야 함\n    Capacity: int = Field(..., ge=0)  # 음수가 아닌 정수여야 함\n    Usage: int = Field(..., ge=0)  # 음수가 아니어야 함\n\n    # 사용량이 용량을 초과하지 않도록 필드 유효성 검사기 추가\n    @field_validator(\"Usage\")\n    def check_usage_not_exceed_capacity(cls, usage, info):\n        capacity = info.data.get(\"Capacity\")\n        if capacity is not None and usage &gt; capacity:\n            raise ValueError(f\"사용량 ({usage})이 용량 ({capacity})을 초과할 수 없습니다.\")\n        return usage\n\ndf = pl.read_parquet(\"cleaned_shelter_usage.parquet\")\n\n# Polars DataFrame을 유효성 검사를 위한 딕셔너리 목록으로 변환\ndata_dicts = df.to_dicts()\n\n# 데이터셋을 일괄적으로 유효성 검사\nvalidated_data = []\nerrors = []\n\n# 일괄 유효성 검사\nfor i, row in enumerate(data_dicts):\n    try:\n        validated_row = ShelterData(**row)  # 각 행 유효성 검사\n        validated_data.append(validated_row)\n    except ValidationError as e:\n        errors.append((i, e))\n\n# 유효성 검사된 데이터를 Polars DataFrame으로 다시 변환\nvalidated_df = pl.DataFrame([row.dict() for row in validated_data])\n\n# 결과 표시\nprint(\"유효성 검사된 행:\")\nprint(validated_df)\n\nif errors:\n    print(\"\\n오류:\")\n    for i, error in errors:\n        print(f\"행 {i}: {error}\")\n\n\n\nB.3.5 탐색\n데이터 조작\n\nimport polars as pl\n\ndf = pl.read_parquet(\"cleaned_shelter_usage.parquet\")\n\n# 날짜 열을 datetime으로 변환하고 명확성을 위해 이름 변경\ndf = df.with_columns(pl.col(\"date\").str.strptime(pl.Date, \"%Y-%m-%d\").alias(\"date\"))\n\n# \"Dates\"로 그룹화하고 총 \"Capacity\" 및 \"Usage\" 계산\naggregated_df = (\n    df.group_by(\"date\")\n    .agg([\n        pl.col(\"Capacity\").sum().alias(\"총 용량\"),\n        pl.col(\"Usage\").sum().alias(\"총 사용량\")\n    ])\n    .sort(\"date\")  # 날짜별로 결과 정렬\n)\n\n# 집계된 DataFrame 표시\nprint(aggregated_df)\n\n그래프 만들기\n\nimport polars as pl\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport matplotlib.dates as mdates\n\n# Parquet 파일에서 Polars DataFrame 읽기\ndf = pl.read_parquet(\"analysis_data.parquet\")\n\n# 'date' 열이 Polars에서 datetime 유형인지 확인\ndf = df.with_columns([\n    pl.col('date').cast(pl.Date)\n])\n\n# 관련 열을 선택하고 DataFrame 재구성\ndf_melted = df.select([\"date\", \"총 용량\", \"총 사용량\"]).melt(\n    id_vars=\"date\",\n    variable_name=\"측정 항목\",\n    value_name=\"값\"\n)\n\n# Polars DataFrame을 Seaborn을 위한 Pandas DataFrame으로 변환\ndf_melted_pd = df_melted.to_pandas()\n\n# Pandas에서 'date' 열이 datetime인지 확인\ndf_melted_pd['date'] = pd.to_datetime(df_melted_pd['date'])\n\n# 플로팅 스타일 설정\nsns.set_theme(style=\"whitegrid\")\n\n# 플롯 생성\nplt.figure(figsize=(12, 6))\nsns.lineplot(\n    data=df_melted_pd,\n    x=\"date\",\n    y=\"값\",\n    hue=\"측정 항목\",\n    linewidth=2.5\n)\n\n# x축 레이블을 보기 좋게 형식 지정\nplt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\nplt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n\n# 가독성을 위해 x축 레이블 회전\nplt.xticks(rotation=45)\n\n# 레이블 및 제목 추가\nplt.xlabel(\"날짜\")\nplt.ylabel(\"값\")\nplt.title(\"시간 경과에 따른 총 용량 및 사용량\")\n\n# 눈금 레이블 잘림 방지를 위해 레이아웃 조정\nplt.tight_layout()\n\n# 플롯 표시\nplt.show()\n\n\n\nB.3.6 공유\n한 가지 좋은 점은 Quarto 문서에서 Python을 사용할 수 있다는 것입니다. 이를 위해 여기에서 Quarto 확장을 설치하여 VS Code에 추가해야 합니다. 터미널에서 quarto preview를 실행하여 문서를 렌더링할 수 있습니다.\nVS Code는 Microsoft에서 만들었으며, Microsoft는 GitHub도 소유하고 있습니다. 따라서 계정으로 이동하여 로그인하여 GitHub 계정을 VS Code에 추가할 수 있습니다.",
    "crumbs": [
      "부록",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Python 필수 사항</span>"
    ]
  },
  {
    "objectID": "21-python_essentials_ko.html#python",
    "href": "21-python_essentials_ko.html#python",
    "title": "Online Appendix B — Python 필수 사항",
    "section": "B.4 Python",
    "text": "B.4 Python\nFor 루프\n리스트 컴프리헨션",
    "crumbs": [
      "부록",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Python 필수 사항</span>"
    ]
  },
  {
    "objectID": "21-python_essentials_ko.html#그래프-만들기",
    "href": "21-python_essentials_ko.html#그래프-만들기",
    "title": "Online Appendix B — Python 필수 사항",
    "section": "B.5 그래프 만들기",
    "text": "B.5 그래프 만들기\nmatplotlib\nseaborn",
    "crumbs": [
      "부록",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Python 필수 사항</span>"
    ]
  },
  {
    "objectID": "21-python_essentials_ko.html#polars-탐색",
    "href": "21-python_essentials_ko.html#polars-탐색",
    "title": "Online Appendix B — Python 필수 사항",
    "section": "B.6 polars 탐색",
    "text": "B.6 polars 탐색\n\nB.6.1 데이터 가져오기\n\n\nB.6.2 결합 및 피벗을 사용한 데이터셋 조작\n\n\nB.6.3 문자열 조작\n\n\nB.6.4 요인 변수",
    "crumbs": [
      "부록",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Python 필수 사항</span>"
    ]
  },
  {
    "objectID": "21-python_essentials_ko.html#연습-문제",
    "href": "21-python_essentials_ko.html#연습-문제",
    "title": "Online Appendix B — Python 필수 사항",
    "section": "B.7 연습 문제",
    "text": "B.7 연습 문제\n\n연습\n\n\n퀴즈\n\n\n과제\n무료 Replit “100일 코딩” Python 과정.\n\n\n\n\nPython Software Foundation. 2024. Python Language Reference, version 3.13.0. https://docs.python.org/3/index.html.",
    "crumbs": [
      "부록",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Python 필수 사항</span>"
    ]
  },
  {
    "objectID": "22-sql_essentials_ko.html",
    "href": "22-sql_essentials_ko.html",
    "title": "Online Appendix C — SQL 필수",
    "section": "",
    "text": "C.1 서론\n선행 조건\n핵심 개념 및 기술\n주요 패키지 및 함수\n구조화된 쿼리 언어(SQL) (“시퀄” 또는 “에스큐엘”)는 관계형 데이터베이스와 함께 사용됩니다. 관계형 데이터베이스는 하나 이상의 테이블 모음이며, 테이블은 행과 열로 구성된 데이터입니다. 데이터베이스에 테이블이 두 개 이상 있는 경우, 테이블을 연결하는 열이 있어야 합니다. 예를 들어, 부록 A 에서 사용되는 AustralianPoliticians 데이터셋이 있습니다. SQL을 사용하는 것은 마크업과 프로그래밍의 중간쯤 되는 HTML/CSS와 비슷하게 느껴집니다. 한 가지 재미있는 점은 관례적으로 명령어가 대문자로 작성된다는 것입니다. 또 다른 점은 줄 공백은 아무 의미가 없다는 것입니다. 포함하든 안 하든 상관없지만, 항상 SQL 명령은 세미콜론으로 끝나야 합니다.\nSQL은 1970년대 IBM에서 개발되었습니다. SQL은 데이터를 다루는 특히 인기 있는 방법입니다. 닫힌 옵션과 열린 옵션을 포함하여 다양한 “SQL 종류”가 있습니다. 여기서는 오픈 소스이며 Mac에 사전 설치된 SQLite를 소개합니다. Windows 사용자는 여기에서 설치할 수 있습니다.\n고급 SQL 사용자는 SQL만으로 많은 작업을 수행하지만, SQL에 대한 실무 지식만 있어도 액세스할 수 있는 데이터셋의 수가 증가합니다. SQL에 대한 실무 지식은 많은 데이터셋이 SQL 서버에 저장되어 있고, 직접 데이터를 가져올 수 있다는 점에서 효율성에 특히 유용합니다.\nRStudio 내에서 SQL을 사용할 수 있습니다. 특히 DBI (R Special Interest Group on Databases (R-SIG-DB), Wickham, 와/과 Müller 2022) 를 활용할 수 있습니다. 그러나 R 기술에 대한 수요와는 별개로 SQL 기술에 대한 수요를 고려할 때, RStudio와 독립적으로 SQL에 대한 실무 지식을 갖는 것이 경력 관점에서 더 나은 아이디어일 수 있습니다. 많은 SQL 명령을 이 책 전체에서 사용한 dplyr 동사의 간단한 변형으로 간주할 수 있습니다. 실제로 R 내에 머물고 싶다면 dbplyr (Wickham, Girlich, 와/과 Ruiz 2022) 를 사용하면 dplyr 함수를 명시적으로 사용할 수 있으며, 그러면 자동으로 SQL로 변환됩니다. tidyverse에서 mutate(), filter(), left_join()을 사용해 본 경험이 있다면 많은 핵심 SQL 명령이 익숙할 것입니다. 이는 SQL이 까다로울 수 있으므로 주요 어려움은 연산 순서를 파악하는 것임을 의미합니다.",
    "crumbs": [
      "부록",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>SQL 필수</span>"
    ]
  },
  {
    "objectID": "22-sql_essentials_ko.html#시작하기",
    "href": "22-sql_essentials_ko.html#시작하기",
    "title": "Online Appendix C — SQL 필수",
    "section": "C.2 시작하기",
    "text": "C.2 시작하기\nSQL을 시작하려면 무료 오픈 소스인 DB Browser for SQLite (DB4S)를 다운로드하여 엽니다(그림 C.1).\n\n\n\n\n\n\n그림 C.1: DB Browser for SQLite 열기\n\n\n\n여기에서 “AustralianPoliticians.db”를 다운로드한 다음 “데이터베이스 열기”로 열고 데이터베이스를 다운로드한 위치로 이동합니다.\n이제 다룰 세 가지 주요 SQL 명령은 SELECT, FROM, WHERE입니다. SELECT를 사용하면 데이터의 특정 열을 지정할 수 있으며, SELECT를 select()와 유사하게 고려할 수 있습니다. select()로 데이터셋을 지정해야 하고 파이프 연산자를 사용하여 그렇게 했던 것과 마찬가지로, FROM으로 데이터셋을 지정합니다. 예를 들어, “SQL 실행”을 열고 다음을 입력한 다음 “실행”을 클릭할 수 있습니다.\n\nSELECT\n    surname   \nFROM\n    politicians;\n\n결과는 성(surname) 열을 얻는 것입니다. 쉼표로 구분하여 여러 열을 선택하거나, 별표를 사용하여 모든 열을 선택할 수 있지만, 데이터셋이 우리가 모르는 사이에 변경될 경우 결과가 달라질 수 있으므로 이는 모범 사례가 아닙니다.\n\nSELECT\n    uniqueID,\n    surname   \nFROM\n    politicians;\n\n\nSELECT\n    *\nFROM\n    politicians;\n\n마지막으로, 반복되는 행이 있다면 DISTINCT를 사용하여 고유한 행만 볼 수 있습니다. 이는 distinct()와 유사합니다.\n\nSELECT\n    DISTINCT surname   \nFROM\n    politicians;\n\n지금까지 SELECT와 FROM을 사용했습니다. 일반적으로 사용되는 세 번째 명령은 WHERE이며, 이는 filter()와 유사하게 특정 행에 초점을 맞출 수 있도록 합니다.\n\nSELECT\n    uniqueID,\n    surname,\n    firstName   \nFROM\n    politicians     \nWHERE\n    firstName = \"Myles\";\n\n“=”, “!=”, “&gt;”, “&lt;”, “&gt;=”, “&lt;=”와 같은 모든 일반적인 논리 연산자는 WHERE와 함께 사용할 수 있습니다. AND와 OR를 사용하여 조건을 결합할 수 있습니다.\n\nSELECT\n    uniqueID,\n    surname,\n    firstName   \nFROM\n    politicians     \nWHERE\n    firstName = \"Myles\" \n    OR firstName = \"Ruth\";\n\n많은 결과를 반환하는 쿼리가 있다면 LIMIT를 사용하여 결과 수를 제한할 수 있습니다.\n\nSELECT\n    uniqueID,\n    surname,\n    firstName        \nFROM\n    politicians          \nWHERE\n    firstName = \"Robert\"       LIMIT 5;\n\n그리고 ORDER를 사용하여 결과의 순서를 지정할 수 있습니다.\n\nSELECT\n    uniqueID,\n    surname,\n    firstName \nFROM\n    politicians \nWHERE\n    firstName = \"Robert\" \nORDER BY\n    surname DESC;\n\n기준에 매우 가까운 행을 확인하십시오:\n\nSELECT\n    uniqueID,\n    surname,\n    firstName      \nFROM\n    politicians      \nWHERE\n    firstName LIKE \"Ma__\";\n\n위의 “_“는 모든 문자와 일치하는 와일드카드입니다. 이는”Mary”와 “Mark”를 포함하는 결과를 제공합니다. LIKE는 대소문자를 구분하지 않습니다: “Ma“와”ma” 모두 동일한 결과를 반환합니다.\n“NULL” 또는 “NOT NULL”을 사용하여 누락된 데이터에 초점을 맞출 수 있습니다.\n\nSELECT\n    uniqueID,\n    surname,\n    firstName,\n    comment\nFROM\n    politicians      \nWHERE\n    comment     IS NULL;\n\n숫자, 날짜, 텍스트 필드에 순서가 적용되어 BETWEEN을 모든 필드에 사용할 수 있습니다. 숫자뿐만 아니라. 예를 들어, X와 Z 사이의 문자로 시작하는 모든 성을 찾을 수 있습니다 (Z는 포함하지 않음).\n\nSELECT\n    uniqueID,\n    surname,\n    firstName\nFROM\n    politicians      \nWHERE\n    surname     BETWEEN \"X\" AND \"Z\";\n\n숫자 변수와 함께 WHERE를 사용하면 BETWEEN이 포함적입니다. 문자와 함께 사용한 예시와는 다릅니다.\n\nSELECT\n    uniqueID,\n    surname,\n    firstName,\n    birthYear\nFROM\n    politicians      \nWHERE\n    birthYear     BETWEEN 1980 AND 1990;\n\n요청한 데이터셋 관측치를 제공하는 것 외에도 데이터셋을 수정할 수 있습니다. 예를 들어, UPDATE와 SET을 사용하여 값을 편집할 수 있습니다.\n\nUPDATE\n    politicians   \nSET\n    displayName = \"John Gilbert Alexander\"\nWHERE\n    uniqueID = \"Alexander1951\";\n\nCASE와 ELSE를 사용하여 if-else 논리를 통합할 수 있습니다. 예를 들어, “Josh Frydenberg”의 경우 “Yes”, “Kevin Rudd”의 경우 “No”, 다른 모든 경우 “Unsure”인 “wasTreasurer”라는 열을 추가할 수 있습니다.\n\nSELECT\n    uniqueID,\n    surname,\n    firstName,\n    birthYear,\n    CASE                  \n        WHEN uniqueID = \"Frydenberg1971\" THEN \"Yes\"                  \n        WHEN surname = \"Rudd\" THEN \"No\"                  \n        ELSE \"Unsure\"            \n    END AS \"wasTreasurer\"        \nFROM\n    politicians;\n\nsummarize() 대신 COUNT, SUM, MAX, MIN, AVG, ROUND와 같은 명령을 사용하여 요약 통계를 생성할 수 있습니다. COUNT는 열 이름을 전달하여 해당 열에 대해 비어 있지 않은 행의 수를 계산하며, MIN 등도 유사하게 작동합니다.\n\nSELECT\n    COUNT(uniqueID)   \nFROM\n    politicians;\n\n\nSELECT\n    MIN(birthYear)   \nFROM\n    politicians;\n\nR의 group_by와 유사하게 GROUP BY를 사용하여 데이터셋의 다른 그룹을 기반으로 결과를 얻을 수 있습니다.\n\nSELECT\n    COUNT(uniqueID)   \nFROM\n    politicians     \nGROUP BY\n    gender;\n\n마지막으로 LEFT JOIN을 사용하여 두 테이블을 결합할 수 있습니다. 점 표기법을 사용하여 일치하는 열을 지정하는 데 주의해야 합니다.\n\nSELECT\n    politicians.uniqueID,\n    politicians.firstName,\n    politicians.surname,\n    party.partySimplifiedName   \nFROM\n    politicians \nLEFT JOIN\n    party     \n        ON politicians.uniqueID = party.uniqueID;\n\nSQL은 우리의 초점이 아니므로 몇 가지 필수 명령에 대한 간략한 개요만 제공했습니다. 경력 관점에서 SQL에 익숙해져야 합니다. SQL은 데이터 과학에 너무나 통합되어 있어서 “SQL 없이는 너무 멀리 갈 수 없을 것” (Robinson 와/과 Nolis 2020, p. 8)이며, “거의 모든” 데이터 과학 인터뷰에는 SQL에 대한 질문이 포함될 것입니다 (Robinson 와/과 Nolis 2020, p. 110).",
    "crumbs": [
      "부록",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>SQL 필수</span>"
    ]
  },
  {
    "objectID": "22-sql_essentials_ko.html#연습-문제",
    "href": "22-sql_essentials_ko.html#연습-문제",
    "title": "Online Appendix C — SQL 필수",
    "section": "C.3 연습 문제",
    "text": "C.3 연습 문제\n\n연습\n무료 w3school SQL 퀴즈에서 최소 70%를 획득한 화면을 제출하십시오. 튜토리얼을 살펴보는 것도 좋지만, 이 장의 SQL 내용(dplyr 경험과 결합)만으로도 70%를 얻기에 충분합니다. 스크린샷에는 시간과 날짜가 포함되어야 합니다. 즉, 브라우저뿐만 아니라 전체 화면을 스크린샷하십시오.\n\n\n퀴즈\n\nSQL은 주로 무엇에 사용됩니까?\n\n웹 페이지 스타일 지정\n관계형 데이터베이스 관리 및 쿼리\n머신러닝 모델 생성\n그래픽 인터페이스 설계\n\nSQL에서 테이블에서 특정 열을 검색하는 데 사용되는 명령은 무엇입니까?\n\nJOIN\nWHERE\nSELECT\nFROM\n\nSQL에서 데이터를 쿼리할 테이블을 지정하는 데 사용되는 절은 무엇입니까?\n\nWHERE\nFROM\nGROUP BY\nSELECT\n\nSQL 명령 WHERE는 무엇을 합니까?\n\n두 테이블을 조인합니다.\n데이터를 정렬합니다.\n동일한 데이터를 가진 레코드를 그룹화합니다.\n지정된 조건에 따라 레코드를 필터링합니다.\n\nemployees라는 테이블에서 모든 열을 선택하려면 어떻게 해야 합니까?\n\nSELECT # FROM employees;\nSELECT * FROM employees;\nSELECT all FROM employees;\nSELECT columns FROM employees;\n\n결과 집합에서 중복 행을 제거하는 데 사용되는 SQL 키워드는 무엇입니까?\n\nUNIQUE\nDISTINCT\nREMOVE\nDELETE\n\nSQL 쿼리에서 LIMIT 절의 목적은 무엇입니까?\n\n열의 최대값을 설정합니다.\n반환되는 행의 수를 제한합니다.\n표시되는 열의 수를 제한합니다.\n액세스 제어를 적용합니다.\n\n쿼리 결과 집합을 정렬하는 데 사용되는 SQL 절은 무엇입니까?\n\nSORT\nORDER BY\nSORT BY\nORDER\n\nSQL에서 LIKE 연산자와 함께 사용될 때 와일드카드 문자 _는 무엇을 나타냅니까?\n\n모든 숫자\n공백 문자\n모든 단일 문자\n0개 이상의 문자\n\nfirstName이 ’Ma’로 시작하고 그 뒤에 두 문자가 오는 레코드를 어떻게 선택합니까?\n\nWHERE firstName LIKE ’Ma*’;\nWHERE firstName LIKE ’Ma__’;\nWHERE firstName LIKE ‘Ma??’;\nWHERE firstName LIKE ‘Ma%’;\n\n데이터베이스 테이블에서 데이터를 업데이트하는 데 사용되는 SQL 문은 무엇입니까?\n\nSET\nCHANGE\nUPDATE\nMODIFY\n\nSQL 쿼리에서 NULL 값을 확인하려면 어떻게 해야 합니까?\n\nWHERE column LIKE NULL\nWHERE column EQUALS NULL\nWHERE column = NULL\nWHERE column IS NULL\n\nSQL에서 BETWEEN 연산자는 무엇을 합니까?\n\n주어진 범위 내의 값을 선택합니다.\n결과 집합을 정렬합니다.\n값이 NULL인지 확인합니다.\n여러 조건을 결합합니다.\n\nSQL에서 각 그룹에 집계 함수를 적용할 수 있도록 속성을 공유하는 행을 그룹화하는 데 사용되는 SQL 키워드는 무엇입니까?\n\nGROUP BY\nHAVING\nDISTINCT\nORDER BY\n\nSQL에서 LEFT JOIN의 목적은 무엇입니까?\n\n두 테이블의 모든 행을 결합합니다.\n오른쪽 테이블의 모든 레코드와 왼쪽 테이블의 일치하는 레코드를 반환합니다.\n왼쪽 테이블의 모든 레코드와 오른쪽 테이블의 일치하는 레코드를 반환합니다.\n두 테이블에서 일치하는 값을 가진 행을 반환합니다.\n\nSQL 쿼리에서 SELECT *를 사용하는 것이 모범 사례가 아닌 이유는 무엇입니까?\n\n열을 지정하는 것보다 느립니다.\n모든 SQL 종류에서 지원되지 않습니다.\n데이터를 반환하지 않습니다.\n데이터베이스 스키마가 변경되면 예상치 못한 결과가 발생할 수 있습니다.\n\n\n\n\n과제\n여기에서 SQL 데이터셋을 가져오십시오.\nSQL(R 또는 Python 아님)을 사용하여 이 관찰 데이터를 사용하여 몇 가지 결과를 도출하십시오. Quarto를 사용하여 짧은 논문을 작성하십시오 (그래프를 만드는 데 R/Python을 사용해도 좋지만, 데이터 준비/조작에는 사용하지 마십시오. 이는 별도의 스크립트에서 SQL로 수행되어야 합니다). 토론에서 다음 각각에 대한 하위 섹션을 하나씩 포함하십시오: 1) 상관 관계 대 인과 관계; 2) 누락된 데이터; 3) 편향의 원인.\n일반적인 기대치를 충족하는 GitHub 저장소 링크(그룹당 하나의 저장소)를 제출하십시오.\n관련된 루브릭 구성 요소는 다음과 같습니다: “R/Python 인용”, “데이터 인용”, “수업 논문”, “LLM 사용 문서화”, “제목”, “저자, 날짜, 저장소”, “초록”, “서론”, “데이터”, “측정”, “결과”, “토론”, “산문”, “교차 참조”, “캡션”, “그래프/표/기타”, “참고 문헌”, “커밋”, “스케치”, “시뮬레이션”, “테스트”, “재현 가능한 워크플로우”.\n\n\n\n\nChamberlin, Donald. 2012. “Early History of SQL”. IEEE Annals of the History of Computing 34 (4): 78–82. https://doi.org/10.1109/mahc.2012.61.\n\n\nR Special Interest Group on Databases (R-SIG-DB), Hadley Wickham, 와/과 Kirill Müller. 2022. DBI: R Database Interface. https://CRAN.R-project.org/package=DBI.\n\n\nRobinson, Emily, 와/과 Jacqueline Nolis. 2020. Build a Career in Data Science. Shelter Island: Manning Publications. https://livebook.manning.com/book/build-a-career-in-data-science.\n\n\nWickham, Hadley, Maximilian Girlich, 와/과 Edgar Ruiz. 2022. dbplyr: A “dplyr” Back End for Databases. https://CRAN.R-project.org/package=dbplyr.",
    "crumbs": [
      "부록",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>SQL 필수</span>"
    ]
  },
  {
    "objectID": "23-datasets_ko.html",
    "href": "23-datasets_ko.html",
    "title": "Online Appendix D — 데이터셋",
    "section": "",
    "text": "학생들이 종종 어려움을 겪는 것 중 하나는 데이터셋을 선택하는 것입니다. 일반적으로 Kaggle, UCI 머신러닝 저장소 및 기타 일반적으로 사용되는 옵션의 데이터셋은 피하는 것이 좋습니다. 데이터 과학 관점에서 이러한 출처에서 제공되는 데이터셋을 사용한다는 것은 거의 모든 중요한 결정이 이미 내려졌으며 잠재적으로 문서화되지 않았다는 것을 의미합니다. 그리고 경력 관점에서, 다른 모든 사람들이 이러한 데이터셋을 사용하기 때문에 여러분의 포트폴리오를 돋보이게 하지 못합니다. 몇 가지 대안은 다음과 같습니다:\n\nAidData는 개발 및 해외 원조 연구와 관련된 많은 데이터셋을 제공합니다.\nAlex Cookson의 데이터셋.\n(andrews2012data는?) 다양한 데이터셋을 제공하며, 여기에서 확인할 수 있습니다.\n사회 과학자를 위한 API는 데이터를 수집하는 데 사용할 수 있는 다양한 API를 제공합니다.\n(Bombieri2023은?) 5,000건 이상의 대형 육식 동물 인간 공격에 대한 데이터셋을 제공합니다.\n영국 도서관의 세계 신문 목록에는 출판 시작 및 종료 연도, 출판 장소, 변형된 제목 및 판, 출판 언어에 대한 정보가 포함되어 있습니다.\nBuzzFeed News는 기사를 뒷받침하는 많은 데이터셋에 대한 접근을 제공합니다.\n캐나다 지방 선거 데이터베이스에는 캐나다 전역의 지방 자치 단체에 대한 완전한 지방 선거 결과가 포함되어 있습니다 (Lucas 기타 2020).\nCongressindata는 2005년부터 2015년까지 미국 의회 의원에 대한 데이터셋을 제공합니다.\nCongress.gov API는 특히 법안 및 기타 텍스트 데이터와 관련하여 미국 의회에 대한 매우 유용한 데이터 소스입니다.\nCOVerAGE-DB는 COVID-19 사례 및 사망에 대한 전 세계 인구 통계 데이터베이스입니다 (Riffe 기타 2021).\ncricketdata (Hyndman 기타 2022)는 국제 및 기타 주요 크리켓 경기에 대한 데이터를 다운로드하는 기능을 제공합니다.\n데이터 및 스토리 라이브러리는 수백 개의 데이터셋에 대한 접근을 제공합니다.\nData Is Plural은 2015년까지의 아카이브와 함께 흥미로운 데이터셋에 대한 주간 뉴스레터를 제공합니다.\nData Liberation Project는 FOI 요청을 사용하여 미국 정부 데이터셋을 구축하는 데 중점을 둡니다.\n인구 통계 및 건강 조사 (DHS) 프로그램은 1984년부터 90개국에 대한 설문조사 데이터를 제공합니다.\nDuolingo는 연구 논문을 뒷받침하는 데이터셋에 대한 접근을 제공합니다.\nThe Economist는 기사를 뒷받침하는 많은 데이터셋에 대한 접근을 제공합니다.\nEH.net은 다양한 흥미로운 역사 경제 데이터셋을 제공합니다.\nEPA는 규제되지 않은 오염 물질 모니터링 규칙에서 발생 데이터를 제공합니다.\n유럽 NUTS-수준 선거 데이터베이스 (EU-NED)는 1990년부터 2020년까지의 국가 및 유럽 의회 선거 결과를 제공합니다.\n연방 준비 은행 경제 데이터 (FRED)는 미국 경제 데이터를 제공하며, API에 액세스하기 위한 R 패키지 fredr (Boysel 와/과 Vaughan 2021)가 있습니다.\nFiveThirtyEight는 기사를 뒷받침하는 많은 데이터셋에 대한 접근을 제공합니다.\nGoodreads 데이터셋은 2017년에 2백만 권 이상의 책에 대한 메타데이터 및 리뷰를 포함한 공개 데이터를 스크랩한 것입니다 (Wan 와/과 McAuley 2018; Wan 기타 2019).\n역사적 사회 갈등 데이터베이스는 주로 유럽에 초점을 맞춘 20,000건 이상의 갈등에 대한 데이터를 제공합니다 (Chambru 와/과 Maneuvrier-Hervieu 2022).\n역사 통계는 역사 통계에 대한 링크를 제공합니다.\n인간 사망률 데이터베이스는 다양한 국가에 대한 상세한 사망률 및 인구 데이터를 제공합니다.\nICANN의 중앙 집중식 영역 데이터 서비스는 며칠이 걸릴 수 있는 신청 및 승인 절차를 거쳐 모든 도메인 이름에 대한 접근을 제공합니다.\nIPCC 데이터 배포 센터.\n아일랜드 사회 과학 데이터 아카이브는 다양한 데이터셋을 제공합니다.\nJ-PAL (압둘 라티프 자밀 빈곤 행동 연구소)은 행정 데이터 카탈로그를 유지합니다.\nNFL Savant는 2013년 이후의 플레이별 데이터, 1999년 이후의 컴바인 데이터, 날씨 데이터를 포함하여 NFL에 대한 팀별 데이터를 제공합니다.\nThe Markup의 Show Your Work 시리즈는 종종 기사를 뒷받침하는 데이터가 포함된 GitHub 저장소 링크를 포함합니다. 몇 가지 주목할 만한 예시는 다음과 같습니다: 모기지 승인 알고리즘에 숨겨진 비밀 편향.\n매사추세츠 수자원 관리국은 폐수 COVID-19 추적 데이터를 여기에서 제공하며, 원시 데이터는 파싱할 수 있는 PDF로 제공됩니다.\n뉴욕 현대 미술관 (MoMA)은 컬렉션 및 전시회에 대한 데이터셋을 제공합니다.\nNASA의 행성 데이터 시스템.\nProPublica 데이터 스토어는 미국에 대한 방대한 데이터셋을 제공하며, 일부는 상당히 큽니다. 예를 들어, Open Payments Data (2016)는 6GB입니다.\n(bhht3의?) Notable People 데이터셋은 기원전 3500년부터 2018년까지의 주목할 만한 인물에 대한 교차 검증된 데이터베이스를 제공합니다.\nOECD는 경제 데이터를 제공합니다.\nParlEE 데이터셋은 EU 입법부 회의실에서 수백만 건의 연설에 대한 주석이 달린 전문을 포함합니다 (Sylvester 기타 2023).\nPrison Policy Initiative는 미국 교도소 및 구치소에 대한 많은 데이터셋을 제공합니다.\nThe Pudding은 기사를 뒷받침하는 많은 데이터셋을 제공합니다. 몇 가지 주목할 만한 예시는 다음과 같습니다: 벌거벗은 진실, 그리고 미국 인구 조사의 진화.\nPushshift Reddit 데이터셋은 2015년 이후의 Reddit 게시물 모음입니다 (Baumgartner 기타 2020).\nRefugee Law Lab은 캐나다 대법원 판결의 전문을 JSON 형식으로 제공합니다 (Rehaag 2023).\nRijksmuseum은 컬렉션에 대한 다양한 데이터를 제공합니다.\nSocioeconomic High-resolution Rural-Urban Geographic Platform (SHRUG)은 인도 전역의 600,000개 마을 및 도시에 대한 사회경제적 개발 데이터를 제공하는 오픈 데이터 플랫폼입니다 (Asher 기타 2021).\n톰 카르도소의 Bias behind bars는 캐나다의 흑인 및 원주민 수감자에 대한 데이터를 제공합니다.\nTracking (In)Justice는 캐나다에서 경찰 관련 사망을 추적하는 데이터셋입니다 (Data and Justice Criminology Lab, Institute of Criminology and Criminal Justice, Carleton University; The Centre for Research & Innovation for Black Survivors of Homicide Victims (The CRIB), at the Factor-Inwentash Faculty of Social Work, University of Toronto; Canadian Civil Liberties Association; Ethics and Technology Lab, Queen’s University 2022).\n미국 질병 통제 예방 센터 (CDC) 국립 생체 통계 시스템은 연결된 출생 및 영아 사망 데이터를 포함한 다양한 데이터셋을 제공합니다.\n케빈 윌슨이 정리하고 준비한 미국 형량 위원회 개별 범죄자 데이터셋.\n무장 반란에 참여하는 여성 활동은 1946-2015년 사이에 반군 조직에 참여한 여성의 측정치에 대한 접근을 제공합니다 (Loken 와/과 Matfess 2023).\n워싱턴 포스트는 기사를 뒷받침하는 많은 데이터셋에 대한 접근을 제공합니다. 특히 흥미로운 것은 의회 노예 소유자, 치명적인 무력 사격, 학교 총격, 그리고 왜 FEMA는 딥 사우스의 흑인 재난 생존자에게 지원을 거부하는가입니다.\nWordbank 데이터베이스는 어린이 어휘 성장에 대한 공개 데이터베이스입니다. wordbankr (Braginsky 2020)를 사용하여 추가적으로 접근할 수 있으며, 앨리슨 프레스메인즈 힐은 유용한 배경 및 정리 코드를 제공합니다.\n세계 은행은 광범위한 글로벌 개발 데이터 및 마이크로데이터 라이브러리를 제공합니다.\n예일 대학교 국제 금융 센터 데이터셋: 역사적 금융 연구 데이터, 그리고 주식 시장 신뢰 지수.\n\n\n\n\n\nAsher, Sam, Tobias Lunt, Ryu Matsuura, 와/과 Paul Novosad. 2021. “Development Research at High Geographic Resolution: An Analysis of Night Lights, Firms, and Poverty in India using the SHRUG Open Data Platform”. World Bank Economic Review 35 (4). https://shrug-assets-ddl.s3.amazonaws.com/static/main/assets/other/almn-shrug.pdf.\n\n\nBaumgartner, Jason, Savvas Zannettou, Brian Keegan, Megan Squire, 와/과 Jeremy Blackburn. 2020. “The Pushshift Reddit Dataset”. arXiv. https://doi.org/10.48550/arxiv.2001.08435.\n\n\nBoysel, Sam, 와/과 Davis Vaughan. 2021. fredr: An R Client for the “FRED” API. https://CRAN.R-project.org/package=fredr.\n\n\nBraginsky, Mika. 2020. wordbankr: Accessing the Wordbank Database. https://CRAN.R-project.org/package=wordbankr.\n\n\nChambru, Cédric, 와/과 Paul Maneuvrier-Hervieu. 2022. “Introducing HiSCoD: A new gateway for the study of historical social conflict”. Working paper series, Department of Economics, University of Zurich. https://doi.org/10.5167/uzh-217109.\n\n\nData and Justice Criminology Lab, Institute of Criminology and Criminal Justice, Carleton University; The Centre for Research & Innovation for Black Survivors of Homicide Victims (The CRIB), at the Factor-Inwentash Faculty of Social Work, University of Toronto; Canadian Civil Liberties Association; Ethics and Technology Lab, Queen’s University. 2022. “Tracking (In)Justice: A Living Data Set Tracking Canadian Police-Involved Deaths”. https://trackinginjustice.ca.\n\n\nHyndman, Rob, Timothy Hyndman, Charles Gray, Sayani Gupta, 와/과 Jacquie Tran. 2022. cricketdata: International Cricket Data. https://CRAN.R-project.org/package=cricketdata.\n\n\nLoken, Meredith, 와/과 Hilary Matfess. 2023. “Introducing the Women’s Activities in Armed Rebellion (WAAR) Project, 1946-2015”. Journal of Peace Research.\n\n\nLucas, Jack, Reed Merrill, Kelly Blidook, Sandra Breux, Laura Conrad, Gabriel Eidelman, Royce Koop, 기타. 2020. “Canadian Municipal Elections Database”. Scholars Portal Dataverse. https://doi.org/10.5683/sp2/4mzjpq.\n\n\nRehaag, Sean. 2023. “Supreme Court of Canada Bulk Decisions Dataset”. Refugee Law Laboratory. https://refugeelab.ca/bulk-data/scc.\n\n\nRiffe, Tim, Enrique Acosta, Enrique José Acosta, Diego Manuel Aburto, Anna Alburez-Gutierrez, Ainhoa Altová, Ugofilippo Alustiza, 기타. 2021. “Data Resource Profile: COVerAGE-DB: a global demographic database of COVID-19 cases and deaths”. International Journal of Epidemiology 50 (2): 390–390f. https://doi.org/10.1093/ije/dyab027.\n\n\nSylvester, Christine, Anastasia Ershova, Aleksandra Khokhlova, Nikoleta Yordanova, 와/과 Zachary Greene. 2023. “ParlEE plenary speeches V2 data set: Annotated full-text of 15.1 million sentence-level plenary speeches of six EU legislative chambers”. Harvard Dataverse. https://doi.org/10.7910/DVN/VOPK0E.\n\n\nWan, Mengting, 와/과 Julian J. McAuley. 2018. “Item recommendation on monotonic behavior chains”. In Proceedings of the 12th ACM Conference on Recommender Systems, RecSys 2018, Vancouver, BC, Canada, October 2-7, 2018, 편집자： Sole Pera, Michael D. Ekstrand, Xavier Amatriain, 와/과 John O’Donovan, 86–94. ACM. https://doi.org/10.1145/3240323.3240369.\n\n\nWan, Mengting, Rishabh Misra, Ndapa Nakashole, 와/과 Julian J. McAuley. 2019. “Fine-Grained Spoiler Detection from Large-Scale Review Corpora”. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, 편집자： Anna Korhonen, David R. Traum, 와/과 Lluı́s Màrquez, 2605–10. Association for Computational Linguistics. https://doi.org/10.18653/v1/p19-1248.",
    "crumbs": [
      "부록",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>데이터셋</span>"
    ]
  },
  {
    "objectID": "24-rmarkdown_ko.html",
    "href": "24-rmarkdown_ko.html",
    "title": "Online Appendix E — R 마크다운",
    "section": "",
    "text": "E.1 R 청크\nQuarto는 R 마크다운의 후속작이지만, 비교적 새로 나왔고 많은 사람들이 여전히 R 마크다운을 사용합니다. 대부분의 경우, 장 3 에서 다루는 측면은 Quarto와 R 마크다운 모두에 적용됩니다. 그러나 이 부록에서는 Quarto에 제공된 지침과 다른 측면이 있는 경우 R 마크다운에 대한 동등한 내용을 제공합니다.\nR 마크다운 문서 내의 코드 청크에 R 및 기타 여러 언어에 대한 코드를 포함할 수 있습니다. 그런 다음 문서를 니트하면 코드가 실행되어 문서에 포함됩니다. 예를 들어, tidyverse와 AER를 로드하고 설문조사 응답자가 지난 2주 동안 의사를 방문한 횟수에 대한 그래프를 만들 수 있습니다.\n```{r}\nlibrary(tidyverse)\nlibrary(AER)\n\ndata(\"DoctorVisits\", package = \"AER\")\n\nDoctorVisits |&gt;\n  ggplot(aes(x = illness)) +\n  geom_histogram(stat = \"count\")\n```\n해당 코드의 출력은 ?fig-doctervisits 입니다.\n1977-1978년 호주 건강 조사를 기반으로 한 지난 2주간의 질병 수\nQuarto와 달리 모든 옵션이 중괄호 안에 맨 위에 있다는 점에 유의하십시오. Quarto의 주석 표기법은 사용할 수 없습니다.",
    "crumbs": [
      "부록",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>R 마크다운</span>"
    ]
  },
  {
    "objectID": "24-rmarkdown_ko.html#교차-참조",
    "href": "24-rmarkdown_ko.html#교차-참조",
    "title": "Online Appendix E — R 마크다운",
    "section": "E.2 교차 참조",
    "text": "E.2 교차 참조\n그림, 표, 방정식을 교차 참조하는 것이 유용할 수 있습니다. 이렇게 하면 텍스트에서 참조하기가 더 쉬워집니다. 그림의 경우 그림을 생성하거나 포함하는 R 청크의 이름을 참조합니다. 예를 들어, (Figure \\@ref(fig:uniquename))은 (?fig-uniquename) 을 생성합니다. 이는 R 청크의 이름이 uniquename이기 때문입니다. 또한 R 마크다운이 이것이 그림임을 알 수 있도록 청크 이름 앞에 ’fig’를 추가해야 합니다. 그런 다음 캡션을 지정하는 ’fig.cap’을 R 청크에 포함합니다.\n\n`r ''````{r uniquename, fig.cap = \"1977-1978년 호주 건강 조사를 기반으로 한 지난 2주간의 질병 수\", echo = TRUE}\n\n\nlibrary(tidyverse)\nlibrary(AER)\n\ndata(\"DoctorVisits\", package = \"AER\")\n\nDoctorVisits |&gt;\n  ggplot(aes(x = illness)) +\n  geom_histogram(stat = \"count\")\n\n\n\n\n1977-1978년 호주 건강 조사를 기반으로 한 지난 2주간의 질병 수\n\n\n\n\n표를 교차 참조하는 유사하지만 약간 다른 접근 방식을 취할 수 있습니다. 예를 들어, (Table \\@ref(tab:docvisittablermarkdown))은 (표 E.1) 을 생성합니다. 이 경우 R 마크다운이 테이블임을 알 수 있도록 테이블의 고유 참조 앞에 ‘tab’을 지정합니다. 테이블의 경우 그림의 경우와 같이 ’fig.cap’ 청크 옵션이 아닌 ’caption’으로 캡션을 본문에 포함해야 합니다.\n\n\n\n\n표 E.1: 1977-1978년 호주 건강 조사를 기반으로 한 지난 2주간의 의사 방문 횟수\n\n\n\n\n\n\nvisits\nn\n\n\n\n\n0\n4141\n\n\n1\n782\n\n\n2\n174\n\n\n3\n30\n\n\n4\n24\n\n\n5\n9\n\n\n6\n12\n\n\n7\n12\n\n\n8\n5\n\n\n9\n1\n\n\n\n\n\n\n\n\n마지막으로, 방정식을 교차 참조할 수도 있습니다. 이를 위해 태그 (\\#eq:macroidentity)를 추가해야 하며, 이를 참조합니다. 예를 들어, Equation \\@ref(eq:macroidentity).를 사용하여 (방정식 E.1) 를 생성합니다.\n\n\\begin{equation}\nY = C + I + G + (X - M) (\\#eq:macroidentity)\n\\end{equation}\n\n\\[\nY = C + I + G + (X - M)\n\\tag{E.1}\\]\n교차 참조를 사용할 때는 R 청크에 간단한 레이블이 있는지 확인하십시오. 일반적으로 이름은 간단하지만 고유하게 유지하고, 가능하면 구두점을 피하고 문자를 사용하십시오. 레이블에 밑줄을 사용하지 마십시오. 오류가 발생합니다.",
    "crumbs": [
      "부록",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>R 마크다운</span>"
    ]
  },
  {
    "objectID": "25-papers_ko.html",
    "href": "25-papers_ko.html",
    "title": "Online Appendix F — 논문",
    "section": "",
    "text": "F.1 도널드슨 논문\n자료를 이해하는 한 가지 방법은 자료를 사용하는 것입니다. 이 논문들의 목적은 여러분이 배운 것을 실제 환경에서 구현할 기회를 제공하는 것입니다. 논문을 완성하는 것은 또한 취업 지원을 위한 포트폴리오를 구축하는 관점에서도 중요합니다.\n기대치는 매년 바뀌므로 “이전 예시”를 템플릿이 아닌 예시로 취급하십시오.",
    "crumbs": [
      "부록",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>논문</span>"
    ]
  },
  {
    "objectID": "25-papers_ko.html#sec-paper-one",
    "href": "25-papers_ko.html#sec-paper-one",
    "title": "Online Appendix F — 논문",
    "section": "",
    "text": "F.1.1 과제\n\n개별적으로 그리고 완전히 재현 가능한 방식으로 Open Data Toronto에서 관심 있는 데이터셋을 찾아 데이터에 대한 이야기를 담은 짧은 논문을 작성하십시오.\n\n적절한 하위 폴더가 있는 잘 정리된 폴더를 만들고 GitHub에 추가하십시오. 이 시작 폴더를 사용해야 합니다.\nOpen Data Toronto에서 관심 있는 데이터셋을 찾으십시오. (금지된 것은 아니지만, 팬데믹에 대한 데이터셋은 사용하지 마십시오.)\n\n관심 있는 데이터셋을 시뮬레이션하고 일부 테스트를 개발하는 R 스크립트 “scripts/00-simulate_data.R”을 작성하십시오. GitHub에 푸시하고 유익한 커밋 메시지를 포함하십시오.\nopendatatoronto (Gelfand 2022)를 사용하여 재현 가능한 방식으로 실제 데이터를 다운로드하는 R 스크립트 “scripts/01-download_data.R”을 작성하십시오. 데이터를 “data/raw_data/unedited_data.csv”로 저장하십시오 (의미 있는 이름과 적절한 파일 형식을 사용하십시오). GitHub에 푸시하고 유익한 커밋 메시지를 포함하십시오.\n\nQuarto “paper/paper.qmd”를 사용하여 제목, 저자, 날짜, 초록, 서론, 데이터, 참고 문헌 섹션이 있는 PDF를 준비하십시오.\n\n제목은 설명적이고 유익하며 구체적이어야 합니다.\n날짜는 모호하지 않은 형식이어야 합니다. 감사의 글에 GitHub 저장소 링크를 추가하십시오.\n초록은 세네 문장이어야 합니다. 초록은 독자에게 최상위 발견을 알려야 합니다. 이 논문 때문에 세상에 대해 배우는 한 가지는 무엇입니까?\n서론은 두세 단락의 내용이어야 합니다. 그리고 논문의 나머지 부분을 설명하는 추가적인 마지막 단락이 있어야 합니다.\n데이터 섹션은 데이터의 출처와 데이터가 발생한 더 넓은 맥락(윤리적, 통계적 등)을 철저하고 정확하게 논의해야 합니다. 텍스트, 그래프, 표를 사용하여 데이터를 포괄적으로 설명하고 요약하십시오. 그래프는 ggplot2 (Wickham 2016)로 만들어야 하며 표는 tinytable (Arel-Bundock 2024)로 만들어야 합니다. 그래프는 요약 통계가 아닌 실제 데이터 또는 가능한 한 실제 데이터에 가깝게 표시해야 합니다. 그래프와 표는 텍스트에서 상호 참조되어야 합니다 (예: “표 1은 …을 보여줍니다.”).\n참고 문헌은 BibTeX를 사용하여 추가해야 합니다. R과 사용하는 모든 R 패키지, 그리고 데이터셋을 참조해야 합니다. 강력한 제출물은 관련 문헌을 활용하고 이를 참조할 것입니다.\n논문은 잘 작성되어야 하며, 관련 문헌을 활용하고 모든 기술 개념을 설명해야 합니다. 교육을 받았지만 비전문가인 독자를 대상으로 하십시오.\n지원 자료이지만 중요하지 않은 자료는 부록을 사용하십시오.\nGitHub에 푸시하고 유익한 커밋 메시지를 포함하십시오.\n\n\nGitHub 저장소 링크를 제출하십시오. 마감일 이후에는 저장소를 업데이트하지 마십시오.\n이것이 수업 과제라는 증거가 없어야 합니다.\n\n\n\nF.1.2 확인 사항\n\n최종 PDF에는 R 코드나 원시 R 출력이 없어야 합니다.\nLLM 사용에 대한 README의 예시 문구는 다음과 같습니다: “LLM 사용에 대한 진술: 코드의 일부는 자동 완성 도구인 Codriver의 도움을 받아 작성되었습니다. 초록과 서론은 ChatHorse의 도움을 받아 작성되었으며 전체 채팅 기록은 other/llm/usage.txt에서 확인할 수 있습니다.”\n논문은 PDF로 직접 렌더링되어야 합니다. 즉, “PDF로 렌더링”을 사용하십시오.\n그래프, 표, 텍스트는 명확해야 하며, 파이낸셜 타임즈와 비슷한 품질이어야 합니다.\n날짜는 최신이고 모호하지 않아야 합니다 (예: 2/3/2024는 모호하지만, 2024년 3월 2일은 그렇지 않습니다).\n전체 워크플로우는 완전히 재현 가능해야 합니다.\n오타가 없어야 합니다.\n이것이 학교 논문이라는 흔적이 없어야 합니다.\n각주를 사용하여 논문의 GitHub 저장소 링크가 있어야 합니다.\nGitHub 저장소는 잘 정리되어 있어야 하며, 유익한 README를 포함해야 합니다.\n논문은 잘 작성되어야 하며, 예를 들어 파이낸셜 타임즈의 일반 독자가 이해할 수 있어야 합니다. 이는 수학적 표기법을 사용할 수 있지만, 모든 것을 평이한 언어로 설명해야 함을 의미합니다. 모든 통계 개념과 용어는 설명되어야 합니다. 독자는 대학 교육을 받았지만, p-값이 무엇인지 반드시 이해하는 사람은 아닙니다.\n초록은 “간결하게” 작성되어야 하며, 거의 간결해야 합니다. 불필요한 단어를 제거하십시오. 네 문장 이상 포함하지 마십시오. (경험이 쌓이면 이 규칙을 어길 수 있습니다.)\n서론에는 단락이 필요합니다 (Quarto 문서에서 줄 사이에 공백을 두십시오).\n서론에서 논문의 나머지 부분을 미리 알려주십시오: “섹션 2…, 섹션 3….”. (경험이 쌓이면 이 규칙을 어길 수 있습니다.)\n서버에서 Quarto 문서로 데이터를 직접 읽어들이지 말고, 저장된 버전을 읽어들이십시오. 이렇게 하는 제출물은 전체 점수 0점을 받습니다.\n서론에서 발견 사항에 대해 더 구체적으로 설명하십시오.\n데이터 섹션은 데이터 정리와 관련된 것이 아니라 데이터에 관한 것입니다. 데이터 정리는 부록에 넣으십시오. 중요한 것이 아니라면 데이터 섹션에서 데이터 정리를 논의하지 마십시오.\n시뮬레이션에는 시드가 필요합니다.\n저장소 이름을 “Paper 1” 등으로 지정하지 마십시오.\n“그래프” 또는 “표”와 같은 섹션을 만들지 마십시오.\nusethis::git_vaccinate()를 사용하여 더 나은 .gitignore 파일을 얻고, 특히 DS_Store를 무시하십시오.\n사용하는 데이터셋과 opendatatoronto를 모두 인용하는 것을 잊지 마십시오. 이들은 별개의 것입니다.\n\n\n\nF.1.3 FAQ\n\nKaggle에서 데이터셋을 사용할 수 있습니까? 아니요, 그들은 이미 여러분을 위해 어려운 작업을 수행했기 때문입니다.\n코드를 사용하여 데이터셋을 다운로드할 수 없습니다. 수동으로 다운로드할 수 있습니까? 아니요, 전체 워크플로우가 재현 가능해야 하기 때문입니다. 다운로드 문제를 해결하거나 다른 데이터셋을 선택하십시오.\n얼마나 작성해야 합니까? 대부분의 학생들은 2~6페이지 범위의 내용을 제출하지만, 이는 여러분에게 달려 있습니다. 정확하고 철저하게 작성하십시오.\n제 데이터는 아파트 블록/NBA/리그 오브 레전드에 관한 것이므로 더 넓은 맥락이 없습니다. 어떻게 해야 합니까? 편향과 윤리를 더 잘 이해하기 위해 관련 장과 자료를 다시 읽으십시오. 정말로 생각할 수 없다면 다른 데이터셋을 선택하는 것이 좋습니다.\nPython을 사용할 수 있습니까? 아니요. 이미 Python을 알고 있다면 다른 언어를 배우는 것이 나쁘지 않습니다.\nWord를 인용할 필요가 없는데 왜 R을 인용해야 합니까? R은 학술적 기원을 가진 무료 통계 프로그래밍 언어이므로 다른 사람들의 작업을 인정하는 것이 적절합니다. 또한 재현성에도 중요합니다.\n어떤 참고 문헌 스타일을 사용해야 합니까? 어떤 주요 참고 문헌 스타일이든 괜찮습니다 (APA, Harvard, Chicago 등). 익숙한 것을 선택하십시오.\n시작 폴더의 논문에는 모델 섹션이 있는데, 모델을 만들어야 합니까? 아니요. 시작 폴더는 모든 논문에 적용할 수 있도록 설계되었으므로 필요 없는 부분은 삭제하십시오.\n시작 폴더의 논문에는 데이터 시트 부록이 있는데, 데이터 시트를 만들어야 합니까? 아니요. 시작 폴더는 모든 논문에 적용할 수 있도록 설계되었으므로 필요 없는 부분은 삭제하십시오.\n“실제 데이터를 그래프로 나타낸다”는 것은 무엇을 의미합니까? 예를 들어 데이터셋에 5,000개의 관측치와 세 개의 변수가 있다면, 모든 변수에 대해 점의 경우 5,000개의 점이 있는 그래프가 있어야 하며, 막대 차트와 히스토그램의 경우 5,000개로 합산되어야 합니다.\n\n\n\nF.1.4 루브릭\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Component\n                Range\n                Requirement\n              \n        \n        \n        \n                \n                  R/Python cited\n                  0 - 'No'; 1 - 'Yes'\n                  R (and/or Python) is properly referenced in the main content and in the reference list. If not, then paper gets 0 overall.\n                \n                \n                  LLM documentation\n                  0 - 'No'; 1 - 'Yes'\n                  A separate paragraph or dot point must be included in the README about whether LLMs were used, and if so how. If auto-complete tools such as co-pilot were used this must be mentioned. If chat tools such as ChatGPT, were used then the entire chat must be included in the usage text file. If not, then paper gets 0 overall.\n                \n                \n                  Title\n                  0 - 'Poor or not done'; 1 - 'Yes'; 2 - 'Exceptional'\n                  An informative title is included that explains the story, and ideally tells the reader what happens at the end of it. 'Paper X' is not an informative title. Use a subtitle to convey the main finding. Do not use puns (you can break this rule once you're experienced).\n                \n                \n                  Author, date, and repo\n                  0 - 'Poor or not done'; 2 - 'Yes'\n                  The author, date of submission in unambiguous format, and a link to a GitHub repo are clearly included. (The later likely, but not necessarily, through a statement such as: 'Code and data supporting this analysis is available at: LINK').\n                \n                \n                  Abstract\n                  0 - 'Poor or not done'; 1 - 'Some issues'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  An abstract is included and appropriately pitched to a non-specialist audience. The abstract answers: 1) what was done, 2) what was found, and 3) why this matters (all at a high level). Likely four sentences. Abstract must make clear what we learn about the world because of this paper.\n                \n                \n                  Introduction\n                  0 - 'Poor or not done'; 1 - 'Some issues'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  The introduction is self-contained and tells a reader everything they need to know including: 1) broader context to motivate; 2) some detail about what the paper is about; 3) a clear gap that needs to be filled; 4) what was done; 5) what was found; 6) why it is important; 7) the structure of the paper. A reader should be able to read only the introduction and know what was done, why, and what was found. Likely 3 or 4 paragraphs, or 10 per cent of total.\n                \n                \n                  Data\n                  0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Acceptable'; 8 - 'Impressive'; 10 - 'Exceptional'\n                  A sense of the dataset should be communicated to the reader. The broader context of the dataset should be discussed. All variables should be thoroughly examined and explained. Explain if there were similar datasets that could have been used and why they were not. If variables were constructed then this should be mentioned, and high-level cleaning aspects of note should be mentioned, but this section should focus on the destination, not the journey. It is important to understand what the variables look like by including graphs, and possibly tables, of all observations, along with discussion of those graphs and the other features of these data. Summary statistics should also be included, and well as any relationships between the variables. You are not doing EDA in this section--you are talking the reader through the variables that are of interest. If this becomes too detailed, then appendices could be used. Basically, for every variable in your dataset that is of interest to your paper there needs to be graphs and explanation and maybe tables.\n                \n                \n                  Measurement\n                  0 - 'Poor or not done'; 2 - 'Some issues'; 3 - 'Acceptable'; 4 - 'Exceptional'\n                  A thorough discussion of measurement, relating to the dataset, is provided in the data section. Please ensure that you explain how we went from some phenomena in the world that happened to an entry in the dataset that you are interested in.\n                \n                \n                  Prose\n                  0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Acceptable'; 6 - 'Exceptional'\n                  All aspects of submission should be free of noticeable typos, spelling mistakes, and be grammatically correct. Prose should be coherent, concise, clear, and mature. Remove unnecessary words. Do not use the following words/phrases: 'actionable insights', 'address/es/ing a/that/the/this gap', 'advanced', 'all-encompassing', 'apt', 'backdrop', 'beg/s the question', 'bridge/s a/that/the/this gap', 'clear gap', 'compelling', 'comprehensive', 'critical', 'crucial', 'data driven', 'data-driven', 'delve/s', 'drastic', 'drive/s forward', 'elucidate/ing', 'embark/s', 'exploration', 'fill/s a/that/the/this gap', 'fresh perspective/s', 'hidden factor/s', 'immense', 'imperative', 'indispensable', 'insight/s', 'insight/s from', 'interrogate', 'intricate', 'intriguing', 'invaluable', 'key driver/s', 'key insight/s', 'kind of', 'leverage', 'meticulous/ly', 'multifaceted', 'novel', 'nuance', 'offer/s/ing crucial insight', 'pivotal', 'plummeted', 'profound', 'rapidly', 'reveals', 'shed/s light', 'shocking', 'soared', 'unparalleled', 'unveiling', 'valuable', 'wanna'.\n                \n                \n                  Cross-references\n                  0 - 'Poor or not done'; 1 - 'Yes'\n                  All figures, tables, and equations, should be numbered, and referred to in the text using cross-references. The telegraphing paragraph in the introduction should cross reference the rest of the paper.\n                \n                \n                  Captions\n                  0 - 'Poor or not done'; 1 - 'Acceptable'; 2 - 'Excellent'\n                  All figures and tables have detailed and meaningful captions. They should be sufficiently detailed so as to make the main point of the figure/table clear even without the accompanying text. Do not say 'Histogram of...' or whatever else the figure type is.\n                \n                \n                  Graphs and tables\n                  0 - 'Poor or not done'; 1 - 'Some issues'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  Graphs and tables must be included in the paper and should be to well-formatted, clear, and digestible. Graphs should be made using ggplot2 and tables should be made using tinytable. They should serve a clear purpose and be fully self-contained. Graphs and tables should be appropriately sized, colored, and labelled. Variable names should not be used as labels. Tables should have an appropriate number of decimal places and use comma separators for thousands. Don't use boxplots, but if you must then you must overlay the actual data.\n                \n                \n                  Referencing\n                  0 - 'Poor or not done'; 3 - 'One minor issue'; 4 - 'Perfect'\n                  All data, software, literature, and any other relevant material, should be cited in-text and included in a properly formatted reference list made using BibTeX. A few lines of code from Stack Overflow or similar, would be acknowledged just with a comment in the script immediately preceding the use of the code rather than here. But larger chunks of code should be fully acknowledged with an in-text citation and appear in the reference list. Check in-text citations and that you have not accidentally used (@my_cite) when you needed [@my_cite]. R packages and all other aspects should be correctly capitalized, and name should be correct e.g. use double braces appropriately in the BibTeX file.\n                \n                \n                  Commits\n                  0 - 'Poor or not done'; 2 - 'Done'\n                  There are at least a handful of different commits, and they have meaningful commit messages.\n                \n                \n                  Sketches\n                  0 - 'Poor or not done'; 2 - 'Done'\n                  Sketches are included in a labelled folder of the repo, appropriate, and of high-quality.\n                \n                \n                  Simulation\n                  0 - 'Poor or not done'; 1 - 'Some issues'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  The script is clearly commented and structured. All variables are appropriately simulated in a sophisticated way including appropriate interaction between simulated variables.\n                \n                \n                  Tests\n                  0 - 'Poor or not done'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  High-quality extensive suites of tests are written for the both the simulated and actual datasets. These suites must be in separate scripts. The suite should be extensive and put together in a sophisticated way using packages like testthat, validate, pointblank, or great expectations.\n                \n                \n                  Reproducible workflow\n                  0 - 'Poor or not done'; 1 - 'Some issues'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  Use an organized repo with a detailed README and an R project. Thoroughly document code and include a preamble, comments, nice structure, and style code with styler or lintr. Use seeds appropriately. Avoid leaving install.packages() in the code unless handled sophisticatedly. Exclude unnecessary files from the repo; avoid hard-coded paths and setwd(). Use base pipe not magrittr pipe. Comment on and close all GitHub issues. Deal with all branches.\n                \n                \n                  Miscellaneous\n                  0 - 'None'; 1 - 'Notable'; 2 - 'Remarkable'; 3 - 'Exceptional'\n                  There are always students that excel in a way that is not anticipated in the rubric. This item accounts for that.\n                \n        \n      \n    \n\n\n\n\n\nF.1.5 이전 예시\n\n2024 (가을): 줄리아 리, 스티븐 리, 그리고 지헝 중.\n2024 (겨울): 아바스 슬레이만, 베니 로크버그, 칼리 펜로즈 (이 논문을 기반으로 한 기사가 나중에 CBC 뉴스에 게재됨 (Penrose 2024)), 하디 아흐마드, 루카 카네기, 사미 엘 사브리, 토마스 폭스, 그리고 티모티우스 프라조기.\n2023: 크리스티나 웨이, 그리고 이네사 드 안젤리스.\n2022: 아담 라바스, 알리시아 양, 알리사 슐라이퍼, 이든 샌섬, 허드슨 위엔, 잭 맥케이, 로이 찬, 토마스 도노프리오, 그리고 윌리엄 게레케.\n2021: 에이미 패로우, 모르가인 웨스틴, 그리고 레이첼 램.",
    "crumbs": [
      "부록",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>논문</span>"
    ]
  },
  {
    "objectID": "25-papers_ko.html#sec-paper-two",
    "href": "25-papers_ko.html#sec-paper-two",
    "title": "Online Appendix F — 논문",
    "section": "F.2 모슨 논문",
    "text": "F.2 모슨 논문\n\nF.2.1 과제\n\n1~3명으로 구성된 팀의 일원으로, 다음에서 코드와 데이터를 사용할 수 있는 관심 있는 논문을 선택하십시오:\n\n2019년 이후에 출판된 미국 경제학회 저널의 논문. 이 저널들은 다음과 같습니다: “American Economic Review”, “AER: Insights”, “AEJ: Applied Economics”, “AEJ: Economic Policy”, “AEJ: Macroeconomics”, “AEJ: Microeconomics”, “Journal of Economic Literature”, “Journal of Economic Perspectives”, “AEA Papers & Proceedings”.\n여기에서 “Looking for replicator” 상태인 Institute for Replication 목록의 모든 논문.\n길라드 펠드만의 논문 중 하나.1\n\n사회 과학에서 계산 재현성 가속화를 위한 가이드에 따라, 사회 과학 재현 플랫폼을 사용하여 해당 논문의 최소 세 개의 그래프, 표 또는 조합의 복제2를 완료하십시오. 복제의 DOI를 기록하십시오.\n완전히 재현 가능한 방식으로 작업한 다음, 논문의 두세 가지 측면을 기반으로 재현을 수행하고, 이에 대한 짧은 논문을 작성하십시오.\n\n적절한 하위 폴더가 있는 잘 정리된 폴더를 만들고 GitHub에 추가한 다음, Quarto를 사용하여 제목, 저자, 날짜, 초록, 서론, 데이터, 결과, 토론, 참고 문헌 섹션이 있는 PDF를 준비하십시오 (이 시작 폴더를 사용해야 합니다).\n논문에서 중점을 두는 측면은 복제한 측면과 동일할 수 있지만, 그럴 필요는 없습니다. 논문의 방향을 따르되, 자신만의 것으로 만드십시오. 즉, 약간 다른 질문을 하거나, 동일한 질문에 약간 다른 방식으로 답하되, 동일한 데이터셋을 사용해야 합니다.\n논문에 복제의 DOI와 논문을 뒷받침하는 GitHub 저장소 링크를 포함하십시오.\n결과 섹션은 발견 사항을 전달해야 합니다.\n토론은 각각 흥미로운 점에 초점을 맞춘 세네 개의 하위 섹션과 논문의 약점에 대한 또 다른 하위 섹션, 그리고 잠재적인 다음 단계에 대한 또 다른 하위 섹션을 포함해야 합니다.\n토론 섹션 및 기타 관련 섹션에서는 관련 문헌을 참조하여 윤리 및 편향을 논의해야 합니다.\n논문은 잘 작성되어야 하며, 관련 문헌을 활용하고 모든 기술 개념을 설명해야 합니다. 교육을 받았지만 비전문가인 독자를 대상으로 하십시오.\n지원 자료이지만 중요하지 않은 자료는 부록을 사용하십시오.\n코드는 완전히 재현 가능하고, 잘 문서화되어 있으며, 읽기 쉬워야 합니다.\n\nGitHub 저장소 링크를 제출하십시오. 마감일 이후에는 저장소를 업데이트하지 마십시오.\n이것이 수업 과제라는 증거가 없어야 합니다.\n\n\n\nF.2.2 확인 사항\n\n논문은 원본 논문의 코드를 단순히 복사/붙여넣기하는 것이 아니라, 이를 기반으로 작업해야 합니다.\n논문에는 관련 GitHub 저장소 링크와 수행한 사회 과학 재현 플랫폼 복제의 DOI가 있어야 합니다.\nR을 포함하여 모든 것을 참조했는지 확인하십시오. 강력한 제출물은 토론(및 기타 섹션)에서 관련 문헌을 활용하고 이를 참조할 것입니다. 참고 문헌 스타일은 일관적이라면 중요하지 않습니다.\n\n\n\nF.2.3 FAQ\n\n얼마나 작성해야 합니까? 대부분의 학생들은 10~15페이지 범위의 내용을 제출하지만, 이는 여러분에게 달려 있습니다. 정확하고 철저하게 작성하십시오.\n모델 결과에 중점을 두어야 합니까? 아니요, 이 시점에서는 그것을 피하고 요약 또는 설명 통계의 표나 그래프에 중점을 두는 것이 가장 좋습니다.\n선택한 논문이 R 이외의 언어로 되어 있다면 어떻게 해야 합니까? 복제 및 재현 코드 모두 R로 작성되어야 합니다. 따라서 복제를 위해 코드를 R로 번역해야 합니다. 그리고 재현은 여러분 자신의 작업이어야 하므로, 그것도 R로 작성되어야 합니다. 일반적인 언어는 Stata이며, Huntington-Klein (2022) 는 R, Python, Stata에 대한 일종의 “로제타 스톤”으로 유용할 수 있습니다. 또는 LLM의 도움을 받을 수도 있습니다.\n혼자 작업할 수 있습니까? 예.\n그래프/표가 원본과 동일하게 보여야 합니까? 아니요, 재현의 일부로 더 좋게 보이도록 만들 수 있습니다. 그리고 복제의 일부로도 동일할 필요는 없으며, 충분히 유사하면 됩니다.\n제 그래프 중 하나에 네 개의 패널이 있는데, 이것이 하나의 요소로 간주되려면 모든 패널을 만들어야 합니까? 아니요, 이 논문의 목적상 모든 패널은 별도의 요소로 간주되므로 세 개의 패널만 만들면 충분합니다.\n데이터가 로그인 뒤에 있다면 어떻게 자동으로 다운로드할 수 있습니까? 데이터가 로그인 뒤에 있다면, 코드 대신 download_data.R R 파일에 다운로드 방법을 설명하는 주석을 추가하십시오.\n원본, 편집되지 않은 데이터가 너무 크다면 GitHub에 커밋해야 합니까? 아니요, 너무 크다면 원본, 편집되지 않은 데이터를 GitHub에 커밋할 필요는 없습니다. README에 상황과 데이터를 얻는 방법을 설명하는 메모를 추가하십시오.\n초록과 서론은 무엇에 관한 것이어야 합니까? 초록과 서론은 원본 논문의 내용이 아닌 여러분 자신의 작업과 발견 사항을 반영해야 합니다 (비록 그것들이 여전히 어떤 역할을 할지라도). 여러분은 (거의 확실히) 전체 논문을 복제하는 것이 아니므로 초록은 달라야 합니다. 지침은 예시를 참조하십시오.\n\n\n\nF.2.4 루브릭\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Component\n                Range\n                Requirement\n              \n        \n        \n        \n                \n                  R/Python cited\n                  0 - 'No'; 1 - 'Yes'\n                  R (and/or Python) is properly referenced in the main content and in the reference list. If not, then paper gets 0 overall.\n                \n                \n                  Data cited\n                  0 - 'No'; 1 - 'Yes'\n                  Data are properly referenced in the main content and in the reference list. If not, then paper gets 0 overall.\n                \n                \n                  Class paper\n                  0 - 'No'; 1 - 'Yes'\n                  There is no sign this is a class project. Check the rproj and folder names, the README, the title, code comments, etc. If there is any sign this is a class paper, then paper gets 0 overall.\n                \n                \n                  LLM documentation\n                  0 - 'No'; 1 - 'Yes'\n                  A separate paragraph or dot point must be included in the README about whether LLMs were used, and if so how. If auto-complete tools such as co-pilot were used this must be mentioned. If chat tools such as ChatGPT, were used then the entire chat must be included in the usage text file. If not, then paper gets 0 overall.\n                \n                \n                  Replication\n                  0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Acceptable'; 8 - 'Impressive'; 10 - 'Exceptional'\n                  SSRP submission needs to be filled out completely for three elements.\n                \n                \n                  Title\n                  0 - 'Poor or not done'; 1 - 'Yes'; 2 - 'Exceptional'\n                  An informative title is included that explains the story, and ideally tells the reader what happens at the end of it. 'Paper X' is not an informative title. Use a subtitle to convey the main finding. Do not use puns (you can break this rule once you're experienced).\n                \n                \n                  Author, date, and repo\n                  0 - 'Poor or not done'; 2 - 'Yes'\n                  The author, date of submission in unambiguous format, and a link to a GitHub repo are clearly included. (The later likely, but not necessarily, through a statement such as: 'Code and data supporting this analysis is available at: LINK').\n                \n                \n                  Abstract\n                  0 - 'Poor or not done'; 1 - 'Some issues'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  An abstract is included and appropriately pitched to a non-specialist audience. The abstract answers: 1) what was done, 2) what was found, and 3) why this matters (all at a high level). Likely four sentences. Abstract must make clear what we learn about the world because of this paper.\n                \n                \n                  Introduction\n                  0 - 'Poor or not done'; 1 - 'Some issues'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  The introduction is self-contained and tells a reader everything they need to know including: 1) broader context to motivate; 2) some detail about what the paper is about; 3) a clear gap that needs to be filled; 4) what was done; 5) what was found; 6) why it is important; 7) the structure of the paper. A reader should be able to read only the introduction and know what was done, why, and what was found. Likely 3 or 4 paragraphs, or 10 per cent of total.\n                \n                \n                  Estimand\n                  0 - 'Poor or not done'; 1 - 'Done'\n                  The estimand is clearly stated, in its own paragraph, in the introduction.\n                \n                \n                  Data\n                  0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Acceptable'; 8 - 'Impressive'; 10 - 'Exceptional'\n                  A sense of the dataset should be communicated to the reader. The broader context of the dataset should be discussed. All variables should be thoroughly examined and explained. Explain if there were similar datasets that could have been used and why they were not. If variables were constructed then this should be mentioned, and high-level cleaning aspects of note should be mentioned, but this section should focus on the destination, not the journey. It is important to understand what the variables look like by including graphs, and possibly tables, of all observations, along with discussion of those graphs and the other features of these data. Summary statistics should also be included, and well as any relationships between the variables. You are not doing EDA in this section--you are talking the reader through the variables that are of interest. If this becomes too detailed, then appendices could be used. Basically, for every variable in your dataset that is of interest to your paper there needs to be graphs and explanation and maybe tables.\n                \n                \n                  Measurement\n                  0 - 'Poor or not done'; 2 - 'Some issues'; 3 - 'Acceptable'; 4 - 'Exceptional'\n                  A thorough discussion of measurement, relating to the dataset, is provided in the data section. Please ensure that you explain how we went from some phenomena in the world that happened to an entry in the dataset that you are interested in.\n                \n                \n                  Results\n                  0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Acceptable'; 8 - 'Impressive'; 10 - 'Exceptional'\n                  Results will likely require summary statistics, tables, graphs, images, and possibly statistical analysis or maps. There should also be text associated with all these aspects. Show the reader the results by plotting them where possible. Talk about them. Explain them. That said, this section should strictly relay results. Regression tables must not contain stars. Use modelsummary to include a table and graph of the estimates.\n                \n                \n                  Discussion\n                  0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Acceptable'; 8 - 'Impressive'; 10 - 'Exceptional'\n                  Some questions that a good discussion would cover include (each of these would be a sub-section of something like half a page to a page): What is done in this paper? What is something that we learn about the world? What is another thing that we learn about the world? What are some weaknesses of what was done? What is left to learn or how should we proceed in the future?\n                \n                \n                  Prose\n                  0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Acceptable'; 6 - 'Exceptional'\n                  All aspects of submission should be free of noticeable typos, spelling mistakes, and be grammatically correct. Prose should be coherent, concise, clear, and mature. Remove unnecessary words. Do not use the following words/phrases: 'actionable insights', 'address/es/ing a/that/the/this gap', 'advanced', 'all-encompassing', 'apt', 'backdrop', 'beg/s the question', 'bridge/s a/that/the/this gap', 'clear gap', 'compelling', 'comprehensive', 'critical', 'crucial', 'data driven', 'data-driven', 'delve/s', 'drastic', 'drive/s forward', 'elucidate/ing', 'embark/s', 'exploration', 'fill/s a/that/the/this gap', 'fresh perspective/s', 'hidden factor/s', 'immense', 'imperative', 'indispensable', 'insight/s', 'insight/s from', 'interrogate', 'intricate', 'intriguing', 'invaluable', 'key driver/s', 'key insight/s', 'kind of', 'leverage', 'meticulous/ly', 'multifaceted', 'novel', 'nuance', 'offer/s/ing crucial insight', 'pivotal', 'plummeted', 'profound', 'rapidly', 'reveals', 'shed/s light', 'shocking', 'soared', 'unparalleled', 'unveiling', 'valuable', 'wanna'.\n                \n                \n                  Cross-references\n                  0 - 'Poor or not done'; 1 - 'Yes'\n                  All figures, tables, and equations, should be numbered, and referred to in the text using cross-references. The telegraphing paragraph in the introduction should cross reference the rest of the paper.\n                \n                \n                  Captions\n                  0 - 'Poor or not done'; 1 - 'Acceptable'; 2 - 'Excellent'\n                  All figures and tables have detailed and meaningful captions. They should be sufficiently detailed so as to make the main point of the figure/table clear even without the accompanying text. Do not say 'Histogram of...' or whatever else the figure type is.\n                \n                \n                  Graphs and tables\n                  0 - 'Poor or not done'; 1 - 'Some issues'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  Graphs and tables must be included in the paper and should be to well-formatted, clear, and digestible. Graphs should be made using ggplot2 and tables should be made using tinytable. They should serve a clear purpose and be fully self-contained. Graphs and tables should be appropriately sized, colored, and labelled. Variable names should not be used as labels. Tables should have an appropriate number of decimal places and use comma separators for thousands. Don't use boxplots, but if you must then you must overlay the actual data.\n                \n                \n                  Referencing\n                  0 - 'Poor or not done'; 3 - 'One minor issue'; 4 - 'Perfect'\n                  All data, software, literature, and any other relevant material, should be cited in-text and included in a properly formatted reference list made using BibTeX. A few lines of code from Stack Overflow or similar, would be acknowledged just with a comment in the script immediately preceding the use of the code rather than here. But larger chunks of code should be fully acknowledged with an in-text citation and appear in the reference list. Check in-text citations and that you have not accidentally used (@my_cite) when you needed [@my_cite]. R packages and all other aspects should be correctly capitalized, and name should be correct e.g. use double braces appropriately in the BibTeX file.\n                \n                \n                  Commits\n                  0 - 'Poor or not done'; 2 - 'Done'\n                  There are at least a handful of different commits, and they have meaningful commit messages.\n                \n                \n                  Sketches\n                  0 - 'Poor or not done'; 2 - 'Done'\n                  Sketches are included in a labelled folder of the repo, appropriate, and of high-quality.\n                \n                \n                  Simulation\n                  0 - 'Poor or not done'; 1 - 'Some issues'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  The script is clearly commented and structured. All variables are appropriately simulated in a sophisticated way including appropriate interaction between simulated variables.\n                \n                \n                  Tests\n                  0 - 'Poor or not done'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  High-quality extensive suites of tests are written for the both the simulated and actual datasets. These suites must be in separate scripts. The suite should be extensive and put together in a sophisticated way using packages like testthat, validate, pointblank, or great expectations.\n                \n                \n                  Reproducible workflow\n                  0 - 'Poor or not done'; 1 - 'Some issues'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  Use an organized repo with a detailed README and an R project. Thoroughly document code and include a preamble, comments, nice structure, and style code with styler or lintr. Use seeds appropriately. Avoid leaving install.packages() in the code unless handled sophisticatedly. Exclude unnecessary files from the repo; avoid hard-coded paths and setwd(). Use base pipe not magrittr pipe. Comment on and close all GitHub issues. Deal with all branches.\n                \n                \n                  Miscellaneous\n                  0 - 'None'; 1 - 'Notable'; 2 - 'Remarkable'; 3 - 'Exceptional'\n                  There are always students that excel in a way that is not anticipated in the rubric. This item accounts for that.\n                \n        \n      \n    \n\n\n\n\n\nF.2.5 이전 예시\n\n2024: 베니 로크버그; 크리시브 자인, 줄리아 김, 아바스 슬레이만; 사미 엘 사브리, 리반 티미르; 토마스 폭스; 그리고 위안이 (레오) 리우, 치 얼 (엠마) 텅.\n2023: 제이든 정, 핀 코롤, 소피아 셀리토.\n2022: 알리사 슐라이퍼, 허드슨 위엔, 탐센 야우; 올라에도 오크파레케, 아르쉬 라칸팔, 스와르나딥 차토파디아이; 그리고 킴린 친.",
    "crumbs": [
      "부록",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>논문</span>"
    ]
  },
  {
    "objectID": "25-papers_ko.html#sec-paper-three",
    "href": "25-papers_ko.html#sec-paper-three",
    "title": "Online Appendix F — 논문",
    "section": "F.3 하우라 논문",
    "text": "F.3 하우라 논문\n\nF.3.1 과제\n\n1~3명으로 구성된 팀의 일원으로, 완전히 재현 가능한 방식으로 미국 일반 사회 조사에서 데이터를 얻으십시오.3 (다른 정부 운영 설문조사를 사용해도 좋지만, 시작하기 전에 허가를 받으십시오.)\n데이터를 얻고, 설문조사의 한 측면에 초점을 맞춘 다음, 이를 사용하여 이야기를 전달하십시오.\n\n적절한 하위 폴더가 있는 잘 정리된 폴더를 만들고 GitHub에 추가한 다음, Quarto를 사용하여 제목, 저자, 날짜, 초록, 서론, 데이터, 결과, 토론, 최소한 설문조사를 포함하는 부록, 참고 문헌 섹션이 있는 PDF를 준비하십시오 (이 시작 폴더를 사용해야 합니다).\n관심 있는 데이터셋에 대한 감각을 전달하는 것 외에도, 데이터 섹션에는 다음이 포함되어야 하지만 이에 국한되지 않습니다:\n\n설문조사 방법론 및 주요 특징, 강점, 약점에 대한 논의. 예를 들어: 모집단, 프레임, 표본은 무엇이며; 표본은 어떻게 모집되며; 어떤 샘플링 접근 방식이 취해지고, 이의 장단점은 무엇이며; 무응답은 어떻게 처리되는가.\n설문지 논의: 무엇이 좋고 나쁜가?\n너무 자세해지면, 지원 자료이지만 필수적이지 않은 측면은 부록을 사용하십시오.\n\n부록에, 논문이 중점을 두는 일반 사회 조사를 보완하는 데 사용할 수 있는 보충 설문조사를 작성하십시오. 보충 설문조사의 목적은 일반 사회 조사에서 수집된 것 외에 논문의 초점인 주제에 대한 추가 정보를 얻는 것입니다. 설문조사는 일반 사회 조사와 동일한 방식으로 배포되지만 독립적으로 존재해야 합니다. 보충 설문조사는 설문조사 플랫폼을 사용하여 작성되어야 합니다. 이에 대한 링크는 부록에 포함되어야 합니다. 또한 설문조사 사본도 부록에 포함되어야 합니다.\n관련 문헌을 참조하여 윤리 및 편향을 논의해야 합니다.\n코드는 완전히 재현 가능하고, 잘 문서화되어 있으며, 읽기 쉬워야 합니다.\n\nGitHub 저장소 링크를 제출하십시오. 마감일 이후에는 저장소를 업데이트하지 마십시오.\n논문은 잘 작성되어야 하며, 관련 문헌을 활용하고 모든 기술 개념을 설명해야 합니다. 대학 교육을 받았지만 비전문가인 독자를 대상으로 하십시오. 설문조사, 샘플링, 통계 용어를 사용하되, 반드시 설명해야 합니다. 논문은 흐름이 자연스럽고 이해하기 쉬워야 합니다.\n이것이 수업 논문이라는 증거가 없어야 합니다.\n\n\n\nF.3.2 확인 사항\n\n부록에는 보충 설문조사 링크와 질문을 포함한 세부 정보가 모두 포함되어야 합니다 (링크가 실패할 경우를 대비하고 논문을 자체 포함하기 위함).\n\n\n\nF.3.3 FAQ\n\n무엇에 중점을 두어야 합니까? 관심 있는 일반 사회 조사의 초점과 제약을 고려하여 합리적인 연도, 측면 또는 지리에 중점을 둘 수 있습니다. 일부 설문조사는 특정 연도에 특정 주제에 중점을 두므로 관심 있는 연도와 주제를 함께 고려하십시오.\n원시 GSS 데이터를 저장소에 포함해야 합니까? 대부분의 일반 사회 조사의 경우 GSS 데이터를 공유할 권한이 없을 것입니다. 이 경우 README에 데이터를 얻는 방법을 설명하는 명확한 세부 정보를 추가해야 합니다.\n그래프는 몇 개가 필요합니까? 일반적으로 변수 수만큼의 그래프가 필요합니다. 모든 변수에 대한 모든 관측치를 보여주어야 하기 때문입니다. 그러나 몇 개를 결합할 수도 있습니다. 또는 그 반대로, 다른 측면이나 관계를 살펴보는 데 관심이 있을 수도 있습니다.\n\n\n\nF.3.4 루브릭\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Component\n                Range\n                Requirement\n              \n        \n        \n        \n                \n                  R/Python cited\n                  0 - 'No'; 1 - 'Yes'\n                  R (and/or Python) is properly referenced in the main content and in the reference list. If not, then paper gets 0 overall.\n                \n                \n                  Data cited\n                  0 - 'No'; 1 - 'Yes'\n                  Data are properly referenced in the main content and in the reference list. If not, then paper gets 0 overall.\n                \n                \n                  Class paper\n                  0 - 'No'; 1 - 'Yes'\n                  There is no sign this is a class project. Check the rproj and folder names, the README, the title, code comments, etc. If there is any sign this is a class paper, then paper gets 0 overall.\n                \n                \n                  LLM documentation\n                  0 - 'No'; 1 - 'Yes'\n                  A separate paragraph or dot point must be included in the README about whether LLMs were used, and if so how. If auto-complete tools such as co-pilot were used this must be mentioned. If chat tools such as ChatGPT, were used then the entire chat must be included in the usage text file. If not, then paper gets 0 overall.\n                \n                \n                  Title\n                  0 - 'Poor or not done'; 1 - 'Yes'; 2 - 'Exceptional'\n                  An informative title is included that explains the story, and ideally tells the reader what happens at the end of it. 'Paper X' is not an informative title. Use a subtitle to convey the main finding. Do not use puns (you can break this rule once you're experienced).\n                \n                \n                  Author, date, and repo\n                  0 - 'Poor or not done'; 2 - 'Yes'\n                  The author, date of submission in unambiguous format, and a link to a GitHub repo are clearly included. (The later likely, but not necessarily, through a statement such as: 'Code and data supporting this analysis is available at: LINK').\n                \n                \n                  Abstract\n                  0 - 'Poor or not done'; 1 - 'Some issues'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  An abstract is included and appropriately pitched to a non-specialist audience. The abstract answers: 1) what was done, 2) what was found, and 3) why this matters (all at a high level). Likely four sentences. Abstract must make clear what we learn about the world because of this paper.\n                \n                \n                  Introduction\n                  0 - 'Poor or not done'; 1 - 'Some issues'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  The introduction is self-contained and tells a reader everything they need to know including: 1) broader context to motivate; 2) some detail about what the paper is about; 3) a clear gap that needs to be filled; 4) what was done; 5) what was found; 6) why it is important; 7) the structure of the paper. A reader should be able to read only the introduction and know what was done, why, and what was found. Likely 3 or 4 paragraphs, or 10 per cent of total.\n                \n                \n                  Estimand\n                  0 - 'Poor or not done'; 1 - 'Done'\n                  The estimand is clearly stated, in its own paragraph, in the introduction.\n                \n                \n                  Data\n                  0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Acceptable'; 8 - 'Impressive'; 10 - 'Exceptional'\n                  A sense of the dataset should be communicated to the reader. The broader context of the dataset should be discussed. All variables should be thoroughly examined and explained. Explain if there were similar datasets that could have been used and why they were not. If variables were constructed then this should be mentioned, and high-level cleaning aspects of note should be mentioned, but this section should focus on the destination, not the journey. It is important to understand what the variables look like by including graphs, and possibly tables, of all observations, along with discussion of those graphs and the other features of these data. Summary statistics should also be included, and well as any relationships between the variables. You are not doing EDA in this section--you are talking the reader through the variables that are of interest. If this becomes too detailed, then appendices could be used. Basically, for every variable in your dataset that is of interest to your paper there needs to be graphs and explanation and maybe tables.\n                \n                \n                  Measurement\n                  0 - 'Poor or not done'; 2 - 'Some issues'; 3 - 'Acceptable'; 4 - 'Exceptional'\n                  A thorough discussion of measurement, relating to the dataset, is provided in the data section. Please ensure that you explain how we went from some phenomena in the world that happened to an entry in the dataset that you are interested in.\n                \n                \n                  Results\n                  0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Acceptable'; 8 - 'Impressive'; 10 - 'Exceptional'\n                  Results will likely require summary statistics, tables, graphs, images, and possibly statistical analysis or maps. There should also be text associated with all these aspects. Show the reader the results by plotting them where possible. Talk about them. Explain them. That said, this section should strictly relay results. Regression tables must not contain stars. Use modelsummary to include a table and graph of the estimates.\n                \n                \n                  Discussion\n                  0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Acceptable'; 8 - 'Impressive'; 10 - 'Exceptional'\n                  Some questions that a good discussion would cover include (each of these would be a sub-section of something like half a page to a page): What is done in this paper? What is something that we learn about the world? What is another thing that we learn about the world? What are some weaknesses of what was done? What is left to learn or how should we proceed in the future?\n                \n                \n                  Prose\n                  0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Acceptable'; 6 - 'Exceptional'\n                  All aspects of submission should be free of noticeable typos, spelling mistakes, and be grammatically correct. Prose should be coherent, concise, clear, and mature. Remove unnecessary words. Do not use the following words/phrases: 'actionable insights', 'address/es/ing a/that/the/this gap', 'advanced', 'all-encompassing', 'apt', 'backdrop', 'beg/s the question', 'bridge/s a/that/the/this gap', 'clear gap', 'compelling', 'comprehensive', 'critical', 'crucial', 'data driven', 'data-driven', 'delve/s', 'drastic', 'drive/s forward', 'elucidate/ing', 'embark/s', 'exploration', 'fill/s a/that/the/this gap', 'fresh perspective/s', 'hidden factor/s', 'immense', 'imperative', 'indispensable', 'insight/s', 'insight/s from', 'interrogate', 'intricate', 'intriguing', 'invaluable', 'key driver/s', 'key insight/s', 'kind of', 'leverage', 'meticulous/ly', 'multifaceted', 'novel', 'nuance', 'offer/s/ing crucial insight', 'pivotal', 'plummeted', 'profound', 'rapidly', 'reveals', 'shed/s light', 'shocking', 'soared', 'unparalleled', 'unveiling', 'valuable', 'wanna'.\n                \n                \n                  Cross-references\n                  0 - 'Poor or not done'; 1 - 'Yes'\n                  All figures, tables, and equations, should be numbered, and referred to in the text using cross-references. The telegraphing paragraph in the introduction should cross reference the rest of the paper.\n                \n                \n                  Captions\n                  0 - 'Poor or not done'; 1 - 'Acceptable'; 2 - 'Excellent'\n                  All figures and tables have detailed and meaningful captions. They should be sufficiently detailed so as to make the main point of the figure/table clear even without the accompanying text. Do not say 'Histogram of...' or whatever else the figure type is.\n                \n                \n                  Graphs and tables\n                  0 - 'Poor or not done'; 1 - 'Some issues'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  Graphs and tables must be included in the paper and should be to well-formatted, clear, and digestible. Graphs should be made using ggplot2 and tables should be made using tinytable. They should serve a clear purpose and be fully self-contained. Graphs and tables should be appropriately sized, colored, and labelled. Variable names should not be used as labels. Tables should have an appropriate number of decimal places and use comma separators for thousands. Don't use boxplots, but if you must then you must overlay the actual data.\n                \n                \n                  Pollster review\n                  0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Acceptable'; 8 - 'Impressive'; 10 - 'Exceptional'\n                  The evaluation provides a thorough understanding of how something goes from being a person's opinion to part of a result for this pollster. Provide a thorough overview and evaluation of the pollster's methodology, and sampling approach, highlighting both its strengths and limitations. Use this section to demonstrate knowledge of surveys and sampling and link your evaluation to the literature. Be precise and scientific--your review should not sound like an ad for the pollster.\n                \n                \n                  Idealized survey\n                  0 - 'Poor or not done'; 1 - 'Some issues'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  The survey should have an introductory section and include the details of a contact person. The survey questions should be well constructed and appropriate to the task. The questions should have an appropriate ordering. A final section should thank the respondent. Question type should be varied and appropriate. Use this section to demonstrate knowledge of surveys.\n                \n                \n                  Referencing\n                  0 - 'Poor or not done'; 3 - 'One minor issue'; 4 - 'Perfect'\n                  All data, software, literature, and any other relevant material, should be cited in-text and included in a properly formatted reference list made using BibTeX. A few lines of code from Stack Overflow or similar, would be acknowledged just with a comment in the script immediately preceding the use of the code rather than here. But larger chunks of code should be fully acknowledged with an in-text citation and appear in the reference list. Check in-text citations and that you have not accidentally used (@my_cite) when you needed [@my_cite]. R packages and all other aspects should be correctly capitalized, and name should be correct e.g. use double braces appropriately in the BibTeX file.\n                \n                \n                  Commits\n                  0 - 'Poor or not done'; 2 - 'Done'\n                  There are at least a handful of different commits, and they have meaningful commit messages.\n                \n                \n                  Sketches\n                  0 - 'Poor or not done'; 2 - 'Done'\n                  Sketches are included in a labelled folder of the repo, appropriate, and of high-quality.\n                \n                \n                  Simulation\n                  0 - 'Poor or not done'; 1 - 'Some issues'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  The script is clearly commented and structured. All variables are appropriately simulated in a sophisticated way including appropriate interaction between simulated variables.\n                \n                \n                  Tests\n                  0 - 'Poor or not done'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  High-quality extensive suites of tests are written for the both the simulated and actual datasets. These suites must be in separate scripts. The suite should be extensive and put together in a sophisticated way using packages like testthat, validate, pointblank, or great expectations.\n                \n                \n                  Reproducible workflow\n                  0 - 'Poor or not done'; 1 - 'Some issues'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  Use an organized repo with a detailed README and an R project. Thoroughly document code and include a preamble, comments, nice structure, and style code with styler or lintr. Use seeds appropriately. Avoid leaving install.packages() in the code unless handled sophisticatedly. Exclude unnecessary files from the repo; avoid hard-coded paths and setwd(). Use base pipe not magrittr pipe. Comment on and close all GitHub issues. Deal with all branches.\n                \n                \n                  Miscellaneous\n                  0 - 'None'; 1 - 'Notable'; 2 - 'Remarkable'; 3 - 'Exceptional'\n                  There are always students that excel in a way that is not anticipated in the rubric. This item accounts for that.\n                \n        \n      \n    \n\n\n\n\n\nF.3.5 이전 예시\n\n2023: 크리스티나 웨이, 미카엘라 드루일라드; 그리고 이네사 드 안젤리스.\n2022: 안나 리, 모하마드 사르다르 셰이크; 차이나 후이, 마르코 차우; 이든 샌섬; 럭키나 로랑, 사미타 프라바사바트, 조이 소; 파스칼 리 슬루, 윤경 박; 그리고 레이 웬, 이스판디아르 비라니, 레이한 왈리아.",
    "crumbs": [
      "부록",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>논문</span>"
    ]
  },
  {
    "objectID": "25-papers_ko.html#sec-paper-four",
    "href": "25-papers_ko.html#sec-paper-four",
    "title": "Online Appendix F — 논문",
    "section": "F.4 다이사트 논문",
    "text": "F.4 다이사트 논문\n\nF.4.1 과제\n\n1~3명으로 구성된 팀의 일원으로, 완전히 재현 가능한 방식으로 다음에서 사용 가능한 DHS 프로그램 “최종 보고서” 중 1980년대 또는 1990년대의 최소 한 페이지 전체 표를 사용 가능한 데이터셋으로 변환한 다음, 데이터를 사용하여 이야기를 담은 짧은 논문을 작성하십시오. 여기\n적절한 하위 폴더가 있는 잘 정리된 폴더를 만들고 GitHub에 추가하십시오. 이 시작 폴더를 사용해야 합니다.\n데이터셋을 생성하고 문서화하십시오:\n\nPDF를 “inputs”에 저장하십시오.\n사용 가능한 데이터셋에 대한 계획 시뮬레이션을 작성하고 스크립트를 “scripts/00-simulation.R”에 저장하십시오.\n적절하게 PDF를 OCR하거나 파싱하는 R 코드를 작성하고 “scripts/01-gather_data.R”로 저장한 다음, 출력을 “outputs/data/first_parse.csv”에 저장하십시오.\n“first_parse.csv”를 기반으로 데이터셋을 정리하고 준비하는 R 코드를 작성하고 “scripts/02-clean_and_prepare_data.R”로 저장하십시오. pointblank를 사용하여 데이터셋이 통과하는 테스트를 작성하십시오 (최소한 모든 변수에 대해 클래스 테스트와 내용 테스트가 있어야 합니다). 데이터셋을 “outputs/data/cleaned_data.parquet”에 저장하십시오.\nGebru 기타 (2021) 에 따라 작성한 데이터셋에 대한 데이터 시트를 작성하십시오 (논문의 부록에 넣으십시오). 시작 폴더의 템플릿 “inputs/data/datasheet_template.qmd”에서 시작해도 좋지만, 다시 말하지만, 독립적인 문서가 아닌 논문의 부록에 추가해야 합니다.\n\nQuarto를 사용하여 제목, 저자, 날짜, 초록, 서론, 데이터, 결과, 토론, 최소한 데이터셋에 대한 데이터 시트를 포함하는 부록, 참고 문헌 섹션이 있는 PDF를 준비하여 데이터로 이야기를 전달하십시오.\n\n관심 있는 데이터셋에 대한 감각을 전달하는 것 외에도, 데이터 섹션에는 사용한 DHS의 방법론 및 주요 특징, 강점, 약점에 대한 세부 정보가 포함되어야 합니다.\n\nGitHub 저장소 링크를 제출하십시오. 마감일 이후에는 저장소를 업데이트하지 마십시오.\n이것이 수업 논문이라는 증거가 없어야 합니다.\n\n\n\nF.4.2 확인 사항\n\n최소한 몇 번의 커밋을 하고 설명적인 커밋 메시지를 사용하여 GitHub를 잘 활용하십시오.\n\n\n\n\nF.4.3 FAQ\n\n\n\n\n\n\nF.4.4 루브릭\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Component\n                Range\n                Requirement\n              \n        \n        \n        \n                \n                  R/Python cited\n                  0 - 'No'; 1 - 'Yes'\n                  R (and/or Python) is properly referenced in the main content and in the reference list. If not, then paper gets 0 overall.\n                \n                \n                  Data cited\n                  0 - 'No'; 1 - 'Yes'\n                  Data are properly referenced in the main content and in the reference list. If not, then paper gets 0 overall.\n                \n                \n                  Class paper\n                  0 - 'No'; 1 - 'Yes'\n                  There is no sign this is a class project. Check the rproj and folder names, the README, the title, code comments, etc. If there is any sign this is a class paper, then paper gets 0 overall.\n                \n                \n                  LLM documentation\n                  0 - 'No'; 1 - 'Yes'\n                  A separate paragraph or dot point must be included in the README about whether LLMs were used, and if so how. If auto-complete tools such as co-pilot were used this must be mentioned. If chat tools such as ChatGPT, were used then the entire chat must be included in the usage text file. If not, then paper gets 0 overall.\n                \n                \n                  Title\n                  0 - 'Poor or not done'; 1 - 'Yes'; 2 - 'Exceptional'\n                  An informative title is included that explains the story, and ideally tells the reader what happens at the end of it. 'Paper X' is not an informative title. Use a subtitle to convey the main finding. Do not use puns (you can break this rule once you're experienced).\n                \n                \n                  Author, date, and repo\n                  0 - 'Poor or not done'; 2 - 'Yes'\n                  The author, date of submission in unambiguous format, and a link to a GitHub repo are clearly included. (The later likely, but not necessarily, through a statement such as: 'Code and data supporting this analysis is available at: LINK').\n                \n                \n                  Abstract\n                  0 - 'Poor or not done'; 1 - 'Some issues'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  An abstract is included and appropriately pitched to a non-specialist audience. The abstract answers: 1) what was done, 2) what was found, and 3) why this matters (all at a high level). Likely four sentences. Abstract must make clear what we learn about the world because of this paper.\n                \n                \n                  Introduction\n                  0 - 'Poor or not done'; 1 - 'Some issues'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  The introduction is self-contained and tells a reader everything they need to know including: 1) broader context to motivate; 2) some detail about what the paper is about; 3) a clear gap that needs to be filled; 4) what was done; 5) what was found; 6) why it is important; 7) the structure of the paper. A reader should be able to read only the introduction and know what was done, why, and what was found. Likely 3 or 4 paragraphs, or 10 per cent of total.\n                \n                \n                  Estimand\n                  0 - 'Poor or not done'; 1 - 'Done'\n                  The estimand is clearly stated, in its own paragraph, in the introduction.\n                \n                \n                  Data\n                  0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Acceptable'; 8 - 'Impressive'; 10 - 'Exceptional'\n                  A sense of the dataset should be communicated to the reader. The broader context of the dataset should be discussed. All variables should be thoroughly examined and explained. Explain if there were similar datasets that could have been used and why they were not. If variables were constructed then this should be mentioned, and high-level cleaning aspects of note should be mentioned, but this section should focus on the destination, not the journey. It is important to understand what the variables look like by including graphs, and possibly tables, of all observations, along with discussion of those graphs and the other features of these data. Summary statistics should also be included, and well as any relationships between the variables. You are not doing EDA in this section--you are talking the reader through the variables that are of interest. If this becomes too detailed, then appendices could be used. Basically, for every variable in your dataset that is of interest to your paper there needs to be graphs and explanation and maybe tables.\n                \n                \n                  Measurement\n                  0 - 'Poor or not done'; 2 - 'Some issues'; 3 - 'Acceptable'; 4 - 'Exceptional'\n                  A thorough discussion of measurement, relating to the dataset, is provided in the data section. Please ensure that you explain how we went from some phenomena in the world that happened to an entry in the dataset that you are interested in.\n                \n                \n                  Results\n                  0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Acceptable'; 8 - 'Impressive'; 10 - 'Exceptional'\n                  Results will likely require summary statistics, tables, graphs, images, and possibly statistical analysis or maps. There should also be text associated with all these aspects. Show the reader the results by plotting them where possible. Talk about them. Explain them. That said, this section should strictly relay results. Regression tables must not contain stars. Use modelsummary to include a table and graph of the estimates.\n                \n                \n                  Discussion\n                  0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Acceptable'; 8 - 'Impressive'; 10 - 'Exceptional'\n                  Some questions that a good discussion would cover include (each of these would be a sub-section of something like half a page to a page): What is done in this paper? What is something that we learn about the world? What is another thing that we learn about the world? What are some weaknesses of what was done? What is left to learn or how should we proceed in the future?\n                \n                \n                  Prose\n                  0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Acceptable'; 6 - 'Exceptional'\n                  All aspects of submission should be free of noticeable typos, spelling mistakes, and be grammatically correct. Prose should be coherent, concise, clear, and mature. Remove unnecessary words. Do not use the following words/phrases: 'actionable insights', 'address/es/ing a/that/the/this gap', 'advanced', 'all-encompassing', 'apt', 'backdrop', 'beg/s the question', 'bridge/s a/that/the/this gap', 'clear gap', 'compelling', 'comprehensive', 'critical', 'crucial', 'data driven', 'data-driven', 'delve/s', 'drastic', 'drive/s forward', 'elucidate/ing', 'embark/s', 'exploration', 'fill/s a/that/the/this gap', 'fresh perspective/s', 'hidden factor/s', 'immense', 'imperative', 'indispensable', 'insight/s', 'insight/s from', 'interrogate', 'intricate', 'intriguing', 'invaluable', 'key driver/s', 'key insight/s', 'kind of', 'leverage', 'meticulous/ly', 'multifaceted', 'novel', 'nuance', 'offer/s/ing crucial insight', 'pivotal', 'plummeted', 'profound', 'rapidly', 'reveals', 'shed/s light', 'shocking', 'soared', 'unparalleled', 'unveiling', 'valuable', 'wanna'.\n                \n                \n                  Cross-references\n                  0 - 'Poor or not done'; 1 - 'Yes'\n                  All figures, tables, and equations, should be numbered, and referred to in the text using cross-references. The telegraphing paragraph in the introduction should cross reference the rest of the paper.\n                \n                \n                  Captions\n                  0 - 'Poor or not done'; 1 - 'Acceptable'; 2 - 'Excellent'\n                  All figures and tables have detailed and meaningful captions. They should be sufficiently detailed so as to make the main point of the figure/table clear even without the accompanying text. Do not say 'Histogram of...' or whatever else the figure type is.\n                \n                \n                  Graphs and tables\n                  0 - 'Poor or not done'; 1 - 'Some issues'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  Graphs and tables must be included in the paper and should be to well-formatted, clear, and digestible. Graphs should be made using ggplot2 and tables should be made using tinytable. They should serve a clear purpose and be fully self-contained. Graphs and tables should be appropriately sized, colored, and labelled. Variable names should not be used as labels. Tables should have an appropriate number of decimal places and use comma separators for thousands. Don't use boxplots, but if you must then you must overlay the actual data.\n                \n                \n                  Referencing\n                  0 - 'Poor or not done'; 3 - 'One minor issue'; 4 - 'Perfect'\n                  All data, software, literature, and any other relevant material, should be cited in-text and included in a properly formatted reference list made using BibTeX. A few lines of code from Stack Overflow or similar, would be acknowledged just with a comment in the script immediately preceding the use of the code rather than here. But larger chunks of code should be fully acknowledged with an in-text citation and appear in the reference list. Check in-text citations and that you have not accidentally used (@my_cite) when you needed [@my_cite]. R packages and all other aspects should be correctly capitalized, and name should be correct e.g. use double braces appropriately in the BibTeX file.\n                \n                \n                  Commits\n                  0 - 'Poor or not done'; 2 - 'Done'\n                  There are at least a handful of different commits, and they have meaningful commit messages.\n                \n                \n                  Sketches\n                  0 - 'Poor or not done'; 2 - 'Done'\n                  Sketches are included in a labelled folder of the repo, appropriate, and of high-quality.\n                \n                \n                  Simulation\n                  0 - 'Poor or not done'; 1 - 'Some issues'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  The script is clearly commented and structured. All variables are appropriately simulated in a sophisticated way including appropriate interaction between simulated variables.\n                \n                \n                  Tests\n                  0 - 'Poor or not done'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  High-quality extensive suites of tests are written for the both the simulated and actual datasets. These suites must be in separate scripts. The suite should be extensive and put together in a sophisticated way using packages like testthat, validate, pointblank, or great expectations.\n                \n                \n                  Parquet\n                  0 - 'No'; 1 - 'Yes'\n                  The analysis dataset is saved as a parquet file. (Note that the raw data should be saved in whatever format it came.)\n                \n                \n                  Reproducible workflow\n                  0 - 'Poor or not done'; 1 - 'Some issues'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  Use an organized repo with a detailed README and an R project. Thoroughly document code and include a preamble, comments, nice structure, and style code with styler or lintr. Use seeds appropriately. Avoid leaving install.packages() in the code unless handled sophisticatedly. Exclude unnecessary files from the repo; avoid hard-coded paths and setwd(). Use base pipe not magrittr pipe. Comment on and close all GitHub issues. Deal with all branches.\n                \n                \n                  Datasheet\n                  0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Acceptable'; 8 - 'Impressive'; 10 - 'Exceptional'\n                  A thorough datasheet for the dataset that was constructed is included.\n                \n                \n                  Miscellaneous\n                  0 - 'None'; 1 - 'Notable'; 2 - 'Remarkable'; 3 - 'Exceptional'\n                  There are always students that excel in a way that is not anticipated in the rubric. This item accounts for that.\n                \n        \n      \n    \n\n\n\n\n\nF.4.5 이전 예시\n\n2022: 빌랄 하크, 릿빅 푸리; 찰스 루, 마하크 자인, 위준 지아오; 제이콥 요크 홍 시; 그리고 파스칼 리 슬루, 윤경 박.",
    "crumbs": [
      "부록",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>논문</span>"
    ]
  },
  {
    "objectID": "25-papers_ko.html#sec-spadina",
    "href": "25-papers_ko.html#sec-spadina",
    "title": "Online Appendix F — 논문",
    "section": "F.5 스파디나 논문",
    "text": "F.5 스파디나 논문\n\nF.5.1 과제\n\n1~3명으로 구성된 팀의 일원으로, 완전히 재현 가능한 방식으로 선형 또는 일반화 선형 모델을 구축한 다음, 이야기를 담은 짧은 논문을 작성하십시오. 다룰 수 있는 측면에 대한 아이디어는 다음과 같습니다:\n\n섹션 F.1 에서 사용한 데이터셋을 다시 방문하십시오. 변수 중 하나에 대한 선형 모델을 구축하고 결과를 고려하십시오.\n장 13 의 예시 중 하나를 선택하고 상황을 약간 변경한 다음, 일반화 선형 모델을 구축하십시오.\n\n이 시작 폴더를 사용해야 합니다.\nGitHub 저장소 링크를 제출하십시오. 마감일 이후에는 저장소를 업데이트하지 마십시오.\n이것이 수업 논문이라는 증거가 없어야 합니다.\n\n\n\nF.5.2 확인 사항\n\n모델을 철저히 설명하도록 주의하십시오. 또한 모델의 가정과 유효성에 대한 위협도 고려하십시오.\n\n\n\nF.5.3 FAQ\n\n“상황을 약간 변경한다”는 것은 무엇을 의미합니까? 동일하거나 유사한 데이터를 사용해도 좋지만, 다른 측면을 고려하십시오. 예를 들어:\n\n미국 정치 지지에 대한 로지스틱 회귀 예시에서, 다른 연도의 CES를 사용하거나 약간 다른 설명 변수를 사용할 수 있습니다.\n제인 에어에 사용된 문자의 포아송 회귀 예시에서, 다른 소설을 고려할 수 있습니다.\n앨버타 사망률의 음이항 회귀에서, 다른 지리적 영역을 고려할 수 있습니다.\n\n앨버타 사망률 데이터를 사용할 수 있습니까? 아니요.\n\n\n\nF.5.4 루브릭\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Component\n                Range\n                Requirement\n              \n        \n        \n        \n                \n                  R/Python cited\n                  0 - 'No'; 1 - 'Yes'\n                  R (and/or Python) is properly referenced in the main content and in the reference list. If not, then paper gets 0 overall.\n                \n                \n                  Data cited\n                  0 - 'No'; 1 - 'Yes'\n                  Data are properly referenced in the main content and in the reference list. If not, then paper gets 0 overall.\n                \n                \n                  Class paper\n                  0 - 'No'; 1 - 'Yes'\n                  There is no sign this is a class project. Check the rproj and folder names, the README, the title, code comments, etc. If there is any sign this is a class paper, then paper gets 0 overall.\n                \n                \n                  LLM documentation\n                  0 - 'No'; 1 - 'Yes'\n                  A separate paragraph or dot point must be included in the README about whether LLMs were used, and if so how. If auto-complete tools such as co-pilot were used this must be mentioned. If chat tools such as ChatGPT, were used then the entire chat must be included in the usage text file. If not, then paper gets 0 overall.\n                \n                \n                  Title\n                  0 - 'Poor or not done'; 1 - 'Yes'; 2 - 'Exceptional'\n                  An informative title is included that explains the story, and ideally tells the reader what happens at the end of it. 'Paper X' is not an informative title. Use a subtitle to convey the main finding. Do not use puns (you can break this rule once you're experienced).\n                \n                \n                  Author, date, and repo\n                  0 - 'Poor or not done'; 2 - 'Yes'\n                  The author, date of submission in unambiguous format, and a link to a GitHub repo are clearly included. (The later likely, but not necessarily, through a statement such as: 'Code and data supporting this analysis is available at: LINK').\n                \n                \n                  Abstract\n                  0 - 'Poor or not done'; 1 - 'Some issues'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  An abstract is included and appropriately pitched to a non-specialist audience. The abstract answers: 1) what was done, 2) what was found, and 3) why this matters (all at a high level). Likely four sentences. Abstract must make clear what we learn about the world because of this paper.\n                \n                \n                  Introduction\n                  0 - 'Poor or not done'; 1 - 'Some issues'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  The introduction is self-contained and tells a reader everything they need to know including: 1) broader context to motivate; 2) some detail about what the paper is about; 3) a clear gap that needs to be filled; 4) what was done; 5) what was found; 6) why it is important; 7) the structure of the paper. A reader should be able to read only the introduction and know what was done, why, and what was found. Likely 3 or 4 paragraphs, or 10 per cent of total.\n                \n                \n                  Estimand\n                  0 - 'Poor or not done'; 1 - 'Done'\n                  The estimand is clearly stated, in its own paragraph, in the introduction.\n                \n                \n                  Data\n                  0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Acceptable'; 8 - 'Impressive'; 10 - 'Exceptional'\n                  A sense of the dataset should be communicated to the reader. The broader context of the dataset should be discussed. All variables should be thoroughly examined and explained. Explain if there were similar datasets that could have been used and why they were not. If variables were constructed then this should be mentioned, and high-level cleaning aspects of note should be mentioned, but this section should focus on the destination, not the journey. It is important to understand what the variables look like by including graphs, and possibly tables, of all observations, along with discussion of those graphs and the other features of these data. Summary statistics should also be included, and well as any relationships between the variables. You are not doing EDA in this section--you are talking the reader through the variables that are of interest. If this becomes too detailed, then appendices could be used. Basically, for every variable in your dataset that is of interest to your paper there needs to be graphs and explanation and maybe tables.\n                \n                \n                  Measurement\n                  0 - 'Poor or not done'; 2 - 'Some issues'; 3 - 'Acceptable'; 4 - 'Exceptional'\n                  A thorough discussion of measurement, relating to the dataset, is provided in the data section. Please ensure that you explain how we went from some phenomena in the world that happened to an entry in the dataset that you are interested in.\n                \n                \n                  Model\n                  0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Acceptable'; 8 - 'Impressive'; 10 - 'Exceptional'\n                  Present the model clearly using appropriate mathematical notation and plain English explanations, defining every component. Ensure the model is well-explained, justified, appropriate, and balanced in complexity—neither overly simplistic nor unnecessarily complicated—for the situation. Variables should be well-defined and correspond with those in the data section. Explain how modeling decisions reflect aspects discussed in the data section, including why specific features are included (e.g., using age rather than age groups, treating province effects as levels, categorizing gender). If applicable, define and justify sensible priors for Bayesian models. Clearly discuss underlying assumptions, potential limitations, and situations where the model may not be appropriate. Mention the software used to implement the model, and provide evidence of model validation and checking—such as out-of-sample testing, RMSE calculations, test/training splits, or sensitivity analyses—addressing model convergence and diagnostics (although much of the detail make be in the appendix). Include any alternative models or variants considered, their strengths and weaknesses, and the rationale for the final model choice.\n                \n                \n                  Results\n                  0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Acceptable'; 8 - 'Impressive'; 10 - 'Exceptional'\n                  Results will likely require summary statistics, tables, graphs, images, and possibly statistical analysis or maps. There should also be text associated with all these aspects. Show the reader the results by plotting them where possible. Talk about them. Explain them. That said, this section should strictly relay results. Regression tables must not contain stars. Use modelsummary to include a table and graph of the estimates.\n                \n                \n                  Discussion\n                  0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Acceptable'; 8 - 'Impressive'; 10 - 'Exceptional'\n                  Some questions that a good discussion would cover include (each of these would be a sub-section of something like half a page to a page): What is done in this paper? What is something that we learn about the world? What is another thing that we learn about the world? What are some weaknesses of what was done? What is left to learn or how should we proceed in the future?\n                \n                \n                  Prose\n                  0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Acceptable'; 6 - 'Exceptional'\n                  All aspects of submission should be free of noticeable typos, spelling mistakes, and be grammatically correct. Prose should be coherent, concise, clear, and mature. Remove unnecessary words. Do not use the following words/phrases: 'actionable insights', 'address/es/ing a/that/the/this gap', 'advanced', 'all-encompassing', 'apt', 'backdrop', 'beg/s the question', 'bridge/s a/that/the/this gap', 'clear gap', 'compelling', 'comprehensive', 'critical', 'crucial', 'data driven', 'data-driven', 'delve/s', 'drastic', 'drive/s forward', 'elucidate/ing', 'embark/s', 'exploration', 'fill/s a/that/the/this gap', 'fresh perspective/s', 'hidden factor/s', 'immense', 'imperative', 'indispensable', 'insight/s', 'insight/s from', 'interrogate', 'intricate', 'intriguing', 'invaluable', 'key driver/s', 'key insight/s', 'kind of', 'leverage', 'meticulous/ly', 'multifaceted', 'novel', 'nuance', 'offer/s/ing crucial insight', 'pivotal', 'plummeted', 'profound', 'rapidly', 'reveals', 'shed/s light', 'shocking', 'soared', 'unparalleled', 'unveiling', 'valuable', 'wanna'.\n                \n                \n                  Cross-references\n                  0 - 'Poor or not done'; 1 - 'Yes'\n                  All figures, tables, and equations, should be numbered, and referred to in the text using cross-references. The telegraphing paragraph in the introduction should cross reference the rest of the paper.\n                \n                \n                  Captions\n                  0 - 'Poor or not done'; 1 - 'Acceptable'; 2 - 'Excellent'\n                  All figures and tables have detailed and meaningful captions. They should be sufficiently detailed so as to make the main point of the figure/table clear even without the accompanying text. Do not say 'Histogram of...' or whatever else the figure type is.\n                \n                \n                  Graphs and tables\n                  0 - 'Poor or not done'; 1 - 'Some issues'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  Graphs and tables must be included in the paper and should be to well-formatted, clear, and digestible. Graphs should be made using ggplot2 and tables should be made using tinytable. They should serve a clear purpose and be fully self-contained. Graphs and tables should be appropriately sized, colored, and labelled. Variable names should not be used as labels. Tables should have an appropriate number of decimal places and use comma separators for thousands. Don't use boxplots, but if you must then you must overlay the actual data.\n                \n                \n                  Referencing\n                  0 - 'Poor or not done'; 3 - 'One minor issue'; 4 - 'Perfect'\n                  All data, software, literature, and any other relevant material, should be cited in-text and included in a properly formatted reference list made using BibTeX. A few lines of code from Stack Overflow or similar, would be acknowledged just with a comment in the script immediately preceding the use of the code rather than here. But larger chunks of code should be fully acknowledged with an in-text citation and appear in the reference list. Check in-text citations and that you have not accidentally used (@my_cite) when you needed [@my_cite]. R packages and all other aspects should be correctly capitalized, and name should be correct e.g. use double braces appropriately in the BibTeX file.\n                \n                \n                  Commits\n                  0 - 'Poor or not done'; 2 - 'Done'\n                  There are at least a handful of different commits, and they have meaningful commit messages.\n                \n                \n                  Sketches\n                  0 - 'Poor or not done'; 2 - 'Done'\n                  Sketches are included in a labelled folder of the repo, appropriate, and of high-quality.\n                \n                \n                  Simulation\n                  0 - 'Poor or not done'; 1 - 'Some issues'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  The script is clearly commented and structured. All variables are appropriately simulated in a sophisticated way including appropriate interaction between simulated variables.\n                \n                \n                  Tests\n                  0 - 'Poor or not done'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  High-quality extensive suites of tests are written for the both the simulated and actual datasets. These suites must be in separate scripts. The suite should be extensive and put together in a sophisticated way using packages like testthat, validate, pointblank, or great expectations.\n                \n                \n                  Parquet\n                  0 - 'No'; 1 - 'Yes'\n                  The analysis dataset is saved as a parquet file. (Note that the raw data should be saved in whatever format it came.)\n                \n                \n                  Reproducible workflow\n                  0 - 'Poor or not done'; 1 - 'Some issues'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  Use an organized repo with a detailed README and an R project. Thoroughly document code and include a preamble, comments, nice structure, and style code with styler or lintr. Use seeds appropriately. Avoid leaving install.packages() in the code unless handled sophisticatedly. Exclude unnecessary files from the repo; avoid hard-coded paths and setwd(). Use base pipe not magrittr pipe. Comment on and close all GitHub issues. Deal with all branches.\n                \n                \n                  Datasheet\n                  0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Acceptable'; 8 - 'Impressive'; 10 - 'Exceptional'\n                  A thorough datasheet for the dataset that was constructed is included.\n                \n                \n                  Miscellaneous\n                  0 - 'None'; 1 - 'Notable'; 2 - 'Remarkable'; 3 - 'Exceptional'\n                  There are always students that excel in a way that is not anticipated in the rubric. This item accounts for that.\n                \n        \n      \n    \n\n\n\n\n\nF.5.5 이전 예시\n\n2024: 알라이나 후; 아이린 후인; 얀센 마이어 람보, 티모티우스 프라조기; 그리고 치 얼 (엠마) 텅, 원타오 선, 양 청.",
    "crumbs": [
      "부록",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>논문</span>"
    ]
  },
  {
    "objectID": "25-papers_ko.html#sec-st-george-paper",
    "href": "25-papers_ko.html#sec-st-george-paper",
    "title": "Online Appendix F — 논문",
    "section": "F.6 세인트 조지 논문",
    "text": "F.6 세인트 조지 논문\n\nF.6.1 과제\n\n1~3명으로 구성된 팀의 일원으로, 완전히 재현 가능한 방식으로 “여론조사 종합” (Blumenthal 2014; Pasek 2015)을 사용하여 다가오는 미국 대통령 선거의 승자를 예측하는 선형 또는 일반화 선형 모델을 구축한 다음, 이야기를 담은 짧은 논문을 작성하십시오.\n이 시작 폴더를 사용해야 합니다.\nR, Python 또는 조합을 사용해도 좋습니다.\n여기에서 여론조사 결과 데이터를 얻을 수 있습니다 (“Download the data”를 검색한 다음, Presidential general election polls (current cycle)를 선택한 다음, “Download”를 선택하십시오).\n표본에서 한 여론조사 기관을 선택하고, 논문의 부록에서 그들의 방법론을 심층적으로 분석하십시오. 특히, 관심 있는 여론조사 기관에 대한 감각을 전달하는 것 외에도, 이 부록에는 설문조사 방법론 및 주요 특징, 강점, 약점에 대한 논의가 포함되어야 합니다. 예를 들어:\n\n모집단, 프레임, 표본은 무엇이며;\n표본은 어떻게 모집되며;\n어떤 샘플링 접근 방식이 취해지고, 이의 장단점은 무엇이며;\n무응답은 어떻게 처리되는가;\n설문지는 무엇이 좋고 나쁜가.\n\n다른 부록에, 10만 달러의 예산과 미국 대통령 선거를 예측하는 임무가 있다면 실행할 이상적인 방법론과 설문조사를 작성하십시오. 사용할 샘플링 접근 방식, 응답자 모집 방법, 데이터 유효성 검사 및 기타 관련 측면을 자세히 설명해야 합니다. 또한 여론조사 집계 또는 방법론의 다른 특징을 신중하게 다루십시오. Google Forms와 같은 설문조사 플랫폼을 사용하여 설문조사를 실제로 구현해야 합니다. 이에 대한 링크는 부록에 포함되어야 합니다. 또한 설문조사 사본도 부록에 포함되어야 합니다.\nGitHub 저장소 링크를 제출하십시오. 마감일 이후에는 저장소를 업데이트하지 마십시오.\n이것이 수업 논문이라는 증거가 없어야 합니다.\n\n\n\nF.6.2 확인 사항\n\n필요한 두 가지 부록이 모두 있는지 확인하십시오.\n\n\n\nF.6.3 FAQ\n\n데이터셋의 모든 예측 변수를 사용해야 합니까? 아니요, 사용하는 예측 변수에 대해 신중하고 사려 깊게 선택해야 합니다.\n선거인단은 어떻습니까? 미국 대통령 선거는 선거인단에 따라 승패가 결정됩니다. 단순히 일반 투표에만 집중해도 괜찮습니다. 그러나 뛰어난 제출물은 주별 일반 투표를 고려하고, 불확실성을 전파하는 데 주의하면서 선거인단 추정치를 구성할 것입니다.\n\n\n\nF.6.4 루브릭\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Component\n                Range\n                Requirement\n              \n        \n        \n        \n                \n                  R/Python cited\n                  0 - 'No'; 1 - 'Yes'\n                  R (and/or Python) is properly referenced in the main content and in the reference list. If not, then paper gets 0 overall.\n                \n                \n                  Data cited\n                  0 - 'No'; 1 - 'Yes'\n                  Data are properly referenced in the main content and in the reference list. If not, then paper gets 0 overall.\n                \n                \n                  Class paper\n                  0 - 'No'; 1 - 'Yes'\n                  There is no sign this is a class project. Check the rproj and folder names, the README, the title, code comments, etc. If there is any sign this is a class paper, then paper gets 0 overall.\n                \n                \n                  LLM documentation\n                  0 - 'No'; 1 - 'Yes'\n                  A separate paragraph or dot point must be included in the README about whether LLMs were used, and if so how. If auto-complete tools such as co-pilot were used this must be mentioned. If chat tools such as ChatGPT, were used then the entire chat must be included in the usage text file. If not, then paper gets 0 overall.\n                \n                \n                  Title\n                  0 - 'Poor or not done'; 1 - 'Yes'; 2 - 'Exceptional'\n                  An informative title is included that explains the story, and ideally tells the reader what happens at the end of it. 'Paper X' is not an informative title. Use a subtitle to convey the main finding. Do not use puns (you can break this rule once you're experienced).\n                \n                \n                  Author, date, and repo\n                  0 - 'Poor or not done'; 2 - 'Yes'\n                  The author, date of submission in unambiguous format, and a link to a GitHub repo are clearly included. (The later likely, but not necessarily, through a statement such as: 'Code and data supporting this analysis is available at: LINK').\n                \n                \n                  Abstract\n                  0 - 'Poor or not done'; 1 - 'Some issues'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  An abstract is included and appropriately pitched to a non-specialist audience. The abstract answers: 1) what was done, 2) what was found, and 3) why this matters (all at a high level). Likely four sentences. Abstract must make clear what we learn about the world because of this paper.\n                \n                \n                  Introduction\n                  0 - 'Poor or not done'; 1 - 'Some issues'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  The introduction is self-contained and tells a reader everything they need to know including: 1) broader context to motivate; 2) some detail about what the paper is about; 3) a clear gap that needs to be filled; 4) what was done; 5) what was found; 6) why it is important; 7) the structure of the paper. A reader should be able to read only the introduction and know what was done, why, and what was found. Likely 3 or 4 paragraphs, or 10 per cent of total.\n                \n                \n                  Estimand\n                  0 - 'Poor or not done'; 1 - 'Done'\n                  The estimand is clearly stated, in its own paragraph, in the introduction.\n                \n                \n                  Data\n                  0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Acceptable'; 8 - 'Impressive'; 10 - 'Exceptional'\n                  A sense of the dataset should be communicated to the reader. The broader context of the dataset should be discussed. All variables should be thoroughly examined and explained. Explain if there were similar datasets that could have been used and why they were not. If variables were constructed then this should be mentioned, and high-level cleaning aspects of note should be mentioned, but this section should focus on the destination, not the journey. It is important to understand what the variables look like by including graphs, and possibly tables, of all observations, along with discussion of those graphs and the other features of these data. Summary statistics should also be included, and well as any relationships between the variables. You are not doing EDA in this section--you are talking the reader through the variables that are of interest. If this becomes too detailed, then appendices could be used. Basically, for every variable in your dataset that is of interest to your paper there needs to be graphs and explanation and maybe tables.\n                \n                \n                  Measurement\n                  0 - 'Poor or not done'; 2 - 'Some issues'; 3 - 'Acceptable'; 4 - 'Exceptional'\n                  A thorough discussion of measurement, relating to the dataset, is provided in the data section. Please ensure that you explain how we went from some phenomena in the world that happened to an entry in the dataset that you are interested in.\n                \n                \n                  Model\n                  0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Acceptable'; 8 - 'Impressive'; 10 - 'Exceptional'\n                  Present the model clearly using appropriate mathematical notation and plain English explanations, defining every component. Ensure the model is well-explained, justified, appropriate, and balanced in complexity—neither overly simplistic nor unnecessarily complicated—for the situation. Variables should be well-defined and correspond with those in the data section. Explain how modeling decisions reflect aspects discussed in the data section, including why specific features are included (e.g., using age rather than age groups, treating province effects as levels, categorizing gender). If applicable, define and justify sensible priors for Bayesian models. Clearly discuss underlying assumptions, potential limitations, and situations where the model may not be appropriate. Mention the software used to implement the model, and provide evidence of model validation and checking—such as out-of-sample testing, RMSE calculations, test/training splits, or sensitivity analyses—addressing model convergence and diagnostics (although much of the detail make be in the appendix). Include any alternative models or variants considered, their strengths and weaknesses, and the rationale for the final model choice.\n                \n                \n                  Results\n                  0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Acceptable'; 8 - 'Impressive'; 10 - 'Exceptional'\n                  Results will likely require summary statistics, tables, graphs, images, and possibly statistical analysis or maps. There should also be text associated with all these aspects. Show the reader the results by plotting them where possible. Talk about them. Explain them. That said, this section should strictly relay results. Regression tables must not contain stars. Use modelsummary to include a table and graph of the estimates.\n                \n                \n                  Discussion\n                  0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Acceptable'; 8 - 'Impressive'; 10 - 'Exceptional'\n                  Some questions that a good discussion would cover include (each of these would be a sub-section of something like half a page to a page): What is done in this paper? What is something that we learn about the world? What is another thing that we learn about the world? What are some weaknesses of what was done? What is left to learn or how should we proceed in the future?\n                \n                \n                  Prose\n                  0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Acceptable'; 6 - 'Exceptional'\n                  All aspects of submission should be free of noticeable typos, spelling mistakes, and be grammatically correct. Prose should be coherent, concise, clear, and mature. Remove unnecessary words. Do not use the following words/phrases: 'actionable insights', 'address/es/ing a/that/the/this gap', 'advanced', 'all-encompassing', 'apt', 'backdrop', 'beg/s the question', 'bridge/s a/that/the/this gap', 'clear gap', 'compelling', 'comprehensive', 'critical', 'crucial', 'data driven', 'data-driven', 'delve/s', 'drastic', 'drive/s forward', 'elucidate/ing', 'embark/s', 'exploration', 'fill/s a/that/the/this gap', 'fresh perspective/s', 'hidden factor/s', 'immense', 'imperative', 'indispensable', 'insight/s', 'insight/s from', 'interrogate', 'intricate', 'intriguing', 'invaluable', 'key driver/s', 'key insight/s', 'kind of', 'leverage', 'meticulous/ly', 'multifaceted', 'novel', 'nuance', 'offer/s/ing crucial insight', 'pivotal', 'plummeted', 'profound', 'rapidly', 'reveals', 'shed/s light', 'shocking', 'soared', 'unparalleled', 'unveiling', 'valuable', 'wanna'.\n                \n                \n                  Cross-references\n                  0 - 'Poor or not done'; 1 - 'Yes'\n                  All figures, tables, and equations, should be numbered, and referred to in the text using cross-references. The telegraphing paragraph in the introduction should cross reference the rest of the paper.\n                \n                \n                  Captions\n                  0 - 'Poor or not done'; 1 - 'Acceptable'; 2 - 'Excellent'\n                  All figures and tables have detailed and meaningful captions. They should be sufficiently detailed so as to make the main point of the figure/table clear even without the accompanying text. Do not say 'Histogram of...' or whatever else the figure type is.\n                \n                \n                  Graphs and tables\n                  0 - 'Poor or not done'; 1 - 'Some issues'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  Graphs and tables must be included in the paper and should be to well-formatted, clear, and digestible. Graphs should be made using ggplot2 and tables should be made using tinytable. They should serve a clear purpose and be fully self-contained. Graphs and tables should be appropriately sized, colored, and labelled. Variable names should not be used as labels. Tables should have an appropriate number of decimal places and use comma separators for thousands. Don't use boxplots, but if you must then you must overlay the actual data.\n                \n                \n                  Pollster review\n                  0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Acceptable'; 8 - 'Impressive'; 10 - 'Exceptional'\n                  The evaluation provides a thorough understanding of how something goes from being a person's opinion to part of a result for this pollster. Provide a thorough overview and evaluation of the pollster's methodology, and sampling approach, highlighting both its strengths and limitations. Use this section to demonstrate knowledge of surveys and sampling and link your evaluation to the literature. Be precise and scientific--your review should not sound like an ad for the pollster.\n                \n                \n                  Idealized methodology\n                  0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Acceptable'; 8 - 'Impressive'; 10 - 'Exceptional'\n                  The proposed methodology is well-thought through, realistic and would achieve the goals.  Use this section to demonstrate knowledge of surveys and sampling and link your evaluation to the literature and simulation.\n                \n                \n                  Idealized survey\n                  0 - 'Poor or not done'; 1 - 'Some issues'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  The survey should have an introductory section and include the details of a contact person. The survey questions should be well constructed and appropriate to the task. The questions should have an appropriate ordering. A final section should thank the respondent. Question type should be varied and appropriate. Use this section to demonstrate knowledge of surveys.\n                \n                \n                  Referencing\n                  0 - 'Poor or not done'; 3 - 'One minor issue'; 4 - 'Perfect'\n                  All data, software, literature, and any other relevant material, should be cited in-text and included in a properly formatted reference list made using BibTeX. A few lines of code from Stack Overflow or similar, would be acknowledged just with a comment in the script immediately preceding the use of the code rather than here. But larger chunks of code should be fully acknowledged with an in-text citation and appear in the reference list. Check in-text citations and that you have not accidentally used (@my_cite) when you needed [@my_cite]. R packages and all other aspects should be correctly capitalized, and name should be correct e.g. use double braces appropriately in the BibTeX file.\n                \n                \n                  Commits\n                  0 - 'Poor or not done'; 2 - 'Done'\n                  There are at least a handful of different commits, and they have meaningful commit messages.\n                \n                \n                  Sketches\n                  0 - 'Poor or not done'; 2 - 'Done'\n                  Sketches are included in a labelled folder of the repo, appropriate, and of high-quality.\n                \n                \n                  Simulation\n                  0 - 'Poor or not done'; 1 - 'Some issues'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  The script is clearly commented and structured. All variables are appropriately simulated in a sophisticated way including appropriate interaction between simulated variables.\n                \n                \n                  Tests\n                  0 - 'Poor or not done'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  High-quality extensive suites of tests are written for the both the simulated and actual datasets. These suites must be in separate scripts. The suite should be extensive and put together in a sophisticated way using packages like testthat, validate, pointblank, or great expectations.\n                \n                \n                  Parquet\n                  0 - 'No'; 1 - 'Yes'\n                  The analysis dataset is saved as a parquet file. (Note that the raw data should be saved in whatever format it came.)\n                \n                \n                  Reproducible workflow\n                  0 - 'Poor or not done'; 1 - 'Some issues'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  Use an organized repo with a detailed README and an R project. Thoroughly document code and include a preamble, comments, nice structure, and style code with styler or lintr. Use seeds appropriately. Avoid leaving install.packages() in the code unless handled sophisticatedly. Exclude unnecessary files from the repo; avoid hard-coded paths and setwd(). Use base pipe not magrittr pipe. Comment on and close all GitHub issues. Deal with all branches.\n                \n                \n                  Miscellaneous\n                  0 - 'None'; 1 - 'Notable'; 2 - 'Remarkable'; 3 - 'Exceptional'\n                  There are always students that excel in a way that is not anticipated in the rubric. This item accounts for that.\n                \n        \n      \n    \n\n\n\n\n\nF.6.5 이전 예시\n\n2024: 탈리아 파브레가스, 알리자 미트와니, 파티마 유누사; 소피아 브라더스, 데이 콩, 라얀 아와드 알림; 콜린 시한 양, 렉순 유, 시다르트 고다; 그리고 위안이 (레오) 리우, 더전 첸, 지위안 션.",
    "crumbs": [
      "부록",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>논문</span>"
    ]
  },
  {
    "objectID": "25-papers_ko.html#sec-paper-five",
    "href": "25-papers_ko.html#sec-paper-five",
    "title": "Online Appendix F — 논문",
    "section": "F.7 스포포스 논문",
    "text": "F.7 스포포스 논문\n\nF.7.1 과제\n\n1~3명으로 구성된 팀의 일원으로, 사후 계층화 다단계 회귀를 사용하여 다가오는 미국 선거의 일반 투표를 예측한 다음, 이야기를 담은 짧은 논문을 작성하십시오.\n이것은 개인 수준 설문조사 데이터, 사후 계층화 데이터, 그리고 이들을 결합하는 모델을 필요로 합니다. 이러한 데이터를 수집하는 데 드는 비용과 이에 접근할 수 있는 특권을 고려하여, 사용하는 모든 데이터셋을 적절하게 인용해야 합니다.\n다음을 수행해야 합니다:\n\n개인 수준 설문조사 데이터셋에 접근하십시오.\n사후 계층화 데이터셋에 접근하십시오.\n이 두 데이터셋을 함께 사용할 수 있도록 정리하고 준비하십시오.\n설문조사 데이터셋을 사용하여 모델을 추정하십시오.\n훈련된 모델을 사후 계층화 데이터셋에 적용하여 선거 결과를 예측하십시오.\n\n이 시작 폴더를 사용해야 합니다.\nGitHub 저장소 링크를 제출하십시오. 마감일 이후에는 저장소를 업데이트하지 마십시오.\n이것이 수업 논문이라는 증거가 없어야 합니다.\n\n\n\nF.7.2 FAQ\n\n얼마나 작성해야 합니까? 대부분의 학생들은 10~15페이지 범위의 내용을 제출하지만, 이는 여러분에게 달려 있습니다. 정확하고 철저하게 작성하십시오.\n\n\n\nF.7.3 루브릭\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Component\n                Range\n                Requirement\n              \n        \n        \n        \n                \n                  R/Python cited\n                  0 - 'No'; 1 - 'Yes'\n                  R (and/or Python) is properly referenced in the main content and in the reference list. If not, then paper gets 0 overall.\n                \n                \n                  Data cited\n                  0 - 'No'; 1 - 'Yes'\n                  Data are properly referenced in the main content and in the reference list. If not, then paper gets 0 overall.\n                \n                \n                  Class paper\n                  0 - 'No'; 1 - 'Yes'\n                  There is no sign this is a class project. Check the rproj and folder names, the README, the title, code comments, etc. If there is any sign this is a class paper, then paper gets 0 overall.\n                \n                \n                  LLM documentation\n                  0 - 'No'; 1 - 'Yes'\n                  A separate paragraph or dot point must be included in the README about whether LLMs were used, and if so how. If auto-complete tools such as co-pilot were used this must be mentioned. If chat tools such as ChatGPT, were used then the entire chat must be included in the usage text file. If not, then paper gets 0 overall.\n                \n                \n                  Title\n                  0 - 'Poor or not done'; 1 - 'Yes'; 2 - 'Exceptional'\n                  An informative title is included that explains the story, and ideally tells the reader what happens at the end of it. 'Paper X' is not an informative title. Use a subtitle to convey the main finding. Do not use puns (you can break this rule once you're experienced).\n                \n                \n                  Author, date, and repo\n                  0 - 'Poor or not done'; 2 - 'Yes'\n                  The author, date of submission in unambiguous format, and a link to a GitHub repo are clearly included. (The later likely, but not necessarily, through a statement such as: 'Code and data supporting this analysis is available at: LINK').\n                \n                \n                  Abstract\n                  0 - 'Poor or not done'; 1 - 'Some issues'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  An abstract is included and appropriately pitched to a non-specialist audience. The abstract answers: 1) what was done, 2) what was found, and 3) why this matters (all at a high level). Likely four sentences. Abstract must make clear what we learn about the world because of this paper.\n                \n                \n                  Introduction\n                  0 - 'Poor or not done'; 1 - 'Some issues'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  The introduction is self-contained and tells a reader everything they need to know including: 1) broader context to motivate; 2) some detail about what the paper is about; 3) a clear gap that needs to be filled; 4) what was done; 5) what was found; 6) why it is important; 7) the structure of the paper. A reader should be able to read only the introduction and know what was done, why, and what was found. Likely 3 or 4 paragraphs, or 10 per cent of total.\n                \n                \n                  Estimand\n                  0 - 'Poor or not done'; 1 - 'Done'\n                  The estimand is clearly stated, in its own paragraph, in the introduction.\n                \n                \n                  Data\n                  0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Acceptable'; 8 - 'Impressive'; 10 - 'Exceptional'\n                  A sense of the dataset should be communicated to the reader. The broader context of the dataset should be discussed. All variables should be thoroughly examined and explained. Explain if there were similar datasets that could have been used and why they were not. If variables were constructed then this should be mentioned, and high-level cleaning aspects of note should be mentioned, but this section should focus on the destination, not the journey. It is important to understand what the variables look like by including graphs, and possibly tables, of all observations, along with discussion of those graphs and the other features of these data. Summary statistics should also be included, and well as any relationships between the variables. You are not doing EDA in this section--you are talking the reader through the variables that are of interest. If this becomes too detailed, then appendices could be used. Basically, for every variable in your dataset that is of interest to your paper there needs to be graphs and explanation and maybe tables.\n                \n                \n                  Measurement\n                  0 - 'Poor or not done'; 2 - 'Some issues'; 3 - 'Acceptable'; 4 - 'Exceptional'\n                  A thorough discussion of measurement, relating to the dataset, is provided in the data section. Please ensure that you explain how we went from some phenomena in the world that happened to an entry in the dataset that you are interested in.\n                \n                \n                  Model\n                  0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Acceptable'; 8 - 'Impressive'; 10 - 'Exceptional'\n                  Present the model clearly using appropriate mathematical notation and plain English explanations, defining every component. Ensure the model is well-explained, justified, appropriate, and balanced in complexity—neither overly simplistic nor unnecessarily complicated—for the situation. Variables should be well-defined and correspond with those in the data section. Explain how modeling decisions reflect aspects discussed in the data section, including why specific features are included (e.g., using age rather than age groups, treating province effects as levels, categorizing gender). If applicable, define and justify sensible priors for Bayesian models. Clearly discuss underlying assumptions, potential limitations, and situations where the model may not be appropriate. Mention the software used to implement the model, and provide evidence of model validation and checking—such as out-of-sample testing, RMSE calculations, test/training splits, or sensitivity analyses—addressing model convergence and diagnostics (although much of the detail make be in the appendix). Include any alternative models or variants considered, their strengths and weaknesses, and the rationale for the final model choice.\n                \n                \n                  Results\n                  0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Acceptable'; 8 - 'Impressive'; 10 - 'Exceptional'\n                  Results will likely require summary statistics, tables, graphs, images, and possibly statistical analysis or maps. There should also be text associated with all these aspects. Show the reader the results by plotting them where possible. Talk about them. Explain them. That said, this section should strictly relay results. Regression tables must not contain stars. Use modelsummary to include a table and graph of the estimates.\n                \n                \n                  Discussion\n                  0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Acceptable'; 8 - 'Impressive'; 10 - 'Exceptional'\n                  Some questions that a good discussion would cover include (each of these would be a sub-section of something like half a page to a page): What is done in this paper? What is something that we learn about the world? What is another thing that we learn about the world? What are some weaknesses of what was done? What is left to learn or how should we proceed in the future?\n                \n                \n                  Prose\n                  0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Acceptable'; 6 - 'Exceptional'\n                  All aspects of submission should be free of noticeable typos, spelling mistakes, and be grammatically correct. Prose should be coherent, concise, clear, and mature. Remove unnecessary words. Do not use the following words/phrases: 'actionable insights', 'address/es/ing a/that/the/this gap', 'advanced', 'all-encompassing', 'apt', 'backdrop', 'beg/s the question', 'bridge/s a/that/the/this gap', 'clear gap', 'compelling', 'comprehensive', 'critical', 'crucial', 'data driven', 'data-driven', 'delve/s', 'drastic', 'drive/s forward', 'elucidate/ing', 'embark/s', 'exploration', 'fill/s a/that/the/this gap', 'fresh perspective/s', 'hidden factor/s', 'immense', 'imperative', 'indispensable', 'insight/s', 'insight/s from', 'interrogate', 'intricate', 'intriguing', 'invaluable', 'key driver/s', 'key insight/s', 'kind of', 'leverage', 'meticulous/ly', 'multifaceted', 'novel', 'nuance', 'offer/s/ing crucial insight', 'pivotal', 'plummeted', 'profound', 'rapidly', 'reveals', 'shed/s light', 'shocking', 'soared', 'unparalleled', 'unveiling', 'valuable', 'wanna'.\n                \n                \n                  Cross-references\n                  0 - 'Poor or not done'; 1 - 'Yes'\n                  All figures, tables, and equations, should be numbered, and referred to in the text using cross-references. The telegraphing paragraph in the introduction should cross reference the rest of the paper.\n                \n                \n                  Captions\n                  0 - 'Poor or not done'; 1 - 'Acceptable'; 2 - 'Excellent'\n                  All figures and tables have detailed and meaningful captions. They should be sufficiently detailed so as to make the main point of the figure/table clear even without the accompanying text. Do not say 'Histogram of...' or whatever else the figure type is.\n                \n                \n                  Graphs and tables\n                  0 - 'Poor or not done'; 1 - 'Some issues'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  Graphs and tables must be included in the paper and should be to well-formatted, clear, and digestible. Graphs should be made using ggplot2 and tables should be made using tinytable. They should serve a clear purpose and be fully self-contained. Graphs and tables should be appropriately sized, colored, and labelled. Variable names should not be used as labels. Tables should have an appropriate number of decimal places and use comma separators for thousands. Don't use boxplots, but if you must then you must overlay the actual data.\n                \n                \n                  Referencing\n                  0 - 'Poor or not done'; 3 - 'One minor issue'; 4 - 'Perfect'\n                  All data, software, literature, and any other relevant material, should be cited in-text and included in a properly formatted reference list made using BibTeX. A few lines of code from Stack Overflow or similar, would be acknowledged just with a comment in the script immediately preceding the use of the code rather than here. But larger chunks of code should be fully acknowledged with an in-text citation and appear in the reference list. Check in-text citations and that you have not accidentally used (@my_cite) when you needed [@my_cite]. R packages and all other aspects should be correctly capitalized, and name should be correct e.g. use double braces appropriately in the BibTeX file.\n                \n                \n                  Commits\n                  0 - 'Poor or not done'; 2 - 'Done'\n                  There are at least a handful of different commits, and they have meaningful commit messages.\n                \n                \n                  Sketches\n                  0 - 'Poor or not done'; 2 - 'Done'\n                  Sketches are included in a labelled folder of the repo, appropriate, and of high-quality.\n                \n                \n                  Simulation\n                  0 - 'Poor or not done'; 1 - 'Some issues'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  The script is clearly commented and structured. All variables are appropriately simulated in a sophisticated way including appropriate interaction between simulated variables.\n                \n                \n                  Tests\n                  0 - 'Poor or not done'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  High-quality extensive suites of tests are written for the both the simulated and actual datasets. These suites must be in separate scripts. The suite should be extensive and put together in a sophisticated way using packages like testthat, validate, pointblank, or great expectations.\n                \n                \n                  Parquet\n                  0 - 'No'; 1 - 'Yes'\n                  The analysis dataset is saved as a parquet file. (Note that the raw data should be saved in whatever format it came.)\n                \n                \n                  Reproducible workflow\n                  0 - 'Poor or not done'; 1 - 'Some issues'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  Use an organized repo with a detailed README and an R project. Thoroughly document code and include a preamble, comments, nice structure, and style code with styler or lintr. Use seeds appropriately. Avoid leaving install.packages() in the code unless handled sophisticatedly. Exclude unnecessary files from the repo; avoid hard-coded paths and setwd(). Use base pipe not magrittr pipe. Comment on and close all GitHub issues. Deal with all branches.\n                \n                \n                  Miscellaneous\n                  0 - 'None'; 1 - 'Notable'; 2 - 'Remarkable'; 3 - 'Exceptional'\n                  There are always students that excel in a way that is not anticipated in the rubric. This item accounts for that.\n                \n        \n      \n    \n\n\n\n\n\nF.7.4 이전 예시\n\n2024: 정우 김, 지원 최; 탈리아 파브레가스, 파티마 유누사, 아미시 순딥 아바르세카르.\n2020: 알렌 미트로프스키, 샤오얀 양, 매튜 완키에비치 (이 논문은 ASA 2020년 12월 학부 통계 연구 프로젝트 대회에서 “우수 언급”을 받았습니다.)",
    "crumbs": [
      "부록",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>논문</span>"
    ]
  },
  {
    "objectID": "25-papers_ko.html#sec-final-paper",
    "href": "25-papers_ko.html#sec-final-paper",
    "title": "Online Appendix F — 논문",
    "section": "F.8 최종 논문",
    "text": "F.8 최종 논문\n\nF.8.1 과제\n\n개별적으로 그리고 완전히 재현 가능한 방식으로 원본 작업을 포함하여 데이터로 이야기를 전달하는 논문을 작성하십시오.\n관심 있는 연구 질문을 개발한 다음, 관련 데이터셋을 얻거나 생성하고 이를 답변하는 논문을 작성하십시오.\n이 시작 폴더를 사용해야 합니다.\nR, Python 또는 조합을 사용해도 좋습니다.\n논문과 관련된 설문조사, 샘플링 또는 관찰 데이터의 측면에 초점을 맞춘 부록을 포함하십시오. 이는 논문 2의 “이상적인 방법론/설문조사/여론조사 기관 방법론” 섹션과 유사하게 심층적인 탐색이어야 하며, 시뮬레이션, 문헌 링크, 탐색 및 비교와 같은 측면을 포함할 수 있습니다.\n데이터셋 아이디어:\n\n제이콥 필립의 식료품 데이터셋 여기.\nIJF 데이터셋 여기 (그러면 IJF 최우수 논문상 자격이 주어집니다).\nOpen Data Toronto 데이터셋 재방문 (그러면 Open Data Toronto 최우수 논문상 자격이 주어집니다).\n부록 D 의 데이터셋.\n\n이전 논문의 모든 지침과 기대치는 이 논문에도 적용됩니다.\nGitHub 저장소 링크를 제출하십시오. 마감일 이후에는 저장소를 업데이트하지 마십시오.\n이것이 수업 논문이라는 증거가 없어야 합니다.\n\n\n\nF.8.2 확인 사항\n\nKaggle, UCI 또는 Statistica의 데이터셋을 사용하지 마십시오. 주로 다른 모든 사람들이 이러한 데이터셋을 사용하므로 고용주에게 눈에 띄는 데 도움이 되지 않지만, 데이터가 오래되었거나 출처를 알 수 없다는 우려도 있습니다.\n\n\n\nF.8.3 FAQ\n\n팀의 일원으로 작업할 수 있습니까? 아니요. 완전히 자신만의 작업이 있어야 합니다. 취업 지원 등을 위해 자신만의 작업이 정말 필요합니다.\n얼마나 작성해야 합니까? 대부분의 학생들은 10~20페이지의 본문 내용과 추가 페이지를 부록에 할애하지만, 이는 여러분에게 달려 있습니다. 간결하지만 철저하게 작성하십시오.\n어떤 모델이든 사용할 수 있습니까? 어떤 모델이든 사용할 수 있지만, 철저히 설명해야 하며 복잡한 모델의 경우 어려울 수 있습니다. 작게 시작하십시오. 한두 개의 예측 변수를 선택하십시오. 그것이 작동하면 복잡하게 만드십시오. 모든 예측 변수와 결과 변수는 데이터 섹션에서 그래프로 표시되고 설명되어야 함을 기억하십시오.\n\n\n\nF.8.4 루브릭\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Component\n                Range\n                Requirement\n              \n        \n        \n        \n                \n                  R/Python cited\n                  0 - 'No'; 1 - 'Yes'\n                  R (and/or Python) is properly referenced in the main content and in the reference list. If not, then paper gets 0 overall.\n                \n                \n                  Data cited\n                  0 - 'No'; 1 - 'Yes'\n                  Data are properly referenced in the main content and in the reference list. If not, then paper gets 0 overall.\n                \n                \n                  Class paper\n                  0 - 'No'; 1 - 'Yes'\n                  There is no sign this is a class project. Check the rproj and folder names, the README, the title, code comments, etc. If there is any sign this is a class paper, then paper gets 0 overall.\n                \n                \n                  LLM documentation\n                  0 - 'No'; 1 - 'Yes'\n                  A separate paragraph or dot point must be included in the README about whether LLMs were used, and if so how. If auto-complete tools such as co-pilot were used this must be mentioned. If chat tools such as ChatGPT, were used then the entire chat must be included in the usage text file. If not, then paper gets 0 overall.\n                \n                \n                  Title\n                  0 - 'Poor or not done'; 1 - 'Yes'; 2 - 'Exceptional'\n                  An informative title is included that explains the story, and ideally tells the reader what happens at the end of it. 'Paper X' is not an informative title. Use a subtitle to convey the main finding. Do not use puns (you can break this rule once you're experienced).\n                \n                \n                  Author, date, and repo\n                  0 - 'Poor or not done'; 2 - 'Yes'\n                  The author, date of submission in unambiguous format, and a link to a GitHub repo are clearly included. (The later likely, but not necessarily, through a statement such as: 'Code and data supporting this analysis is available at: LINK').\n                \n                \n                  Abstract\n                  0 - 'Poor or not done'; 1 - 'Some issues'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  An abstract is included and appropriately pitched to a non-specialist audience. The abstract answers: 1) what was done, 2) what was found, and 3) why this matters (all at a high level). Likely four sentences. Abstract must make clear what we learn about the world because of this paper.\n                \n                \n                  Introduction\n                  0 - 'Poor or not done'; 1 - 'Some issues'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  The introduction is self-contained and tells a reader everything they need to know including: 1) broader context to motivate; 2) some detail about what the paper is about; 3) a clear gap that needs to be filled; 4) what was done; 5) what was found; 6) why it is important; 7) the structure of the paper. A reader should be able to read only the introduction and know what was done, why, and what was found. Likely 3 or 4 paragraphs, or 10 per cent of total.\n                \n                \n                  Estimand\n                  0 - 'Poor or not done'; 1 - 'Done'\n                  The estimand is clearly stated, in its own paragraph, in the introduction.\n                \n                \n                  Data\n                  0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Acceptable'; 8 - 'Impressive'; 10 - 'Exceptional'\n                  A sense of the dataset should be communicated to the reader. The broader context of the dataset should be discussed. All variables should be thoroughly examined and explained. Explain if there were similar datasets that could have been used and why they were not. If variables were constructed then this should be mentioned, and high-level cleaning aspects of note should be mentioned, but this section should focus on the destination, not the journey. It is important to understand what the variables look like by including graphs, and possibly tables, of all observations, along with discussion of those graphs and the other features of these data. Summary statistics should also be included, and well as any relationships between the variables. You are not doing EDA in this section--you are talking the reader through the variables that are of interest. If this becomes too detailed, then appendices could be used. Basically, for every variable in your dataset that is of interest to your paper there needs to be graphs and explanation and maybe tables.\n                \n                \n                  Measurement\n                  0 - 'Poor or not done'; 2 - 'Some issues'; 3 - 'Acceptable'; 4 - 'Exceptional'\n                  A thorough discussion of measurement, relating to the dataset, is provided in the data section. Please ensure that you explain how we went from some phenomena in the world that happened to an entry in the dataset that you are interested in.\n                \n                \n                  Model\n                  0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Acceptable'; 8 - 'Impressive'; 10 - 'Exceptional'\n                  Present the model clearly using appropriate mathematical notation and plain English explanations, defining every component. Ensure the model is well-explained, justified, appropriate, and balanced in complexity—neither overly simplistic nor unnecessarily complicated—for the situation. Variables should be well-defined and correspond with those in the data section. Explain how modeling decisions reflect aspects discussed in the data section, including why specific features are included (e.g., using age rather than age groups, treating province effects as levels, categorizing gender). If applicable, define and justify sensible priors for Bayesian models. Clearly discuss underlying assumptions, potential limitations, and situations where the model may not be appropriate. Mention the software used to implement the model, and provide evidence of model validation and checking—such as out-of-sample testing, RMSE calculations, test/training splits, or sensitivity analyses—addressing model convergence and diagnostics (although much of the detail make be in the appendix). Include any alternative models or variants considered, their strengths and weaknesses, and the rationale for the final model choice.\n                \n                \n                  Results\n                  0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Acceptable'; 8 - 'Impressive'; 10 - 'Exceptional'\n                  Results will likely require summary statistics, tables, graphs, images, and possibly statistical analysis or maps. There should also be text associated with all these aspects. Show the reader the results by plotting them where possible. Talk about them. Explain them. That said, this section should strictly relay results. Regression tables must not contain stars. Use modelsummary to include a table and graph of the estimates.\n                \n                \n                  Discussion\n                  0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Acceptable'; 8 - 'Impressive'; 10 - 'Exceptional'\n                  Some questions that a good discussion would cover include (each of these would be a sub-section of something like half a page to a page): What is done in this paper? What is something that we learn about the world? What is another thing that we learn about the world? What are some weaknesses of what was done? What is left to learn or how should we proceed in the future?\n                \n                \n                  Prose\n                  0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Acceptable'; 6 - 'Exceptional'\n                  All aspects of submission should be free of noticeable typos, spelling mistakes, and be grammatically correct. Prose should be coherent, concise, clear, and mature. Remove unnecessary words. Do not use the following words/phrases: 'actionable insights', 'address/es/ing a/that/the/this gap', 'advanced', 'all-encompassing', 'apt', 'backdrop', 'beg/s the question', 'bridge/s a/that/the/this gap', 'clear gap', 'compelling', 'comprehensive', 'critical', 'crucial', 'data driven', 'data-driven', 'delve/s', 'drastic', 'drive/s forward', 'elucidate/ing', 'embark/s', 'exploration', 'fill/s a/that/the/this gap', 'fresh perspective/s', 'hidden factor/s', 'immense', 'imperative', 'indispensable', 'insight/s', 'insight/s from', 'interrogate', 'intricate', 'intriguing', 'invaluable', 'key driver/s', 'key insight/s', 'kind of', 'leverage', 'meticulous/ly', 'multifaceted', 'novel', 'nuance', 'offer/s/ing crucial insight', 'pivotal', 'plummeted', 'profound', 'rapidly', 'reveals', 'shed/s light', 'shocking', 'soared', 'unparalleled', 'unveiling', 'valuable', 'wanna'.\n                \n                \n                  Cross-references\n                  0 - 'Poor or not done'; 1 - 'Yes'\n                  All figures, tables, and equations, should be numbered, and referred to in the text using cross-references. The telegraphing paragraph in the introduction should cross reference the rest of the paper.\n                \n                \n                  Captions\n                  0 - 'Poor or not done'; 1 - 'Acceptable'; 2 - 'Excellent'\n                  All figures and tables have detailed and meaningful captions. They should be sufficiently detailed so as to make the main point of the figure/table clear even without the accompanying text. Do not say 'Histogram of...' or whatever else the figure type is.\n                \n                \n                  Graphs and tables\n                  0 - 'Poor or not done'; 1 - 'Some issues'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  Graphs and tables must be included in the paper and should be to well-formatted, clear, and digestible. Graphs should be made using ggplot2 and tables should be made using tinytable. They should serve a clear purpose and be fully self-contained. Graphs and tables should be appropriately sized, colored, and labelled. Variable names should not be used as labels. Tables should have an appropriate number of decimal places and use comma separators for thousands. Don't use boxplots, but if you must then you must overlay the actual data.\n                \n                \n                  Surveys, sampling, and observational data\n                  0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Acceptable'; 8 - 'Impressive'; 10 - 'Exceptional'\n                  Please include an appendix where you focus on some aspect of surveys, sampling or observational data, related to your paper. This should be an in-depth exploration, akin to the idealized methodology/survey/pollster methodology sections of Paper 2. Some aspect of this is likely covered in the Measurement sub-section of your Data section, but this would be much more detailed, and might include aspects like simulation and linkages to the literature, among other aspects.\n                \n                \n                  Referencing\n                  0 - 'Poor or not done'; 3 - 'One minor issue'; 4 - 'Perfect'\n                  All data, software, literature, and any other relevant material, should be cited in-text and included in a properly formatted reference list made using BibTeX. A few lines of code from Stack Overflow or similar, would be acknowledged just with a comment in the script immediately preceding the use of the code rather than here. But larger chunks of code should be fully acknowledged with an in-text citation and appear in the reference list. Check in-text citations and that you have not accidentally used (@my_cite) when you needed [@my_cite]. R packages and all other aspects should be correctly capitalized, and name should be correct e.g. use double braces appropriately in the BibTeX file.\n                \n                \n                  Commits\n                  0 - 'Poor or not done'; 2 - 'Done'\n                  There are at least a handful of different commits, and they have meaningful commit messages.\n                \n                \n                  Sketches\n                  0 - 'Poor or not done'; 2 - 'Done'\n                  Sketches are included in a labelled folder of the repo, appropriate, and of high-quality.\n                \n                \n                  Simulation\n                  0 - 'Poor or not done'; 1 - 'Some issues'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  The script is clearly commented and structured. All variables are appropriately simulated in a sophisticated way including appropriate interaction between simulated variables.\n                \n                \n                  Tests\n                  0 - 'Poor or not done'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  High-quality extensive suites of tests are written for the both the simulated and actual datasets. These suites must be in separate scripts. The suite should be extensive and put together in a sophisticated way using packages like testthat, validate, pointblank, or great expectations.\n                \n                \n                  Parquet\n                  0 - 'No'; 1 - 'Yes'\n                  The analysis dataset is saved as a parquet file. (Note that the raw data should be saved in whatever format it came.)\n                \n                \n                  Reproducible workflow\n                  0 - 'Poor or not done'; 1 - 'Some issues'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  Use an organized repo with a detailed README and an R project. Thoroughly document code and include a preamble, comments, nice structure, and style code with styler or lintr. Use seeds appropriately. Avoid leaving install.packages() in the code unless handled sophisticatedly. Exclude unnecessary files from the repo; avoid hard-coded paths and setwd(). Use base pipe not magrittr pipe. Comment on and close all GitHub issues. Deal with all branches.\n                \n                \n                  Enhancements\n                  0 - 'Poor or not done'; 1 - 'Some issues'; 2 - 'Acceptable'; 3 - 'Impressive'; 4 - 'Exceptional'\n                  You should pick at least one of the following and include it to enhance your submission: 1) a datasheet for the dataset; 2) a model card for the model; 3) a Shiny application; 4) an R package; or 5) an API for the model. If you would like to include an enhancement not on this list please email the instructor with your idea.\n                \n                \n                  Miscellaneous\n                  0 - 'None'; 1 - 'Notable'; 2 - 'Remarkable'; 3 - 'Exceptional'\n                  There are always students that excel in a way that is not anticipated in the rubric. This item accounts for that.\n                \n        \n      \n    \n\n\n\n\n\nF.8.5 이전 예시\n\n2024: 메리 청; 카비야 칼라니; 줄리아 김; 윈자오 리; 티모티우스 프라조기; 베니 로크버그; 아바스 슬레이만; 에밀리 수; 그리고 한나 유.\n2023: 알리야 막신 라모스; 클로이 티어스타인; 제이슨 응오; 제니 션; 로라 리-추; 그리고 세바스찬 로드리게스.\n2022: 알리시아 양; 이든 샌섬; 이반 리; 잭 맥케이; 올라에도 오크파레케; 그리고 티안 이 장.\n2021: 에이미 패로우; 지아 지아 지; 로라 클라인; 로레나 알마라즈 데 라 가르자; 그리고 레이첼 램.\n2020: 애니 콜린스.\n\n\n\n\n\nArel-Bundock, Vincent. 2024. tinytable: Simple and Configurable Tables in “HTML”, “LaTeX”, “Markdown”, “Word”, “PNG”, “PDF”, and “Typst” Formats. https://vincentarelbundock.github.io/tinytable/.\n\n\nBarba, Lorena. 2018. “Terminologies for Reproducible Research”. https://arxiv.org/abs/1802.03311.\n\n\nBlumenthal, Mark. 2014. “Polls, Forecasts, and Aggregators”. PS: Political Science & Politics 47 (02): 297–300. https://doi.org/10.1017/s1049096514000055.\n\n\nGebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé III, 와/과 Kate Crawford. 2021. “Datasheets for datasets”. Communications of the ACM 64 (12): 86–92. https://doi.org/10.1145/3458723.\n\n\nGelfand, Sharla. 2022. opendatatoronto: Access the City of Toronto Open Data Portal. https://CRAN.R-project.org/package=opendatatoronto.\n\n\nHuntington-Klein, Nick. 2022. “Library of Statistical Techniques”. https://lost-stats.github.io.\n\n\nPasek, Josh. 2015. “Predicting Elections: Considering Tools to Pool the Polls”. Public Opinion Quarterly 79 (2): 594–619. https://doi.org/10.1093/poq/nfu060.\n\n\nPenrose, Carly. 2024. “Deadly fires: Risk of death, injury highest in Toronto’s poor neighbourhoods”. CBC News, 4월. https://www.cbc.ca/news/canada/toronto/fatal-fires-lower-income-1.7177356.\n\n\nWickham, Hadley. 2016. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.",
    "crumbs": [
      "부록",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>논문</span>"
    ]
  },
  {
    "objectID": "25-papers_ko.html#footnotes",
    "href": "25-papers_ko.html#footnotes",
    "title": "Online Appendix F — 논문",
    "section": "",
    "text": "길라드는 이 목록에 포함되는 것에 대한 명시적인 허가와 격려를 주었습니다.↩︎\n이 용어는 Barba (2018) 를 따르지만, BITSS에서 사용되는 용어와는 반대입니다.↩︎\n미국 GSS는 개인 수준 데이터가 공개적으로 사용 가능하고 데이터셋이 잘 문서화되어 있기 때문에 여기에 권장됩니다. 그러나 특정 국가의 대학생들은 일반에 공개되지 않는 개인 수준 데이터에 접근할 수 있는 경우가 많으며, 이 경우 대신 해당 데이터를 사용해도 좋습니다. 호주 대학생들은 호주 일반 사회 조사의 개인 수준 데이터에 접근할 수 있을 것이며, 이를 사용할 수 있습니다. 캐나다 대학생들은 캐나다 일반 사회 조사의 개인 수준 데이터에 접근할 수 있을 것이며, 이를 사용하고 싶을 수 있습니다.↩︎",
    "crumbs": [
      "부록",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>논문</span>"
    ]
  },
  {
    "objectID": "20-r_essentials_ko.html#시작하기",
    "href": "20-r_essentials_ko.html#시작하기",
    "title": "Online Appendix A — R 필수 사항",
    "section": "A.3 시작하기",
    "text": "A.3 시작하기\n이제 코드를 살펴보겠습니다. 직접 모든 것을 작성하십시오.\n콘솔에서 한 줄씩 작업하는 것도 괜찮지만, 전체 스크립트를 작성한 다음 실행하는 것이 더 쉽습니다. R 스크립트(“파일” \\(\\rightarrow\\) “새 파일” \\(\\rightarrow\\) “R 스크립트”)를 만들어 이를 수행할 것입니다. 콘솔 창은 왼쪽 하단으로 이동하고 R 스크립트가 왼쪽 상단에 열릴 것입니다. 호주 연방 정치인 전체를 가져온 다음 총리의 성별에 대한 작은 표를 만드는 코드를 작성할 것입니다. 이 코드 중 일부는 이 단계에서는 이해가 안 될 수도 있지만, 습관을 들이기 위해 모두 입력한 다음 실행하십시오. 전체 스크립트를 실행하려면 “실행”을 클릭하거나 특정 줄을 강조 표시한 다음 “실행”을 클릭하여 해당 줄만 실행할 수 있습니다.\n\n# 필요한 패키지 설치\ninstall.packages(\"tidyverse\")\ninstall.packages(\"AustralianPoliticians\")\n\n\n# 이번에 사용할 패키지 로드\nlibrary(tidyverse)\nlibrary(AustralianPoliticians)\n\n# 총리의 성별 개수 표 만들기\nget_auspol(\"all\") |&gt; # GitHub에서 데이터 가져오기\n  as_tibble() |&gt;\n  filter(wasPrimeMinister == 1) |&gt;\n  count(gender)\n\n# A tibble: 2 × 2\n  gender     n\n  &lt;chr&gt;  &lt;int&gt;\n1 female     1\n2 male      29\n\n\n2021년 말 기준으로 여성 총리는 한 명(줄리아 길라드)이었고, 나머지 29명의 총리는 남성이었습니다.\n프로그래밍에서 중요한 연산자 중 하나는 “파이프”: |&gt;. 우리는 이것을 “그리고 나서”라고 읽습니다. 이것은 코드 줄의 출력을 가져와 다음 코드 줄의 첫 번째 입력으로 사용합니다. 코드를 더 읽기 쉽게 만듭니다. 배경 지식으로, 수년 동안 R 사용자들은 magrittr(Bache 와/과 Wickham 2022)에서 온 %&gt;%를 파이프로 사용했으며, 이는 tidyverse의 일부입니다. Base R은 2021년에 우리가 이 책에서 사용하는 파이프 |&gt;를 추가했으므로, 오래된 코드를 보면 이전 파이프가 사용되는 것을 볼 수 있습니다. 대부분의 경우, 서로 교환 가능합니다.\n파이프의 아이디어는 데이터셋을 가져와서 무언가를 하는 것입니다. 우리는 이전 예시에서 이것을 사용했습니다. 다음은 head()에 파이프하여 데이터셋의 처음 6줄을 살펴보는 또 다른 예시입니다. head()는 이 예시에서 명시적으로 인수를 취하지 않는다는 점에 유의하십시오. 파이프가 암시적으로 표시할 데이터를 알려주기 때문에 어떤 데이터를 표시할지 알고 있습니다.\n\nget_auspol(\"all\") |&gt; # GitHub에서 데이터 가져오기\n  head()\n\n# A tibble: 6 × 20\n  uniqueID   surname allOtherNames          firstName commonName displayName    \n  &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;                  &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;          \n1 Abbott1859 Abbott  Richard Hartley Smith  Richard   &lt;NA&gt;       Abbott, Richard\n2 Abbott1869 Abbott  Percy Phipps           Percy     &lt;NA&gt;       Abbott, Percy  \n3 Abbott1877 Abbott  Macartney              Macartney Mac        Abbott, Mac    \n4 Abbott1886 Abbott  Charles Lydiard Aubrey Charles   Aubrey     Abbott, Aubrey \n5 Abbott1891 Abbott  Joseph Palmer          Joseph    &lt;NA&gt;       Abbott, Joseph \n6 Abbott1957 Abbott  Anthony John           Anthony   Tony       Abbott, Tony   \n# ℹ 14 more variables: earlierOrLaterNames &lt;chr&gt;, title &lt;chr&gt;, gender &lt;chr&gt;,\n#   birthDate &lt;date&gt;, birthYear &lt;dbl&gt;, birthPlace &lt;chr&gt;, deathDate &lt;date&gt;,\n#   member &lt;dbl&gt;, senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;, wikidataID &lt;chr&gt;,\n#   wikipedia &lt;chr&gt;, adb &lt;chr&gt;, comments &lt;chr&gt;\n\n\n이 R 스크립트를 “my_first_r_script.R”로 저장할 수 있습니다(“파일” \\(\\rightarrow\\) “다른 이름으로 저장”). 이 시점에서 작업 공간은 그림 A.2 와 같이 보일 것입니다.\n\n\n\n\n\n\n그림 A.2: R 스크립트 실행 후\n\n\n\n각 Posit Cloud 작업 공간은 본질적으로 새로운 컴퓨터라는 점을 알아두십시오. 이 때문에 각 작업 공간에서 사용하려는 모든 패키지를 설치해야 합니다. 예를 들어, tidyverse를 사용하기 전에 install.packages(\"tidyverse\")로 설치해야 합니다. 이는 자신의 컴퓨터를 사용하는 것과 대조됩니다.\nPosit Cloud에 대한 몇 가지 마지막 참고 사항:\n\n호주 정치인 예시에서 우리는 GitHub 웹사이트에서 R 패키지를 사용하여 데이터를 가져왔지만, 다양한 방법으로 로컬 컴퓨터에서 작업 공간으로 데이터를 가져올 수 있습니다. 한 가지 방법은 “파일” 패널의 “업로드” 버튼을 사용하는 것입니다. 다른 방법은 tidyverse(Wickham 기타 2019)의 일부인 readr(Wickham, Hester, 와/과 Bryan 2022)를 사용하는 것입니다.\nPosit Cloud는 어느 정도의 협업을 허용합니다. 예를 들어, 다른 사람에게 자신이 만든 작업 공간에 대한 접근 권한을 부여하고 동시에 동일한 작업 공간에 있을 수도 있습니다. 이는 협업에 유용할 수 있습니다.\nPosit Cloud에는 다양한 약점이 있습니다. 특히 RAM 제한이 있습니다. 또한, 다른 웹 애플리케이션과 마찬가지로 때때로 문제가 발생하거나 중단될 수 있습니다.",
    "crumbs": [
      "부록",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R 필수 사항</span>"
    ]
  },
  {
    "objectID": "20-r_essentials_ko.html#tidyverse-탐색",
    "href": "20-r_essentials_ko.html#tidyverse-탐색",
    "title": "Online Appendix A — R 필수 사항",
    "section": "A.7 tidyverse 탐색",
    "text": "A.7 tidyverse 탐색\ntidyverse의 두 가지 측면인 dplyr와 ggplot2에 초점을 맞췄습니다. 그러나 tidyverse는 다양한 패키지와 함수로 구성되어 있습니다. 이제 네 가지 일반적인 측면을 살펴보겠습니다.\n\n데이터 가져오기 및 tibble();\n데이터셋 결합 및 피벗;\n문자열 조작 및 stringr;\n요인 변수 및 forcats.\n\n그러나 첫 번째 작업은 명명법을 다루는 것이며, 특히 “tidyverse”에서 “tidy”가 무엇을 의미하는지 명확히 하는 것입니다. 이름은 정돈된 데이터를 의미하며, 그 이점은 데이터가 지저분할 수 있는 다양한 방법이 있지만, 정돈된 데이터는 세 가지 규칙을 충족한다는 것입니다. 이는 데이터셋의 구조가 세부 사항에 관계없이 일관되며, 특정 유형의 입력을 예상하는 함수를 적용하기가 더 쉽다는 것을 의미합니다. 정돈된 데이터는 다음을 충족하는 데이터셋을 의미합니다(Wickham, Çetinkaya-Rundel, 와/과 Grolemund [2016년] 2023; Wickham 2014, p. 4).\n\n모든 변수는 자체 열에 있습니다.\n모든 관측치는 자체 행에 있습니다.\n모든 값은 자체 셀에 있습니다.\n\n표 A.1 는 연령과 머리카락이 열을 공유하므로 정돈되지 않았습니다. 표 A.2 는 정돈된 데이터입니다.\n\n\n\n\n표 A.1: 정돈되지 않은 데이터의 예시\n\n\n\n\n\n\n사람\n변수\n값\n\n\n\n\n로한\n나이\n35\n\n\n로한\n머리카락\n검정\n\n\n모니카\n나이\n35\n\n\n모니카\n머리카락\n금발\n\n\n에드워드\n나이\n3\n\n\n에드워드\n머리카락\n갈색\n\n\n휴고\n나이\n1\n\n\n휴고\n머리카락\n금발\n\n\n\n\n\n\n\n\n\n\n\n\n표 A.2: 정돈된 데이터의 예시\n\n\n\n\n\n\n사람\n나이\n머리카락\n\n\n\n\n로한\n35\n검정\n\n\n모니카\n35\n금발\n\n\n에드워드\n3\n갈색\n\n\n휴고\n1\n금발\n\n\n\n\n\n\n\n\n\nA.7.1 데이터 가져오기 및 tibble()\n데이터를 R로 가져와 사용할 수 있는 다양한 방법이 있습니다. CSV 파일의 경우 readr(Wickham, Hester, 와/과 Bryan 2022)의 read_csv()가 있고, Stata 파일의 경우 haven(Wickham, Miller, 와/과 Smith 2023)의 read_dta()가 있습니다.\nCSV는 일반적인 형식이며, 데이터를 수정하지 않는다는 점을 포함하여 많은 장점이 있습니다. 각 열은 쉼표로 구분되고, 각 행은 레코드입니다. read_csv()에 URL 또는 로컬 파일을 제공하여 읽을 수 있습니다. read_csv()에 전달할 수 있는 다양한 옵션이 있으며, 데이터셋에 열 이름이 있는지, 열의 유형, 건너뛸 줄 수를 지정할 수 있습니다. 열의 유형을 지정하지 않으면 read_csv()는 데이터셋을 보고 추측합니다.\n우리는 read_dta()를 사용하여 .dta 파일을 읽습니다. 이 파일은 통계 프로그램 Stata에서 일반적으로 생성됩니다. 이는 사회학, 정치학, 경제학과 같은 분야에서 흔히 사용된다는 것을 의미합니다. 이 형식은 데이터를 레이블과 분리하므로, 일반적으로 labelled(Larmarange 2023)의 to_factor()를 사용하여 다시 결합합니다. haven은 tidyverse의 일부이지만, ggplot2와 같은 패키지와 달리 기본적으로 자동으로 로드되지 않으므로 library(haven)을 실행해야 합니다.\n일반적으로 데이터셋은 R에 “data.frame”으로 들어옵니다. 이것은 유용할 수 있지만, 데이터셋에 대한 또 다른 유용한 클래스는 “tibble”입니다. 이것은 tidyverse의 일부인 tibble의 tibble()을 사용하여 생성할 수 있습니다. 티블은 데이터 프레임이지만, 기본적으로 문자열을 요인으로 변환하지 않고, 열의 클래스를 표시하고, 깔끔하게 인쇄하는 등 작업하기 쉽게 만드는 몇 가지 특정 변경 사항이 있습니다.\n필요한 경우 수동으로 티블을 만들 수 있습니다. 예를 들어, 데이터를 시뮬레이션할 때입니다. 그러나 일반적으로 read_csv()를 사용할 때와 같이 데이터를 티블로 직접 가져옵니다.\n\npeople_as_dataframe &lt;-\n  data.frame(\n    names = c(\"로한\", \"모니카\"),\n    website = c(\"rohanalexander.com\", \"monicaalexander.com\"),\n    fav_color = c(\"파란색\", \"흰색\")\n  )\nclass(people_as_dataframe)\n\n[1] \"data.frame\"\n\npeople_as_dataframe\n\n   names             website fav_color\n1   로한  rohanalexander.com    파란색\n2 모니카 monicaalexander.com      흰색\n\npeople_as_tibble &lt;-\n  tibble(\n    names = c(\"로한\", \"모니카\"),\n    website = c(\"rohanalexander.com\", \"monicaalexander.com\"),\n    fav_color = c(\"파란색\", \"흰색\")\n  )\npeople_as_tibble\n\n# A tibble: 2 × 3\n  names  website             fav_color\n  &lt;chr&gt;  &lt;chr&gt;               &lt;chr&gt;    \n1 로한   rohanalexander.com  파란색   \n2 모니카 monicaalexander.com 흰색     \n\nclass(people_as_tibble)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n\n\nA.7.2 결합 및 피벗을 사용한 데이터셋 조작\n자주 필요한 데이터셋 조작은 결합과 피벗입니다.\n두 개 이상의 데이터셋이 있고 그것들을 결합하는 데 관심이 있는 경우가 많습니다. 다양한 방법으로 데이터셋을 결합할 수 있습니다. 일반적인 방법은 dplyr(Wickham 기타 2022)의 left_join()을 사용하는 것입니다. 이것은 우리가 사용하는 하나의 주요 데이터셋이 있고, 거기에 추가하고 싶은 유용한 변수가 있는 다른 데이터셋이 있을 때 가장 유용합니다. 중요한 측면은 두 데이터셋을 연결하는 데 사용할 수 있는 열 또는 열이 있다는 것입니다. 여기서는 두 개의 티블을 만든 다음 이름을 기준으로 결합할 것입니다.\n\nmain_dataset &lt;-\n  tibble(\n    names = c(\"로한\", \"모니카\", \"에드워드\", \"휴고\"),\n    status = c(\"성인\", \"성인\", \"어린이\", \"유아\")\n  )\nmain_dataset\n\n# A tibble: 4 × 2\n  names    status\n  &lt;chr&gt;    &lt;chr&gt; \n1 로한     성인  \n2 모니카   성인  \n3 에드워드 어린이\n4 휴고     유아  \n\nsupplementary_dataset &lt;-\n  tibble(\n    names = c(\"로한\", \"모니카\", \"에드워드\", \"휴고\"),\n    favorite_food = c(\"파스타\", \"연어\", \"피자\", \"우유\")\n  )\nsupplementary_dataset\n\n# A tibble: 4 × 2\n  names    favorite_food\n  &lt;chr&gt;    &lt;chr&gt;        \n1 로한     파스타       \n2 모니카   연어         \n3 에드워드 피자         \n4 휴고     우유         \n\nmain_dataset &lt;-\n  main_dataset |&gt;\n  left_join(supplementary_dataset, by = \"names\")\n\nmain_dataset\n\n# A tibble: 4 × 3\n  names    status favorite_food\n  &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;        \n1 로한     성인   파스타       \n2 모니카   성인   연어         \n3 에드워드 어린이 피자         \n4 휴고     유아   우유         \n\n\ninner_join(), right_join(), full_join()을 포함하여 데이터셋을 결합하는 다양한 다른 옵션이 있습니다.\n또 다른 일반적인 데이터셋 조작 작업은 피벗입니다. 데이터셋은 길거나 넓은 경향이 있습니다. 긴 데이터는 각 변수가 행에 있으므로 반복이 있을 수 있는 반면, 넓은 데이터는 각 변수가 열이므로 일반적으로 반복이 거의 없습니다(Wickham 2009). 예를 들어, “anscombe”는 넓고, “anscombe_long”은 깁니다.\n\nanscombe\n\n   x1 x2 x3 x4    y1   y2    y3    y4\n1  10 10 10  8  8.04 9.14  7.46  6.58\n2   8  8  8  8  6.95 8.14  6.77  5.76\n3  13 13 13  8  7.58 8.74 12.74  7.71\n4   9  9  9  8  8.81 8.77  7.11  8.84\n5  11 11 11  8  8.33 9.26  7.81  8.47\n6  14 14 14  8  9.96 8.10  8.84  7.04\n7   6  6  6  8  7.24 6.13  6.08  5.25\n8   4  4  4 19  4.26 3.10  5.39 12.50\n9  12 12 12  8 10.84 9.13  8.15  5.56\n10  7  7  7  8  4.82 7.26  6.42  7.91\n11  5  5  5  8  5.68 4.74  5.73  6.89\n\n\n\nanscombe_long\n\n# A tibble: 44 × 3\n   set       x     y\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 1        10  8.04\n 2 2        10  9.14\n 3 3        10  7.46\n 4 4         8  6.58\n 5 1         8  6.95\n 6 2         8  8.14\n 7 3         8  6.77\n 8 4         8  5.76\n 9 1        13  7.58\n10 2        13  8.74\n# ℹ 34 more rows\n\n\n일반적으로 tidyverse에서, 그리고 확실히 ggplot2에서는 긴 데이터가 필요합니다. 한 형식에서 다른 형식으로 이동하려면 tidyr(Wickham, Vaughan, 와/과 Girlich 2023)의 pivot_longer()와 pivot_wider()를 사용합니다.\n세 년도 각각에서 “마크” 또는 “로렌”이 달리기 경주에서 우승했는지에 대한 넓은 데이터를 생성할 것입니다.\n\npivot_example_data &lt;-\n  tibble(\n    year = c(2019, 2020, 2021),\n    mark = c(\"1위\", \"2위\", \"1위\"),\n    lauren = c(\"2위\", \"1위\", \"2위\")\n  )\n\npivot_example_data\n\n# A tibble: 3 × 3\n   year mark  lauren\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n1  2019 1위   2위   \n2  2020 2위   1위   \n3  2021 1위   2위   \n\n\n이 데이터셋은 현재 넓은 형식입니다. 긴 형식으로 만들려면 사람을 지정하는 열과 결과를 지정하는 열이 필요합니다. pivot_longer()를 사용하여 이를 달성합니다.\n\ndata_pivoted_longer &lt;-\n  pivot_example_data |&gt;\n  pivot_longer(\n    cols = c(\"mark\", \"lauren\"),\n    names_to = \"person\",\n    values_to = \"position\"\n  )\n\nhead(data_pivoted_longer)\n\n# A tibble: 6 × 3\n   year person position\n  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;   \n1  2019 mark   1위     \n2  2019 lauren 2위     \n3  2020 mark   2위     \n4  2020 lauren 1위     \n5  2021 mark   1위     \n6  2021 lauren 2위     \n\n\n때때로 긴 데이터에서 넓은 데이터로 이동해야 합니다. pivot_wider()를 사용하여 이를 수행합니다.\n\ndata_pivoted_wider &lt;-\n  data_pivoted_longer |&gt;\n  pivot_wider(\n    names_from = \"person\",\n    values_from = \"position\"\n  )\n\nhead(data_pivoted_wider)\n\n# A tibble: 3 × 3\n   year mark  lauren\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n1  2019 1위   2위   \n2  2020 2위   1위   \n3  2021 1위   2위   \n\n\n\n\nA.7.3 문자열 조작 및 stringr\nR에서는 종종 큰따옴표로 문자열을 생성하지만, 작은따옴표도 작동합니다. 예를 들어 c(\"a\", \"b\")는 문자 벡터에 포함된 두 개의 문자열 “a”와 “b”로 구성됩니다. R에서 문자열을 조작하는 다양한 방법이 있으며, stringr(Wickham 2022)에 초점을 맞춥니다. 이것은 tidyverse를 로드할 때 자동으로 로드됩니다.\n문자열에 특정 내용이 포함되어 있는지 확인하려면 str_detect()를 사용할 수 있습니다. 그리고 특정 내용을 제거하거나 변경하려면 str_remove() 또는 str_replace()를 사용할 수 있습니다.\n\ndataset_of_strings &lt;-\n  tibble(\n    names = c(\n      \"로한 알렉산더\",\n      \"모니카 알렉산더\",\n      \"에드워드 알렉산더\",\n      \"휴고 알렉산더\"\n    )\n  )\n\ndataset_of_strings |&gt;\n  mutate(\n    is_rohan = str_detect(names, \"로한\"),\n    make_howlett = str_replace(names, \"알렉산더\", \"하울렛\"),\n    remove_rohan = str_remove(names, \"로한\")\n  )\n\n# A tibble: 4 × 4\n  names             is_rohan make_howlett    remove_rohan       \n  &lt;chr&gt;             &lt;lgl&gt;    &lt;chr&gt;           &lt;chr&gt;              \n1 로한 알렉산더     TRUE     로한 하울렛     \" 알렉산더\"        \n2 모니카 알렉산더   FALSE    모니카 하울렛   \"모니카 알렉산더\"  \n3 에드워드 알렉산더 FALSE    에드워드 하울렛 \"에드워드 알렉산더\"\n4 휴고 알렉산더     FALSE    휴고 하울렛     \"휴고 알렉산더\"    \n\n\n데이터 정리에서 특히 유용한 다양한 다른 함수가 있습니다. 예를 들어, str_length()를 사용하여 문자열의 길이를 찾고, str_c()를 사용하여 문자열을 결합할 수 있습니다.\n\ndataset_of_strings |&gt;\n  mutate(\n    length_is = str_length(string = names),\n    name_and_length = str_c(names, length_is, sep = \" - \")\n  )\n\n# A tibble: 4 × 3\n  names             length_is name_and_length      \n  &lt;chr&gt;                 &lt;int&gt; &lt;chr&gt;                \n1 로한 알렉산더             7 로한 알렉산더 - 7    \n2 모니카 알렉산더           8 모니카 알렉산더 - 8  \n3 에드워드 알렉산더         9 에드워드 알렉산더 - 9\n4 휴고 알렉산더             7 휴고 알렉산더 - 7    \n\n\n마지막으로, tidyr의 separate()는 stringr의 일부는 아니지만, 문자열 조작에 필수적입니다. 하나의 문자 열을 여러 개로 변환합니다.\n\ndataset_of_strings |&gt;\n  separate(\n    col = names,\n    into = c(\"first\", \"last\"),\n    sep = \" \",\n    remove = FALSE\n  )\n\n# A tibble: 4 × 3\n  names             first    last    \n  &lt;chr&gt;             &lt;chr&gt;    &lt;chr&gt;   \n1 로한 알렉산더     로한     알렉산더\n2 모니카 알렉산더   모니카   알렉산더\n3 에드워드 알렉산더 에드워드 알렉산더\n4 휴고 알렉산더     휴고     알렉산더\n\n\n\n\nA.7.4 요인 변수 및 forcats\n요인은 범주인 문자열 모음입니다. 때로는 고유한 순서가 있을 것입니다. 예를 들어, 요일은 월요일, 화요일, 수요일…과 같은 순서를 가지며, 이는 알파벳순이 아닙니다. 그러나 항상 그럴 필요는 없습니다. 예를 들어 임신 상태: 임신 또는 비임신. 요인은 base R에서 두드러지게 나타납니다. 적절한 문자열만 허용되도록 보장하므로 유용할 수 있습니다. 예를 들어, “days_of_the_week”가 요인 변수라면 “January”는 허용되지 않을 것입니다. 그러나 많은 복잡성을 추가할 수 있으므로 tidyverse에서는 덜 중요한 역할을 합니다. 그럼에도 불구하고 특정 상황에서는 요인을 활용하는 것이 유용합니다. 예를 들어, 요일을 플로팅할 때 문자 변수로 가지고 있다면 알파벳순으로 정렬되는 것보다 일반적인 순서로 정렬되기를 원할 것입니다. 요인은 base R에 내장되어 있지만, 요인을 사용할 때 특히 유용한 tidyverse 패키지 중 하나는 forcats(Wickham 2023)입니다.\n때로는 문자 벡터가 있고 특정 방식으로 정렬되기를 원할 것입니다. 기본값은 문자 벡터가 알파벳순으로 정렬되는 것이지만, 우리는 그것을 원하지 않을 수 있습니다. 예를 들어, 요일이 알파벳순으로 정렬되면 그래프에서 이상하게 보일 것입니다: 금요일, 월요일, 토요일, 일요일, 목요일, 화요일, 수요일!\n정렬을 변경하는 방법은 변수를 문자에서 요인으로 변경하는 것입니다. forcats(Wickham 2023)의 fct_relevel()을 사용하여 정렬을 지정할 수 있습니다.\n\nset.seed(853)\n\ndays_data &lt;-\n  tibble(\n    days =\n      c(\n        \"월요일\",\n        \"화요일\",\n        \"수요일\",\n        \"목요일\",\n        \"금요일\",\n        \"토요일\",\n        \"일요일\"\n      ),\n    some_value = c(sample.int(100, 7))\n  )\n\ndays_data &lt;-\n  days_data |&gt;\n  mutate(\n    days_as_factor = factor(days),\n    days_as_factor = fct_relevel(\n      days,\n      \"월요일\",\n      \"화요일\",\n      \"수요일\",\n      \"목요일\",\n      \"금요일\",\n      \"토요일\",\n      \"일요일\"\n    )\n  )\n\n그리고 x축에 원본 문자 벡터를 사용하여 그래프를 그린 다음, x축에 요인 벡터를 사용하여 다른 그래프를 그려 결과를 비교할 수 있습니다.\n\ndays_data |&gt;\n  ggplot(aes(x = days, y = some_value)) +\n  geom_point()\n\ndays_data |&gt;\n  ggplot(aes(x = days_as_factor, y = some_value)) +\n  geom_point()",
    "crumbs": [
      "부록",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R 필수 사항</span>"
    ]
  },
  {
    "objectID": "99-references_ko.html",
    "href": "99-references_ko.html",
    "title": "참고 문헌",
    "section": "",
    "text": "Abelson, Harold, and Gerald Jay Sussman. 1996. Structure and\nInterpretation of Computer Programs. Cambridge: The MIT Press.\n\n\nAcemoglu, Daron, Simon Johnson, and James Robinson. 2001. “The\nColonial Origins of Comparative Development: An Empirical\nInvestigation.” American Economic Review 91\n(5): 1369–1401. https://doi.org/10.1257/aer.91.5.1369.\n\n\nAchen, Christopher. 1978. “Measuring Representation.”\nAmerican Journal of Political Science 22 (3): 475–510. https://doi.org/10.2307/2110458.\n\n\nAkerlof, George. 1970. “The Market for ‘Lemons’:\nQuality Uncertainty and the Market Mechanism.” The Quarterly\nJournal of Economics 84 (3): 488–500. https://doi.org/10.2307/1879431.\n\n\nAlexander, Monica. 2019. “Analyzing Name Changes After Marriage\nUsing a Non-Representative Survey,” August. https://www.monicaalexander.com/posts/2019-08-07-mrp/.\n\n\n———. 2021. “Overcoming Barriers to Sharing Code.”\nYouTube, February. https://youtu.be/yvM2C6aZ94k.\n\n\nAlexander, Rohan, and Paul Hodgetts. 2021.\nAustralianPoliticians: Provides Datasets About Australian\nPoliticians. https://CRAN.R-project.org/package=AustralianPoliticians.\n\n\nAlexander, Rohan, and A Mahfouz. 2021. heapsofpapers: Easily Download Heaps of PDF and CSV\nFiles. https://CRAN.R-project.org/package=heapsofpapers.\n\n\nAlexopoulos, Michelle, and Jon Cohen. 2015. “The power of print: Uncertainty shocks, markets, and the\neconomy.” International Review of Economics\n& Finance 40 (November): 8–28. https://doi.org/10.1016/j.iref.2015.02.002.\n\n\nAllen, Jeff. 2021. plumberDeploy: Plumber\nDeployment. https://CRAN.R-project.org/package=plumberDeploy.\n\n\nAlsan, Marcella, and Amy Finkelstein. 2021. “Beyond Causality:\nAdditional Benefits of Randomized Controlled Trials for Improving Health\nCare Delivery.” The Milbank Quarterly 99 (4): 864–81. https://doi.org/10.1111/1468-0009.12521.\n\n\nAmaka, Ofunne, and Amber Thomas. 2021. “The Naked Truth: How the\nNames of 6,816 Complexion Products Can Reveal Bias in Beauty.”\nThe Pudding, March. https://pudding.cool/2021/03/foundation-names/.\n\n\nAmerican Medical Association and New York Academy of Medicine. 1848.\nCode of Medical Ethics. Academy of Medicine. https://hdl.handle.net/2027/chi.57108026.\n\n\nAndersen, Robert, and David Armstrong. 2021. Presenting Statistical\nResults Effectively. London: Sage.\n\n\nAnderson, Margo, and Stephen Fienberg. 1999. Who Counts?: The Politics of Census-Taking in\nContemporary America. Russell Sage Foundation. http://www.jstor.org/stable/10.7758/9781610440059.\n\n\nAngelucci, Charles, and Julia Cagé. 2019. “Newspapers in Times of\nLow Advertising Revenues.” American Economic Journal:\nMicroeconomics 11 (3): 319–64. https://doi.org/10.1257/mic.20170306.\n\n\nAngrist, Joshua, and Jörn-Steffen Pischke. 2010. “The Credibility\nRevolution in Empirical Economics: How Better Research Design Is Taking\nthe Con Out of Econometrics.” Journal of Economic\nPerspectives 24 (2): 3–30. https://doi.org/10.1257/jep.24.2.3.\n\n\nAnnas, George. 2003. “HIPAA Regulations: A New Era of\nMedical-Record Privacy?” New England Journal of Medicine\n348 (15): 1486–90. https://doi.org/10.1056/NEJMlim035027.\n\n\nAnsolabehere, Stephen, Brian Schaffner, and Sam Luks. 2021. “Guide to the 2020 Cooperative Election\nStudy.” https://doi.org/10.7910/DVN/E9N6PH.\n\n\nArel-Bundock, Vincent. 2021. WDI: World\nDevelopment Indicators and Other World Bank Data. https://CRAN.R-project.org/package=WDI.\n\n\n———. 2022. “modelsummary: Data and\nModel Summaries in R.” Journal of Statistical\nSoftware 103 (1): 1–23. https://doi.org/10.18637/jss.v103.i01.\n\n\n———. 2023. marginaleffects: Predictions,\nComparisons, Slopes, Marginal Means, and Hypothesis Tests.\nhttps://vincentarelbundock.github.io/marginaleffects/.\n\n\n———. 2024. tinytable: Simple and Configurable\nTables in “HTML,” “LaTeX,”\n“Markdown,” “Word,” “PNG,”\n“PDF,” and “Typst” Formats. https://vincentarelbundock.github.io/tinytable/.\n\n\nArmstrong, Zan. 2022. “Stop Aggregating Away the Signal in Your\nData.” The Overflow, March. https://stackoverflow.blog/2022/03/03/stop-aggregating-away-the-signal-in-your-data/.\n\n\nArnold, Jeffrey. 2021. ggthemes: Extra Themes,\nScales and Geoms for “ggplot2”. https://CRAN.R-project.org/package=ggthemes.\n\n\nAsher, Sam, Tobias Lunt, Ryu Matsuura, and Paul Novosad. 2021.\n“Development Research at High Geographic Resolution: An Analysis\nof Night Lights, Firms, and Poverty in India Using the SHRUG Open Data\nPlatform.” World Bank Economic Review 35 (4). https://shrug-assets-ddl.s3.amazonaws.com/static/main/assets/other/almn-shrug.pdf.\n\n\nAu, Randy. 2020. “Data Cleaning IS Analysis, Not Grunt\nWork,” September. https://counting.substack.com/p/data-cleaning-is-analysis-not-grunt.\n\n\nBååth, Rasmus. 2018. beepr: Easily Play\nNotification Sounds on any Platform. https://CRAN.R-project.org/package=beepr.\n\n\nBache, Stefan Milton, and Hadley Wickham. 2022. magrittr: A Forward-Pipe Operator for R. https://CRAN.R-project.org/package=magrittr.\n\n\nBackus, John. 1981. “The History of FORTRAN\nI, II, and III.” In History of Programming\nLanguages, edited by Richard Wexelblat, 25–74. Academic Press.\n\n\nBailey, Rosemary. 2008. Design of Comparative Experiments.\nCambridge: Cambridge University Press. https://doi.org/10.1017/CBO9780511611483.\n\n\nBaker, Reg, Michael Brick, Nancy Bates, Mike Battaglia, Mick Couper,\nJill Dever, Krista Gile, and Roger Tourangeau. 2013. “Summary Report of the AAPOR Task Force on Non-Probability\nSampling.” Journal of Survey Statistics and\nMethodology 1 (2): 90–143. https://doi.org/10.1093/jssam/smt008.\n\n\nBandy, John, and Nicholas Vincent. 2021. “Addressing\n‘Documentation Debt’ in Machine Learning: A Retrospective\nDatasheet for BookCorpus.” In Proceedings of the Neural\nInformation Processing Systems Track on Datasets and Benchmarks,\nedited by J. Vanschoren and S. Yeung. Vol. 1. https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/file/54229abfcfa5649e7003b83dd4755294-Paper-round1.pdf.\n\n\nBanerjee, Abhijit, and Esther Duflo. 2011. Poor Economics: A Radical\nRethinking of the Way to Fight Global Poverty. New York:\nPublicAffairs.\n\n\nBarba, Lorena. 2018. “Terminologies for Reproducible\nResearch.” https://arxiv.org/abs/1802.03311.\n\n\nBarrett, Malcolm. 2021a. Data Science as an Atomic Habit. https://malco.io/articles/2021-01-04-data-science-as-an-atomic-habit.\n\n\n———. 2021b. ggdag: Analyze and Create Elegant\nDirected Acyclic Graphs. https://CRAN.R-project.org/package=ggdag.\n\n\nBarron, Alexander, Jenny Huang, Rebecca Spang, and Simon DeDeo. 2018.\n“Individuals, Institutions, and Innovation in the Debates of the\nFrench Revolution.” Proceedings of the National Academy of\nSciences 115 (18): 4607–12. https://doi.org/10.1073/pnas.1717729115.\n\n\nBaumgartner, Jason, Savvas Zannettou, Brian Keegan, Megan Squire, and\nJeremy Blackburn. 2020. “The Pushshift Reddit Dataset.”\narXiv. https://doi.org/10.48550/arxiv.2001.08435.\n\n\nBecker, Richard, Allan Wilks, Ray Brownrigg, Thomas Minka, and Alex\nDeckmyn. 2022. maps: Draw Geographical\nMaps. https://CRAN.R-project.org/package=maps.\n\n\nBegley, Glenn, and Lee Ellis. 2012. “Raise Standards for\nPreclinical Cancer Research.” Nature 483 (7391):\n531--533. https://doi.org/10.1038/483531a.\n\n\nBengtsson, Henrik. 2021. “A Unifying\nFramework for Parallel and Distributed Processing in R using\nFutures.” The R Journal 13 (2): 208–27. https://doi.org/10.32614/RJ-2021-048.\n\n\nBenoit, Kenneth. 2020. “Text as Data: An Overview.” In\nThe SAGE Handbook of Research Methods in Political Science and\nInternational Relations, edited by Luigi Curini and Robert\nFranzese, 461–97. London: SAGE Publishing. https://doi.org/10.4135/9781526486387.n29.\n\n\nBenoit, Kenneth, Kohei Watanabe, Haiyan Wang, Paul Nulty, Adam Obeng,\nStefan Müller, and Akitaka Matsuo. 2018. “quanteda: An R package for the quantitative analysis of\ntextual data.” Journal of Open Source Software 3\n(30): 774. https://doi.org/10.21105/joss.00774.\n\n\nBensinger, Greg. 2020. “Google Redraws the Borders on Maps\nDepending on Who’s Looking.” The Washington Post,\nFebruary. https://www.washingtonpost.com/technology/2020/02/14/google-maps-political-borders/.\n\n\nBerdine, Gilbert, Vincent Geloso, and Benjamin Powell. 2018.\n“Cuban Infant Mortality and Longevity: Health Care or\nRepression?” Health Policy and Planning 33 (6): 755–57.\nhttps://doi.org/10.1093/heapol/czy033.\n\n\nBerkson, Joseph. 1946. “Limitations of the Application of Fourfold\nTable Analysis to Hospital Data.” Biometrics Bulletin 2\n(3): 47–53. https://doi.org/10.2307/3002000.\n\n\nBerners-Lee, Timothy. 1989. “Information Management: A\nProposal.” https://www.w3.org/History/1989/proposal.html.\n\n\nBickel, Peter, Eugene Hammel, and William O’Connell. 1975. “Sex\nBias in Graduate Admissions: Data from Berkeley: Measuring Bias Is\nHarder Than Is Usually Assumed, and the Evidence Is Sometimes Contrary\nto Expectation.” Science 187 (4175): 398–404. https://doi.org/10.1126/science.187.4175.398.\n\n\nBiderman, Stella, Kieran Bicheno, and Leo Gao. 2022. “Datasheet\nfor the Pile.” https://arxiv.org/abs/2201.07311.\n\n\nBirkmeyer, John, Jonathan Finks, Amanda O’Reilly, Mary Oerline, Arthur\nCarlin, Andre Nunn, Justin Dimick, Mousumi Banerjee, and Nancy\nBirkmeyer. 2013. “Surgical Skill and Complication Rates After\nBariatric Surgery.” New England Journal of Medicine 369\n(15): 1434–42. https://doi.org/10.1056/nejmsa1300625.\n\n\nBlair, Graeme, Jasper Cooper, Alexander Coppock, and Macartan Humphreys.\n2019. “Declaring and Diagnosing Research Designs.”\nAmerican Political Science Review 113 (3): 838–59. https://doi.org/10.1017/S0003055419000194.\n\n\nBlair, Graeme, Jasper Cooper, Alexander Coppock, Macartan Humphreys, and\nLuke Sonnet. 2021. estimatr: Fast Estimators\nfor Design-Based Inference. https://CRAN.R-project.org/package=estimatr.\n\n\nBlair, James. 2019. Democratizing R with\nPlumber APIs. https://posit.co/resources/videos/democratizing-r-with-plumber-apis/.\n\n\nBland, Martin, and Douglas Altman. 1986. “Statistical Methods for\nAssessing Agreement Between Two Methods of Clinical Measurement.”\nThe Lancet 327 (8476): 307–10. https://doi.org/10.1016/S0140-6736(86)90837-8.\n\n\nBlei, David. 2012. “Probabilistic Topic Models.”\nCommunications of the ACM 55 (4): 77–84. https://doi.org/10.1145/2133806.2133826.\n\n\nBloom, Howard, Andrew Bell, and Kayla Reiman. 2020. “Using Data\nfrom Randomized Trials to Assess the Likely Generalizability of\nEducational Treatment-Effect Estimates from Regression Discontinuity\nDesigns.” Journal of Research on Educational\nEffectiveness 13 (3): 488–517. https://doi.org/10.1080/19345747.2019.1634169.\n\n\nBlumenthal, Mark. 2014. “Polls, Forecasts, and\nAggregators.” PS: Political Science & Politics 47\n(02): 297–300. https://doi.org/10.1017/s1049096514000055.\n\n\nBoland, Philip. 1984. “A Biographical Glimpse of William Sealy\nGosset.” The American Statistician 38 (3): 179–83. https://doi.org/10.2307/2683648.\n\n\nBolker, Ben, and David Robinson. 2022. broom.mixed: Tidying Methods for Mixed\nModels. https://CRAN.R-project.org/package=broom.mixed.\n\n\nBorer, Elizabeth T., Eric W. Seabloom, Matthew B. Jones, and Mark\nSchildhauer. 2009. “Some Simple Guidelines for Effective Data\nManagement.” Bulletin of the Ecological Society of\nAmerica 90 (2): 205–14. https://doi.org/10.1890/0012-9623-90.2.205.\n\n\nBorghi, John, and Ana Van Gulick. 2022. “Promoting Open Science\nThrough Research Data Management.” Harvard Data Science\nReview 4 (3). https://doi.org/10.1162/99608f92.9497f68e.\n\n\nBowen, Claire McKay. 2022. Protecting Your\nPrivacy in a Data-Driven World. 1st ed. Chapman; Hall/CRC.\nhttps://doi.org/10.1201/9781003122043.\n\n\nBowers, Jake, and Maarten Voors. 2016. “How to Improve Your\nRelationship with Your Future Self.” Revista de Ciencia\nPolı́tica 36 (3): 829–48. https://doi.org/10.4067/S0718-090X2016000300011.\n\n\nBowley, Arthur Lyon. 1901. Elements of Statistics. London: P.\nS. King.\n\n\n———. 1913. “Working-Class Households in Reading.”\nJournal of the Royal Statistical Society 76 (7): 672–701. https://doi.org/10.2307/2339708.\n\n\nBox, George E. P. 1976. “Science and Statistics.”\nJournal of the American Statistical Association 71 (356):\n791–99. https://doi.org/10.1080/01621459.1976.10480949.\n\n\nBoykis, Vicki. 2019. “A Deep Dive on Python Type Hints,”\nJuly. https://vickiboykis.com/2019/07/08/a-deep-dive-on-python-type-hints/.\n\n\nBoysel, Sam, and Davis Vaughan. 2021. fredr: An\nR Client for the “FRED” API. https://CRAN.R-project.org/package=fredr.\n\n\nBradley, Valerie, Shiro Kuriwaki, Michael Isakov, Dino Sejdinovic,\nXiao-Li Meng, and Seth Flaxman. 2021. “Unrepresentative Big\nSurveys Significantly Overestimated US Vaccine\nUptake.” Nature 600 (7890): 695–700. https://doi.org/10.1038/s41586-021-04198-4.\n\n\nBraginsky, Mika. 2020. wordbankr: Accessing the\nWordbank Database. https://CRAN.R-project.org/package=wordbankr.\n\n\nBremer, Nadieh, and Shirley Wu. 2021. Data Sketches. A K\nPeters/CRC Press. https://doi.org/10.1201/9780429445019.\n\n\nBrewer, Cynthia. 2015. Designing Better Maps: A Guide for GIS\nUsers. 2nd ed.\n\n\nBrewer, Ken. 2013. “Three Controversies in the History of Survey\nSampling.” Survey Methodology 39 (2): 249–63.\n\n\nBreznau, Nate, Eike Mark Rinke, Alexander Wuttke, Hung HV Nguyen, Muna\nAdem, Jule Adriaans, Amalia Alvarez-Benjumea, et al. 2022.\n“Observing Many Researchers Using the Same Data and Hypothesis\nReveals a Hidden Universe of Uncertainty.” Proceedings of the\nNational Academy of Sciences 119 (44): e2203150119. https://doi.org/10.1073/pnas.2203150119.\n\n\nBrokowski, Carolyn, and Mazhar Adli. 2019. “CRISPR Ethics: Moral\nConsiderations for Applications of a Powerful Tool.” Journal\nof Molecular Biology 431 (1): 88–101. https://doi.org/10.1016/j.jmb.2018.05.044.\n\n\nBronner, Laura. 2021. “Quantitative Editing.”\nYouTube, June. https://youtu.be/LI5m9RzJgWc.\n\n\nBrontë, Charlotte. 1847. Jane Eyre. https://www.gutenberg.org/files/1260/1260-h/1260-h.htm.\n\n\n———. 1857. The Professor. https://www.gutenberg.org/files/1028/1028-h/1028-h.htm.\n\n\nBrown, Zack. 2018. “A Git Origin Story.” Linux\nJournal, July. https://www.linuxjournal.com/content/git-origin-story.\n\n\nBryan, Jenny. 2018a. “Excuse Me, Do You Have a Moment to Talk\nabout Version Control?” The American Statistician 72\n(1): 20–27. https://doi.org/10.1080/00031305.2017.1399928.\n\n\n———. 2018b. “Code Smells and Feels.” YouTube,\nJuly. https://youtu.be/7oyiPBjLAWY.\n\n\n———. 2020. Happy Git and GitHub for the\nuseR. https://happygitwithr.com.\n\n\nBryan, Jenny, and Jim Hester. 2020. What They\nForgot to Teach You About R. https://rstats.wtf/index.html.\n\n\nBryan, Jenny, Jim Hester, David Robinson, Hadley Wickham, and Christophe\nDervieux. 2022. reprex: Prepare Reproducible\nExample Code via the Clipboard. https://CRAN.R-project.org/package=reprex.\n\n\nBryan, Jenny, and Hadley Wickham. 2021. gh:\nGitHub API. https://CRAN.R-project.org/package=gh.\n\n\nBueno de Mesquita, Ethan, and Anthony Fowler. 2021. Thinking Clearly\nwith Data: A Guide to Quantitative Reasoning and Analysis. New\nJersey: Princeton University Press.\n\n\nBuja, Andreas, Dianne Cook, Heike Hofmann, Michael Lawrence, Eun-Kyung\nLee, Deborah F. Swayne, and Hadley Wickham. 2009. “Statistical\nInference for Exploratory Data Analysis and Model Diagnostics.”\nPhilosophical Transactions of the Royal Society A:\nMathematical, Physical and Engineering Sciences 367 (1906):\n4361–83. https://doi.org/10.1098/rsta.2009.0120.\n\n\nBuja, Andreas, Dianne Cook, and Deborah Swayne. 1996. “Interactive\nHigh-Dimensional Data Visualization.” Journal of\nComputational and Graphical Statistics 5 (1): 78–99. https://doi.org/10.2307/1390754.\n\n\nBuneman, Peter, Sanjeev Khanna, and Tan Wang-Chiew. 2001. “Why and\nWhere: A Characterization of Data Provenance.” In Database\nTheory  ICDT 2001, 316–30. Springer\nBerlin Heidelberg. https://doi.org/10.1007/3-540-44503-x_20.\n\n\nBush, Vannevar. 1945. “As We May Think.” The Atlantic\nMonthly, July. https://www.theatlantic.com/magazine/archive/1945/07/as-we-may-think/303881/.\n\n\nByrd, James Brian, Anna Greene, Deepashree Venkatesh Prasad, Xiaoqian\nJiang, and Casey Greene. 2020. “Responsible, Practical Genomic\nData Sharing That Accelerates Research.” Nature Reviews\nGenetics 21 (10): 615–29. https://doi.org/10.1038/s41576-020-0257-5.\n\n\nCalonico, Sebastian, Matias Cattaneo, Max Farrell, and Rocio Titiunik.\n2021. rdrobust: Robust Data-Driven Statistical\nInference in Regression-Discontinuity Designs. https://CRAN.R-project.org/package=rdrobust.\n\n\nCambon, Jesse, and Christopher Belanger. 2021. “tidygeocoder: Geocoding Made Easy.” Zenodo.\nhttps://doi.org/10.5281/zenodo.3981510.\n\n\nCanty, Angelo, and B. D. Ripley. 2021. boot:\nBootstrap R (S-Plus) Functions.\n\n\nCarl, Sebastian, Ben Baldwin, Lee Sharpe, Tan Ho, and John Edwards.\n2023. Nflverse: Easily Install and Load the ’Nflverse’. https://CRAN.R-project.org/package=nflverse.\n\n\nCaro, Robert. 2019. Working. 1st ed. New York: Knopf.\n\n\nCarpenter, Christopher, and Carlos Dobkin. 2014. “Replication data for: The Minimum Legal Drinking Age and\nCrime.” https://doi.org/10.7910/DVN/27070.\n\n\nCarroll, Lewis. 1871. Through the Looking-Glass. Macmillan. https://www.gutenberg.org/files/12/12-h/12-h.htm.\n\n\nChamberlain, Scott, Hadley Wickham, Winston Chang, and Mauricio Vargas.\n2022. Analogsea: Interface to “Digital Ocean”. https://CRAN.R-project.org/package=analogsea.\n\n\nChamberlin, Donald. 2012. “Early History of\nSQL.” IEEE Annals of the History of\nComputing 34 (4): 78–82. https://doi.org/10.1109/mahc.2012.61.\n\n\nChambliss, Daniel. 1989. “The Mundanity of Excellence: An\nEthnographic Report on Stratification and Olympic Swimmers.”\nSociological Theory 7 (1): 70–86. https://doi.org/10.2307/202063.\n\n\nChambru, Cédric, and Paul Maneuvrier-Hervieu. 2022. “Introducing HiSCoD: A new gateway for the study of\nhistorical social conflict.” Working Paper Series,\nDepartment of Economics, University of Zurich. https://doi.org/10.5167/uzh-217109.\n\n\nChan, Duo. 2021. “Combining Statistical, Physical, and Historical\nEvidence to Improve Historical Sea-Surface Temperature Records.”\nHarvard Data Science Review 3 (1). https://doi.org/10.1162/99608f92.edcee38f.\n\n\nChang, Winston, Joe Cheng, JJ Allaire, Carson Sievert, Barret Schloerke,\nYihui Xie, Jeff Allen, Jonathan McPherson, Alan Dipert, and Barbara\nBorges. 2021. shiny: Web Application Framework\nfor R. https://CRAN.R-project.org/package=shiny.\n\n\nChase, William. 2020. “The Glamour of Graphics.”\nRStudio Conference, January. https://posit.co/resources/videos/the-glamour-of-graphics/.\n\n\nChen, Heng, Marie-Hélène Felt, and Christopher Henry. 2018. “2017\nMethods-of-Payment Survey: Sample Calibration and Variance\nEstimation.” Bank of Canada. https://doi.org/10.34989/tr-114.\n\n\nChen, Wei, Xilu Chen, Chang-Tai Hsieh, and Zheng Song. 2019. “A\nForensic Examination of China’s National Accounts.” Brookings\nPapers on Economic Activity, 77–127. https://www.jstor.org/stable/26798817.\n\n\nCheng, Joe, Bhaskar Karambelkar, and Yihui Xie. 2021. leaflet: Create Interactive Web Maps with the JavaScript\n“Leaflet” Library. https://CRAN.R-project.org/package=leaflet.\n\n\nCheriet, Mohamed, Nawwaf Kharma, Cheng-Lin Liu, and Ching Suen. 2007.\nCharacter Recognition Systems: A Guide for Students and\nPractitioner. Wiley.\n\n\nChouldechova, Alexandra, Diana Benavides-Prado, Oleksandr Fialko, and\nRhema Vaithianathan. 2018. “A Case Study of Algorithm-Assisted\nDecision Making in Child Maltreatment Hotline Screening\nDecisions.” In Proceedings of the 1st Conference on Fairness,\nAccountability and Transparency, edited by Sorelle Friedler and\nChristo Wilson, 81:134–48. Proceedings of Machine Learning Research. https://proceedings.mlr.press/v81/chouldechova18a.html.\n\n\nChrétien, Jean. 2007. My Years as Prime Minister. 1st ed.\nToronto: Knopf Canada.\n\n\nChristensen, Garret, Jeremy Freese, and Edward Miguel. 2019.\nTransparent and Reproducible Social Science Research.\nCalifornia: University of California Press.\n\n\nChristian, Brian. 2012. “The A/B Test: Inside\nthe Technology That’s Changing the Rules of Business.”\nWired, April. https://www.wired.com/2012/04/ff-abtesting/.\n\n\nCirone, Alexandra, and Arthur Spirling. 2021. “Turning History\ninto Data: Data Collection, Measurement, and Inference in HPE.”\nJournal of Historical Political Economy 1 (1): 127–54. https://doi.org/10.1561/115.00000005.\n\n\nCity of Toronto. 2021. 2021 Street Needs Assessment. https://www.toronto.ca/city-government/data-research-maps/research-reports/housing-and-homelessness-research-and-reports/.\n\n\nCleveland, William. (1985) 1994. The Elements of Graphing Data.\n2nd ed. New Jersey: Hobart Press.\n\n\nCohen, Glenn, and Michelle Mello. 2018. “HIPAA and\nProtecting Health Information in the 21st Century.”\nJAMA 320 (3): 231. https://doi.org/10.1001/jama.2018.5630.\n\n\nCohen, Jason, Steven Teleki, and Eric Brown. 2006. Best Kept Secrets\nof Peer Code Review. Smart Bear Incorporated.\n\n\nCohn, Alain. 2019. “Data and code for: Civic\nHonesty Around the Globe.” Harvard Dataverse. https://doi.org/10.7910/dvn/ykbodn.\n\n\nCohn, Alain, Michel André Maréchal, David Tannenbaum, and Christian\nLukas Zünd. 2019a. “Civic Honesty Around the Globe.”\nScience 365 (6448): 70–73. https://doi.org/10.1126/science.aau8712.\n\n\n———. 2019b. “Supplementary Materials for: Civic Honesty Around the\nGlobe.” Science 365 (6448): 70–73.\n\n\nCohn, Nate. 2016. “We Gave Four Good Pollsters the Same Raw Data.\nThey Had Four Different Results.” The New York Times,\nSeptember. https://www.nytimes.com/interactive/2016/09/20/upshot/the-error-the-polling-world-rarely-talks-about.html.\n\n\nColombo, Tommaso, Holger Fröning, Pedro Javier Garcı̀a, and Wainer\nVandelli. 2016. “Optimizing the Data-Collection Time of a\nLarge-Scale Data-Acquisition System Through a Simulation\nFramework.” The Journal of Supercomputing 72 (12):\n4546–72. https://doi.org/10.1007/s11227-016-1764-1.\n\n\nCongelio, Bradley. 2024. Introduction to NFL\nAnalytics with R. 1st ed. Chapman; Hall/CRC. https://bradcongelio.com/nfl-analytics-with-r-book/.\n\n\nCook, Dianne, Andreas Buja, Javier Cabrera, and Catherine Hurley. 1995.\n“Grand Tour and Projection\nPursuit.” Journal of Computational and Graphical\nStatistics 4 (3): 155–72. https://doi.org/10.1080/10618600.1995.10474674.\n\n\nCook, Dianne, Nancy Reid, and Emi Tanaka. 2021. “The Foundation Is\nAvailable for Thinking about Data Visualization Inferentially.”\nHarvard Data Science Review 3 (3). https://doi.org/10.1162/99608f92.8453435d.\n\n\nCook, Dianne, and Deborah Swayne. 2007. Interactive and Dynamic Graphics for Data Analysis: With\nR and GGobi. 1st ed. Springer.\n\n\nCooley, David. 2020. mapdeck: Interactive Maps\nUsing “Mapbox GL JS” and\n“Deck.gl”. https://CRAN.R-project.org/package=mapdeck.\n\n\nCouncil of European Union. 2016. “General Data Protection\nRegulation 2016/679.” https://eur-lex.europa.eu/eli/reg/2016/679/oj.\n\n\nCowen, Tyler. 2021. “Episode 132: Amia Srinivasan on Utopian\nFeminism.” Conversations with Tyler, September. https://conversationswithtyler.com/episodes/amia-srinivasan/.\n\n\n———. 2023. “Episode 168: Katherine Rundell on the Art of\nWords.” Conversations with Tyler, January. https://conversationswithtyler.com/episodes/katherine-rundell/.\n\n\nCox, Murray. 2021. “Inside Airbnb—Toronto\nData.” http://insideairbnb.com/get-the-data.html.\n\n\nCoyle, Edward, Andrew Coggan, Mari Hopper, and Thomas Walters. 1988.\n“Determinants of Endurance in Well-Trained\nCyclists.” Journal of Applied Physiology 64 (6):\n2622–30. https://doi.org/10.1152/jappl.1988.64.6.2622.\n\n\nCraiu, Radu. 2019. “The Hiring Gambit: In Search of the Twofer\nData Scientist.” Harvard Data Science Review 1 (1). https://doi.org/10.1162/99608f92.440445cb.\n\n\nCrawford, Kate. 2021. Atlas of AI.\n1st ed. New Haven: Yale University Press.\n\n\nCrosby, Alfred. 1997. The Measure of Reality: Quantification in\nWestern Europe, 1250-1600. Cambridge: Cambridge University Press.\n\n\nCsárdi, Gábor. 2022. gitcreds: Query\n“git” Credentials from “R”. https://CRAN.R-project.org/package=gitcreds.\n\n\nCsárdi, Gábor, Jim Hester, Hadley Wickham, Winston Chang, Martin Morgan,\nand Dan Tenenbaum. 2021. remotes: R Package\nInstallation from Remote Repositories, Including\n“GitHub”. https://CRAN.R-project.org/package=remotes.\n\n\nCunningham, Scott. 2021. Causal Inference: The Mixtape. 1st ed.\nNew Haven: Yale Press. https://mixtape.scunning.com.\n\n\nD’Ignazio, Catherine, and Lauren Klein. 2020. Data Feminism.\nMassachusetts: The MIT Press. https://data-feminism.mitpress.mit.edu.\n\n\nDagan, Noa, Noam Barda, Eldad Kepten, Oren Miron, Shay Perchik, Mark\nKatz, Miguel Hernán, Marc Lipsitch, Ben Reis, and Ran Balicer. 2021.\n“BNT162b2 mRNA Covid-19 Vaccine in a Nationwide Mass Vaccination\nSetting.” New England Journal of Medicine 384 (15):\n1412–23. https://doi.org/10.1056/NEJMoa2101765.\n\n\nData and Justice Criminology Lab, Institute of Criminology and Criminal\nJustice, Carleton University; The Centre for Research & Innovation\nfor Black Survivors of Homicide Victims (The CRIB), at the\nFactor-Inwentash Faculty of Social Work, University of Toronto; Canadian\nCivil Liberties Association; Ethics and Technology Lab, Queen’s\nUniversity. 2022. “Tracking (in)justice: A Living Data Set\nTracking Canadian Police-Involved Deaths.” https://trackinginjustice.ca.\n\n\nDattani, Saloni. 2024. “The Rise in Reported Maternal Mortality\nRates in the US Is Largely Due to a Change in Measurement.”\nOur World in Data.\n\n\nDavies, Rhian, Steph Locke, and Lucy D’Agostino McGowan. 2022. datasauRus: Datasets from the Datasaurus\nDozen. https://CRAN.R-project.org/package=datasauRus.\n\n\nDavis, Darren. 1997. “Nonrandom Measurement Error and Race of\nInterviewer Effects Among African Americans.” The Public\nOpinion Quarterly 61 (1): 183–207. https://doi.org/10.1086/297792.\n\n\nDavison, A. C., and D. V. Hinkley. 1997. Bootstrap Methods and Their\nApplications. Cambridge: Cambridge University Press. http://statwww.epfl.ch/davison/BMA/.\n\n\nDe Jonge, Edwin, and Mark van der Loo. 2013. An\nintroduction to data cleaning with R. Statistics Netherlands\nHeerlen. https://cran.r-project.org/doc/contrib/de%5FJonge+van%5Fder%5FLoo-Introduction%5Fto%5Fdata%5Fcleaning%5Fwith%5FR.pdf.\n\n\nDeaton, Angus. 2010. “Instruments, Randomization, and Learning\nabout Development.” Journal of Economic Literature 48\n(2): 424–55. https://doi.org/10.1257/jel.48.2.424.\n\n\nDeWitt, Helen. 2000. The Last Samurai. 1st ed. United States:\nTalk Mirimax Books.\n\n\nDoggers, Peter. 2021. “Carlsen Wins Game 6, Longest World Chess\nChampionship Game of All Time,” December. https://www.chess.com/news/view/fide-world-chess-championship-2021-game-6.\n\n\nDolatsara, Hamidreza Ahady, Ying-Ju Chen, Robert Leonard, Fadel Megahed,\nand Allison Jones-Farmer. 2021. “Explaining Predictive Model\nPerformance: An Experimental Study of Data Preparation and Model\nChoice.” Big Data, October. https://doi.org/10.1089/big.2021.0067.\n\n\nDoll, Richard, and Bradford Hill. 1950. “Smoking and Carcinoma of\nthe Lung.” British Medical Journal 2 (4682): 739–48. https://doi.org/10.1136/bmj.2.4682.739.\n\n\nDruckman, James, and Donald Green. 2021. “A New Era of\nExperimental Political Science.” In Advances in Experimental\nPolitical Science, 1–16. Cambridge: Cambridge University Press. https://doi.org/10.1017/9781108777919.002.\n\n\nDuflo, Esther. 2020. “Field Experiments and the Practice of\nPolicy.” American Economic Review 110 (7): 1952–73. https://doi.org/10.1257/aer.110.7.1952.\n\n\nDwork, Cynthia, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006.\n“Calibrating Noise to Sensitivity in Private Data\nAnalysis.” In Theory of Cryptography Conference, 265–84.\nSpringer. https://doi.org/10.1007/11681878_14.\n\n\nDwork, Cynthia, and Aaron Roth. 2013. “The Algorithmic Foundations\nof Differential Privacy.” Foundations and Trends in\nTheoretical Computer Science 9 (3-4): 211–407. https://doi.org/10.1561/0400000042.\n\n\nEdgeworth, Francis Ysidro. 1885. “Methods of Statistics.”\nJournal of the Statistical Society of London, 181–217.\n\n\nEdwards, Jonathan. 2017. “PACE team response\nshows a disregard for the principles of science.”\nJournal of Health Psychology 22 (9): 1155–58. https://doi.org/10.1177/1359105317700886.\n\n\nEghbal, Nadia. 2020. Working in Public: The Making and Maintenance\nof Open Source Software. California: Stripe Press.\n\n\nEisenstein, Michael. 2022. “Need Web Data? Here’s How to Harvest\nThem.” Nature 607: 200–201. https://doi.org/10.1038/d41586-022-01830-9.\n\n\nFirke, Sam. 2023. janitor: Simple Tools for\nExamining and Cleaning Dirty Data. https://CRAN.R-project.org/package=janitor.\n\n\nFisher, Ronald. (1925) 1928. Statistical Methods for Research\nWorkers. 2nd ed. London: Oliver; Boyd.\n\n\nFlynn, Michael. 2022. troopdata: Tools for\nAnalyzing Cross-National Military Deployment and Basing\nData. https://CRAN.R-project.org/package=troopdata.\n\n\nFord, Paul. 2015. “What Is Code?” Bloomberg\nBusinessweek, June. https://www.bloomberg.com/graphics/2015-paul-ford-what-is-code/.\n\n\nForster, Edward Morgan. 1927. Aspects of the Novel. London:\nEdward Arnold.\n\n\nFourcade, Marion, and Kieran Healy. 2017. “Seeing Like a\nMarket.” Socio-Economic Review 15 (1): 9–29. https://doi.org/10.1093/ser/mww033.\n\n\nFowler, Martin, and Kent Beck. 2018. Refactoring: Improving the Design of Existing\nCode. 2nd ed. New York: Addison-Wesley Professional.\n\n\nFox, John, Sanford Weisberg, and Brad Price. 2022. carData:\nCompanion to Applied Regression Data Sets. https://CRAN.R-project.org/package=carData.\n\n\nFranconeri, Steven, Lace Padilla, Priti Shah, Jeffrey Zacks, and Jessica\nHullman. 2021. “The Science of Visual Data Communication: What\nWorks.” Psychological Science in the Public Interest 22\n(3): 110–61. https://doi.org/10.1177/15291006211051956.\n\n\nFrandell, Ashlee, Mary Feeney, Timothy Johnson, Eric Welch, Lesley\nMichalegko, and Heyjie Jung. 2021. “The Effects of Electronic\nAlert Letters for Internet Surveys of Academic Scientists.”\nScientometrics 126 (8): 7167–81. https://doi.org/10.1007/s11192-021-04029-3.\n\n\nFranklin, Laura. 2005. “Exploratory Experiments.”\nPhilosophy of Science 72 (5): 888–99. https://doi.org/10.1086/508117.\n\n\nFrick, Hannah, Fanny Chow, Max Kuhn, Michael Mahoney, Julia Silge, and\nHadley Wickham. 2022. rsample: General\nResampling Infrastructure. https://CRAN.R-project.org/package=rsample.\n\n\nFriedman, Jerome, Robert Tibshirani, and Trevor Hastie. 2009. The\nElements of Statistical Learning. 2nd ed. Springer. https://hastie.su.domains/ElemStatLearn/.\n\n\nFriendly, Michael. 2021. HistData: Data Sets from the History of\nStatistics and Data Visualization. https://CRAN.R-project.org/package=HistData.\n\n\nFriendly, Michael, and Howard Wainer. 2021. A History of Data\nVisualization and Graphic Communication. 1st ed. Massachusetts:\nHarvard University Press.\n\n\nFry, Hannah. 2020. “Big Tech Is Testing You.” The New\nYorker, February, 61–65. https://www.newyorker.com/magazine/2020/03/02/big-tech-is-testing-you.\n\n\nFryzlewicz, Piotr. 2024. “Telling Stories\nwith Data: With Applications in R.” The American\nStatistician, April, 1–5. https://doi.org/10.1080/00031305.2024.2339562.\n\n\nFunkhouser, Gray. 1937. “Historical Development of the Graphical\nRepresentation of Statistical Data.” Osiris 3: 269–404.\nhttps://doi.org/10.1086/368480.\n\n\nGagolewski, Marek. 2022. “stringi:\nFast and Portable Character String Processing in\nR.” Journal of Statistical Software 103\n(2): 1–59. https://doi.org/10.18637/jss.v103.i02.\n\n\nGalef, Julia. 2020. “Episode 248: Are Democrats Being Irrational?\n(David Shor).” Rationally Speaking, December. http://rationallyspeakingpodcast.org/248-are-democrats-being-irrational-david-shor/.\n\n\nGao, Lucy, Jacob Bien, and Daniela Witten. 2022. “Selective\nInference for Hierarchical Clustering.” Journal of the\nAmerican Statistical Association, October, 1–11. https://doi.org/10.1080/01621459.2022.2116331.\n\n\nGarfinkel, Irwin, Lee Rainwater, and Timothy Smeeding. 2006. “A\nRe-Examination of Welfare States and Inequality in Rich Nations: How\nin-Kind Transfers and Indirect Taxes Change the Story.”\nJournal of Policy Analysis and Management 25 (4): 897–919. https://doi.org/10.1002/pam.20213.\n\n\nGarnier, Simon, Noam Ross, Robert Rudis, Antônio Camargo, Marco Sciaini,\nand Cédric Scherer. 2021. viridis –\nColorblind-Friendly Color Maps for R. https://doi.org/10.5281/zenodo.4679424.\n\n\nGazeley, Ursula, Georges Reniers, Hallie Eilerts-Spinelli, Julio Romero\nPrieto, Momodou Jasseh, Sammy Khagayi, and Veronique Filippi. 2022.\n“Women’s Risk of Death Beyond 42 Days Post Partum: A Pooled\nAnalysis of Longitudinal Health and Demographic Surveillance System Data\nin Sub-Saharan Africa.” The Lancet Global Health 10\n(11): e1582–89. https://doi.org/10.1016/s2214-109x(22)00339-4.\n\n\nGebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman\nVaughan, Hanna Wallach, Hal Daumé III, and Kate Crawford. 2021.\n“Datasheets for Datasets.” Communications of the\nACM 64 (12): 86–92. https://doi.org/10.1145/3458723.\n\n\nGelfand, Sharla. 2021. “Make a ReprEx... Please.”\nYouTube, February. https://youtu.be/G5Nm-GpmrLw.\n\n\n———. 2022a. Astrologer: Chani Nicholas Weekly Horoscopes\n(2013-2017). http://github.com/sharlagelfand/astrologer.\n\n\n———. 2022b. opendatatoronto: Access the City of\nToronto Open Data Portal. https://CRAN.R-project.org/package=opendatatoronto.\n\n\nGelman, Andrew. 2016. “What has happened down\nhere is the winds have changed,” September. https://statmodeling.stat.columbia.edu/2016/09/21/what-has-happened-down-here-is-the-winds-have-changed/.\n\n\n———. 2019. “Another Regression Discontinuity Disaster and What Can\nWe Learn from It,” June. https://statmodeling.stat.columbia.edu/2019/06/25/another-regression-discontinuity-disaster-and-what-can-we-learn-from-it/.\n\n\n———. 2020. “Statistical Models of Election Outcomes.”\nYouTube, August. https://youtu.be/7gjDnrbLQ4k.\n\n\nGelman, Andrew, John Carlin, Hal Stern, David Dunson, Aki Vehtari, and\nDonald Rubin. (1995) 2014. Bayesian Data Analysis. 3rd ed.\nChapman; Hall/CRC.\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using\nRegression and Multilevel/Hierarchical Models. 1st ed. Cambridge\nUniversity Press.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and\nOther Stories. Cambridge University Press. https://avehtari.github.io/ROS-Examples/.\n\n\nGelman, Andrew, and Guido Imbens. 2019. “Why High-Order\nPolynomials Should Not Be Used in Regression Discontinuity\nDesigns.” Journal of Business & Economic Statistics\n37 (3): 447–56. https://doi.org/10.1080/07350015.2017.1366909.\n\n\nGelman, Andrew, and Eric Loken. 2013. “The Garden of Forking\nPaths: Why Multiple Comparisons Can Be a Problem, Even When There Is No\n‘Fishing Expedition’ or ‘p-Hacking’ and the\nResearch Hypothesis Was Posited Ahead of Time.” Department of\nStatistics, Columbia University. http://www.stat.columbia.edu/~gelman/research/unpublished/p%5Fhacking.pdf.\n\n\nGelman, Andrew, Cristian Pasarica, and Rahul Dodhia. 2002. “Let’s\nPractice What We Preach: Turning Tables into Graphs.” The\nAmerican Statistician 56 (2): 121–30. https://doi.org/10.1198/000313002317572790.\n\n\nGelman, Andrew, and Aki Vehtari. 2024. Active\nStatistics: Stories, Games, Problems, and Hands-on Demonstrations for\nApplied Regression and Causal Inference. Cambridge\nUniversity Press. https://doi.org/10.1017/9781009436243.\n\n\nGelman, Andrew, Aki Vehtari, Daniel Simpson, Charles Margossian, Bob\nCarpenter, Yuling Yao, Lauren Kennedy, Jonah Gabry, Paul-Christian\nBürkner, and Martin Modrák. 2020. “Bayesian Workflow.”\narXiv. https://doi.org/10.48550/arXiv.2011.01808.\n\n\nGentemann, Chelle Leigh, Chris Holdgraf, Ryan Abernathey, Daniel\nCrichton, James Colliander, Edward Joseph Kearns, Yuvi Panda, and\nRichard Signell. 2021. “Science Storms the Cloud.”\nAGU Advances 2 (2). https://doi.org/10.1029/2020av000354.\n\n\nGerber, Alan, and Donald Green. 2012. Field Experiments: Design,\nAnalysis, and Interpretation. New York: WW Norton.\n\n\nGertler, Paul, Sebastian Martinez, Patrick Premand, Laura Rawlings, and\nChristel Vermeersch. 2016. Impact Evaluation in Practice. 2nd\ned. The World Bank. https://doi.org/10.1596/978-1-4648-0779-4.\n\n\nGibney, Elizabeth. 2022. “The leap second’s\ntime is up: world votes to stop pausing clocks.”\nNature 612 (7938): 18–18. https://doi.org/10.1038/d41586-022-03783-5.\n\n\nGodfrey, Ernest. 1918. “History and Development of Statistics in\nCanada.” In The History of Statistics–Their Development and\nProgress in Many Countries. New York: Macmillan, edited by John\nKoren, 179–98. Macmillan Company of New York.\n\n\nGoodrich, Ben, Jonah Gabry, Imad Ali, and Sam Brilleman. 2023.\n“rstanarm: Bayesian applied\nregression modeling via Stan.” https://mc-stan.org/rstanarm.\n\n\nGoogle. 2022. “What to Look for in a Code Review.” Google\nEngineering Practices Documentation. https://google.github.io/eng-practices/review/reviewer/looking-for.html.\n\n\nGordon, Brett, Robert Moakler, and Florian Zettelmeyer. 2022.\n“Close Enough? A Large-Scale Exploration of Non-Experimental\nApproaches to Advertising Measurement.” Marketing\nScience, November. https://doi.org/10.1287/mksc.2022.1413.\n\n\nGordon, Brett, Florian Zettelmeyer, Neha Bhargava, and Dan Chapsky.\n2019. “A Comparison of Approaches to Advertising Measurement:\nEvidence from Big Field Experiments at Facebook.” Marketing\nScience 38 (2): 193–225. https://doi.org/10.1287/mksc.2018.1135.\n\n\nGould, Elliot, Hannah Fraser, Timothy Parker, Shinichi Nakagawa, Simon\nGriffith, Peter Vesk, and Fiona Fidler. 2023. “Same Data,\nDifferent Analysts: Variation in Effect Sizes Due to Analytical\nDecisions in Ecology and Evolutionary Biology,” October. https://doi.org/10.32942/x2gg62.\n\n\nGraham, Paul. 2020. “How to Write Usefully,” February. http://paulgraham.com/useful.html.\n\n\nGray, Charles T., and Ben Marwick. 2019. “Truth, Proof, and\nReproducibility: There’s No Counter-Attack for the Codeless.” In\nCommunications in Computer and Information Science, 111–29.\nSpringer Singapore. https://doi.org/10.1007/978-981-15-1960-4_8.\n\n\nGreen, Donald, Terence Leong, Holger Kern, Alan Gerber, and Christopher\nLarimer. 2009. “Testing the Accuracy of Regression Discontinuity\nAnalysis Using Experimental Benchmarks.” Political\nAnalysis 17 (4): 400–417. https://doi.org/10.1093/pan/mpp018.\n\n\nGreen, Eric. 2020. “Nivi Research: Mister P\nhelps us understand vaccine hesitancy,” December. https://research.nivi.io/posts/2020-12-08-mister-p-helps-us-understand-vaccine-hesitancy/.\n\n\nGreenberg, Bernard, Abdel-Latif Abul-Ela, Walt Simmons, and Daniel\nHorvitz. 1969. “The Unrelated Question Randomized Response Model:\nTheoretical Framework.” Journal of the American Statistical\nAssociation 64 (326): 520–39. https://doi.org/10.1080/01621459.1969.10500991.\n\n\nGreenland, Sander, Stephen Senn, Kenneth Rothman, John Carlin, Charles\nPoole, Steven Goodman, and Douglas Altman. 2016. “Statistical Tests, P values, Confidence Intervals, and\nPower: A Guide to Misinterpretations.” European\nJournal of Epidemiology 31 (4): 337–50. https://doi.org/10.1007/s10654-016-0149-3.\n\n\nGreifer, Noah. 2021. “Why Do We Do Matching for Causal Inference\nVs Regressing on Confounders?” Cross Validated,\nSeptember. https://stats.stackexchange.com/q/544958.\n\n\nGrimmer, Justin, Margaret Roberts, and Brandon Stewart. 2022. Text As Data: A New Framework for Machine Learning and\nthe Social Sciences. New Jersey: Princeton University Press.\n\n\nGrolemund, Garrett, and Hadley Wickham. 2011. “Dates and Times\nMade Easy with lubridate.”\nJournal of Statistical Software 40 (3): 1–25. https://doi.org/10.18637/jss.v040.i03.\n\n\nGronsbell, Jessica, Jessica Minnier, Sheng Yu, Katherine Liao, and\nTianxi Cai. 2019. “Automated Feature Selection of Predictors in\nElectronic Medical Records Data.” Biometrics 75 (1):\n268–77. https://doi.org/10.1111/biom.12987.\n\n\nGrün, Bettina, and Kurt Hornik. 2011. “topicmodels: An R Package for Fitting\nTopic Models.” Journal of Statistical Software 40 (13):\n1–30. https://doi.org/10.18637/jss.v040.i13.\n\n\nGustafsson, Karl, and Linus Hagström. 2017. “What Is the Point?\nTeaching Graduate Students How to Construct Political Science Research\nPuzzles.” European Political Science 17 (4): 634–48. https://doi.org/10.1057/s41304-017-0130-y.\n\n\nGutman, Robert. 1958. “Birth and Death Registration in\nMassachusetts: II. The Inauguration of a Modern System,\n1800-1849.” The Milbank Memorial Fund Quarterly 36 (4):\n373–402.\n\n\nHackett, Robert. 2016. “Researchers Caused an\nUproar By Publishing Data From 70,000 OkCupid Users.”\nFortune, May. https://fortune.com/2016/05/18/okcupid-data-research/.\n\n\nHalberstam, David. 1972. The Best and the\nBrightest. 1st ed. New York: Random House.\n\n\nHamming, Richard. (1997) 2020. The Art of Doing\nScience and Engineering. 2nd ed. Stripe Press.\n\n\nHand, David. 2018. “Statistical Challenges of Administrative and\nTransaction Data.” Journal of the Royal Statistical Society:\nSeries A (Statistics in Society) 181 (3): 555–605. https://doi.org/10.1111/rssa.12315.\n\n\nHao, Karen. 2019. “This is How AI Bias Really\nHappens—And Why It’s So Hard To Fix.” MIT Technology\nReview, February. https://www.technologyreview.com/2019/02/04/137602/this-is-how-ai-bias-really-happensand-why-its-so-hard-to-fix/.\n\n\nHart, Edmund, Pauline Barmby, David LeBauer, François Michonneau, Sarah\nMount, Patrick Mulrooney, Timothée Poisot, Kara Woo, Naupaka Zimmerman,\nand Jeffrey Hollister. 2016. “Ten Simple Rules for Digital Data\nStorage.” PLOS Computational Biology 12\n(10): e1005097. https://doi.org/10.1371/journal.pcbi.1005097.\n\n\nHartocollis, Anemona. 2022. “U.S. News Ranked\nColumbia No. 2, but a Math Professor Has His Doubts.”\nThe New York Times, March. https://www.nytimes.com/2022/03/17/us/columbia-university-rank.html.\n\n\nHassan, Mai. 2022. “New Insights on Africa’s Autocratic\nPast.” African Affairs 121 (483): 321–33. https://doi.org/10.1093/afraf/adac002.\n\n\nHawes, Michael. 2020. “Implementing Differential\nPrivacy: Seven Lessons From the\n2020 United States\nCensus.” Harvard Data Science Review 2 (2).\nhttps://doi.org/10.1162/99608f92.353c6f99.\n\n\nHayot, Eric. 2014. The Elements of Academic Style. New York:\nColumbia University Press.\n\n\nHealy, Kieran. 2018. Data Visualization. New Jersey: Princeton\nUniversity Press. https://socviz.co.\n\n\n———. 2020. “The Kitchen Counter Observatory,” May. https://kieranhealy.org/blog/archives/2020/05/21/the-kitchen-counter-observatory/.\n\n\nHeil, Benjamin, Michael Hoffman, Florian Markowetz, Su-In Lee, Casey\nGreene, and Stephanie Hicks. 2021. “Reproducibility Standards for\nMachine Learning in the Life Sciences.” Nature Methods\n18 (10): 1132–35. https://doi.org/10.1038/s41592-021-01256-7.\n\n\nHermans, Felienne. 2017. “Peter Hilton on Naming.” IEEE\nSoftware 34 (3): 117–20. https://doi.org/10.1109/MS.2017.81.\n\n\n———. 2021. The Programmer’s Brain: What Every Programmer Needs to\nKnow about Cognition. 1st ed. New York: Simon; Schuster. https://www.manning.com/books/the-programmers-brain.\n\n\nHernán, Miguel, and James Robins. 2023. What If. 1st ed. Boca\nRaton: Chapman & Hall/CRC. https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/.\n\n\nHester, Jim, Florent Angly, Russ Hyde, Michael Chirico, Kun Ren,\nAlexander Rosenstock, and Indrajeet Patil. 2022. lintr: A “Linter” for R Code. https://CRAN.R-project.org/package=lintr.\n\n\nHester, Jim, Hadley Wickham, and Gábor Csárdi. 2021. fs: Cross-Platform File System Operations Based on\n“libuv”. https://CRAN.R-project.org/package=fs.\n\n\nHill, Austin Bradford. 1965. “The Environment and Disease:\nAssociation or Causation?” Proceedings of the Royal Society\nof Medicine 58 (5): 295–300.\n\n\nHillel, Wayne. 2017. How Do We Trust Our Science Code? https://www.hillelwayne.com/how-do-we-trust-science-code/.\n\n\nHo, Daniel, Kosuke Imai, Gary King, and Elizabeth Stuart. 2011.\n“MatchIt: Nonparametric Preprocessing for Parametric\nCausal Inference.” Journal of Statistical Software 42\n(8): 1–28. https://doi.org/10.18637/jss.v042.i08.\n\n\nHodgetts, Paul. 2022. “The Negative Space of Data,” March.\nhttps://hodgettsp.netlify.app/post/data-negativespace/.\n\n\nHofmeister, Johannes, Janet Siegmund, and Daniel Holt. 2017.\n“Shorter Identifier Names Take Longer to Comprehend.” In\n2017 IEEE 24th International Conference on Software Analysis,\nEvolution and Reengineering (SANER), 217–27. https://doi.org/10.1109/saner.2017.7884623.\n\n\nHolland, Paul. 1986. “Statistics and Causal Inference.”\nJournal of the American Statistical Association 81 (396):\n945–60. https://doi.org/10.2307/2289064.\n\n\nHorst, Allison Marie, Alison Presmanes Hill, and Kristen Gorman. 2020.\npalmerpenguins: Palmer Archipelago (Antarctica)\npenguin data. https://doi.org/10.5281/zenodo.3960218.\n\n\nHorton, Nicholas, Rohan Alexander, Micaela Parker, Aneta Piekut, and\nColin Rundel. 2022. “The Growing Importance of Reproducibility and\nResponsible Workflow in the Data Science and Statistics\nCurriculum.” Journal of Statistics and Data Science\nEducation 30 (3): 207–8. https://doi.org/10.1080/26939169.2022.2141001.\n\n\nHorton, Nicholas, and Stuart Lipsitz. 2001. “Multiple Imputation\nin Practice.” The American Statistician 55 (3): 244–54.\nhttps://doi.org/10.1198/000313001317098266.\n\n\nHotz, Joseph, Christopher Bollinger, Tatiana Komarova, Charles Manski,\nRobert Moffitt, Denis Nekipelov, Aaron Sojourner, and Bruce Spencer.\n2022. “Balancing Data Privacy and Usability in the Federal\nStatistical System.” Proceedings of the National Academy of\nSciences 119 (31): 1–10. https://doi.org/10.1073/pnas.2104906119.\n\n\nHowes, Adam. 2022. “Representing Uncertainty Using Significant\nFigures,” April. https://athowes.github.io/posts/2022-04-24-representing-uncertainty-using-significant-figures/.\n\n\nHulley, Stephen, Steven Cummings, Warren Browner, Deborah Grady, and\nThomas Newman. 2007. Designing Clinical Research. 3rd ed.\nLippincott Williams & Wilkins.\n\n\nHullman, Jessica, and Andrew Gelman. 2021. “Designing for\nInteractive Exploratory Data Analysis Requires Theories of Graphical\nInference.” Harvard Data Science Review 3 (3). https://doi.org/10.1162/99608f92.3ab8a587.\n\n\nHuntington-Klein, Nick. 2021. The Effect: An Introduction to\nResearch Design and Causality. 1st ed. Chapman & Hall. https://theeffectbook.net.\n\n\n———. 2022. “Library of Statistical Techniques.” https://lost-stats.github.io.\n\n\nHuntington-Klein, Nick, Andreu Arenas, Emily Beam, Marco Bertoni,\nJeffrey Bloem, Pralhad Burli, Naibin Chen, et al. 2021. “The\nInfluence of Hidden Researcher Decisions in Applied\nMicroeconomics.” Economic Inquiry 59: 944–60. https://doi.org/10.1111/ecin.12992.\n\n\nHuyen, Chip. 2020. “Machine Learning Is Going Real-Time,”\nDecember. https://huyenchip.com/2020/12/27/real-time-machine-learning.html.\n\n\nHvitfeldt, Emil, and Julia Silge. 2021. Supervised Machine Learning for Text Analysis in\nR. 1st ed. Chapman; Hall/CRC. https://doi.org/10.1201/9781003093459.\n\n\nHyman, Michael, Luca Sartore, and Linda J Young. 2021. “Capture-Recapture Estimation of Characteristics of U.S.\nLocal Food Farms Using a Web-Scraped List Frame.”\nJournal of Survey Statistics and Methodology 10 (4): 979–1004.\nhttps://doi.org/10.1093/jssam/smab008.\n\n\nHyndman, Rob, Timothy Hyndman, Charles Gray, Sayani Gupta, and Jacquie\nTran. 2022. cricketdata: International Cricket\nData. https://CRAN.R-project.org/package=cricketdata.\n\n\nIannone, Richard. 2022. DiagrammeR: Graph/Network\nVisualization. https://CRAN.R-project.org/package=DiagrammeR.\n\n\nIannone, Richard, and Mauricio Vargas. 2022. pointblank: Data Validation and Organization of Metadata\nfor Local and Remote Tables. https://CRAN.R-project.org/package=pointblank.\n\n\nInternational Organization Of Legal Metrology. 2007. International\nVocabulary of Metrology – Basic and General Concepts and Associated\nTerms. 3rd ed. https://www.oiml.org/en/files/pdf%5Fv/v002-200-e07.pdf.\n\n\nIoannidis, John. 2005. “Why Most Published Research Findings Are\nFalse.” PLOS Medicine 2 (8): e124. https://doi.org/10.1371/journal.pmed.0020124.\n\n\nIrizarry, Rafael. 2020. “The Role of Academia\nin Data Science Education.” Harvard Data Science\nReview 2 (1). https://doi.org/10.1162/99608f92.dd363929.\n\n\nIrving, Damien, Kate Hertweck, Luke Johnston, Joel Ostblom, Charlotte\nWickham, and Greg Wilson. 2021. Research Software Engineering with\nPython. Chapman; Hall/CRC.\n\n\nIsaacson, Walter. 2011. Steve Jobs. 1st ed. Simon &\nSchuster.\n\n\nIshiguro, Kazuo. 1989. The Remains of the Day. 1st ed. Faber;\nFaber.\n\n\nIzrailev, Sergei. 2022. tictoc: Functions for\nTiming R Scripts, as Well as Implementations of “Stack” and\n“List” Structures. https://CRAN.R-project.org/package=tictoc.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n(2013) 2021. An Introduction to Statistical\nLearning with Applications in R. 2nd ed. Springer. https://www.statlearning.com.\n\n\nJet Propulsion Laboratory. 2009. “JPL\nInstitutional Coding Standard for the C Programming\nLanguage.” Document Number D-60411, March. https://web.archive.org/web/20111015064908/http://lars-lab.jpl.nasa.gov/JPL_Coding_Standard_C.pdf.\n\n\nJohnson, Alicia, Miles Ott, and Mine Dogucu. 2022. Bayes Rules! An Introduction to Bayesian Modeling with\nR. 1st ed. Chapman; Hall/CRC. https://www.bayesrulesbook.com.\n\n\nJohnson, Kaneesha. 2021. “Two Regimes of Prison Data\nCollection.” Harvard Data Science Review 3 (3). https://doi.org/10.1162/99608f92.72825001.\n\n\nJohnston, Myfanwy, and David Robinson. 2022. gutenbergr: Download and Process Public Domain Works from\nProject Gutenberg. https://CRAN.R-project.org/package=gutenbergr.\n\n\nJordan, Michael. 2019. “Artificial\nIntelligence–The Revolution Hasn’t Happened Yet.”\nHarvard Data Science Review 1 (1). https://doi.org/10.1162/99608f92.f06c6e61.\n\n\nJoyner, Michael. 1991. “Modeling: Optimal Marathon Performance on\nthe Basis of Physiological Factors.” Journal of Applied\nPhysiology 70 (2): 683–87. https://doi.org/10.1152/jappl.1991.70.2.683.\n\n\nJurafsky, Dan, and James Martin. (2000) 2023. Speech and Language\nProcessing. 3rd ed. https://web.stanford.edu/~jurafsky/slp3/.\n\n\nKahan, Brennan, Suzie Cro, Fan Li, and Michael Harhay. 2023.\n“Eliminating Ambiguous Treatment Effects Using Estimands.”\nAmerican Journal of Epidemiology, February. https://doi.org/10.1093/aje/kwad036.\n\n\nKahan, Brennan, Joanna Hindley, Mark Edwards, Suzie Cro, and Tim Morris.\n2024. “The estimands framework: a primer on\nthe ICH E9(R1) addendum.” BMJ, January, e076316.\nhttps://doi.org/10.1136/bmj-2023-076316.\n\n\nKahan, Brennan, Fan Li, Andrew Copas, and Michael Harhay. 2022.\n“Estimands in Cluster-Randomized Trials: Choosing Analyses That\nAnswer the Right Question.” International Journal of\nEpidemiology, July. https://doi.org/10.1093/ije/dyac131.\n\n\nKahle, David, and Hadley Wickham. 2013. “ggmap: Spatial Visualization with ggplot2.”\nThe R Journal 5 (1): 144–61. http://journal.r-project.org/archive/2013-1/kahle-wickham.pdf.\n\n\nKalamara, Eleni, Arthur Turrell, Chris Redl, George Kapetanios, and\nSujit Kapadia. 2022. “Making text count:\nEconomic forecasting using newspaper text.”\nJournal of Applied Econometrics 37 (5): 896–919.\nhttps://doi.org/10.1002/jae.2907.\n\n\nKalgin, Alexander. 2014. “Implementation of\nPerformance Management in Regional Government in Russia: Evidence of\nData Manipulation.” Public Management Review 18\n(1): 110–38. https://doi.org/10.1080/14719037.2014.965271.\n\n\nKarsten, Karl. 1923. Charts and Graphs. New York:\nPrentice-Hall.\n\n\nKay, Matthew. 2022. tidybayes: Tidy Data\nand Geoms for Bayesian Models. https://doi.org/10.5281/zenodo.1308151.\n\n\nKennedy, Lauren, Katharine Khanna, Daniel Simpson, Andrew Gelman, Yajun\nJia, and Julien Teitler. 2022. “He, She, They: Using Sex and\nGender in Survey Adjustment.” https://arxiv.org/abs/2009.14401.\n\n\nKenny, Christopher T., Shiro Kuriwaki, Cory McCartan, Evan T. R.\nRosenman, Tyler Simko, and Kosuke Imai. 2021. “The use of differential privacy for census data and its\nimpact on redistricting: The case of the 2020 U.S.\nCensus.” Science Advances 7 (41). https://doi.org/10.1126/sciadv.abk3283.\n\n\n———. 2023. “Comment: The Essential Role of Policy Evaluation for\nthe 2020 Census Disclosure Avoidance System.” Harvard Data\nScience Review, no. Special Issue 2. https://doi.org/10.1162/99608f92.abc2c765.\n\n\nKent, William. 1993. “My Height: A Model for Numeric\nInformation.” https://www.bkent.net/Doc/myheight.htm.\n\n\nKeshav, Srinivasan. 2007. “How to Read a Paper.”\nACM SIGCOMM Computer Communication\nReview 37 (3): 83–84. https://doi.org/10.1145/1273445.1273458.\n\n\nKeyes, Os. 2019. “Counting the Countless.” Real\nLife. https://reallifemag.com/counting-the-countless/.\n\n\nKharecha, Pushker, and James Hansen. 2013. “Prevented Mortality\nand Greenhouse Gas Emissions from Historical and Projected Nuclear\nPower.” Environmental Science & Technology 47 (9):\n4889–95. https://doi.org/10.1021/es3051197.\n\n\nKing, Gary. 2006. “Publication, Publication.” PS:\nPolitical Science & Politics 39 (1): 119–25. https://doi.org/10.1017/S1049096506060252.\n\n\nKing, Gary, and Richard Nielsen. 2019. “Why Propensity Scores\nShould Not Be Used for Matching.” Political Analysis 27\n(4): 435–54. https://doi.org/10.1017/pan.2019.11.\n\n\nKing, Stephen. 2000. On Writing: A Memoir of the Craft. 1st ed.\nScribner.\n\n\nKleiber, Christian, and Achim Zeileis. 2008. Applied Econometrics\nwith R. New York: Springer-Verlag. https://CRAN.R-project.org/package=AER.\n\n\nKnuth, Donald. 1984. “Literate Programming.” The\nComputer Journal 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.\n\n\n———. 1998. Art of Computer Programming, Volume 2: Seminumerical\nAlgorithms. 2nd ed.\n\n\nKoenker, Roger, and Achim Zeileis. 2009. “On Reproducible\nEconometric Research.” Journal of Applied Econometrics\n24 (5): 833–47. https://doi.org/10.1002/jae.1083.\n\n\nKoerner, Lisbet. 2000. Linnaeus: Nature and Nation. Cambridge:\nHarvard University Press.\n\n\nKohavi, Ron, Alex Deng, Brian Frasca, Roger Longbotham, Toby Walker, and\nYa Xu. 2012. “Trustworthy Online Controlled Experiments.”\nIn Proceedings of the 18th ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining -\nKDD 12, 1st ed. ACM Press.\nhttps://doi.org/10.1145/2339530.2339653.\n\n\nKohavi, Ron, Diane Tang, and Ya Xu. 2020. Trustworthy Online Controlled Experiments: A Practical\nGuide to A/B Testing. Cambridge University Press.\n\n\nKoitsalu, Marie, Martin Eklund, Jan Adolfsson, Henrik Grönberg, and\nYvonne Brandberg. 2018. “Effects of Pre-Notification, Invitation\nLength, Questionnaire Length and Reminder on Participation Rate: A\nQuasi-Randomised Controlled Trial.” BMC Medical Research\nMethodology 18 (3): 1–5. https://doi.org/10.1186/s12874-017-0467-5.\n\n\nKrantz, Sebastian. 2023. collapse: Advanced and\nFast Data Transformation. https://CRAN.R-project.org/package=collapse.\n\n\nKuhn, Max. 2022. tune: Tidy Tuning\nTools. https://CRAN.R-project.org/package=tune.\n\n\nKuhn, Max, and Hannah Frick. 2022. poissonreg:\nModel Wrappers for Poisson Regression. https://CRAN.R-project.org/package=poissonreg.\n\n\nKuhn, Max, and Davis Vaughan. 2022. parsnip: A\nCommon API to Modeling and Analysis Functions. https://CRAN.R-project.org/package=parsnip.\n\n\nKuhn, Max, Davis Vaughan, and Emil Hvitfeldt. 2022. yardstick: Tidy Characterizations of Model\nPerformance. https://CRAN.R-project.org/package=yardstick.\n\n\nKuhn, Max, and Hadley Wickham. 2020. tidymodels: a collection of packages for modeling and\nmachine learning using tidyverse principles. https://www.tidymodels.org.\n\n\n———. 2022. recipes: Preprocessing and Feature\nEngineering Steps for Modeling. https://CRAN.R-project.org/package=recipes.\n\n\nKuriwaki, Shiro, Will Beasley, and Thomas Leeper. 2023. dataverse: R Client for Dataverse 4+\nRepositories.\n\n\nKuznets, Simon, Lillian Epstein, and Elizabeth Jenks. 1941. National Income and Its Composition,\n1919-1938. National Bureau of Economic Research.\n\n\nLamott, Anne. 1994. Bird by Bird: Some Instructions on Writing and\nLife. Anchor Books.\n\n\nLandau, William Michael. 2021. “The targets R\nPackage: A Dynamic Make-Like Function-Oriented Pipeline Toolkit for\nReproducibility and High-Performance Computing.”\nJournal of Open Source Software 6 (57): 2959. https://doi.org/10.21105/joss.02959.\n\n\nLane, Nick. 2015. “The Unseen World: Reflections on Leeuwenhoek\n(1677) ‘Concerning Little Animals’.”\nPhilosophical Transactions of the Royal Society B: Biological\nSciences 370 (1666): 20140344. https://doi.org/10.1098/rstb.2014.0344.\n\n\nLarmarange, Joseph. 2023. labelled:\nManipulating Labelled Data. https://CRAN.R-project.org/package=labelled.\n\n\nLatour, Bruno. 1996. “On Actor-Network Theory: A Few\nClarifications.” Soziale Welt 47 (4): 369–81. http://www.jstor.org/stable/40878163.\n\n\nLauderdale, Benjamin, Delia Bailey, Jack Blumenau, and Douglas Rivers.\n2020. “Model-Based Pre-Election Polling for National and\nSub-National Outcomes in the US and UK.” International\nJournal of Forecasting 36 (2): 399–413. https://doi.org/10.1016/j.ijforecast.2019.05.012.\n\n\nLeek, Jeff, Blakeley McShane, Andrew Gelman, David Colquhoun, Michèle\nNuijten, and Steven Goodman. 2017. “Five Ways to Fix\nStatistics.” Nature 551 (7682): 557–59. https://doi.org/10.1038/d41586-017-07522-z.\n\n\nLeonelli, Sabina. 2020. “Learning from Data Journeys.” In\nData Journeys in the Sciences, 1–24. Springer International\nPublishing. https://doi.org/10.1007/978-3-030-37177-7_1.\n\n\nLetterman, Clark. 2021. “Q&A: How Pew\nResearch Center surveyed nearly 30,000 people in India,”\nJuly. https://medium.com/pew-research-center-decoded/q-a-how-pew-research-center-surveyed-nearly-30-000-people-in-india-7c778f6d650e.\n\n\nLevine, Judah, Patrizia Tavella, and Martin Milton. 2022. “Towards\na Consensus on a Continuous Coordinated Universal Time.”\nMetrologia 60 (1): 014001. https://doi.org/10.1088/1681-7575/ac9da5.\n\n\nLewis, Crystal. 2024. Data Management in Large-Scale Education\nResearch. 1st ed. Chapman; Hall/CRC. https://datamgmtinedresearch.com/index.html.\n\n\nLight, Richard, Judith Singer, and John Willett. 1990. By Design: Planning Research on Higher\nEducation. 1st ed. Cambridge: Harvard University Press.\n\n\nLima, Renato de, Oliver Phillips, Alvaro Duque, Sebastian Tello, Stuart\nDavies, Alexandre Adalardo de Oliveira, Sandra Muller, et al. 2022.\n“Making Forest Data Fair and Open.” Nature Ecology\n& Evolution 6 (April): 656–58. https://doi.org/10.1038/s41559-022-01738-7.\n\n\nLin, Herbert. 2014. “A Proposal to Reduce Government\nOverclassification of Information Related to National Security.”\nJournal of National Security Law and Policy 7: 443–63.\n\n\nLin, Sarah, Ibraheem Ali, and Greg Wilson. 2021. “Ten Quick Tips\nfor Making Things Findable.” PLOS Computational Biology\n16 (12): 1–10. https://doi.org/10.1371/journal.pcbi.1008469.\n\n\nLips, Hilary. 2020. Sex and Gender: An Introduction. 7th ed.\nIllinois: Waveland Press.\n\n\nLittle, Roderick, and Roger Lewis. 2021. “Estimands, Estimators,\nand Estimates.” JAMA 326 (10): 967. https://doi.org/10.1001/jama.2021.2886.\n\n\nLiu, Emily, Lenny Bronner, and Jeremy Bowers. 2022. “What the\nWashington Post Elections Engineering Team Had to Learn about Election\nData.” Washington Post Engineering, April. https://washpost.engineering/what-the-washington-post-elections-engineering-team-had-to-learn-about-election-data-a41603daf9ca.\n\n\nLockheed Martin. 2005. “Joint Strike Fighter Air Vehicle C++\nCoding Standards For The System Development And Demonstration\nProgram.” Document Number 2RDU00001 Rev C,\nDecember. https://www.stroustrup.com/JSF-AV-rules.pdf.\n\n\nLohr, Sharon. (1999) 2022. Sampling: Design and Analysis. 3rd\ned. Chapman; Hall/CRC.\n\n\nLoken, Meredith, and Hilary Matfess. 2023. “Introducing the\nWomen’s Activities in Armed Rebellion (WAAR) Project, 1946-2015.”\nJournal of Peace Research.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019. Geocomputation with R. 1st ed. Chapman;\nHall/CRC. https://geocompr.robinlovelace.net.\n\n\nLucas, Jack, Reed Merrill, Kelly Blidook, Sandra Breux, Laura Conrad,\nGabriel Eidelman, Royce Koop, et al. 2020. “Canadian\nMunicipal Elections Database.” Scholars Portal Dataverse.\nhttps://doi.org/10.5683/sp2/4mzjpq.\n\n\nLucas, Robert. 1978. “Asset Prices in an Exchange Economy.”\nEconometrica 46 (6): 1429–45. https://doi.org/10.2307/1913837.\n\n\nLuebke, David Martin, and Sybil Milton. 1994. “Locating the\nVictim: An Overview of Census-Taking, Tabulation Technology, and\nPersecution in Nazi Germany.” IEEE Annals of the History of\nComputing 16 (3): 25–39. https://doi.org/10.1109/MAHC.1994.298418.\n\n\nLumley, Thomas. 2020. “survey: analysis of\ncomplex survey samples.” https://cran.r-project.org/web/packages/survey/index.html.\n\n\nLundberg, Ian, Rebecca Johnson, and Brandon Stewart. 2021. “What\nIs Your Estimand? Defining the Target Quantity Connects Statistical\nEvidence to Theory.” American Sociological Review 86\n(3): 532–65. https://doi.org/10.1177/00031224211004187.\n\n\nLuscombe, Alex, Kevin Dick, and Kevin Walby. 2021. “Algorithmic\nThinking in the Public Interest: Navigating Technical, Legal, and\nEthical Hurdles to Web Scraping in the Social Sciences.”\nQuality & Quantity 56 (3): 1–22. https://doi.org/10.1007/s11135-021-01164-0.\n\n\nLyman, Frank. 1981. “The Responsive Classroom Discussion: The\nInclusion of All Students.” Mainstreaming Digest 109:\n109–13.\n\n\nMacDorman, Marian, and Eugene Declercq. 2018. “The Failure of\nUnited States Maternal Mortality Reporting and Its Impact on Women’s\nLives.” Birth 45 (2): 105–8. https://doi.org/1111/birt.12333.\n\n\nMaher, Michael. 1982. “Modelling Association Football\nScores.” Statistica Neerlandica 36 (3): 109–18. https://doi.org/10.1111/j.1467-9574.1982.tb00782.x.\n\n\nMaier, Maximilian, František Bartoš, Tom Stanley, David Shanks, Adam\nHarris, and Eric-Jan Wagenmakers. 2022. “No Evidence for Nudging\nAfter Adjusting for Publication Bias.” Proceedings of the\nNational Academy of Sciences 119 (31): e2200300119. https://doi.org/10.1073/pnas.2200300119.\n\n\nMammoliti, Anthony, Petr Smirnov, Minoru Nakano, Zhaleh Safikhani,\nChristopher Eeles, Heewon Seo, Sisira Kadambat Nair, et al. 2021.\n“Orchestrating and Sharing Large Multimodal Data for Transparent\nand Reproducible Research.” Nature Communications 12\n(1). https://doi.org/10.1038/s41467-021-25974-w.\n\n\nManski, Charles. 2022. “Inference with Imputed Data: The Allure of\nMaking Stuff Up.” arXiv. https://doi.org/10.48550/arXiv.2205.07388.\n\n\nMarchese, David. 2022. “Her Discovery Changed the World. How Does\nShe Think We Should Use It?” The New York Times, August.\nhttps://www.nytimes.com/interactive/2022/08/15/magazine/jennifer-doudna-crispr-interview.html.\n\n\nMartin, Charles, and Ben Popper. 2021. “Don’t Push That Button:\nExploring the Software That Flies SpaceX Rockets and Starships.”\nThe Overflow, December. https://stackoverflow.blog/2021/12/27/dont-push-that-button-exploring-the-software-that-flies-spacex-starships/.\n\n\nMatsumoto, Yukihiro. 2007. “Treating Code as\nan Essay.” In Beautiful Code, edited by Andy Oram\nand Greg Wilson, 477–81. O’Reilly.\n\n\nMattson, Greggor. 2017. “Artificial Intelligence Discovers\nGayface. Sigh.” https://greggormattson.com/2017/09/09/artificial-intelligence-discovers-gayface/amp/.\n\n\nMcCarthy, Fiona M., Tamsin E. M. Jones, Anne E. Kwitek, Cynthia L.\nSmith, Peter D. Vize, Monte Westerfield, and Elspeth A. Bruford. 2023.\n“The Case for Standardizing Gene Nomenclature in\nVertebrates.” Nature 614 (7948): E31–32. https://doi.org/10.1038/s41586-022-05633-w.\n\n\nMcElreath, Richard. (2015) 2020. Statistical\nRethinking: A Bayesian Course with Examples in R and Stan.\n2nd ed. Chapman; Hall/CRC.\n\n\n———. 2020. “Science as Amateur Software Development.”\nYouTube, September. https://youtu.be/zwRdO9%5FGGhY.\n\n\nMcIlroy, Doug, Ray Brownrigg, Thomas Minka, and Roger Bivand. 2023.\nmapproj: Map Projections. https://CRAN.R-project.org/package=mapproj.\n\n\nMcKinney, Wes. (2011) 2022. Python for Data Analysis. 3rd ed.\nhttps://wesmckinney.com/book/.\n\n\nMcPhee, John. 2017. Draft No. 4. 1st ed. Farrar, Straus;\nGiroux.\n\n\nMeng, Xiao-Li. 1994. “Multiple-Imputation Inferences with\nUncongenial Sources of Input.” Statistical Science 9\n(4): 538–58. https://doi.org/10.1214/ss/1177010269.\n\n\n———. 2012. “You Want Me to Analyze Data i Don’t Have? Are You\nInsane?” Shanghai Archives of Psychiatry 24 (5):\n297–301. https://doi.org/10.3969/j.issn.1002-0829.2012.05.011.\n\n\n———. 2018. “Statistical Paradises and Paradoxes in Big Data (i):\nLaw of Large Populations, Big Data Paradox, and the 2016 US Presidential\nElection.” The Annals of Applied Statistics 12 (2):\n685–726. https://doi.org/10.1214/18-AOAS1161SF.\n\n\n———. 2021. “What Are the Values of Data, Data Science, or Data\nScientists?” Harvard Data Science Review 3 (1). https://doi.org/10.1162/99608f92.ee717cf7.\n\n\nMerali, Zeeya. 2010. “Computational Science:... Error.”\nNature 467 (7317): 775–77. https://doi.org/10.1038/467775a.\n\n\nMichener, William. 2015. “Ten Simple Rules for Creating a Good\nData Management Plan.” PLOS Computational Biology 11\n(10): e1004525. https://doi.org/10.1371/journal.pcbi.1004525.\n\n\nMill, James. 1817. The History of British India. 1st ed. https://books.google.ca/books?id=Orw_AAAAcAAJ.\n\n\nMiller, Greg. 2014. “The Cartographer Who’s\nTransforming Map Design.” Wired, October. https://www.wired.com/2014/10/cindy-brewer-map-design/.\n\n\nMiller, Michael, and Joseph Sutherland. 2022. “The Effect of\nGender on Interruptions at Congressional Hearings.” American\nPolitical Science Review, 1–19. https://doi.org/10.1017/S0003055422000260.\n\n\nMindell, David. 2008. Digital Apollo: Human and\nMachine in Spaceflight. 1st ed. New York: The MIT Press.\n\n\nMinsky, Yaron. 2011. “OCaml for the\nmasses.” Communications of the ACM 54 (11):\n53–58. https://doi.org/10.1145/2018396.2018413.\n\n\n———. 2015. “Automated Trading and OCaml with Yaron Minsky.”\nHackers — Software Engineering Daily, November. https://softwareengineeringdaily.com/2015/11/09/automated-trading-and-ocaml-with-yaron-minsky/.\n\n\nMitchell, Alanna. 2022a. “Get Ready for the New, Improved\nSecond.” The New York Times, April. https://www.nytimes.com/2022/04/25/science/time-second-measurement.html.\n\n\n———. 2022b. “Time Has Run Out for the Leap Second.” The\nNew York Times, November. https://www.nytimes.com/2022/11/14/science/time-leap-second.html.\n\n\nMitchell, Margaret, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy\nVasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and\nTimnit Gebru. 2019. “Model Cards for Model Reporting.”\nProceedings of the Conference on Fairness, Accountability, and\nTransparency, January. https://doi.org/10.1145/3287560.3287596.\n\n\nMiyakawa, Tsuyoshi. 2020. “No Raw Data, No Science: Another\nPossible Source of the Reproducibility Crisis.” Molecular\nBrain 13 (1): 1–6. https://doi.org/10.1186/s13041-020-0552-2.\n\n\nMolanphy, Chris. 2012. “100 & Single: Three Rules to Define\nthe Term ‘One-Hit Wonder’ in 2012.” The Village\nVoice, September. https://www.villagevoice.com/2012/09/10/100-single-three-rules-to-define-the-term-one-hit-wonder-in-2012/.\n\n\nMorange, Michel. 2016. A History of Biology. New Jersey:\nPrinceton University Press.\n\n\nMoyer, Brian, and Abe Dunn. 2020. “Measuring the\nGross Domestic Product\n(GDP): The Ultimate Data\nScience Project.” Harvard Data\nScience Review 2 (1). https://doi.org/10.1162/99608f92.414caadb.\n\n\nMullard, Asher. 2021. “Half of Top Cancer Studies Fail\nHigh-Profile Reproducibility Effort.” Nature 600 (7889):\n368--369. https://doi.org/10.1038/d41586-021-03691-0.\n\n\nMüller, Kirill. 2020. here: A Simpler Way to\nFind Your Files. https://CRAN.R-project.org/package=here.\n\n\nMüller, Kirill, Tobias Schieferdecker, and Patrick Schratz. 2019.\nVisualization, Transformation and Reporting with the Tidyverse.\nhttps://krlmlr.github.io/vistransrep/.\n\n\nMüller, Kirill, and Lorenz Walthert. 2022. styler: Non-Invasive Pretty Printing of R\nCode. https://CRAN.R-project.org/package=styler.\n\n\nMüller, Kirill, and Hadley Wickham. 2022. tibble: Simple Data Frames. https://CRAN.R-project.org/package=tibble.\n\n\nNational Academies of Sciences, Engineering, and Medicine. 2019.\nReproducibility and Replicability in Science. 1st ed. National\nAcademies Press. https://doi.org/10.17226/25303.\n\n\nNelder, John. 1999. “From Statistics to Statistical\nScience.” Journal of the Royal Statistical Society: Series D\n(The Statistician) 48 (2): 257–69. https://doi.org/10.1111/1467-9884.00187.\n\n\nNeufeld, Anna, and Daniela Witten. 2021. “Discussion of Breiman’s\n\"Two Cultures\": From Two Cultures to One.” Observational\nStudies 7 (1): 171–74. https://doi.org/10.1353/obs.2021.0004.\n\n\nNeufeld, Michael. 2002. “Wernher von Braun, the SS, and\nConcentration Camp Labor: Questions of Moral, Political, and Criminal\nResponsibility.” German Studies Review 25 (1): 57–78. https://doi.org/10.2307/1433245.\n\n\nNeuwirth, Erich. 2022. RColorBrewer: ColorBrewer\nPalettes. https://CRAN.R-project.org/package=RColorBrewer.\n\n\nNewman, Daniel. 2014. “Missing Data: Five Practical\nGuidelines.” Organizational Research Methods 17 (4):\n372–411. https://doi.org/10.1177/1094428114548590.\n\n\nNeyman, Jerzy. 1934. “On the Two Different Aspects of the\nRepresentative Method: The Method of Stratified Sampling and the Method\nof Purposive Selection.” Journal of the Royal Statistical\nSociety 97 (4): 558–625. https://doi.org/10.2307/2342192.\n\n\nNobles, Melissa. 2002. “Racial Categorization and\nCensuses.” In Census and Identity: The Politics of Race,\nEthnicity, and Language in National Censuses, edited by David\nKertzer and Dominique Arel, 43–70. Cambridge: Cambridge University\nPress. https://doi.org/10.1017/CBO9780511606045.003.\n\n\nOberski, Daniel, and Frauke Kreuter. 2020. “Differential Privacy\nand Social Science: An Urgent\nPuzzle.” Harvard Data Science Review 2 (1).\nhttps://doi.org/10.1162/99608f92.63a22079.\n\n\nOECD. 2014. “The Essential Macroeconomic Aggregates.” In\nUnderstanding National Accounts, 13–46. OECD. https://doi.org/10.1787/9789264214637-2-en.\n\n\n———. 2022. Quarterly GDP. https://data.oecd.org/gdp/quarterly-gdp.htm.\n\n\nOoms, Jeroen. 2014. “The jsonlite Package: A\nPractical and Consistent Mapping Between JSON Data and R\nObjects.” arXiv:1403.2805 [Stat.CO]. https://arxiv.org/abs/1403.2805.\n\n\n———. 2022a. openssl: Toolkit for Encryption,\nSignatures and Certificates Based on OpenSSL. https://CRAN.R-project.org/package=openssl.\n\n\n———. 2022b. pdftools: Text Extraction,\nRendering and Converting of PDF Documents. https://CRAN.R-project.org/package=pdftools.\n\n\n———. 2022c. ssh: Secure Shell (SSH) Client for\nR. https://CRAN.R-project.org/package=ssh.\n\n\n———. 2022d. tesseract: Open Source OCR\nEngine. https://CRAN.R-project.org/package=tesseract.\n\n\nOpen Science Collaboration. 2015. “Estimating the Reproducibility\nof Psychological Science.” Science 349 (6251): aac4716.\nhttps://doi.org/10.1126/science.aac4716.\n\n\nOsborne, Jason. 2012. Best Practices in Data\nCleaning: A Complete Guide to Everything You Need to Do Before and After\nCollecting Your Data. SAGE Publications.\n\n\nPalmer Station Antarctica LTER, and Gorman, Kristen. 2020.\n“Structural Size Measurements and Isotopic Signatures of Foraging\nAmong Adult Male and Female Adélie Penguins (Pygoscelis Adeliae) Nesting\nAlong the Palmer Archipelago Near Palmer Station, 2007-2009.” https://doi.org/10.6073/PASTA/98B16D7D563F265CB52372C8CA99E60F.\n\n\nPasek, Josh. 2015. “Predicting Elections:\nConsidering Tools to Pool the Polls.” Public Opinion\nQuarterly 79 (2): 594–619. https://doi.org/10.1093/poq/nfu060.\n\n\nPatki, Neha, Roy Wedge, and Kalyan Veeramachaneni. 2016. “The\nSynthetic Data Vault.” In 2016 IEEE International Conference\non Data Science and Advanced Analytics (DSAA), 399–410. https://doi.org/10.1109/DSAA.2016.49.\n\n\nPaullada, Amandalynne, Inioluwa Deborah Raji, Emily Bender, Emily\nDenton, and Alex Hanna. 2021. “Data and Its (Dis)contents: A\nSurvey of Dataset Development and Use in Machine Learning\nResearch.” Patterns 2 (11): 100336. https://doi.org/10.1016/j.patter.2021.100336.\n\n\nPedersen, Thomas Lin. 2022. patchwork: The\nComposer of Plots. https://CRAN.R-project.org/package=patchwork.\n\n\nPenrose, Carly. 2024. “Deadly Fires: Risk of Death, Injury Highest\nin Toronto’s Poor Neighbourhoods.” CBC News, April. https://www.cbc.ca/news/canada/toronto/fatal-fires-lower-income-1.7177356.\n\n\nPerepolkin, Dmytro. 2022. polite: Be Nice on\nthe Web. https://CRAN.R-project.org/package=polite.\n\n\nPerkel, Jeffrey. 2021. “Ten Computer Codes That Transformed\nScience.” Nature 589 (7842): 344–48. https://doi.org/10.1038/d41586-021-00075-2.\n\n\n———. 2023. “The Sleight-of-Hand Trick That Can Simplify Scientific\nComputing.” Nature 617 (7959): 212--213. https://doi.org/10.1038/d41586-023-01469-0.\n\n\nPhillips, Alban. 1958. “The Relation Between Unemployment and the\nRate of Change of Money Wage Rates in the United Kingdom,\n1861-1957.” Economica 25 (100): 283–99. https://doi.org/10.1111/j.1468-0335.1958.tb00003.x.\n\n\nPineau, Joelle, Philippe Vincent-Lamarre, Koustuv Sinha, Vincent\nLarivière, Alina Beygelzimer, Florence d’Alché-Buc, Emily Fox, and Hugo\nLarochelle. 2021. “Improving Reproducibility in Machine Learning\nResearch (a Report from the NeurIPS 2019 Reproducibility\nProgram).” Journal of Machine Learning Research 22\n(164): 1–20. http://jmlr.org/papers/v22/20-303.html.\n\n\nPitman, Jim. 1993. Probability. 1st ed. New York: Springer. https://doi.org/10.1007/978-1-4612-4374-8.\n\n\nPlant, Anne, and Robert Hanisch. 2020. “Reproducibility in\nScience: A Metrology Perspective.” Harvard Data Science\nReview 2 (4). https://doi.org/10.1162/99608f92.eb6ddee4.\n\n\nPodlogar, Tim, Peter Leo, and James Spragg. 2022. “Using VO2max as a marker of training status in\nathletes—Can we do better?” Journal of Applied\nPhysiology 133 (6): 144–47. https://doi.org/10.1152/japplphysiol.00723.2021.\n\n\nPrévost, Jean-Guy, and Jean-Pierre Beaud. 2015. Statistics, Public\nDebate and the State, 1800–1945: A Social, Political and Intellectual\nHistory of Numbers. Routledge.\n\n\nPython Software Foundation. 2024. Python\nLanguage Reference, version 3.13.0. https://docs.python.org/3/index.html.\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical\nComputing. Vienna, Austria: R Foundation for Statistical Computing.\nhttps://www.R-project.org/.\n\n\nR Special Interest Group on Databases (R-SIG-DB), Hadley Wickham, and\nKirill Müller. 2022. DBI: R Database Interface. https://CRAN.R-project.org/package=DBI.\n\n\nRegister, Yim. 2020a. “Introduction to Sampling and\nRandomization.” YouTube, November. https://youtu.be/U272FFxG8LE.\n\n\n———. 2020b. “Data Science Ethics in 6 Minutes.”\nYouTube, December. https://youtu.be/mA4gypAiRYU.\n\n\nRehaag, Sean. 2023. “Supreme Court of Canada Bulk Decisions\nDataset.” Refugee Law Laboratory. https://refugeelab.ca/bulk-data/scc.\n\n\nRichardson, Neal, Ian Cook, Nic Crane, Dewey Dunnington, Romain\nFrançois, Jonathan Keane, Dragoș Moldovan-Grünfeld, Jeroen Ooms, and\nApache Arrow. 2023. arrow: Integration to\nApache Arrow. https://CRAN.R-project.org/package=arrow.\n\n\nRiederer, Emily. 2020. “Column Names as Contracts,”\nSeptember. https://emilyriederer.netlify.app/post/column-name-contracts/.\n\n\n———. 2021. “Causal Design Patterns for Data Analysts,”\nJanuary. https://emilyriederer.netlify.app/post/causal-design-patterns/.\n\n\nRiffe, Tim, Enrique Acosta, Enrique José Acosta, Diego Manuel Aburto,\nAnna Alburez-Gutierrez, Ainhoa Altová, Ugofilippo Alustiza, et al. 2021.\n“Data Resource Profile: COVerAGE-DB: A\nGlobal Demographic Database of COVID-19 Cases and\nDeaths.” International Journal of Epidemiology 50 (2):\n390–390f. https://doi.org/10.1093/ije/dyab027.\n\n\nRilke, Rainer Maria. (1929) 2014. Letters to a Young Poet.\nPenguin Classics.\n\n\nRoberts, Margaret, Brandon Stewart, and Dustin Tingley. 2019.\n“stm: An R Package for\nStructural Topic Models.” Journal of Statistical\nSoftware 91 (2): 1–40. https://doi.org/10.18637/jss.v091.i02.\n\n\nRobinson, David, Alex Hayes, and Simon Couch. 2022. broom: Convert Statistical Objects into Tidy\nTibbles. https://CRAN.R-project.org/package=broom.\n\n\nRobinson, Emily, and Jacqueline Nolis. 2020. Build a Career in Data\nScience. Shelter Island: Manning Publications. https://livebook.manning.com/book/build-a-career-in-data-science.\n\n\nRosenau, James N. 1999. “A Transformed Observer in a Transforming\nWorld.” Studia Diplomatica 52 (1/2): 5–14. http://www.jstor.org/stable/44838096.\n\n\nRoss, Casey. 2022. “How a Decades-Old Database Became a Hugely\nProfitable Dossier on the Health of 270 Million Americans.”\nStat, February. https://www.statnews.com/2022/02/01/ibm-watson-health-marketscan-data/.\n\n\nRubinstein, Benjamin, and Francesco Alda. 2017. “Pain-Free Random\nDifferential Privacy with Sensitivity Sampling.” In 34th\nInternational Conference on Machine Learning (ICML’2017).\n\n\nRudis, Bob. 2020. hrbrthemes: Additional\nThemes, Theme Components and Utilities for\n“ggplot2”. https://CRAN.R-project.org/package=hrbrthemes.\n\n\nRuggles, Steven, Catherine Fitch, Diana Magnuson, and Jonathan\nSchroeder. 2019. “Differential Privacy and Census Data:\nImplications for Social and Economic Research.” AEA Papers\nand Proceedings 109 (May): 403–8. https://doi.org/10.1257/pandp.20191107.\n\n\nRyan, Philip. 2015. “Keeping a Lab Notebook.”\nYouTube, May. https://youtu.be/-MAIuaOL64I.\n\n\nSadowski, Caitlin, Emma Söderberg, Luke Church, Michal Sipko, and\nAlberto Bacchelli. 2018. “Modern Code Review: A Case Study at\nGoogle.” In Proceedings of the 40th International Conference\non Software Engineering: Software Engineering in Practice, 181–90.\nICSE-SEIP ’18. New York, NY, USA: Association for Computing Machinery.\nhttps://doi.org/10.1145/3183519.3183525.\n\n\nSalganik, Matthew. 2018. Bit by Bit: Social Research in the Digital\nAge. New Jersey: Princeton University Press.\n\n\nSalganik, Matthew, and Douglas Heckathorn. 2004. “Sampling and\nEstimation in Hidden Populations Using Respondent-Driven\nSampling.” Sociological Methodology 34 (1): 193–240. https://doi.org/10.1111/j.0081-1750.2004.00152.x.\n\n\nSambasivan, Nithya, Shivani Kapania, Hannah Highfill, Diana Akrong,\nPraveen Paritosh, and Lora Aroyo. 2021. “‘Everyone Wants to\nDo the Model Work, Not the Data Work’: Data Cascades in\nHigh-Stakes AI.” In Proceedings of the 2021\nCHI Conference on Human Factors in Computing Systems.\nACM. https://doi.org/10.1145/3411764.3445518.\n\n\nSamuel, Arthur. 1959. “Some Studies in Machine Learning Using the\nGame of Checkers.” IBM Journal of Research and\nDevelopment 3 (3): 210–29. https://doi.org/10.1147/rd.33.0210.\n\n\nSavage, Van, and Pamela Yeh. 2019. “Novelist Cormac\nMcCarthy’s Tips on How to Write a Great Science\nPaper.” Nature 574 (7778): 441–42. https://doi.org/10.1038/d41586-019-02918-5.\n\n\nSchaffner, Brian, Stephen Ansolabehere, and Sam Luks. 2021.\n“Cooperative Election Study Common Content,\n2020.” Harvard Dataverse. https://doi.org/10.7910/DVN/E9N6PH.\n\n\nSchloerke, Barret, and Jeff Allen. 2022. plumber: An API Generator for R. https://CRAN.R-project.org/package=plumber.\n\n\nSchofield, Alexandra, Måns Magnusson, and David Mimno. 2017.\n“Pulling Out the Stops: Rethinking Stopword Removal for Topic\nModels.” In Proceedings of the 15th Conference of the\nEuropean Chapter of the Association for Computational\nLinguistics: Volume 2, Short Papers, 432–36. Valencia, Spain:\nAssociation for Computational Linguistics. https://aclanthology.org/E17-2069.\n\n\nScott, James. 1998. Seeing Like a State. Yale University Press.\n\n\nSekhon, Jasjeet, and Rocío Titiunik. 2017. “Understanding\nRegression Discontinuity Designs as Observational Studies.”\nObservational Studies 3 (2): 174–82. https://doi.org/10.1353/obs.2017.0005.\n\n\nSen, Amartya. 1980. “Description as\nChoice.” Oxford Economic Papers 32 (3): 353–69.\nhttps://doi.org/10.1093/oxfordjournals.oep.a041484.\n\n\nShankar, Shreya, Rolando Garcia, Joseph Hellerstein, and Aditya\nParameswaran. 2022. “Operationalizing Machine Learning: An\nInterview Study.” arXiv. https://doi.org/10.48550/ARXIV.2209.09125.\n\n\nSilberzahn, Raphael, Eric Uhlmann, Daniel Martin, Pasquale Anselmi,\nFrederik Aust, Eli Awtrey, Štěpán Bahnı́k, et al. 2018. “Many\nAnalysts, One Data Set: Making Transparent How Variations in Analytic\nChoices Affect Results.” Advances in Methods and Practices in\nPsychological Science 1 (3): 337–56. https://doi.org/10.1177/2515245917747646.\n\n\nSilge, Julia, and David Robinson. 2016. “tidytext: Text Mining and Analysis Using Tidy Data\nPrinciples in R.” The Journal of Open Source\nSoftware 1 (3). https://doi.org/10.21105/joss.00037.\n\n\nSilver, Nate. 2020. “We Fixed an Issue with How Our Primary\nForecast Was Calculating Candidates’ Demographic Strengths.”\nFiveThirtyEight, February. https://fivethirtyeight.com/features/we-fixed-a-mistake-in-how-our-primary-forecast-was-calculating-candidates-demographic-strengths/.\n\n\nSimonsohn, Uri. 2013. “Just Post It: The Lesson from Two Cases of\nFabricated Data Detected by Statistics Alone.” Psychological\nScience 24 (10): 1875–88. https://doi.org/10.1177/0956797613480366.\n\n\nSimpkinson, Scott. 1971. “Testing to Ensure\nMission Success.” In What Made Apollo a Success,\nedited by NASA, 21–29.\n\n\nSimpson, Edward. 1951. “The Interpretation of Interaction in\nContingency Tables.” Journal of the Royal Statistical\nSociety: Series B (Methodological) 13 (2): 238–41. https://doi.org/10.1111/j.2517-6161.1951.tb00088.x.\n\n\nSmith, Richard. 2002. “A Statistical Assessment of Buchanan’s Vote\nin Palm Beach County.” Statistical Science 17 (4):\n441–57. https://doi.org/10.1214/ss/1049993203.\n\n\nSobek, Matthew, and Steven Ruggles. 1999. “The IPUMS Project: An\nUpdate.” Historical Methods: A Journal of Quantitative and\nInterdisciplinary History 32 (3): 102–10. https://doi.org/10.1080/01615449909598930.\n\n\nSomers, James. 2015. “Toolkits for the\nMind.” MIT Technology Review, April. https://www.technologyreview.com/2015/04/02/168469/toolkits-for-the-mind/.\n\n\n———. 2017. “Torching the Modern-Day Library of Alexandria.”\nThe Atlantic, April. https://www.theatlantic.com/technology/archive/2017/04/the-tragedy-of-google-books/523320/.\n\n\nSpear, Mary Eleanor. 1952. Charting Statistics. https://archive.org/details/ChartingStatistics_201801/.\n\n\nSprint, Gina, and Jason Conci. 2019. “Mining GitHub Classroom\nCommit Behavior in Elective and Introductory Computer Science\nCourses.” Journal of Computing Sciences in Colleges 35\n(1): 76–84.\n\n\nStaicu, Ana-Maria. 2017. “Interview with Nancy Reid.”\nInternational Statistical Review 85 (3): 381–403. https://doi.org/10.1111/insr.12237.\n\n\nStaniak, Mateusz, and Przemysław Biecek. 2019. “The Landscape of R Packages for Automated Exploratory\nData Analysis.” The R Journal 11\n(2): 347–69. https://doi.org/10.32614/RJ-2019-033.\n\n\nStantcheva, Stefanie. 2023. “How to Run Surveys: A Guide to\nCreating Your Own Identifying Variation and Revealing the\nInvisible.” Annual Review of Economics 15 (1): 205–34.\nhttps://doi.org/10.1146/annurev-economics-091622-010157.\n\n\nStatistics Canada. 2023. “Guide to the Census of Population,\n2021.” Statistics Canada. https://www12.statcan.gc.ca/census-recensement/2021/ref/98-304/98-304-x2021001-eng.pdf.\n\n\nSteckel, Richard. 1991. “The Quality of Census Data for Historical\nInquiry: A Research Agenda.” Social Science History 15\n(4): 579–99. https://doi.org/10.2307/1171470.\n\n\nStevens, Wallace. 1934. The Idea of Order at Key West. https://www.poetryfoundation.org/poems/43431/the-idea-of-order-at-key-west.\n\n\nSteyvers, Mark, and Tom Griffiths. 2006. “Probabilistic Topic\nModels.” In Latent Semantic Analysis: A Road to Meaning,\nedited by T. Landauer, D McNamara, S. Dennis, and W. Kintsch. https://cocosci.princeton.edu/tom/papers/SteyversGriffiths.pdf.\n\n\nStigler, Stephen. 1978. “Francis Ysidro Edgeworth,\nStatistician.” Journal of the Royal Statistical\nSociety. Series A (General) 141 (3): 287–322. https://doi.org/10.2307/2344804.\n\n\n———. 1986. The History of Statistics. Massachusetts: Belknap\nHarvard.\n\n\nStock, James, and Francesco Trebbi. 2003. “Retrospectives: Who\nInvented Instrumental Variable Regression?” Journal of\nEconomic Perspectives 17 (3): 177–94. https://doi.org/10.1257/089533003769204416.\n\n\nStoler, Ann Laura. 2002. “Colonial Archives and the Arts of\nGovernance.” Archival Science 2 (March): 87–109. https://doi.org/10.1007/bf02435632.\n\n\nStommes, Drew, P. M. Aronow, and Fredrik Sävje. 2023. “On the\nReliability of Published Findings Using the Regression Discontinuity\nDesign in Political Science.” Research & Politics 10\n(2). https://doi.org/https://doi.org/10.1177/2053168023116645.\n\n\nStudent. 1908. “The Probable Error of a Mean.”\nBiometrika 6 (1): 1–25. https://doi.org/10.2307/2331554.\n\n\nSunstein, Cass, and Lucia Reisch. 2017. The Economics of Nudge.\nRoutledge.\n\n\nSylvester, Christine, Anastasia Ershova, Aleksandra Khokhlova, Nikoleta\nYordanova, and Zachary Greene. 2023. “ParlEE\nplenary speeches V2 data set: Annotated full-text of 15.1 million\nsentence-level plenary speeches of six EU legislative\nchambers.” Harvard Dataverse. https://doi.org/10.7910/DVN/VOPK0E.\n\n\nSzaszi, Barnabas, Anthony Higney, Aaron Charlton, Andrew Gelman, Ignazio\nZiano, Balazs Aczel, Daniel Goldstein, David Yeager, and Elizabeth\nTipton. 2022. “No Reason to Expect Large and Consistent Effects of\nNudge Interventions.” Proceedings of the National Academy of\nSciences 119 (31): e2200732119. https://doi.org/10.1073/pnas.2200732119.\n\n\nTaddy, Matt. 2019. Business Data Science. 1st ed. McGraw Hill.\n\n\nTal, Eran. 2020. “Measurement in\nScience.” In The Stanford Encyclopedia of\nPhilosophy, edited by Edward Zalta, Fall 2020. https://plato.stanford.edu/archives/fall2020/entries/measurement-science/;\nMetaphysics Research Lab, Stanford University.\n\n\nTang, John. 2015. “Pollution havens and the\ntrade in toxic chemicals: Evidence from U.S. trade flows.”\nEcological Economics 112 (April): 150–60. https://doi.org/10.1016/j.ecolecon.2015.02.022.\n\n\nTaylor, Adam. 2015. “New Zealand Says No to Jedis.” The\nWashington Post, September. https://www.washingtonpost.com/news/worldviews/wp/2015/09/29/new-zealand-says-no-to-jedis/.\n\n\nTeate, Renée. 2022. SQL for Data Scientists. Wiley.\n\n\nThe Washington Post. 2023. “Fatal Force Database.” https://github.com/washingtonpost/data-police-shootings.\n\n\nThieme, Nick. 2018. “R Generation.” Significance\n15 (4): 14–19. https://doi.org/10.1111/j.1740-9713.2018.01169.x.\n\n\nThompson, Charlie, Daniel Antal, Josiah Parry, Donal Phipps, and Tom\nWolff. 2022. spotifyr: R Wrapper for the\n“Spotify” Web API. https://CRAN.R-project.org/package=spotifyr.\n\n\nThomson-DeVeaux, Amelia, Laura Bronner, and Damini Sharma. 2021.\n“Cities Spend Millions On Police Misconduct\nEvery Year. Here’s Why It’s So Difficult to Hold Departments\nAccountable.” FiveThirtyEight, February. https://fivethirtyeight.com/features/police-misconduct-costs-cities-millions-every-year-but-thats-where-the-accountability-ends/.\n\n\nThornhill, John. 2021. “Lunch with the FT: Mathematician Hannah\nFry.” Financial Times, July. https://www.ft.com/content/a5e33e5a-99b9-4bbc-948f-8a527c7675c3.\n\n\nTierney, Nicholas, Di Cook, Miles McBain, and Colin Fay. 2021. naniar: Data Structures, Summaries, and Visualisations\nfor Missing Data. https://CRAN.R-project.org/package=naniar.\n\n\nTierney, Nicholas, and Karthik Ram. 2020. “A Realistic Guide to\nMaking Data Available Alongside Code to Improve Reproducibility.”\nhttps://arxiv.org/abs/2002.11626.\n\n\nTimbers, Tiffany. 2020. canlang: Canadian\nCensus language data. https://ttimbers.github.io/canlang/.\n\n\nTimbers, Tiffany, Trevor Campbell, and Melissa Lee. 2022. Data\nScience: A First Introduction. Chapman; Hall/CRC. https://datasciencebook.ca.\n\n\nTourangeau, Roger, Lance Rips, and Kenneth Rasinski. 2000. The\nPsychology of Survey Response. 1st ed. Cambridge University Press.\nhttps://doi.org/10.1017/CBO9780511819322.003.\n\n\nTukey, John. 1962. “The Future of Data Analysis.” The\nAnnals of Mathematical Statistics 33 (1): 1–67. https://doi.org/10.1214/aoms/1177704711.\n\n\nUN IGME. 2021. “Levels and Trends in Child Mortality,\n2021.” https://childmortality.org/wp-content/uploads/2021/12/UNICEF-2021-Child-Mortality-Report.pdf.\n\n\nUshey, Kevin. 2022. renv: Project\nEnvironments. https://CRAN.R-project.org/package=renv.\n\n\nvan Buuren, Stef, and Karin Groothuis-Oudshoorn. 2011. “mice: Multivariate Imputation by Chained Equations in\nR.” Journal of Statistical Software 45 (3): 1–67.\nhttps://doi.org/10.18637/jss.v045.i03.\n\n\nVan den Broeck, Jan, Solveig Argeseanu Cunningham, Roger Eeckels, and\nKobus Herbst. 2005. “Data Cleaning: Detecting, Diagnosing, and\nEditing Data Abnormalities.” PLOS Medicine 2 (10): e267.\nhttps://doi.org/10.1371/journal.pmed.0020267.\n\n\nvan der Loo, Mark, and Edwin De Jonge. 2021. “Data Validation Infrastructure for R.”\nJournal of Statistical Software 97 (10): 1–33. https://doi.org/10.18637/jss.v097.i10.\n\n\nVanderplas, Susan, Dianne Cook, and Heike Hofmann. 2020. “Testing\nStatistical Charts: What Makes a Good Graph?” Annual Review\nof Statistics and Its Application 7: 61–88. https://doi.org/10.1146/annurev-statistics-031219-041252.\n\n\nVanhoenacker, Mark. 2015. Skyfaring: A Journey with a Pilot.\n1st ed. Alfred A. Knopf.\n\n\nVavreck, Lynn, and Chris Tausanovitch. 2021. “Democracy Fund\n+ UCLA Nationscape Project User Guide.” https://www.voterstudygroup.org/data/nationscape.\n\n\nVidoni, Melina. 2021. “Evaluating Unit\nTesting Practices in R Packages.” In 2021 IEEE/ACM\n43rd International Conference on Software Engineering (ICSE),\n1523–34. https://doi.org/10.1109/ICSE43902.2021.00136.\n\n\nvon Bergmann, Jens, Dmitry Shkolnik, and Aaron Jacobs. 2021. cancensus: R package to access, retrieve, and work with\nCanadian Census data and geography. https://mountainmath.github.io/cancensus/.\n\n\nWalby, Kevin, and Alex Luscombe. 2019. Freedom of Information and\nSocial Science Research Design. Routledge.\n\n\nWalker, Kyle, and Matt Herman. 2022. tidycensus: Load US Census Boundary and Attribute Data as\n“tidyverse” and “sf”-Ready Data\nFrames. https://CRAN.R-project.org/package=tidycensus.\n\n\nWallach, Hanna. 2018. “Computational Social Science ≠ Computer Science + Social Data.”\nCommunications of the ACM 61 (3): 42–44. https://doi.org/10.1145/3132698.\n\n\nWan, Mengting, and Julian J. McAuley. 2018. “Item Recommendation\non Monotonic Behavior Chains.” In Proceedings of the 12th\nACM Conference on Recommender Systems, RecSys 2018,\nVancouver, BC, Canada, October 2-7, 2018, edited by Sole Pera,\nMichael D. Ekstrand, Xavier Amatriain, and John O’Donovan, 86–94.\nACM. https://doi.org/10.1145/3240323.3240369.\n\n\nWan, Mengting, Rishabh Misra, Ndapa Nakashole, and Julian J. McAuley.\n2019. “Fine-Grained Spoiler Detection from Large-Scale Review\nCorpora.” In Proceedings of the 57th Conference of the\nAssociation for Computational Linguistics, ACL 2019,\nFlorence, Italy, July 28- August 2, 2019, Volume 1: Long Papers,\nedited by Anna Korhonen, David R. Traum, and Lluı́s Màrquez, 2605–10.\nAssociation for Computational Linguistics. https://doi.org/10.18653/v1/p19-1248.\n\n\nWang, Wei, David Rothschild, Sharad Goel, and Andrew Gelman. 2015.\n“Forecasting Elections with Non-Representative Polls.”\nInternational Journal of Forecasting 31 (3): 980–91. https://doi.org/10.1016/j.ijforecast.2014.06.001.\n\n\nWang, Yilun, and Michal Kosinski. 2018. “Deep Neural Networks Are\nMore Accurate Than Humans at Detecting Sexual Orientation from Facial\nImages.” Journal of Personality and Social Psychology\n114 (2): 246–57. https://doi.org/10.1037/pspa0000098.\n\n\nWardrop, Robert. 1995. “Simpson’s Paradox and the Hot Hand in\nBasketball.” The American Statistician 49 (1): 24–28. https://doi.org/10.2307/2684806.\n\n\nWare, James. 1989. “Investigating Therapies of Potentially Great\nBenefit: ECMO.” Statistical Science 4 (4): 298–306. https://doi.org/10.1214/ss/1177012384.\n\n\nWasserman, Larry. 2005. All of Statistics. Springer.\n\n\nWei, Eugene. 2017. Remove the Legend to Become One. https://www.eugenewei.com/blog/2017/11/13/remove-the-legend.\n\n\nWeinberg, Gerald. 1971. The Psychology of Computer Programming.\nNew York: Van Nostrand Reinhold Company.\n\n\nWeissgerber, Tracey, Natasa Milic, Stacey Winham, and Vesna Garovic.\n2015. “Beyond Bar and Line Graphs: Time for a New Data\nPresentation Paradigm.” PLoS Biology 13 (4): e1002128.\nhttps://doi.org/10.1371/journal.pbio.1002128.\n\n\nWhitby, Andrew. 2020. The Sum of the\nPeople. New York: Basic Books.\n\n\nWhitelaw, James. 1805. An Essay on the Population of Dublin. Being\nthe Result of an Actual Survey Taken in 1798, with Great Care and\nPrecision, and Arranged in a Manner Entirely New. Graisberry;\nCampbell.\n\n\nWickham, Hadley. 2009. “Manipulating Data.” In ggplot2, 157–75. Springer New York. https://doi.org/10.1007/978-0-387-98141-3_9.\n\n\n———. 2011. “testthat: Get Started with\nTesting.” The R Journal 3: 5–10. https://journal.r-project.org/archive/2011-1/RJournal%5F2011-1%5FWickham.pdf.\n\n\n———. 2014. “Tidy Data.” Journal of Statistical\nSoftware 59 (1): 1–23. https://doi.org/10.18637/jss.v059.i10.\n\n\n———. 2016. ggplot2: Elegant Graphics for Data\nAnalysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\n———. 2017. tidyverse: Easily Install and Load\nthe “Tidyverse”. https://CRAN.R-project.org/package=tidyverse.\n\n\n———. 2018. “Whole Game.” YouTube, January. https://youtu.be/go5Au01Jrvs.\n\n\n———. 2019. Advanced R. 2nd ed. Chapman; Hall/CRC.\nhttps://adv-r.hadley.nz.\n\n\n———. 2020. Tidyverse. https://www.tidyverse.org/.\n\n\n———. 2021a. babynames: US Baby Names\n1880-2017. https://CRAN.R-project.org/package=babynames.\n\n\n———. 2021b. Mastering Shiny. 1st ed. O’Reilly Media. https://mastering-shiny.org.\n\n\n———. 2021c. The Tidyverse Style Guide. https://style.tidyverse.org/index.html.\n\n\n———. 2022a. R Packages. 2nd ed. O’Reilly Media. https://r-pkgs.org.\n\n\n———. 2022b. rvest: Easily Harvest (Scrape) Web\nPages. https://CRAN.R-project.org/package=rvest.\n\n\n———. 2022c. stringr: Simple, Consistent\nWrappers for Common String Operations. https://CRAN.R-project.org/package=stringr.\n\n\n———. 2023a. forcats: Tools for Working with\nCategorical Variables (Factors). https://CRAN.R-project.org/package=forcats.\n\n\n———. 2023b. httr: Tools for Working with URLs\nand HTTP. https://CRAN.R-project.org/package=httr.\n\n\nWickham, Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019.\n“Welcome to the Tidyverse.”\nJournal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, and Jennifer Bryan. 2023. readxl: Read Excel Files. https://CRAN.R-project.org/package=readxl.\n\n\nWickham, Hadley, Jennifer Bryan, and Malcolm Barrett. 2022. usethis: Automate Package and Project Setup.\nhttps://CRAN.R-project.org/package=usethis.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. (2016)\n2023. R for Data Science. 2nd ed. O’Reilly Media. https://r4ds.hadley.nz.\n\n\nWickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2022.\ndplyr: A Grammar of Data\nManipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, Maximilian Girlich, and Edgar Ruiz. 2022. dbplyr: A “dplyr” Back End for\nDatabases. https://CRAN.R-project.org/package=dbplyr.\n\n\nWickham, Hadley, and Lionel Henry. 2022. purrr:\nFunctional Programming Tools. https://CRAN.R-project.org/package=purrr.\n\n\nWickham, Hadley, Jim Hester, and Jenny Bryan. 2022. readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr.\n\n\nWickham, Hadley, Jim Hester, Winston Chang, and Jenny Bryan. 2022.\ndevtools: Tools to Make Developing R Packages\nEasier. https://CRAN.R-project.org/package=devtools.\n\n\nWickham, Hadley, Jim Hester, and Jeroen Ooms. 2021. xml2: Parse XML. https://CRAN.R-project.org/package=xml2.\n\n\nWickham, Hadley, Evan Miller, and Danny Smith. 2023. haven: Import and Export “SPSS”\n“Stata” and “SAS” Files. https://CRAN.R-project.org/package=haven.\n\n\nWickham, Hadley, and Dana Seidel. 2022. scales:\nScale Functions for Visualization. https://CRAN.R-project.org/package=scales.\n\n\nWickham, Hadley, and Lisa Stryjewski. 2011. “40 Years of\nBoxplots,” November. https://vita.had.co.nz/papers/boxplots.pdf.\n\n\nWickham, Hadley, Davis Vaughan, and Maximilian Girlich. 2023. tidyr: Tidy Messy Data. https://CRAN.R-project.org/package=tidyr.\n\n\nWiessner, Polly. 2014. “Embers of Society: Firelight Talk Among\nthe Ju/’hoansi Bushmen.” Proceedings of the National Academy\nof Sciences 111 (39): 14027–35. https://doi.org/10.1073/pnas.1404212111.\n\n\nWilde, Oscar. 1891. The Picture of Dorian Gray. https://www.gutenberg.org/files/174/174-h/174-h.htm.\n\n\nWilford, John Noble. 1977. “Wernher von Braun, Rocket Pioneer,\nDies.” The New York Times, June. https://www.nytimes.com/1977/06/18/archives/wernher-von-braun-rocket-pioneer-dies-wernher-von-braun-pioneer-in.html.\n\n\nWilkinson, Leland. 2005. The Grammar of Graphics. 2nd ed.\nSpringer.\n\n\nWilkinson, Mark, Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle\nAppleton, Myles Axton, Arie Baak, Niklas Blomberg, et al. 2016.\n“The FAIR Guiding Principles for Scientific Data Management and\nStewardship.” Scientific Data 3 (1): 1–9. https://doi.org/10.1038/sdata.2016.18.\n\n\nWilson, Greg, Jenny Bryan, Karen Cranston, Justin Kitzes, Lex\nNederbragt, and Tracy Teal. 2017. “Good Enough Practices in\nScientific Computing.” PLOS Computational Biology 13\n(6): 1–20. https://doi.org/10.1371/journal.pcbi.1005510.\n\n\nWood, Simon. 2015. Core Statistics. Cambridge University Press.\nhttps://www.maths.ed.ac.uk/\\%7Eswood34/core-statistics.pdf.\n\n\nWorld Health Organization. 2019. “Trends in Maternal Mortality\n2000 to 2017: Estimates by WHO, UNICEF, UNFPA, World Bank Group and the\nUnited Nations Population Division.” https://apps.who.int/iris/handle/10665/327596.\n\n\nWu, Changbao, and Mary Thompson. 2020. Sampling Theory and\nPractice. Springer.\n\n\nXie, Yihui. 2019. “TinyTeX: A lightweight,\ncross-platform, and easy-to-maintain LaTeX distribution based on TeX\nLive.” TUGboat, no. 1: 30–32. https://tug.org/TUGboat/Contents/contents40-1.html.\n\n\n———. 2023. knitr: A General-Purpose Package for\nDynamic Report Generation in R. https://yihui.org/knitr/.\n\n\nXu, Ya. 2020. “Causal Inference Challenges in Industry: A\nPerspective from Experiences at LinkedIn.” YouTube,\nJuly. https://youtu.be/OoKsLAvyIYA.\n\n\nYoshioka, Alan. 1998. “Use of Randomisation in the Medical\nResearch Council’s Clinical Trial of Streptomycin in Pulmonary\nTuberculosis in the 1940s.” BMJ 317 (7167): 1220–23. https://doi.org/10.1136/bmj.317.7167.1220.\n\n\nZhang, Ping, XunPeng Shi, YongPing Sun, Jingbo Cui, and Shuai Shao.\n2019. “Have China’s provinces achieved their\ntargets of energy intensity reduction? Reassessment based on nighttime\nlighting data.” Energy Policy 128 (May): 276–83.\nhttps://doi.org/10.1016/j.enpol.2019.01.014.\n\n\nZinsser, William. 1976. On Writing Well. New York:\nHarperCollins.",
    "crumbs": [
      "부록",
      "참고 문헌"
    ]
  }
]