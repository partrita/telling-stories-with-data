---
engine: knitr
---

# 데이터로서의 텍스트 {#sec-text-as-data}

**선수 지식**

- *데이터로서의 텍스트: 개요*, [@benoit2020text]
  - 이 장은 텍스트를 데이터로 사용하는 것에 대한 개요를 제공합니다.
- *R을 사용한 텍스트 분석을 위한 지도 기계 학습*, [@hvitfeldt2021supervised]
  - 텍스트를 데이터로 사용하여 선형 및 일반화 선형 모델을 구현하는 6장 "회귀" 및 7장 "분류"에 집중하세요.
- *벌거벗은 진실: 6,816개 피부색 제품 이름이 아름다움의 편향을 어떻게 드러낼 수 있는가*, [@thenakedtruth]
  - 화장품 텍스트 분석.

**핵심 개념 및 기술**

- 텍스트를 분석할 수 있는 데이터 소스로 이해하면 많은 흥미로운 질문을 고려할 수 있습니다.
- 텍스트 정리 및 준비는 가능한 결과가 많기 때문에 특히 중요합니다. 이 단계에서 많은 결정을 내려야 하며, 이는 분석 후반에 중요한 영향을 미칩니다.
- 텍스트 데이터셋을 고려하는 한 가지 방법은 특정 문서를 구별하는 단어를 살펴보는 것입니다.
- 또 다른 방법은 문서에 포함된 주제를 고려하는 것입니다.

**소프트웨어 및 패키지**

- Base R [@citeR]
- `astrologer` [@astrologer] (이 패키지는 CRAN에 없으므로 `devtools::install_github("sharlagelfand/astrologer")`로 설치하세요)
- `beepr` [@beepr]
- `fs` [@fs]
- `gutenbergr` [@gutenbergr]
- `quanteda` [@quanteda]
- `stm` [@stm]
- `tidytext` [@SilgeRobinson2016]
- `tidyverse` [@tidyverse]
- `tinytable` [@tinytable]

```{r}
#| message: false
#| warning: false

library(astrologer)
# library(beepr)
library(fs)
# library(gutenbergr)
# library(quanteda)
# library(stm)
library(tidytext)
library(tidyverse)
library(tinytable)
```

## 서론

텍스트는 우리 주변에 있습니다.\index{text!data} 많은 경우, 텍스트는 우리가 접하는 가장 초기 유형의 데이터입니다. 계산 능력의 증가, 새로운 방법의 개발, 그리고 텍스트의 엄청난 가용성은 텍스트를 데이터로 사용하는 데 큰 관심을 불러일으켰습니다. 텍스트를 데이터로 사용하면 독특한 분석 기회를 제공합니다. 예를 들어:

- 아프리카 국가의 국영 신문 텍스트 분석은 정부의 조작을 식별할 수 있습니다[@Hassan2022];
- 영국 일간 신문 텍스트는 GDP 및 인플레이션에 대한 더 나은 예측을 생성하는 데 사용될 수 있으며[@Kalamara2022], 마찬가지로 *뉴욕 타임즈*\index{text!newspapers}는 미국 경제 활동과 상관 관계가 있는 불확실성 지수를 생성하는 데 사용될 수 있습니다[@Alexopoulos2015];
- 전자 건강 기록(EHR)\index{Electronic Health Records (EHR)}의 메모 분석은 질병 예측의 효율성을 향상시킬 수 있습니다[@jessgronsbell]; 그리고
- 미국 의회 기록 분석은 여성 의원들이 남성에게 얼마나 자주 방해받는지 보여줍니다[@millersutherland2022].

텍스트 분석에 대한 초기 접근 방식은 컨텍스트와 분리된 단어를 숫자로 변환하는 경향이 있었습니다.\index{text!context} 그런 다음 로지스틱 회귀의 변형과 같은 전통적인 접근 방식을 사용하여 분석할 수 있었습니다. 더 최근의 방법은 텍스트에 내재된 구조를 활용하려고 시도하며, 이는 추가적인 의미를 가져올 수 있습니다.\index{text!structure} 그 차이는 아마도 유사한 색상을 그룹화할 수 있는 아이와 어떤 물체가 무엇인지 아는 아이의 차이와 같을 것입니다. 악어와 나무는 모두 녹색이며, 그 지식으로 무언가를 할 수 있지만, 악어가 당신을 잡아먹을 수 있고 나무는 아마도 그렇지 않을 것이라는 것을 아는 것이 유용합니다.

텍스트는 이 책 전체에서 사용한 데이터셋의 다루기 어렵지만 유사한 버전으로 간주될 수 있습니다. 주요 차이점은 일반적으로 각 변수가 단어 또는 더 일반적으로 토큰인 넓은 데이터로 시작한다는 것입니다. 종종 각 항목은 개수입니다. 그런 다음 일반적으로 이를 단어 변수와 개수 변수가 있는 상당히 긴 데이터로 변환합니다. 텍스트를 데이터로 간주하는 것은 자연스럽게 컨텍스트에서 일부 추상화를 필요로 합니다. 그러나 이는 역사적 불평등을 영속화할 수 있으므로 완전히 분리되어서는 안 됩니다. 예를 들어, @koenecke2020은 자동 음성 인식 시스템이 백인 화자에 비해 흑인 화자에 대해 훨씬 더 나쁜 성능을 보인다는 것을 발견했으며, @davidson2019racial은 특정하게 정의된 기술 용어인 흑인 미국 영어\index{Black American!English}를 사용하는 트윗이 표준 미국 영어(다시 기술 용어임)의 유사한 트윗보다 더 높은 비율로 혐오 발언으로 분류된다는 것을 발견했습니다.

텍스트 데이터의 흥미로운 측면 중 하나는 일반적으로 분석 목적으로 생성되지 않는다는 것입니다. 단점은 우리가 작업할 수 있는 형태로 만들기 위해 일반적으로 더 많은 작업을 해야 한다는 것입니다. 데이터 정리 및 준비 단계에서 많은 결정을 내려야 합니다.\index{data cleaning}

텍스트 데이터셋의 크기가 커질수록 분석에 있어서 시뮬레이션하고 작게 시작하는 것이 특히 중요합니다.\index{simulation} 텍스트를 데이터로 사용하는 것은 우리에게 사용 가능한 텍스트의 양과 다양성 때문에 흥미롭습니다. 그러나 일반적으로 텍스트 데이터셋을 다루는 것은 지저분합니다. 일반적으로 많은 정리 및 준비가 필요합니다. 종종 텍스트 데이터셋은 큽니다. 따라서 재현 가능한 워크플로를 마련하고 발견한 내용을 명확하게 전달하는 것이 중요합니다.\index{reproducibility}\index{workflow} 그럼에도 불구하고, 흥미로운 분야입니다.

:::{.callout-note}
## 거인의 어깨 위에 서서

케네스 브누아 교수는 런던 경제 정치 대학(LSE)의 계산 사회 과학 교수이자 데이터 과학 연구소 소장입니다. 1998년 하버드 대학교에서 게리 킹과 케네스 셰플의 지도를 받아 정부학 박사 학위를 받은 후, 더블린 트리니티 칼리지에서 자리를 잡았고 2007년 교수로 승진했습니다. 2020년 LSE로 옮겼습니다. 그는 텍스트 데이터, 특히 정치 텍스트 및 소셜 미디어를 분석하는 정량적 방법의 전문가입니다. 그의 중요한 논문 중 일부는 정치 텍스트에서 정책 입장을 추출하고 정치학에서 "데이터로서의 텍스트" 하위 분야를 시작하는 데 도움이 된 @laver2003을 포함합니다. 그는 또한 수십 개국에서 원본 전문가 설문 조사 입장을 제공한 @benoitbook과 전문가 설문 조사를 당 정책 입장의 수동 코딩 분석과 비교한 @benoit2007과 같은 정책 입장을 추정하는 다른 방법에도 광범위하게 작업했습니다. 핵심 기여는 "텍스트 데이터의 정량적 분석"을 위한 `quanteda`[@quanteda]로 알려진 R 패키지 제품군으로, 텍스트 데이터를 쉽게 분석할 수 있게 해줍니다.
:::

이 장에서는 먼저 텍스트 데이터셋 준비를 고려합니다. 그런 다음 용어 빈도-역 문서 빈도(TF-IDF) 및 주제 모델을 고려합니다.

## 텍스트 정리 및 준비

텍스트 모델링은 흥미로운 연구 분야입니다.\index{text!cleaning} 그러나 일반적으로 말하자면, 정리 및 준비 측면은 모델링만큼이나 어렵습니다.\index{data!cleaning} 몇 가지 필수 사항을 다루고 구축할 수 있는 기반을 제공할 것입니다.

첫 번째 단계는 데이터를 가져오는 것입니다. [@sec-gather-data]에서 데이터 수집에 대해 논의했으며, 다음을 포함한 많은 출처를 언급했습니다.

- 리뷰에서 텍스트를 제공하는 *Inside Airbnb* 사용.
- 저작권이 만료된 책에서 텍스트를 제공하는 프로젝트 구텐베르크.
- 위키피디아 또는 기타 웹사이트 스크래핑.

텍스트 정리 및 준비에 필요한 핵심 패키지는 `tidyverse`의 일부인 `stringr`와 `quanteda`입니다.

설명을 위해 토니 모리슨의 *빌러브드*, 헬렌 드윗의 *마지막 사무라이*, 샬럿 브론테의 *제인 에어* 세 권의 책에서 첫 문장 또는 두 문장으로 구성된 코퍼스를 구성합니다.

```{r}
#| message: false
#| warning: false

last_samurai <-"우리 아버지의 아버지는 감리교 목사였다."

beloved <- "124는 악의적이었다. 아기의 독으로 가득했다."

jane_eyre <- "그날은 산책을 할 가능성이 없었다."

bookshelf <-
  tibble(
    book = c("마지막 사무라이", "빌러브드", "제인 에어"),
    first_sentence = c(last_samurai, beloved, jane_eyre)
  )

bookshelf
```

우리는 일반적으로 각 관측치에 문서, 각 열에 단어, 그리고 관련 메타데이터와 함께 각 조합에 대한 개수를 포함하는 문서-특징 행렬을 구성하고자 합니다.\index{text!document-feature matrix} 예를 들어, 코퍼스가 에어비앤비 리뷰 텍스트였다면, 각 문서는 리뷰일 수 있으며, 일반적인 특징에는 "The", "Airbnb", "was", "great" 등이 포함될 수 있습니다. 여기서 문장이 다른 단어로 분리되었다는 점에 유의하십시오. 우리는 일반적으로 관심 있는 다양한 측면 때문에 단어에서 벗어나 "토큰"이라는 용어를 사용하지만, 단어가 일반적으로 사용됩니다.\index{text!tokens}

```{r}
#| eval: false
books_corpus <-
  corpus(bookshelf,
         docid_field = "book",
         text_field = "first_sentence")

books_corpus
```

`quanteda`[@quanteda]의 `dfm()`을 사용하여 코퍼스의 토큰을 사용하여 문서-특징 행렬(DFM)을 구성합니다.\index{text!document-feature matrix}

```{r}
#| message: false
#| warning: false
#| eval: false
books_dfm <-
  books_corpus |>
  tokens() |>
  dfm()

books_dfm
```

이제 이 과정의 일부로 내려야 할 많은 결정을 고려합니다. 명확한 정답이나 오답은 없습니다. 대신, 데이터셋을 무엇에 사용할지에 따라 결정을 내립니다.

### 불용어

불용어는 "the", "and", "a"와 같은 단어입니다.\index{text!stop words} 오랫동안 불용어는 많은 의미를 전달하지 않는다고 생각되었고, 메모리 제약 계산에 대한 우려가 있었습니다. 텍스트 데이터셋을 준비하는 일반적인 단계는 불용어를 제거하는 것이었습니다. 이제 불용어가 많은 의미를 가질 수 있다는 것을 알고 있습니다[@schofield2017]. 불용어를 제거할지 여부에 대한 결정은 상황에 따라 달라지는 미묘한 결정입니다.

`quanteda`의 `stopwords()`를 사용하여 불용어 목록을 얻을 수 있습니다.

```{r}
#| eval: false
stopwords(source = "snowball")[1:10]
```

그런 다음 해당 목록의 모든 단어 인스턴스를 찾아 `str_replace_all()`을 사용하여 대략적으로 제거할 수 있습니다.

```{r}
#| eval: false
stop_word_list <-
  paste(stopwords(source = "snowball"), collapse = " | ")

bookshelf |>
  mutate(no_stops = str_replace_all(
    string = first_sentence,
    pattern = stop_word_list,
    replacement = " ")
  ) |>
  select(no_stops, first_sentence)
```

다른 사람들이 만든 많은 불용어 목록이 있습니다.\index{text!stop words} 예를 들어, `stopwords()`는 "snowball", "stopwords-iso", "smart", "marimo", "ancient", "nltk"를 포함한 목록을 사용할 수 있습니다. 더 일반적으로, 불용어를 사용하기로 결정하면 종종 프로젝트별 단어로 그러한 목록을 보강해야 합니다. 코퍼스에서 개별 단어의 개수를 생성한 다음, 가장 흔한 단어를 기준으로 정렬하고 적절하게 불용어 목록에 추가하여 이를 수행할 수 있습니다.

```{r}
#| eval: false
stop_word_list_updated <-
  paste(
    "Methodist |",
    "spiteful |",
    "possibility |",
    stop_word_list,
    collapse = " | "
  )

bookshelf |>
  mutate(no_stops = str_replace_all(
    string = first_sentence,
    pattern = stop_word_list_updated,
    replacement = " ")
  ) |>
  select(no_stops)
```

`quanteda`의 `dfm_remove()`를 사용하여 불용어 제거를 DFM 구성에 통합할 수 있습니다.

```{r}
#| message: false
#| warning: false
#| eval: false
books_dfm |>
dfm_remove(stopwords(source = "snowball"))
```

불용어를 제거하면 데이터셋을 인위적으로 조정합니다.\index{text!stop words} 때로는 그렇게 하는 것이 좋은 이유가 있을 수 있습니다. 그러나 무심코 해서는 안 됩니다. 예를 들어, @sec-farm-data 및 @sec-store-and-share에서 데이터셋이 응답자의 개인 정보 보호를 위해 검열, 절단 또는 유사한 방식으로 조작되어야 할 수 있다고 논의했습니다. 자연어 처리에서 불용어 제거를 기본 단계로 통합한 것은 이러한 방법이 개발되었을 때 더 제한적이었을 수 있는 계산 능력 때문일 수 있습니다. 어쨌든, @jurafskymartin [p. 62]은 불용어 제거가 텍스트 분류 성능을 향상시키지 않는다고 결론 내립니다. 관련하여, @schofield2017은 나중에 다룰 주제 모델링의 맥락에서, 어간 추출이 거의 영향을 미치지 않으며, 가장 흔한 단어를 제외한 다른 단어를 제거할 필요가 거의 없다는 것을 발견했습니다. 불용어를 제거해야 한다면, 주제가 구성된 후에 제거할 것을 권장합니다.

### 대소문자, 숫자 및 구두점

단어만 중요하고 대소문자나 구두점은 중요하지 않은 경우가 있습니다.\index{text!case}\index{text!numbers}\index{text!punctuation}\index{text!cleaning} 예를 들어, 텍스트 코퍼스가 특히 지저분하거나 특정 단어의 존재가 유익한 경우입니다. 정보 손실을 감수하고 더 간단하게 만드는 이점을 얻습니다. `str_to_lower()`로 소문자로 변환하고, `str_replace_all()`을 사용하여 "[:punct:]"로 구두점을 제거하고, "[:digit:]"로 숫자를 제거할 수 있습니다.

```{r}
bookshelf |>
  mutate(lower_sentence = str_to_lower(string = first_sentence)) |>
  select(lower_sentence)
```


```{r}
bookshelf |>
  mutate(no_punctuation_numbers = str_replace_all(
    string = first_sentence,
    pattern = "[:punct:]|[:digit:]",
    replacement = " "
  )) |>
  select(no_punctuation_numbers)
```

여담으로, `str_replace_all()`에서 "[:graph:]"를 사용하여 문자, 숫자 및 구두점을 제거할 수 있습니다. 이는 교과서 예시에서는 거의 필요하지 않지만, 실제 데이터셋에서는 특히 유용합니다. 왜냐하면 일반적으로 식별하고 제거해야 할 소수의 예상치 못한 기호가 있기 때문입니다. 우리는 익숙한 모든 것을 제거하고, 익숙하지 않은 것만 남겨둡니다.

더 일반적으로, `quanteda()`의 `tokens()`에서 인수를 사용하여 이를 수행할 수 있습니다.

```{r}
#| eval: false
books_corpus |>
  tokens(remove_numbers = TRUE, remove_punct = TRUE)
```

### 오타 및 흔하지 않은 단어

그런 다음 오타 및 기타 사소한 문제에 대해 무엇을 할지 결정해야 합니다.\index{text!typos}\index{text!cleaning} 모든 실제 텍스트에는 오타가 있습니다. 때로는 명확하게 수정해야 합니다. 그러나 체계적인 방식으로 만들어진 경우, 예를 들어 특정 작가가 항상 동일한 실수를 하는 경우, 작가별로 그룹화하는 데 관심이 있다면 가치가 있을 수 있습니다. OCR\index{Optical Character Recognition} 사용은 @sec-gather-data에서 보았듯이 일반적인 문제를 야기할 것입니다. 예를 들어, "the"는 일반적으로 "thc"로 잘못 인식됩니다.

불용어를 수정하는 것과 동일한 방식으로 오타를 수정할 수 있습니다. 즉, 수정 목록을 사용하는 것입니다. 흔하지 않은 단어의 경우, `dfm_trim()`을 사용하여 문서-특징 행렬 생성에 이를 포함할 수 있습니다. 예를 들어, 최소 두 번 이상 나타나지 않는 단어를 제거하기 위해 "min_termfreq = 2"를 사용하거나, 문서의 5% 이상에 나타나지 않는 단어를 제거하기 위해 "min_docfreq = 0.05"를 사용하거나, 문서의 90% 이상에 나타나는 단어를 제거하기 위해 "max_docfreq = 0.90"을 사용할 수 있습니다.

```{r}
#| message: false
#| warning: false
#| eval: false
books_corpus |>
  tokens(remove_numbers = TRUE, remove_punct = TRUE) |>
  dfm(tolower = TRUE) |>
  dfm_trim(min_termfreq = 2)
```

### 튜플

튜플은 정렬된 요소 목록입니다. 텍스트의 맥락에서는 일련의 단어입니다.\index{text!tuples} 튜플이 두 단어로 구성되면 "바이그램", 세 단어는 "트라이그램" 등으로 부릅니다. 이것들은 텍스트 정리 및 준비와 관련하여 문제입니다. 왜냐하면 우리는 종종 공백을 기준으로 용어를 분리하기 때문입니다. 이는 부적절한 분리로 이어질 것입니다.

이는 지명과 관련하여 명확한 문제입니다. 예를 들어, "British Columbia", "New Hampshire", "United Kingdom", "Port Hedland"를 고려해 보십시오. 한 가지 방법은 그러한 장소 목록을 만든 다음 `str_replace_all()`을 사용하여 밑줄을 추가하는 것입니다. 예를 들어, "British_Columbia", "New_Hampshire", "United_Kingdom", "Port_Hedland"입니다. 또 다른 옵션은 `quanteda`의 `tokens_compound()`를 사용하는 것입니다.

```{r}
#| eval: false
some_places <- c("British Columbia",
                 "New Hampshire",
                 "United Kingdom",
                 "Port Hedland")
a_sentence <-
c("밴쿠버는 브리티시 컬럼비아에 있고 뉴햄프셔는 아니다.")

tokens(a_sentence) |>
  tokens_compound(pattern = phrase(some_places))
```

이 경우 우리는 튜플이 무엇인지 알고 있었습니다. 그러나 코퍼스에서 일반적인 튜플이 무엇인지 확신하지 못했을 수도 있습니다. `tokens_ngrams()`를 사용하여 식별할 수 있습니다. 예를 들어, *제인 에어*의 발췌문에서 모든 바이그램을 요청할 수 있습니다.\index{Brontë, Charlotte!Jane Eyre} @sec-its-just-a-generalized-linear-model에서 프로젝트 구텐베르크\index{Project Gutenberg}에서 이 책의 텍스트를 다운로드하는 방법을 보여주었으므로, 여기서는 이전에 저장한 로컬 버전을 로드합니다.

```{r}
#| eval: false
#| echo: true

jane_eyre <- read_csv(
  "jane_eyre.csv",
  col_types = cols(
    gutenberg_id = col_integer(),
    text = col_character()
  )
)

jane_eyre
```

```{r}
#| eval: true
#| echo: false

# INTERNAL

jane_eyre <- read_csv(
  "inputs/jane_eyre.csv",
  col_types = cols(
    gutenberg_id = col_integer(),
    text = col_character()
  )
)

jane_eyre
```

빈 줄이 많으므로 제거할 것입니다.

```{r}
jane_eyre <-
  jane_eyre |>
  filter(!is.na(text))
```


```{r}
#| eval: false
jane_eyre_text <- tibble(
  book = "제인 에어",
  text = paste(jane_eyre$text, collapse = " ") |>
    str_replace_all(pattern = "[:punct:]",
                    replacement = " ") |>
    str_replace_all(pattern = stop_word_list,
                    replacement = " ")
)

jane_eyre_corpus <-
  corpus(jane_eyre_text, docid_field = "book", text_field = "text")
ngrams <- tokens_ngrams(tokens(jane_eyre_corpus), n = 2)
ngram_counts <-
  tibble(ngrams = unlist(ngrams)) |>
  count(ngrams, sort = TRUE)

head(ngram_counts)
```

일부 일반적인 바이그램을 식별했으므로 변경할 목록에 추가할 수 있습니다. 이 예시에는 "Mr Rochester" 및 "St John"과 같은 이름이 포함되어 있으며, 분석을 위해 함께 유지되어야 합니다.

### 어간 추출 및 표제어 추출

단어의 어간 추출 및 표제어 추출은 텍스트 데이터셋의 차원 축소를 위한 또 다른 일반적인 접근 방식입니다.\index{text!stemming} 어간 추출은 단어의 마지막 부분을 제거하여 더 일반적인 단어를 얻을 것으로 예상하는 것을 의미합니다. 예를 들어, "Canadians", "Canadian", "Canada"는 모두 "Canad"로 어간 추출됩니다. 표제어 추출\index{text!lemmatizing}은 유사하지만 더 복잡합니다. 이는 단어를 철자뿐만 아니라 정식 형태로 변경하는 것을 의미합니다[@textasdata, p. 54]. 예를 들어, "Canadians", "Canadian", "Canucks", "Canuck"는 모두 "Canada"로 변경될 수 있습니다.

`dfm_wordstem()`을 사용하여 이를 수행할 수 있습니다. 예를 들어, "minister"가 "minist"로 변경되었음을 알 수 있습니다.

```{r}
#| eval: false
char_wordstem(c("Canadians", "Canadian", "Canada"))

books_corpus |>
  tokens(remove_numbers = TRUE, remove_punct = TRUE) |>
  dfm(tolower = TRUE) |>
  dfm_wordstem()
```

이것은 텍스트를 데이터로 사용하는 일반적인 단계이지만, @schofield2017understanding은 나중에 다룰 주제 모델링의 맥락에서, 어간 추출이 거의 영향을 미치지 않으며, 이를 수행할 필요가 거의 없다는 것을 발견했습니다.

### 중복

중복은 텍스트 데이터셋의 크기 때문에 주요 관심사입니다.\index{text!duplication} 예를 들어, @bandy2021addressing은 BookCorpus 데이터셋의 약 30%가 부적절하게 중복되었음을 보여주었으며,\index{computer science} @schofield2017quantifying은 이것이 주요 관심사이며 결과에 상당한 영향을 미칠 수 있음을 보여줍니다. 그러나 이는 미묘하고 진단하기 어려운 문제일 수 있습니다. 예를 들어, @sec-its-just-a-generalized-linear-model에서 포아송 회귀의 맥락에서 다양한 작가의 페이지 번호 개수를 고려했을 때, 각 셰익스피어 항목을 실수로 두 번 포함했을 수 있습니다. 왜냐하면 각 희곡에 대한 항목뿐만 아니라 모든 희곡을 포함하는 많은 선집도 있기 때문입니다. 데이터셋에 대한 신중한 고려는 문제를 식별했지만, 대규모에서는 어려울 것입니다.

## 용어 빈도-역 문서 빈도 (TF-IDF)

### 점성술 구별

실제 데이터셋을 탐색하기 위해 점성술 데이터셋인 `astrologer`를 설치하고 로드합니다.\index{zodiac}\index{astrology}

그런 다음 "horoscopes" 데이터셋에 접근할 수 있습니다.

```{r}
horoscopes
```

네 가지 변수가 있습니다: "startdate", "zodiacsign", "horoscope", "url" (URL은 웹사이트가 업데이트되었기 때문에 오래된 것입니다. 예를 들어, 첫 번째 URL은 [여기](https://chaninicholas.com/horoscopes-week-january-5th/)를 참조합니다). 우리는 각 별자리의 운세를 구별하는 데 사용되는 단어에 관심이 있습니다.

```{r}
horoscopes |>
  count(zodiacsign)
```

각 별자리에 대해 106개의 운세가 있습니다. 이 예시에서는 먼저 단어별로 토큰화한 다음, 날짜가 아닌 별자리만 기준으로 개수를 생성합니다. @hvitfeldt2021supervised에서 광범위하게 사용되므로 `tidytext`를 사용합니다.

```{r}
horoscopes_by_word <-
  horoscopes |>
  select(-startdate,-url) |>
  unnest_tokens(output = word,
                input = horoscope,
                token = "words")

horoscopes_counts_by_word <-
  horoscopes_by_word |>
  count(zodiacsign, word, sort = TRUE)

horoscopes_counts_by_word
```

가장 인기 있는 단어들이 다른 별자리에서도 비슷하게 나타나는 것을 알 수 있습니다. 이 시점에서 다양한 방식으로 데이터를 사용할 수 있습니다.

각 그룹을 특징짓는 단어가 무엇인지, 즉 각 그룹에서만 일반적으로 사용되는 단어가 무엇인지 알고 싶을 수 있습니다.\index{term frequency–inverse document frequency} 이를 위해 먼저 단어의 용어 빈도(TF)를 살펴볼 수 있습니다. 이는 각 별자리의 운세에서 단어가 사용된 횟수입니다. 문제는 컨텍스트에 관계없이 일반적으로 사용되는 단어가 많다는 것입니다. 따라서 많은 별자리의 운세에 나타나는 단어에 "벌점"을 주는 역 문서 빈도(IDF)도 살펴보고 싶을 수 있습니다. 많은 별자리의 운세에 나타나는 단어는 하나의 별자리의 운세에만 나타나는 단어보다 낮은 IDF를 가질 것입니다. 용어 빈도-역 문서 빈도(tf-idf)는 이들의 곱입니다.

`tidytext`의 `bind_tf_idf()`를 사용하여 이 값을 생성할 수 있습니다. 각 측정값에 대한 새 변수를 생성합니다.

```{r}
horoscopes_counts_by_word_tf_idf <-
  horoscopes_counts_by_word |>
  bind_tf_idf(
    term = word,
    document = zodiacsign,
    n = n
  ) |>
  arrange(-tf_idf)

horoscopes_counts_by_word_tf_idf
```

@tbl-zodiac에서 각 별자리의 운세를 구별하는 단어를 살펴봅니다. 가장 먼저 주목할 점은 일부 별자리가 자체 별자리 이름을 가지고 있다는 것입니다. 한편으로는 이를 제거해야 한다는 주장이 있지만, 다른 한편으로는 모든 별자리에 해당되지 않는다는 사실이 각 별자리의 운세의 본질에 대한 정보를 제공할 수 있습니다.

```{r}
#| label: tbl-zodiac
#| tbl-cap: "특정 별자리에 고유한 운세에서 가장 흔한 단어"

horoscopes_counts_by_word_tf_idf |>
  slice(1:5,
        .by = zodiacsign) |>
  select(zodiacsign, word) |>
  summarise(all = paste0(word, collapse = "; "),
            .by = zodiacsign) |>
  tt() |>
  style_tt(j = 1:2, align = "lr") |>
  setNames(c("별자리", "해당 별자리에 고유한 가장 흔한 단어"))
```

## 주제 모델

주제 모델\index{text!topic models}은 많은 문장이 있고 유사한 단어를 사용하는 문장을 기반으로 그룹을 만들고 싶을 때 유용합니다. 우리는 유사한 단어 그룹을 주제로 정의합니다. 각 문장의 주제에 대한 일관된 추정치를 얻는 한 가지 방법은 주제 모델을 사용하는 것입니다. 많은 변형이 있지만, 한 가지 방법은 `stm`에 의해 구현된 @Blei2003latent의 잠재 디리클레 할당(LDA)\index{latent Dirichlet allocation} 방법을 사용하는 것입니다. 명확성을 위해 이 장의 맥락에서 LDA는 잠재 디리클레 할당을 의미하며, 선형 판별 분석을 의미하지는 않습니다. 비록 이것이 LDA 약어와 관련된 또 다른 일반적인 주제이지만 말입니다.

LDA 방법의 핵심 가정은 각 문장, 즉 문서가 해당 문서에서 이야기하고 싶은 주제를 결정하고, 그런 다음 해당 주제에 적합한 단어, 즉 용어를 선택하는 사람에 의해 만들어진다는 것입니다. 주제는 용어의 모음으로 생각할 수 있고, 문서는 주제의 모음으로 생각할 수 있습니다. 주제는 *사전에* 지정되지 않습니다. 그것들은 방법의 결과입니다. 용어는 반드시 특정 주제에 고유하지 않으며, 문서는 하나 이상의 주제에 관한 것일 수 있습니다. 이는 엄격한 단어 개수 방법과 같은 다른 접근 방식보다 더 많은 유연성을 제공합니다. 목표는 문서에서 발견된 단어들이 스스로 그룹화되어 주제를 정의하도록 하는 것입니다.

LDA는 각 문장을 사람이 이야기하고 싶은 주제를 먼저 선택하는 과정의 결과로 간주합니다. 주제를 선택한 후, 사람은 각 주제에 사용할 적절한 단어를 선택합니다. 더 일반적으로, LDA 주제 모델은 각 문서를 주제에 대한 어떤 확률 분포에 의해 생성된 것으로 간주하여 작동합니다. 예를 들어, 5개의 주제와 2개의 문서가 있다면, 첫 번째 문서는 주로 처음 몇 개의 주제로 구성될 수 있고, 다른 문서는 주로 마지막 몇 개의 주제에 관한 것일 수 있습니다(@fig-topicsoverdocuments).

```{r}
#| echo: false
#| fig-cap: "주제에 대한 확률 분포"
#| label: fig-topicsoverdocuments
#| layout-ncol: 2
#| fig-subcap: ["문서 1에 대한 분포", "문서 2에 대한 분포"]

topics <- c("주제 1", "주제 2", "주제 3", "주제 4", "주제 5")

document_1 <- tibble(
  Topics = topics,
  Probability = c(0.40, 0.40, 0.1, 0.05, 0.05)
)

document_2 <- tibble(
  Topics = topics,
  Probability = c(0.01, 0.04, 0.35, 0.20, 0.4)
)

ggplot(document_1, aes(Topics, Probability)) +
  geom_point() +
  theme_classic() +
  coord_cartesian(ylim = c(0, 0.4))

ggplot(document_2, aes(Topics, Probability)) +
  geom_point() +
  theme_classic() +
  coord_cartesian(ylim = c(0, 0.4))
```

마찬가지로, 각 주제는 용어에 대한 확률 분포로 간주될 수 있습니다. 각 문서에서 사용되는 용어를 선택하기 위해 화자는 각 주제에서 적절한 비율로 용어를 선택합니다. 예를 들어, 10개의 용어가 있다면, 한 주제는 이민과 관련된 용어에 더 많은 가중치를 부여하여 정의될 수 있고, 다른 주제는 경제와 관련된 용어에 더 많은 가중치를 부여할 수 있습니다(@fig-topicsoverterms).

```{r}
#| echo: false
#| fig-cap: "용어에 대한 확률 분포"
#| label: fig-topicsoverterms
#| layout-ncol: 2
#| fig-subcap: ["주제 1에 대한 분포", "주제 2에 대한 분포"]

some_terms <- c(
  "이민", "인종", "유입", "대출", "부",
  "저축", "중국인", "프랑스", "영국인", "영어")

topic_1 <- tibble(
  Terms = some_terms,
  Probability = c(0.0083, 0.0083, 0.0083, 0.0083, 0.0083, 0.0083, 0.2, 0.15, 0.4, 0.2)
)

topic_2 <- tibble(
  Terms = some_terms,
  Probability = c(0.0142, 0.0142, 0.0142, 0.25, 0.35, 0.30, 0.0142, 0.0142, 0.0142, 0.0142)
)

ggplot(topic_1, aes(Terms, Probability)) +
  geom_point() +
  theme_classic() +
  coord_cartesian(ylim = c(0, 0.4))
ggplot(topic_2, aes(Terms, Probability)) +
  geom_point() +
  theme_classic() +
  coord_cartesian(ylim = c(0, 0.4))
```

<!-- @BleiLafferty2009, @blei2012 및 @GriffithsSteyvers2004에 따르면, 문서가 생성되는 과정은 더 공식적으로 다음과 같이 간주됩니다. -->

<!-- 1. $1, 2, \dots, k, \dots, K$개의 주제가 있고 어휘는 $1, 2, \dots, V$개의 용어로 구성됩니다. 각 주제에 대해 용어에 대한 분포를 무작위로 추출하여 주제가 사용하는 용어를 결정합니다. $k$번째 주제에 대한 용어 분포는 $\beta_k$입니다. 일반적으로 주제는 소수의 용어이므로 하이퍼파라미터 $0<\eta<1$을 가진 디리클레 분포가 사용됩니다: $\beta_k \sim \mbox{Dirichlet}(\eta)$.[^Dirichletfootnote] 엄밀히 말하면, $\eta$는 각 $K$에 대한 하이퍼파라미터 벡터이지만, 실제로는 모두 동일한 값을 가지는 경향이 있습니다. -->
<!-- 2. $1, 2, \dots, d, \dots, D$개의 문서 각각에 대해 $K$개의 주제에 대한 분포를 무작위로 추출하여 각 문서가 다룰 주제를 결정합니다. $d$번째 문서에 대한 주제 분포는 $\theta_d$이고, $\theta_{d,k}$는 문서 $d$의 주제 $k$에 대한 주제 분포입니다. 다시, 일반적으로 문서가 소수의 주제만 다루므로 하이퍼파라미터 $0<\alpha<1$을 가진 디리클레 분포가 여기에 사용됩니다: $\theta_d \sim \mbox{Dirichlet}(\alpha)$. 다시, 엄밀히 말하면 $\alpha$는 길이 $K$의 하이퍼파라미터 벡터이지만, 실제로는 각 값이 동일한 값을 가지는 경향이 있습니다. -->
<!-- 3. $d$번째 문서에 $1, 2, \dots, n, \dots, N$개의 용어가 있다면, $n$번째 용어 $w_{d, n}$을 선택하려면: -->
<!--     a. 해당 문서 $d$에서 해당 용어 $n$에 대한 주제 $z_{d,n}$을 해당 문서의 주제에 대한 다항 분포에서 무작위로 선택합니다: $z_{d,n} \sim \mbox{Multinomial}(\theta_d)$. -->
<!--     b. 해당 주제에 대한 용어에 대한 관련 다항 분포에서 용어를 무작위로 선택합니다: $w_{d,n} \sim \mbox{Multinomial}(\beta_{z_{d,n}})$. -->

배경 지식으로, 디리클레 분포\index{distribution!Dirichlet}는 범주형 및 다항 변수에 대한 사전 분포로 일반적으로 사용되는 베타 분포의 변형입니다.\index{distribution!beta} 두 가지 범주만 있다면 디리클레 분포와 베타 분포는 동일합니다. 대칭 디리클레 분포의 특수한 경우인 $\eta=1$일 때, 이는 균일 분포와 동일합니다. $\eta<1$이면 분포는 희소하고 더 적은 수의 값에 집중되며, 이 수는 $\eta$가 감소함에 따라 감소합니다. 이 용법에서 하이퍼파라미터는 사전 분포의 매개변수입니다.

<!-- 이 설정을 고려할 때, 변수에 대한 결합 분포는 다음과 같습니다[@blei2012, p.6]. -->
<!-- $$p(\beta_{1:K}, \theta_{1:D}, z_{1:D, 1:N}, w_{1:D, 1:N}) = \prod^{K}_{i=1}p(\beta_i) \prod^{D}_{d=1}p(\theta_d) \left(\prod^N_{n=1}p(z_{d,n}|\theta_d)p\left(w_{d,n}|\beta_{1:K},z_{d,n}\right) \right).$$ -->

문서가 생성된 후, 그것들이 우리가 분석할 수 있는 전부입니다. 각 문서의 용어 사용은 관찰되지만, 주제는 숨겨져 있거나 "잠재적"입니다. 우리는 각 문서의 주제를 모르고, 용어가 주제를 어떻게 정의했는지도 모릅니다. 즉, @fig-topicsoverdocuments 또는 @fig-topicsoverterms의 확률 분포를 모릅니다. 어떤 의미에서 우리는 문서 생성 프로세스를 역전시키려고 노력하고 있습니다. 즉, 용어를 가지고 있고, 주제를 발견하고 싶습니다.

각 문서의 용어를 관찰하면 주제의 추정치를 얻을 수 있습니다[@SteyversGriffiths2006]. LDA 프로세스의 결과는 확률 분포입니다. 이 분포가 주제를 정의합니다. 각 용어는 특정 주제의 구성원일 확률을 부여받고, 각 문서는 특정 주제에 관한 것일 확률을 부여받습니다.

<!-- 즉, 각 문서에서 관찰된 용어가 주어졌을 때 주제의 사후 분포를 계산하려고 합니다(@blei2012, p.7). -->
<!-- $$p(\beta_{1:K}, \theta_{1:D}, z_{1:D, 1:N} | w_{1:D, 1:N}) = \frac{p\left(\beta_{1:K}, \theta_{1:D}, z_{1:D, 1:N}, w_{1:D, 1:N}\right)}{p(w_{1:D, 1:N})}.$$ -->

코퍼스 문서가 주어졌을 때 LDA를 구현할 때의 초기 실용적인 단계는 일반적으로 불용어를 제거하는 것입니다. 비록 앞서 언급했듯이 이것이 필수는 아니며, 그룹이 생성된 후에 수행하는 것이 더 나을 수 있습니다. 우리는 종종 구두점과 대소문자도 제거합니다. 그런 다음 `quanteda`의 `dfm()`을 사용하여 문서-특징 행렬을 구성합니다.

데이터셋이 준비되면 `stm`을 사용하여 LDA를 구현하고 사후 분포를 근사할 수 있습니다.
<!-- 깁스 샘플링 또는 변분 기대-최대화 알고리즘을 사용하여 이를 수행합니다. @SteyversGriffiths2006 및 @Darling2011에 따르면, 깁스 샘플링 -->
이 프로세스는 특정 문서의 특정 용어에 대한 주제를 찾으려고 시도합니다. 다른 모든 문서의 다른 모든 용어의 주제가 주어졌을 때 말입니다. 광범위하게 말하면, 먼저 모든 문서의 모든 용어를 디리클레 사전 분포에 의해 지정된 무작위 주제에 할당합니다.
<!-- $\alpha = \frac{50}{K}$ 및 $\eta = 0.1$ (@SteyversGriffiths2006은 $\eta = 0.01$을 권장), 여기서 $\alpha$는 주제에 대한 분포를 나타내고 $\eta$는 용어에 대한 분포를 나타냅니다(@Grun2011, p.7). -->
그런 다음 특정 문서의 특정 용어를 선택하고, 다른 모든 문서의 다른 모든 용어에 대한 주제가 주어진 조건부 분포를 기반으로 새 주제에 할당합니다[@Grun2011, p.6].
<!-- $$p(z_{d, n}=k | w_{1:D, 1:N}, z'_{d, n}) \propto \frac{\lambda'_{n\rightarrow k}+\eta}{\lambda'_{.\rightarrow k}+V\eta} \frac{\lambda'^{(d)}_{n\rightarrow k}+\alpha}{\lambda'^{(d)}_{-i}+K\alpha} $$ -->
<!-- 여기서 $z'_{d, n}$은 다른 모든 주제 할당을 나타냅니다. $\lambda'_{n\rightarrow k}$는 해당 용어가 주제 $k$에 할당된 다른 횟수를 나타냅니다. $\lambda'_{.\rightarrow k}$는 어떤 용어가 주제 $k$에 할당된 다른 횟수를 나타냅니다. $\lambda'^{(d)}_{n\rightarrow k}$는 해당 용어가 해당 특정 문서에서 주제 $k$에 할당된 다른 횟수를 나타냅니다. 그리고 $\lambda'^{(d)}_{-i}$는 해당 문서에서 해당 용어가 할당된 다른 횟수를 나타냅니다. -->
이것이 추정되면, 단어의 주제 분포와 문서의 주제 분포에 대한 추정치를 역추적할 수 있습니다.

조건부 분포는 용어가 이전에 해당 주제에 얼마나 자주 할당되었는지, 그리고 해당 문서에서 주제가 얼마나 흔한지에 따라 주제를 할당합니다[@SteyversGriffiths2ash]. 문서 코퍼스를 통한 초기 통과 결과는 좋지 않지만, 충분한 시간이 주어지면 알고리즘은 적절한 추정치로 수렴합니다.

주제 수 $k$의 선택은 결과에 영향을 미치며, *사전에* 지정되어야 합니다. 특정 수에 대한 강력한 이유가 있다면 이를 사용할 수 있습니다. 그렇지 않으면 적절한 수를 선택하는 한 가지 방법은 테스트 및 훈련 세트 프로세스를 사용하는 것입니다. 본질적으로, 이는 *k*의 다양한 가능한 값에 대해 프로세스를 실행한 다음, 잘 수행되는 적절한 값을 선택하는 것을 의미합니다.

LDA 방법의 한 가지 약점은 단어의 순서가 중요하지 않은 "단어 가방"을 고려한다는 것입니다[@blei2012]. 모델을 확장하여 단어 가방 가정의 영향을 줄이고 단어 순서에 조건성을 추가할 수 있습니다. 또한 디리클레 분포의 대안을 사용하여 모델을 확장하여 상관 관계를 허용할 수 있습니다.

### 캐나다 의회에서 무엇이 논의되는가?

영국 의회의 예를 따라, 캐나다 의회에서 말한 내용을 기록한 것은 "한사드"라고 불립니다. 완전히 그대로는 아니지만, 매우 가깝습니다. @BeelenEtc2017이 구축한 [LiPaD](https://www.lipad.ca)에서 CSV 형식으로 사용할 수 있습니다.\index{Hansard}\index{Canada!Hansard}

2018년 캐나다 의회에서 무엇이 논의되었는지에 관심이 있습니다.\index{Canada!parliament}\index{political science} 시작하려면 [여기](https://www.lipad.ca/data/)에서 전체 코퍼스를 다운로드한 다음, 2018년을 제외한 모든 연도를 버릴 수 있습니다. 데이터셋이 "2018"이라는 폴더에 있다면, `read_csv()`를 사용하여 모든 CSV를 읽고 결합할 수 있습니다.

```{r}
#| echo: true
#| eval: false

files_of_interest <-
  dir_ls(path = "2018/", glob = "*.csv", recurse = 2)

hansard_canada_2018 <-
  read_csv(
    files_of_interest,
    col_types = cols(
      basepk = col_integer(),
      speechdate = col_date(),
      speechtext = col_character(),
      speakerparty = col_character(),
      speakerriding = col_character(),
      speakername = col_character()
    ),
    col_select =
      c(basepk, speechdate, speechtext, speakername, speakerparty,
        speakerriding)) |>
  filter(!is.na(speakername))

hansard_canada_2018
```

```{r}
#| echo: false
#| eval: true

files_of_interest <-
  dir_ls(
    path = "inputs/data/2018/",
    glob = "*.csv",
    recurse = 2
  )

hansard_canada_2018 <-
  read_csv(
    files_of_interest,
    col_types = cols(
      basepk = col_integer(),
      speechdate = col_date(),
      speechtext = col_character(),
      speakerparty = col_character(),
      speakerriding = col_character(),
      speakername = col_character()
    ),
    col_select = c(
      basepk,
      speechdate,
      speechtext,
      speakername,
      speakerparty,
      speakerriding
    )
  ) |>
  filter(!is.na(speakername))

hansard_canada_2018
```

`filter()`를 사용하는 것은 "지시"와 같은 비연설 측면이 한사드에 포함되는 경우가 있기 때문에 필요합니다. 예를 들어, `filter()`를 포함하지 않으면 첫 줄은 "하원은 2017년 11월 9일부터 동의안 심의를 재개했습니다."입니다. 그런 다음 코퍼스를 구성할 수 있습니다.

```{r}
#| eval: false
hansard_canada_2018_corpus <-
  corpus(hansard_canada_2018,
         docid_field = "basepk",
         text_field = "speechtext")

hansard_canada_2018_corpus
```

코퍼스의 토큰을 사용하여 문서-특징 행렬을 구성합니다. 계산적으로 좀 더 쉽게 하기 위해, 최소 두 번 이상 나타나지 않는 단어와 최소 두 문서에 나타나지 않는 단어는 모두 제거합니다.

```{r}
#| message: false
#| warning: false
#| eval: false
hansard_dfm <-
  hansard_canada_2018_corpus |>
  tokens(
    remove_punct = TRUE,
    remove_symbols = TRUE
  ) |>
  dfm() |>
  dfm_trim(min_termfreq = 2, min_docfreq = 2) |>
  dfm_remove(stopwords(source = "snowball"))

hansard_dfm
```

이 시점에서 `stm`의 `stm()`을 사용하여 LDA 모델을 구현할 수 있습니다.\index{latent Dirichlet allocation} 문서-특징 행렬과 주제 수를 지정해야 합니다. 주제 모델은 본질적으로 요약일 뿐입니다. 문서가 단어 모음이 되는 대신, 각 주제와 관련된 어떤 확률을 가진 주제 모음이 됩니다. 그러나 실제 기본 의미보다는 유사한 시기에 사용되는 경향이 있는 단어 모음을 제공하기 때문에, 우리가 관심 있는 주제 수를 지정해야 합니다. 이 결정은 큰 영향을 미칠 것이며, 몇 가지 다른 숫자를 고려해야 합니다.

```{r}
#| echo: true
#| eval: false

hansard_topics <- stm(documents = hansard_dfm, K = 10)

beepr::beep()

write_rds(
  hansard_topics,
  file = "hansard_topics.rda"
)
```

```{r}
#| echo: false
#| eval: false

# INTERNAL

hansard_topics <- stm(documents = hansard_dfm, K = 10)

beep()

write_rds(
  hansard_topics,
  file = "outputs/hansard_topics.rda"
)
```

이것은 시간이 좀 걸릴 것입니다. 아마 15-30분 정도 걸릴 것이므로, 완료되면 `write_rds()`를 사용하여 모델을 저장하고, `beep`를 사용하여 완료 알림을 받는 것이 유용합니다. 그런 다음 `read_rds()`를 사용하여 결과를 다시 읽어들일 수 있습니다.

```{r}
#| echo: true
#| eval: false

hansard_topics <- read_rds(
  file = "hansard_topics.rda"
)
```

```{r}
#| echo: false
#| eval: true

hansard_topics <- read_rds(
  file = "outputs/hansard_topics.rda"
)
```

`labelTopics()`를 사용하여 각 주제의 단어를 살펴볼 수 있습니다.

:::{.content-visible when-format="pdf"}
```{r}
#| message: false
#| eval: false

labelTopics(hansard_topics)
```
:::

:::{.content-visible unless-format="pdf"}
```{r}
#| eval: false
labelTopics(hansard_topics)
```
:::



## 연습 문제

### 연습 {.unnumbered}

1. *(계획)* 다음 시나리오를 고려하십시오: *뉴스 웹사이트를 운영하고 있으며 익명 댓글을 허용할지 여부를 파악하려고 합니다. A/B 테스트를 수행하기로 결정했습니다. 모든 것을 동일하게 유지하되, 사이트의 한 버전에서만 익명 댓글을 허용합니다. 테스트에서 얻는 텍스트 데이터만 결정하면 됩니다.* 데이터셋이 어떻게 생겼을지 스케치한 다음, 모든 관측치를 보여주기 위해 만들 수 있는 그래프를 스케치하십시오.
2. *(시뮬레이션)* 설명된 시나리오를 더 고려하고 상황을 시뮬레이션하십시오. 시뮬레이션된 데이터를 기반으로 최소 10개의 테스트를 포함하십시오.
3. *(수집)* 그러한 데이터셋의 가능한 출처를 설명하십시오.
4. *(탐색)* `ggplot2`를 사용하여 스케치한 그래프를 만드십시오. `rstanarm`을 사용하여 모델을 구축하십시오.
5. *(소통)* 자신이 한 일에 대해 두 단락을 작성하십시오.

### 퀴즈 {.unnumbered}

1. `str_replace_all()`의 어떤 인수가 구두점을 제거합니까?
    a.  "[:punct:]"
    b. "[:digit:]"
    c. "[:alpha:]"
    d. "[:lower:]"
2. `stopwords(source = "snowball")[1:10]`를 변경하여 "nltk" 목록에서 아홉 번째 불용어를 찾으십시오.
    a. "her"
    b. "my"
    c.  "you"
    d. "i"
3. `quanteda()`의 어떤 함수가 코퍼스를 토큰화합니까?
    a. `tokenizer()`
    b. `token()`
    c. `tokenize()`
    d.  `tokens()`
4. `dfm_trim()`의 어떤 인수를 사용해야 최소 두 번 이상 나타나는 용어만 포함할 수 있습니까? = 2)
    a. "min_wordfreq"
    b.  "min_termfreq"
    c. "min_term_occur"
    d. "min_ occurrence"
5. 가장 좋아하는 트라이그램 예시는 무엇입니까?
6. 게자리 운세에서 두 번째로 흔하게 사용되는 단어는 무엇입니까?
    a. to
    b. your
    c. the
    d.  you
7. 물고기자리 운세에서 여섯 번째로 흔하게 사용되는 단어는 무엇이며, 해당 별자리에만 고유한 단어는 무엇입까?
    a. shoes
    b.  prayer
    c. fishes
    d. pisces
8. 캐나다 주제 모델을 다시 실행하되, 5개의 주제만 포함하십시오. 각 주제의 단어를 보고, 각각이 무엇에 관한 것인지 어떻게 설명하시겠습니까?


### 수업 활동 {.unnumbered}

- 아이들은 "개", "고양이", 또는 "새" 중 무엇을 먼저 배웁니까? Wordbank 데이터베이스를 사용하십시오.

### 과제 {.unnumbered}

*R을 사용한 텍스트 분석을 위한 지도 기계 학습*의 5.2장 "단어 임베딩을 직접 찾아 이해하기"에 있는 @hvitfeldt2021supervised의 코드를 따라 [LiPaD](https://www.lipad.ca)에서 1년치 데이터를 사용하여 자신만의 단어 임베딩을 구현하십시오.
