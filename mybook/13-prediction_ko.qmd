---
engine: knitr
---

# 예측 {#sec-predictingpythons}

**선수 지식**

- *R을 사용한 통계 학습 소개*, [@islr]
  - 6장 "선형 모델 선택 및 정규화"에 집중하세요. 이는 릿지 및 라쏘 회귀에 대한 개요를 제공합니다.
- *데이터 분석을 위한 Python*, [@pythonfordataanalysis]
  - Python에서 데이터 분석의 작업 예시를 제공하는 13장에 집중하세요.
- *R을 사용한 NFL 분석 소개*, [@congelio2024]
  - 3장 "`nflverse` 패키지 제품군을 사용한 NFL 분석" 및 5장 "NFL 데이터를 사용한 고급 모델 생성"에 집중하세요.

**핵심 개념 및 기술**

**소프트웨어 및 패키지**

- `arrow` [@arrow]
- `nflverse` [@nflverse]
- `poissonreg` [@poissonreg]
- `tidymodels` [@citeTidymodels]
  - `parsnip` [@parsnip]
  - `recipes` [@recipes]
  - `rsample` [@rsample]
  - `tune` [@tune]
  - `yarkdstick` [@yardstick]
- `tidyverse` [@tidyverse]
- `tinytable` [@tinytable]

```{r}
#| message: false
#| warning: false

library(arrow)
# library(nflverse)
library(poissonreg)
library(tidymodels)
library(tidyverse)
library(tinytable)
```

## 서론

[@sec-its-just-a-linear-model]에서 논의했듯이, 모델은 추론 또는 예측에 중점을 두는 경향이 있습니다. 일반적으로 초점에 따라 다른 문화가 있습니다. 이에 대한 한 가지 이유는 [@sec-causality-from-observational-data]에서 소개될 인과 관계에 대한 다른 강조 때문입니다. 여기서 매우 일반적으로 말하자면, 추론에서는 인과 관계에 대해 매우 우려하지만, 예측에서는 덜 우려합니다. 이는 모델이 예상했던 것과 조건이 상당히 다를 때 예측의 품질이 저하된다는 것을 의미합니다. 그러나 조건이 충분히 다른지 어떻게 알 수 있을까요?

이러한 문화적 차이에 대한 또 다른 이유는 데이터 과학, 특히 기계 학습의 부상이 컴퓨터 과학 또는 공학 배경을 가진 사람들이 Python으로 모델을 개발하는 데 크게 기여했기 때문입니다. 이는 추론의 대부분이 통계학에서 나왔기 때문에 추가적인 어휘 차이가 있다는 것을 의미합니다. 다시 말하지만, 이 모든 것은 매우 일반적으로 말하는 것입니다.

이 장에서는 `tidymodels`의 R 접근 방식을 사용하여 예측에 중점을 둡니다. 그런 다음 회색 영역 중 하나인 라쏘 회귀를 소개합니다. 이는 통계학자들이 개발했지만 주로 예측에 사용됩니다. 마지막으로, 이 모든 것을 Python으로 소개합니다.

## `tidymodels`를 사용한 예측

### 선형 모델

예측에 초점을 맞출 때, 종종 많은 모델을 적합하고 싶을 것입니다. 이를 수행하는 한 가지 방법은 코드를 여러 번 복사하여 붙여넣는 것입니다. 이것은 괜찮고, 대부분의 사람들이 시작하는 방법이지만, 찾기 어려운 오류를 만들 가능성이 높습니다. 더 나은 접근 방식은 다음과 같습니다.

1) 더 쉽게 확장 가능;
2) 과적합에 대해 신중하게 생각할 수 있도록; 그리고
3) 모델 평가 추가.

`tidymodels`[@citeTidymodels]의 사용은 다양한 모델을 쉽게 적합할 수 있는 일관된 문법을 제공하여 이러한 기준을 충족합니다. `tidyverse`와 마찬가지로, 패키지 모음입니다.

설명을 위해, 시뮬레이션된 달리기 데이터에 대해 다음 모델을 추정하고자 합니다.

$$
\begin{aligned}
y_i | \mu_i &\sim \mbox{Normal}(\mu_i, \sigma) \\
\mu_i &= \beta_0 +\beta_1x_i
\end{aligned}
$$

여기서 $y_i$는 어떤 개인 $i$의 마라톤 시간을 나타내고 $x_i$는 그들의 5킬로미터 시간을 나타냅니다. 여기서 우리는 어떤 개인 $i$의 마라톤 시간이 평균 $\mu$와 표준 편차 $\sigma$를 가진 정규 분포를 따르며, 평균은 두 매개변수 $\beta_0$와 $\beta_1$ 및 그들의 5킬로미터 시간에 따라 달라진다고 말합니다. 여기서 "~"는 "~로 분포된다"는 의미입니다. 사용되는 분포에 대해 더 명시적으로 하기 위해 이전과 약간 다른 표기법을 사용하지만, 이 모델은 $y_i=\beta_0+\beta_1 x_i + \epsilon_i$와 동일하며, 여기서 $\epsilon$은 정규 분포를 따릅니다.

예측에 초점을 맞추고 있으므로, 데이터 과적합에 대해 우려합니다. 이는 다른 데이터셋에 대한 주장을 할 수 있는 능력을 제한할 것입니다. 이를 부분적으로 해결하는 한 가지 방법은 `initial_split()`을 사용하여 데이터셋을 두 개로 나누는 것입니다.

```{r}
#| message: false
#| warning: false

sim_run_data <-
  read_parquet(file = here::here("outputs/data/running_data.parquet"))

set.seed(853)

sim_run_data_split <-
  initial_split(
    data = sim_run_data,
    prop = 0.80
  )

sim_run_data_split
```

데이터를 분할한 후, `training()`과 `testing()`을 사용하여 훈련 및 테스트 데이터셋을 생성합니다.

```{r}
sim_run_data_train <- training(sim_run_data_split)

sim_run_data_test <- testing(sim_run_data_split)
```

데이터셋의 80%를 훈련 데이터셋에 배치했습니다. 이를 사용하여 모델의 매개변수를 추정할 것입니다. 나머지 20%는 남겨두고, 이를 사용하여 모델을 평가할 것입니다. 왜 이렇게 할까요? 우리의 관심사는 모델링의 모든 측면을 괴롭히는 편향-분산 트레이드오프입니다. 우리는 결과가 우리가 가진 데이터셋에 너무 특수하여 다른 데이터셋에 적용할 수 없을까 봐 우려합니다. 극단적인 예를 들자면, 10개의 관측치를 가진 데이터셋을 생각해 보십시오. 우리는 그 관측치를 완벽하게 맞추는 모델을 만들 수 있습니다. 그러나 그 모델을 다른 데이터셋에 적용하면, 동일한 기본 프로세스에 의해 생성된 데이터셋이라 할지라도 정확하지 않을 것입니다.

이러한 우려를 해결하는 한 가지 방법은 데이터를 이런 식으로 분할하는 것입니다. 우리는 훈련 데이터를 사용하여 계수 추정치를 얻고, 테스트 데이터를 사용하여 모델을 평가합니다. 훈련 데이터에 너무 밀접하게 일치하는 모델은 테스트 데이터에서 잘 작동하지 않을 것입니다. 왜냐하면 훈련 데이터에 너무 특수하기 때문입니다. 이러한 테스트-훈련 분할을 사용하면 적절한 모델을 구축할 기회를 얻을 수 있습니다.

이러한 분리를 적절하게 수행하는 것은 처음 생각하는 것보다 더 어렵습니다. 테스트 데이터셋의 측면이 훈련 데이터셋에 존재하여 부적절하게 앞으로 일어날 일을 미리 알려주는 상황을 피하고 싶습니다. 이를 데이터 누출이라고 합니다. 그러나 전체 데이터셋을 포함할 가능성이 있는 데이터 정리 및 준비를 고려하면, 각 데이터셋의 일부 기능이 서로 영향을 미칠 수 있습니다. @kapoornarayanan2022 는 많은 연구를 무효화할 수 있는 기계 학습 응용 프로그램에서 광범위한 데이터 누출을 발견했습니다.

`tidymodels`를 사용하려면 먼저 `linear_reg()`로 선형 회귀에 관심이 있음을 지정합니다. 그런 다음 `set_engine()`로 선형 회귀 유형(이 경우 다중 선형 회귀)을 지정합니다. 마지막으로 `fit()`으로 모델을 지정합니다. 이것은 위에서 자세히 설명한 기본 R 접근 방식보다 훨씬 더 많은 인프라를 필요로 하지만, 이 접근 방식의 장점은 많은 모델을 적합하는 데 사용할 수 있다는 것입니다. 말하자면, 모델 공장을 만든 것입니다.

```{r}
sim_run_data_first_model_tidymodels <-
  linear_reg() |>
  set_engine(engine = "lm") |>
  fit(
    marathon_time ~ five_km_time + was_raining,
    data = sim_run_data_train
  )
```

추정된 계수는 @tbl-modelsummarybayesbetter 의 첫 번째 열에 요약되어 있습니다. 예를 들어, 데이터셋에서 5킬로미터 달리기 시간이 1분 더 길면 마라톤 시간이 약 8분 더 길어진다는 것을 발견합니다.

### 로지스틱 회귀

로지스틱 회귀 문제에도 `tidymodels`를 사용할 수 있습니다. 이를 위해 먼저 종속 변수의 클래스를 요인으로 변경해야 합니다. 이는 분류 모델에 필요하기 때문입니다.

```{r}
#| message: false
#| warning: false

week_or_weekday <-
  read_parquet(file = "outputs/data/week_or_weekday.parquet")

set.seed(853)

week_or_weekday <-
  week_or_weekday |>
  mutate(is_weekday = as_factor(is_weekday))

week_or_weekday_split <- initial_split(week_or_weekday, prop = 0.80)
week_or_weekday_train <- training(week_or_weekday_split)
week_or_weekday_test <- testing(week_or_weekday_split)

week_or_weekday_tidymodels <-
  logistic_reg(mode = "classification") |>
  set_engine("glm") |>
  fit(
    is_weekday ~ number_of_cars,
    data = week_or_weekday_train
  )
```

이전과 마찬가지로, 추정치와 비교하여 실제 결과를 그래프로 그릴 수 있습니다. 그러나 이의 좋은 점 중 하나는 테스트 데이터셋을 사용하여 모델의 예측 능력을 더 철저히 평가할 수 있다는 것입니다. 예를 들어, 혼동 행렬을 통해 각 예측의 개수를 실제 값과 비교하여 지정할 수 있습니다. 모델이 보류된 데이터셋에서 잘 작동한다는 것을 발견했습니다. 모델이 주중이라고 예측한 관측치는 90개였고, 실제로 주중이었습니다. 모델이 주말이라고 예측한 관측치는 95개였고, 실제로 주말이었습니다. 15개의 관측치에서 오류가 있었고, 이들은 주중이라고 예측했지만 주말이었던 7개와 그 반대였던 8개로 나뉘었습니다.

```{r}
week_or_weekday_tidymodels |>
  predict(new_data = week_or_weekday_test) |>
  cbind(week_or_weekday_test) |>
  conf_mat(truth = is_weekday, estimate = .pred_class)
```

#### 미국 정치적 지지

한 가지 접근 방식은 `tidymodels`를 사용하여 이전과 동일한 방식으로 예측 중심 로지스틱 회귀 모델을 구축하는 것입니다. 즉, 검증 세트 접근 방식입니다[@islr, p. 176]. 이 경우 확률은 바이든에게 투표할 확률이 됩니다.

```{r}

ces2020 <-
  read_parquet(file = "outputs/data/ces2020.parquet")

set.seed(853)

ces2020_split <- initial_split(ces2020, prop = 0.80)
ces2020_train <- training(ces2020_split)
ces2020_test <- testing(ces2020_split)

ces_tidymodels <-
  logistic_reg(mode = "classification") |>
  set_engine("glm") |>
  fit(
    voted_for ~ gender + education,
    data = ces2020_train
  )

ces_tidymodels
```

그런 다음 테스트 세트에서 평가합니다. 모델이 트럼프 지지자를 식별하는 데 어려움을 겪는 것으로 보입니다.

```{r}
ces_tidymodels |>
  predict(new_data = ces2020_test) |>
  cbind(ces2020_test) |>
  conf_mat(truth = voted_for, estimate = .pred_class)
```

`tidymodels`를 소개할 때, 훈련 및 테스트 세트를 무작위로 구성하는 것의 중요성에 대해 논의했습니다. 우리는 훈련 데이터셋을 사용하여 매개변수를 추정하고, 테스트 세트에서 모델을 평가합니다. 무작위성의 변덕에 따라야 하는 이유와 데이터를 최대한 활용하고 있는지 묻는 것은 당연합니다. 예를 들어, 좋은 모델이 테스트 세트에 무작위로 포함되어 제대로 평가되지 않으면 어떻게 될까요? 또한, 테스트 세트가 크지 않으면 어떻게 될까요?

이를 해결하는 데 도움이 되는 일반적으로 사용되는 리샘플링 방법 중 하나는 $k$-겹 교차 검증입니다. 이 접근 방식에서는 데이터셋에서 $k$개의 다른 샘플 또는 "폴드"를 대체 없이 생성합니다. 그런 다음 모델을 처음 $k-1$개의 폴드에 적합하고, 마지막 폴드에서 평가합니다. 모든 관측치가 $k-1$번 훈련에 사용되고 한 번 테스트에 사용되도록 각 폴드에 대해 $k$번 이 작업을 수행합니다. $k$-겹 교차 검증 추정치는 평균 제곱 오차의 평균입니다[@islr, p. 181]. 예를 들어, `tidymodels`의 `vfold_cv()`를 사용하여 10개의 폴드를 생성할 수 있습니다.

```{r}
set.seed(853)

ces2020_10_folds <- vfold_cv(ces2020, v = 10)
```

그런 다음 `fit_resamples()`를 사용하여 폴드의 다른 조합에 걸쳐 모델을 적합할 수 있습니다. 이 경우 모델은 10번 적합됩니다.

```{r}
ces2020_cross_validation <-
  fit_resamples(
    object = logistic_reg(mode = "classification") |> set_engine("glm"),
    preprocessor = recipe(voted_for ~ gender + education,
                          data = ces2020),
    resamples = ces2020_10_folds,
    metrics = metric_set(accuracy, sens),
    control = control_resamples(save_pred = TRUE)
  )
```

모델의 성능을 이해하는 데 관심이 있을 수 있으며, `collect_metrics()`를 사용하여 폴드 전체에서 집계할 수 있습니다(@tbl-metricsvoters-1). 이러한 유형의 세부 정보는 일반적으로 논문의 본문에서 간략하게 언급되지만, 부록에 자세히 포함됩니다. 폴드 전체에서 모델의 평균 정확도는 0.61이고, 평균 민감도는 0.19이며, 평균 특이도는 0.90입니다.

```{r}
#| label: tbl-metricsvoters
#| tbl-cap: "유권자 선호도를 예측하기 위한 로지스틱 회귀의 10겹 교차 검증에 대한 평균 지표"
#| layout-ncol: 2
#| tbl-subcap: ["주요 성능 지표", "혼동 행렬"]

collect_metrics(ces2020_cross_validation) |>
  select(.metric, mean) |>
  tt() |>
  style_tt(j = 1:2, align = "lr") |>
  format_tt(digits = 2, num_mark_big = ",", num_fmt = "decimal") |>
  setNames(c("지표", "평균"))

conf_mat_resampled(ces2020_cross_validation) |>
  mutate(Proportion = Freq / sum(Freq)) |>
  tt() |>
  style_tt(j = 1:4, align = "llrr") |>
  format_tt(digits = 2, num_mark_big = ",", num_fmt = "decimal")
```

이것은 무엇을 의미합니까? 정확도는 올바르게 분류된 관측치의 비율입니다. 0.61이라는 결과는 모델이 동전 던지기보다 잘 작동하지만, 그 이상은 아니라는 것을 시사합니다. 민감도는 참으로 식별된 참 관측치의 비율입니다[@islr, p. 145]. 이 경우 모델이 응답자가 트럼프에게 투표했다고 예측했고, 실제로 그랬다는 것을 의미합니다. 특이도는 거짓으로 식별된 거짓 관측치의 비율입니다[@islr, p. 145]. 이 경우 바이든에게 투표한 유권자 중 바이든에게 투표할 것으로 예측된 유권자의 비율입니다. 이는 모델이 트럼프 지지자를 식별하는 데 어려움을 겪고 있다는 우리의 초기 생각을 확인시켜 줍니다.

혼동 행렬을 보면 더 자세히 알 수 있습니다(@tbl-metricsvoters-2). 교차 검증과 같은 리샘플링 접근 방식과 함께 사용될 때, 혼동 행렬은 각 폴드에 대해 계산된 다음 평균화됩니다. 모델은 2020년 선거가 얼마나 근접했는지에 대한 우리의 지식에서 예상할 수 있는 것보다 훨씬 더 많이 바이든을 예측하고 있습니다. 이는 모델이 더 나은 작업을 수행하기 위해 추가 변수가 필요할 수 있음을 시사합니다.

마지막으로, 개별 수준 결과에 관심이 있는 경우 `collect_predictions()`를 사용하여 데이터셋에 추가할 수 있습니다.

```{r}
ces2020_with_predictions <-
  cbind(
    ces2020,
    collect_predictions(ces2020_cross_validation) |>
      arrange(.row) |>
      select(.pred_class)
  ) |>
  as_tibble()
```

예를 들어, 모델이 고등학교를 졸업하지 않았거나, 고등학교를 졸업했거나, 2년제 대학을 졸업한 남성을 제외한 모든 개인에 대해 바이든 지지를 예측하고 있음을 알 수 있습니다(@tbl-omgthismodelishorriblelol).

```{r}
#| label: tbl-omgthismodelishorriblelol
#| tbl-cap: "모델은 교육 수준에 관계없이 모든 여성과 많은 남성에 대해 바이든 지지를 예측하고 있습니다."

ces2020_with_predictions |>
  group_by(gender, education, voted_for) |>
  count(.pred_class) |>
  tt() |>
  style_tt(j = 1:5, align = "llllr") |>
  format_tt(digits = 0, num_mark_big = ",", num_fmt = "decimal") |>
  setNames(c(
      "성별",
      "교육",
      "투표 대상",
      "예측 투표",
      "수"
    ))
```


### 포아송 회귀

`tidymodels`를 사용하여 `poissonreg`[@poissonreg]로 포아송 모델을 추정할 수 있습니다(@tbl-modelsummarypoisson).

```{r}
count_of_A <-
  read_parquet(file = "outputs/data/count_of_A.parquet")

set.seed(853)

count_of_A_split <-
  initial_split(count_of_A, prop = 0.80)
count_of_A_train <- training(count_of_A_split)
count_of_A_test <- testing(count_of_A_split)

grades_tidymodels <-
  poisson_reg(mode = "regression") |>
  set_engine("glm") |>
  fit(
    number_of_As ~ department,
    data = count_of_A_train
  )
```

이 추정 결과는 @tbl-modelsummarypoisson의 두 번째 열에 있습니다. `glm()`의 추정치와 유사하지만, 분할로 인해 관측치 수가 적습니다.




## 라쏘 회귀

<!-- 텍스트의 좋은 점 중 하나는 기존 방법을 사용하여 입력으로 사용할 수 있다는 것입니다. 여기서는 로지스틱 회귀의 변형과 텍스트 입력을 사용하여 예측할 것입니다. @silge2018에서 영감을 받아 두 가지 다른 텍스트 입력을 사용하여 각각의 텍스트 샘플에서 모델을 훈련한 다음, 해당 모델을 사용하여 훈련 세트의 텍스트를 예측하려고 합니다. 이것은 임의의 예시이지만, 많은 실제 응용 프로그램을 상상할 수 있습니다. 예를 들어, 어떤 텍스트가 봇 또는 인간에 의해 작성되었을 가능성이 있는지에 관심이 있을 수 있습니다. -->


:::{.callout-note}
## 거인의 어깨 위에 서서

로버트 팁시라니 박사\index{Tibshirani, Robert}는 스탠포드 대학교 통계학 및 생의학 데이터 과학과 교수입니다.\index{statistics} 1981년 스탠포드 대학교에서 통계학 박사 학위를 받은 후, 토론토 대학교에 조교수로 합류했습니다.\index{University of Toronto} 1994년 정교수로 승진했으며 1998년 스탠포드로 옮겼습니다. 그는 위에서 언급된 GAM과 자동 변수 선택 방법인 라쏘 회귀를 포함한 근본적인 기여를 했습니다. 그는 @islr 의 저자입니다. 그는 1996년 COPSS 회장상\index{COPSS Presidents' Award}을 수상했으며 2019년 왕립 학회 펠로우로 임명되었습니다.
:::


<!-- 먼저 데이터를 가져와야 합니다. `gutenbergr`[@gutenbergr]의 `gutenberg_download()`를 사용하여 프로젝트 구텐베르크에서 책을 가져옵니다. *제인 에어*[@janeeyre]와 *이상한 나라의 앨리스*[@citealice]를 고려합니다. -->

<!-- ```{r} -->
<!-- #| include: true -->
<!-- #| message: false -->
<!-- #| warning: false -->
<!-- #| eval: false -->

<!-- library(gutenbergr) -->
<!-- library(tidyverse) -->

<!-- # 관심 있는 책은 각각 1260과 11의 키를 가집니다. -->
<!-- alice_and_jane <- -->
<!--   gutenberg_download( -->
<!--     gutenberg_id = c(1260, 11), -->
<!--     meta_fields = "title" -->
<!--   ) -->

<!-- write_csv(alice_and_jane, "alice_and_jane.csv") -->

<!-- head(alice_and_jane) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- #| include: false -->
<!-- #| eval: true -->
<!-- #| warning: false -->
<!-- #| message: false -->

<!-- library(gutenbergr) -->
<!-- library(tidyverse) -->

<!-- alice_and_jane <- read_csv("inputs/books/alice_and_jane.csv") -->

<!-- head(alice_and_jane) -->
<!-- ``` -->

<!-- 이것의 좋은 점 중 하나는 데이터셋이 티블이라는 것입니다. 따라서 익숙한 모든 기술을 사용할 수 있습니다. 책의 각 줄은 데이터셋의 다른 행으로 읽어들여집니다. 여기서 두 권의 책을 동시에 다운로드했으므로 제목을 추가했습니다. 두 권의 책은 서로 뒤따릅니다. -->

<!-- 각 책의 줄 수를 보면 *제인 에어*가 *이상한 나라의 앨리스*보다 훨씬 길다는 것을 알 수 있습니다. `janitor`[@janitor]의 `remove_empty()`를 사용하여 빈 줄을 제거하는 것으로 시작합니다. -->

<!-- ```{r} -->
<!-- library(janitor) -->

<!-- alice_and_jane <- -->
<!--   alice_and_jane |> -->
<!--   mutate(blank_line = if_else(text == "", 1, 0)) |> -->
<!--   filter(blank_line == 0) |> -->
<!--   select(-blank_line) -->

<!-- table(alice_and_jane$title) -->
<!-- ``` -->

<!-- *제인 에어*는 *이상한 나라의 앨리스*에 비해 여전히 압도적으로 많으므로, 더 균등하게 만들기 위해 *제인 에어*에서 샘플링합니다. -->

<!-- ```{r} -->
<!-- set.seed(853) -->

<!-- alice_and_jane$rows <- c(1:nrow(alice_and_jane)) -->
<!-- sample_from_me <- alice_and_jane |> filter(title == "Jane Eyre: An Autobiography") -->
<!-- keep_me <- sample(x = sample_from_me$rows, size = 2481, replace = FALSE) -->

<!-- alice_and_jane <- -->
<!--   alice_and_jane |>
<!--   filter(title == "Alice's Adventures in Wonderland" | rows %in% keep_me) |>
<!--   select(-rows) -->

<!-- table(alice_and_jane$title) -->
<!-- ``` -->

<!-- 여기에는 다양한 문제가 있습니다. 예를 들어, 앨리스는 전체를 가지고 있지만, 제인은 무작위 부분만 가지고 있습니다. 그럼에도 불구하고 계속해서 각 책의 줄 번호로 카운터를 추가합니다. -->

<!-- ```{r} -->
<!-- alice_and_jane <- -->
<!--   alice_and_jane |> -->
<!--   group_by(title) |> -->
<!--   mutate(line_number = paste(gutenberg_id, row_number(), sep = "_")) |> -->
<!--   ungroup() -->
<!-- ``` -->

<!-- 이제 토큰을 언네스트하고 싶습니다. `tidytext`[@SilgeRobinson2016]의 `unnest_tokens()`를 사용합니다. -->

<!-- ```{r} -->
<!-- #| message: false -->
<!-- #| warning: false -->

<!-- library(tidytext) -->

<!-- alice_and_jane_by_word <- -->
<!--   alice_and_jane |> -->
<!--   unnest_tokens(word, text) |>
<!--   group_by(word) |>
<!--   filter(n() > 10) |>
<!--   ungroup() -->
<!-- ``` -->

<!-- 10번 이상 사용되지 않은 단어는 모두 제거합니다. 그럼에도 불구하고 여전히 500개 이상의 고유 단어가 있습니다. (작가가 최소 10번 단어를 사용하도록 요구하지 않았다면 6,000개 이상의 단어가 남게 됩니다.) -->

<!-- 이것이 관련성이 있는 이유는 이것들이 우리의 독립 변수이기 때문입니다. 따라서 10개 미만의 설명 변수를 사용하는 데 익숙할 수 있지만, 이 경우 585개를 가질 것입니다. 따라서 이를 처리할 수 있는 모델이 필요합니다. -->

<!-- 그러나 앞에서 언급했듯이, 본질적으로 단어 하나만 있는 행이 있을 것입니다. 따라서 모델이 최소한 일부 단어를 사용할 수 있도록 필터링합니다. -->

<!-- ```{r} -->
<!-- alice_and_jane_by_word <- -->
<!--   alice_and_jane_by_word |> -->
<!--   group_by(title, line_number) |> -->
<!--   mutate(number_of_words_in_line = n()) |> -->
<!--   ungroup() |> -->
<!--   filter(number_of_words_in_line > 2) |> -->
<!--   select(-number_of_words_in_line) -->
<!-- ``` -->

<!-- 테스트/훈련 분할을 생성하고 `tidymodels`를 로드합니다. -->

<!-- ```{r} -->
<!-- #| message: false -->
<!-- #| warning: false -->

<!-- library(tidymodels) -->

<!-- set.seed(853) -->

<!-- alice_and_jane_by_word_split <- -->
<!--   alice_and_jane_by_word |> -->
<!--   select(title, line_number) |>
<!--   distinct() |>
<!--   initial_split(prop = 3 / 4, strata = title) -->
<!-- ``` -->

<!-- 그런 다음 `cast_dtm()`을 사용하여 문서-용어 행렬을 생성할 수 있습니다. 이것은 각 단어가 사용된 횟수를 제공합니다. -->

<!-- ```{r} -->
<!-- alice_and_jane_dtm_training <- -->
<!--   alice_and_jane_by_word |> -->
<!--   count(line_number, word) |>
<!--   inner_join(training(alice_and_jane_by_word_split) |> select(line_number)) |>
<!--   cast_dtm(term = word, document = line_number, value = n) -->

<!-- dim(alice_and_jane_dtm_training) -->
<!-- ``` -->

<!-- 따라서 독립 변수는 정렬되었고, 이제 이진 종속 변수가 필요합니다. 즉, 책이 이상한 나라의 앨리스인지 제인 에어인지 여부입니다. -->

<!-- ```{r} -->
<!-- response <- -->
<!--   data.frame(id = dimnames(alice_and_jane_dtm_training)[[1]]) |>
<!--   separate(id, into = c("book", "line", sep = "_")) |>
<!--   mutate(is_alice = if_else(book == 11, 1, 0)) -->

<!-- predictor <- alice_and_jane_dtm_training[] |> as.matrix() -->
<!-- ``` -->


<!-- 이제 모델을 실행할 수 있습니다. -->

<!-- ```{r} -->
<!-- #| eval: false -->

<!-- library(glmnet) -->

<!-- model <- cv.glmnet( -->
<!--   x = predictor, -->
<!--   y = response$is_alice, -->
<!--   family = "binomial", -->
<!--   keep = TRUE -->
<!-- ) -->

<!-- save(model, file = "alice_vs_jane.rda") -->
<!-- ``` -->

<!-- ```{r}  -->
<!-- #| echo: false -->

<!-- load("outputs/model/alice_vs_jane.rda") -->
<!-- ``` -->


<!-- ```{r} -->
<!-- library(glmnet) -->
<!-- library(broom) -->

<!-- coefs <- model$glmnet.fit |> -->
<!--   tidy() |> -->
<!--   filter(lambda == model$lambda.1se) -->

<!-- coefs |> head() -->
<!-- ``` -->

<!-- ```{r} -->
<!-- coefs |> -->
<!--   group_by(estimate > 0) |> -->
<!--   top_n(10, abs(estimate)) |> -->
<!--   ungroup() |> -->
<!--   ggplot(aes(fct_reorder(term, estimate), estimate, fill = estimate > 0)) + -->
<!--   geom_col(alpha = 0.8, show.legend = FALSE) + -->
<!--   coord_flip() + -->
<!--   theme_minimal() + -->
<!--   labs( -->
<!--     x = "계수", -->
<!--     y = "단어" -->
<!--   ) + -->
<!--   scale_fill_brewer(palette = "Set1") -->
<!-- ``` -->

<!-- 아마도 놀랍지 않게도, 줄에 앨리스가 언급되면 이상한 나라의 앨리스일 가능성이 높고, 제인이 언급되면 제인 에어일 가능성이 높습니다. -->




## Python을 사용한 예측

### 설정

Microsoft에서 제공하는 무료 IDE인 VSCode 내에서 Python을 사용할 것입니다. [여기](https://code.visualstudio.com)에서 다운로드할 수 있습니다. 그런 다음 Quarto 및 Python 확장을 설치합니다. 이 과정에서 어려움이 있다면, Posit Cloud에서 시작하여 로컬 머신으로 전환했던 것과 마찬가지로, [여기](https://colab.google)에서 Google Colab을 처음 사용할 수 있습니다.

### 데이터

*parquet을 사용하여 데이터 읽기.*

*pandas를 사용하여 조작*

### 모델

#### scikit-learn

#### TensorFlow


## 연습 문제

### 연습 {.unnumbered}

1. *(계획)* 다음 시나리오를 고려하십시오: *1년 동안 매일 삼촌과 당신은 다트를 합니다. 각 라운드는 각각 세 개의 다트를 던지는 것으로 구성됩니다. 각 라운드 끝에 세 개의 다트가 맞춘 점수를 합산합니다. 따라서 3, 5, 10을 맞추면 해당 라운드의 총 점수는 18입니다. 삼촌은 다소 자비로워서 5점 미만을 맞추면 못 본 척하고 해당 다트를 다시 던질 기회를 줍니다. 매일 15라운드를 한다고 가정하십시오.* 데이터셋이 어떻게 생겼을지 스케치한 다음, 모든 관측치를 보여주기 위해 만들 수 있는 그래프를 스케치하십시오.
2. *(시뮬레이션)* 설명된 시나리오를 더 고려하고 상황을 시뮬레이션하십시오. 삼촌의 총점과 다시 던질 기회가 없었을 때의 총점, 그리고 실제로 얻게 되는 총점을 비교하십시오. 시뮬레이션된 데이터를 기반으로 최소 10개의 테스트를 포함하십시오.
3. *(수집)* 그러한 데이터셋의 가능한 출처를 설명하십시오(또는 관심 있는 동등한 스포츠 또는 상황).
4. *(탐색)* `ggplot2`를 사용하여 스케치한 그래프를 만드십시오. 그런 다음 `tidymodels`를 사용하여 누가 이길지 예측하는 모델을 구축하십시오.
5. *(소통)* 자신이 한 일에 대해 두 단락을 작성하십시오.

### 퀴즈 {.unnumbered}


### 수업 활동 {.unnumbered}

### 과제 {.unnumbered}

`nflverse`를 사용하여 정규 시즌 동안 NFL 쿼터백에 대한 일부 통계를 로드하십시오. [데이터 사전](https://nflreadr.nflverse.com/articles/dictionary_player_stats.html)은 데이터를 이해하는 데 유용할 것입니다.

```{r}
#| eval: false
qb_regular_season_stats <-
  load_player_stats(seasons = TRUE) |>
  filter(season_type == "REG" & position == "QB")
```

NFL 분석가라고 가정하고 2023년 정규 시즌의 절반, 즉 9주차가 막 끝났다고 가정하십시오. 시즌의 나머지 부분, 즉 10-18주차에 각 팀에 대해 생성할 수 있는 `passing_epa`의 최상의 예측 모델에 관심이 있습니다.

Quarto를 사용하고, 적절한 제목, 저자, 날짜, GitHub 리포지토리 링크, 섹션 및 인용을 포함하고, 경영진을 위한 2-3페이지 보고서를 작성하십시오. 최상의 성능은 창의적인 기능 엔지니어링을 필요로 할 것입니다. R 또는 Python, 어떤 모델이든 사용할 수 있지만, 모델을 지정하고 작동 방식을 높은 수준에서 설명하는 데 주의해야 합니다. 데이터 누출에 주의하십시오!
